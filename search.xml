<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>A Journey to the Underworld</title>
    <url>/blog/A-Journey-to-the-Underworld/</url>
    <content><![CDATA[<p><img src="/images/writing/underworld.png"></p>
<h2 id="Character-list"><a href="#Character-list" class="headerlink" title="Character list"></a>Character list</h2><p><img src="/images/writing/character_list.png"></p>
<h2 id="A-Normal-Evening"><a href="#A-Normal-Evening" class="headerlink" title="A Normal Evening"></a>A Normal Evening</h2><p>On a Saturday evening, Tom and Bradley were watching a soccer game broadcasted on TV in their apartment. Bradley, in his early teens, yearned to be a scholar, while Tom, in his late teens, gambled ruthlessly and somehow managed to win all of the major games. Suddenly, a player on the TV scored, and Tom cheered. <span id="more"></span><br>“Wow, the game is 20 to none. The blue team is being publicly humiliated right now!” Bradley laughed as one member from the blue team made a face but tripped over the soccer ball.<br>Tom nodded, “I still can’t understand why they chose the red team. Any fool would know that they couldn’t win.” He filled his mouth with another handful of popcorn so that it looked like a piñata ready to explode.<br>Just then, there was a banging on the door. Bradley heard it, and nudged Tom in the cheek. Tom’s mouth exploded open and all of the popcorn bits fell out. “What?” he yelled, frustrated and annoyed about having to clean the floor of the apartment again.<br>“Did you hear a bang?” Bradley asked nervously.<br>“No.” Tom shrugged, and that was the end of that.<br>However, not long after, there came a louder bang outside. Even Tom, who was usually unaware of his surroundings, heard it. But before he could stand up, the door collapsed inward on itself, and the silhouette of a burly man with a club contrasted against the moonlit night. The man raised his club and charged into the apartment. Bradley screamed. Then…BANG.</p>
<h2 id="A-New-Home"><a href="#A-New-Home" class="headerlink" title="A New Home"></a>A New Home</h2><p>Bradley regained consciousness first. He opened his eyes, but his vision was still too blurred by his pain for him to see anything clear. The only thing drifting around in Bradley’s mind was pain. So he fell back into a coma.<br>After an unknown amount of days, Bradley woke up again. This time, his vision was clearer. He looked around and found himself resting on a chair in the corner of a room. Suddenly, he found himself face to face with the tip of a club. He blinked again, and he saw what he supposed to be the same man who had kidnapped him and his brother. Bradley climbed out of the chair and edged away slowly until he hit into another wall. The mysterious man, still holding his club and smirking, closed in even more slowly until he was a few feet from Bradley.<br>“Who are you?” asked Bradley, trying to control his emotions.<br>“The real question is, do you want to live?” The man snickered in a threatening tone, pointing his club at Bradley’s nose. The man was around forty, wore ragged clothes, and had a pierced nose. Bradley also noticed that the man had plenty of scars, both new and old.<br>The man hit him roughly in the chest and shouted menacingly, “I ASKED, DO YOU WANT TO LIVE?”<br>Bradley stammered, “y-yes.”<br>“Then follow me!” The man whisked around and marched outside. Bradley followed him outside and looked around. He was standing on a volcanic island. The sand beneath him was black and charred, but the volcano was apparently dormant, for there were a variety of trees and bushes scattered around the island that could not have grown in the duration of one night. The man stopped before a group of around thirty people and shoved Bradley into the crowd. Bradley watched as that man stalked away and joined another group of five people.<br>Of the thirty people, one was Tom. Tom had woken up just a few hours earlier, and the same man had also treated him harshly before leading him to the crowd.<br>Presently, a man stepped onto a high rock, surrounded by his four supporters, including the one Bradley saw earlier. He shouted in a menacing voice, “My name is Derickson, but you will call me Lord Derick. But first of all, do you all want to go free?”<br>A few feeble calls of yes’s arose here and there in the crowd.<br>“In your dreams!” Lord Derick cackled wickedly. “But…I will grant you freedom if you follow me on a dreadful journey and survive. Now, you might ask, what is the dreadful journey? It is to a place down below - a place I like to call…hell.” Lord Derick spoke in a frightful half-whisper, and heavily emphasized on the last word.<br>“Go to hell yourself!” A defiant voice arose.<br>“Now, now, now.” Lord Derick grinned darkly as two supporters, carrying heavy sticks, beat the life out of the disobedient man and forcefully dragged him into a nearby makeshift grave.<br>“One casualty already? Wow, we really are going to have fun!” A few low mutters of complaints arose, only to be quieted down immediately with threats from Lord Derick’s supporters. A hand rose.<br>“Yes?” Lord Derick asked in the same menacingly calm voice.<br>A timid voice called, “H-how will we go to, uh… the underworld, D-derickson?”<br>“CALL ME LORD DERICK!” Lord Derick shrieked.<br>“How will w-we possibly go to the u-underworld, L-lord Derick?”<br>“Ah - that is a wonderful question. Do you not see that volcano just behind me? Yes? Well, the answer is simple: we travel through one of the side vents.”<br>A few people gasped.<br>“B-but I have a wife and children!” the man complained. A few others wailed in agreement.<br>“What about your life?” Lord Derick asked wickedly in response. The man quieted down, and before any other complaint could be voiced, Lord Derick led his followers away into another cottage.</p>
<h2 id="To-Go-or-Not-To-Go"><a href="#To-Go-or-Not-To-Go" class="headerlink" title="To Go or Not To Go"></a>To Go or Not To Go</h2><p>There were shouts of, “We shall not go!”<br>The same man from before, whose name was Arthur, called out, “It is too risky!”<br>Tom thought for a moment before replying thoughtfully, “It is not too risky unless of course, the volcano erupts. But, believe me, I do not think that monstrous beast should wake up anytime soon. I gambled a bit too much during the past few years, yet I have never tasted a single loss before.”<br>“I agree with my brother. We shall not fear death, if our reward is freedom. Think of the American Revolution, fellow people!” Bradley put in. This put most back into good spirits.<br>But before Tom could get too awed by his debate skills, Arthur grunted, “Tom, you say you have never tasted loss before, eh?”<br>“Not by gambling.”<br>“Well, today, I’m going to give you a taste of loss. Pass me the cards, Little Timmy!”<br>An overweight teenager waddled up and handed Arthur a deck of cards. “Now, here is the bet. If I win, we do not go. If I lose, we go. But don’t get overly excited. I have a whole life’s worth of experience on gambling. Deal?”<br>“Deal!” Tom replied instinctively.<br>Arthur took the cards and shuffled. However, Tom immediately recognised the shuffling pattern, and the fact that the man was not changing the front of the deck made Tom conclude that he was rigging the deck. He pounced on Arthur and shouted in his ear, “I should’ve known that you were a fraud.”<br>The man forced himself to look innocent. “I am not cheating, am I, Little Timmy?” The harassed looking teenager swung his chubby face back and forth.<br>“JUST. ADMIT. IT.” Tom lost his sanity.<br>“Am I cheating, fellow people?” Arthur pretended to look hurt, then asked his surrounding people. There were mutters of no’s.<br>For a split second, even the afternoon breeze seemed to stop as Tom dealt a blow on Arthur’s chest. Crack! Arthur fell onto the black sand.<br>“Help!” Arthur pleaded to his surrounding people. Nobody moved; Everyone was paralyzed by the events flashing before their eyes. Seeing that Tom was about to deal another possibly fatal blow, Arthur begged, “Okay, I admit I was cheating! We will go!”<br>Tom recovered his sanity at the last moment and stalked away to Bradley.<br>Arthur glared at him for a moment before standing up and dusting himself off.</p>
<h2 id="To-the-Mammoth-Beast"><a href="#To-the-Mammoth-Beast" class="headerlink" title="To the Mammoth Beast"></a>To the Mammoth Beast</h2><p>Standing under the afternoon sun seemed like eternity for Bradley, and everyone seemed to go abnormally silent after that brawl. At last, Lord Derick came out with his and asked threateningly, “You had your time to think. So, would it be a yes or a no?”<br>Most people spoke, “Yes.” Bradley glanced at Arthur. He was sulking silently, so Bradley convinced himself that even the most angry of men could not cause too big of a trouble. He turned his attention back to Lord Derick.<br>“…Follow me…” Lord Derick was saying. “If you don’t, you will most likely get lost and die.”<br>So Bradley, along with the crowd, moved along nervously. They climbed over boulders and hills and crossed narrow streams. Finally, they arrived at the base of a giant volcano. Bradley stared up at the majestic yet deadly beast. A small nudge from one of the people nearby brought his attention to Lord Derick.<br>Lord Derick was climbing the volcano, and stopped near a middle-sized vent. “Finding a vent to go through is easy. However, you need to be prepared.” At this, Lord Derick stopped and grinned his same dark smile. Two of his supporters, wearing oxygen masks and special suits, brought up an unwilling person from the crowd. Lord Derick stepped aside, and the victim was exposed to the gas in the vents. “AHH” he screamed, then crumbled down into a heap. The others gasped in horror. “Don’t worry, he only fainted,” Lord Derick smirked. He kicked the body out of the way.<br>A question suddenly arrived in Bradley’s mind: A moment ago, Lord Derick was exposed to the harmful gasses, but why did he not faint or die? Bradley was about to tell Tom about this weird discovery when he saw Lord Derick’s evil eyes stare at him as if to say, “I know what you are thinking, but do not tell anyone, or else you will have an undesirable ending.” Bradley stared into Lord Derick’s dark eyes.<br>“Enough fooling around! Now, the special suits!” Lord Derick summoned the other two minions, who started to throw oxygen masks and special suits at the crowd. An oxygen mask hit Bradley in the back, and he put it on. A few moments later, he found a suit drifting through the air, and caught it before putting it on too. After everyone prepared themselves, Lord Derick motioned for them to follow him into the vents.</p>
<h2 id="Into-the-Underworld"><a href="#Into-the-Underworld" class="headerlink" title="Into the Underworld"></a>Into the Underworld</h2><p>Inside, it was blurry with gas. Tom couldn’t see anything outside, as it was blocked by Lord Derick’s four supporters. The person in front of Tom tripped on a loose rock, and he stumbled to the ground. To everyone’s horror and surprise, his oxygen mask cracked! Although his scream was muffled by the thick stream of gasses, Tom could imagine how much agony that person felt. From then on, he walked more carefully, as if his life depended on it - because his life depended on it.<br>The group walked on for hours. A man stumbled, but luckily, he didn’t break his suit or oxygen mask. Still, it was a grim journey. No one talked, not even Lord Derick. Tom stared at Lord Derick. He was muttering something to himself. Tom wasn’t sure what to make of that, so he pretended nothing had happened, after all, nothing big did happen, and nothing big will happen for seemingly the next decade.<br>But wait - was the narrow tunnel widening? Then, an exhausted Tom saw a streak of dancing red light on the cave wall. Another streak appeared. And so did another. As he took another step forward, he found himself glimpsing at a seemingly endless cave, with an ocean of lava in the middle. He and the others walked out of the tunnel and into the cave.</p>
<h2 id="The-Bridge"><a href="#The-Bridge" class="headerlink" title="The Bridge"></a>The Bridge</h2><p>“At last!” Lord Derick cheered with a short laugh. As he neared the cave, Bradley noticed a basalt bridge emerge from the lava. Lord Derick crossed the bridge. As everyone walked out into the cave, the tunnel behind them suddenly sealed itself with a huge, towering boulder.<br>“Welcome.” Lord Derick stretched his hand out, as if to embrace the cave. “For you, this would be called a magma chamber. Big, isn’t it?”<br>Arthur was staring at the boulder. “How will we get out?” he asked angrily. “You scammed us!”<br>“Be careful of your words, or else…you will pay with your life,” Lord Derick said with a smile. “But…I feel nice right now, so I don’t mind telling you: the bridge ahead of you will judge your cruelty. If you are not cruel enough, the bridge will collapse and you will die a burning death. So who is going first?”<br>A boy in the crowd hesitantly stepped onto the bridge. Seeing that the bridge had, unexpectedly, not collapsed yet, the boy gathered up his courage and ran as fast as he could. He had not run far when the bridge suddenly collapsed, and within moments, the boy disappeared into the eagerly awaiting lava below.<br>Then, Lord Derick pointed at Arthur. Arthur, though timid, walked up to the bridge. Slowly, he crossed it, and surprisingly, he actually made it to the other side!<br>Suddenly, Bradley had a thought. Taking chances, he sneaked to Tom unnoticed and whispered, “I believe that Arthur may have been planning for revenge.”<br>“I would guess that, but how do you know for sure?”<br>“Lord Derick said that only the cruelest could cross the bridge safely. And I am guessing that he might have been dwelling on thoughts of revenge, which were cruel enough to let him pass.”<br>“True enough-”<br>“I get it now! To cross the bridge safely, you will have to dwell on bad thoughts. I am a genius!” Bradley clapped his hands in excitement.<br>Suddenly, he was spotted by the evil eyes of Lord Derick. Lord Derick immediately asked suspiciously, “Why and what were you whispering to each other?”<br>Bradley stammered, “We w-were just-”<br>“You! Go onto the bridge now!”<br>“M-me?” Bradley asked.<br>“Yes, you!”<br>Bradley slowly walked to the bridge. Remembering what he had said before, Bradley focused his thoughts on bad memories - the night when it had all happened.<br>In an instant, he was across the bridge and in the center of the cave.<br>He watched as person after person crossed the bridge. Most people who overheard Bradley’s conversation crossed the bridge safely. Unfortunately, many did not survive. Finally, it was Tom’s turn.<br>Tom thought of his brawl with Arthur, and tried crossing the bridge. The bridge wavered, and Tom heard cracking noises. So he fixed his thoughts on the people who fell into the lava. Still, the bridge continued to produce cracking noises. Is Lord Derick purposefully making this harder? A voice, seemingly inside the bridge, whispered, just give up, just give up! Tom, after hearing this, grew angry, and he, not caring about his life, landed his fists onto the bridge. The bridge, who could not endure public humiliation and Tom’s fury at the same time, pleaded, okay, I will let you pass! So Tom walked across the island unscathed. Lord Derick looked furious.</p>
<h2 id="A-Trivial-Brawl"><a href="#A-Trivial-Brawl" class="headerlink" title="A Trivial Brawl"></a>A Trivial Brawl</h2><p>On the island where the shaggy survivors stood, there was a town. In the center of town there was a castle. Due to the foggy heat waves, even Lord Derick could not see much other than that. However, anyone could also see that Lord Derick was not happy with Tom for surviving the bridge. 21 people? Damn. But the more people, the better, I guess. And that Tom… he’s really strong. Well, so is that Arthur. They will make excellent generals for his majesty. The group walked to the entrance of the town, where two guards wielding heavy swords blocked the entrance. Seeing the ragged group of strangers, they flashed their swords out.<br>“Halt!” they shouted. “Or else, hehe…”<br>“I am Lord Derick! Let us in, or else you will meet a fatal ending.”<br>Realizing that the stranger was no other than Lord Derick, they bowed solemnly and let the group in. A man chuckled, “At least we won’t get pushed around by other people.” He was met by the tip of a sword.<br>“What did you say?” One guard asked grimly. They advanced upon the person, who instantly regretted his life. Suddenly, the first guard jumped up and swung his sword at the man. Lord Derick, spotting action behind him, whipped out his sword and with a light swing, he knocked out the first guard. The first guard fell with an “oof”. The second guard, a bit nervous, backed away, only to be blown right next to the first guard. A third guard came out of a house near the entrance.<br>“Who is this underdeveloped soldier, I may ask?” He mocked.<br>Lord Derick swung his sword at him, only to be blocked with a “ching” by the guard’s spear. Then the guard charged toward Lord Derick with his spear. Lord Derick blocked with his sword. At first, Lord Derick was pretty confident. However, his confidence faded instantly as he recognised the pattern on the spear - like his own sword, that spear was one of the few divine weapons in the entirety of the underworld! A second later, his sword broke in half. Fortunately, the guard was also stung, and seeing the opportunity, Lord Derick knocked him out with the handle of his sword. “This thing will take a long time to heal,” Lord Derick sighed to what was left of his sword.<br>However, the guards didn’t give up that easily. The first guard woke up silently, and seeing that Lord Derick was distracted, picked up his sword and silently charged toward him. However, Lord Derick noticed and kicked him in the chest, but the sword pierced him. Both stumbled back, and the guard fell with a grunt. The fight was over.</p>
<h2 id="To-the-Castle"><a href="#To-the-Castle" class="headerlink" title="To the Castle"></a>To the Castle</h2><p>“The fun is over. Now let us hurry.” Lord Derick grunted, plucking the sword out. His wound instantly healed, but his bad mood did not. The group walked silently. Arthur glanced around. He was still sulking over his fight with Tom, but presently he stuffed it in the back of his mind, for what was in front of him was more to his interest. On both sides, two story buildings were neatly arranged, with narrow alleys littered with garbage in between. Those buildings, though tall, were seriously lacking repair, and looked as if they would collapse at any moment.<br>Everywhere, Arthur could see people. People fighting, people screaming, people on fire, people dying, and people drinking others’ blood due to thirst. To the right, two men were fighting with kitchen knives, and as they spotted the group, they paused and stared at the people, their eyes full of loath and hatred. Arthur stared at them. One of them gave a Chesire-Cat smile and pointed his knife at Arthur. The other did the same. As they advanced toward Arthur, the two suddenly howled with pain and flinched back. After continuous attempts and howls of pain, they resigned and resumed their eternal fighting.<br>One minute later, another man also attempted vainly to attack the group, and received a similar fate. Like the last two, he was constantly bounced back. Is this a curse? Wow. To Arthur’s surprise, the group suddenly halted, and Arthur crashed into the teenager in front.<br>“Ow!” The teenager shouted, and Arthur could tell that he was tired and depressed. However, despite that, Arthur had a very high self esteem, and shouted back, “Who cares about you?”<br>“Be quiet, will you?”<br>“No, and I never will until you apologize.”<br>“Never.”<br>As soon as the teenager muttered that, Arthur dealt a deft blow on the teenager’s forehead, unbalancing him and knocking him down.<br>“Now, now, now. What do we have here? Fools who can’t learn to respect the king’s castle?” Arthur turned around to see Lord Derick looking at him.<br>“I…was… …. fighting a kid? Teaching him a lesson?” Arthur expected to be met with a face of hatred. Instead, Lord Derick looked at him curiously.<br>“Hmm… you would definitely make a great general for his majesty…” Lord Derick turned around. Arthur did not know what to make of that, but he thought it was a good thing, and decided that he would work hard to become a general for his majesty.<br>Suddenly, the teenager kicked at Arthur. Arthur blocked it and was about to retaliate when Lord Derick flung the teenager onto the roof of a random building, where he was met by an angry mob of roof-dwellers. The teenager, scared out of his wits, jumped down the building, broke a leg, and limped back to join the crowd.<br>“Welcome to the entrance of the Kingdom of the Underworld, home of our Lord. Unfortunately, some people cannot learn to respect him,” Lord Derick paused to glare at the teenager. “Anyways, this is where your actual test begins. Twenty people against twenty zombies. The key is to learn that killing is normal. Don’t be afraid. I expect only ten of you to survive. NOW!”</p>
<h2 id="The-Entrance-Exam"><a href="#The-Entrance-Exam" class="headerlink" title="The Entrance Exam"></a>The Entrance Exam</h2><p>Suddenly, the entrance was blocked by a huge boulder, similar to the one that blocked the tunnel. Three huge walls appeared out of nowhere, and a red mist appeared in the center. Within seconds, it expanded and solidified into a horde of ragged people, like those earlier seen in the town. However, Arthur soon noticed that those people were broken from their curse. They moved around freely and attacked the stragglers.<br>Arthur, seeing a zombie advance on him, kicked his chest. The zombie grabbed his foot at the last moment. Seeing his disadvantageous situation, Arthur barely managed to control his overwhelming thoughts of the ways he could die. However, he decided to continue living, so with a sudden jerk, Arthur put his whole weight onto his captive leg, which knocked the zombie back. Then, with a smash, Arthur hit the zombie’s head off. Arthur looked around. People were screaming in agony all around him as zombies bit into their flesh or tore their fingers off. Then, Arthur saw Tom. Tom was fighting bravely, crashing zombies together, smashing zombies into walls, and saving Bradley’s life. Arthur had a sudden evil thought. What if I could sabotage Bradley? I would surely become his majesty’s right hand man!<br>Arthur waited until Tom faced away from Bradley. Then, he snuck behind Bradley and pushed him so that he was inches away from a zombie. He watched as Bradley struggled with the zombie for a moment, then snuck away. Unfortunately, Tom noticed Bradley, and immediately swooped to the rescue. However, after Tom finished off the zombie, he noticed that Bradley made no signs of movement.<br>Touch. No movement. Shake. No movement. Tom felt Bradley’s pulse. No, he didn’t faint - Bradley had died. And Tom knew that even if he cried that entire day, reality is reality. Tom laid the blame on the zombies, and fought them hard until every last zombie collapsed.<br>Finally, as every zombie died, Arthur glanced around. Everywhere, he could see survivors huddling together. All of them were cheering except for Tom, who was silently making a prayer. There were fourteen left, including Arthur. For them, their common thought was to get rid of Lord Derick and to escape as quickly as possible. But for Arthur, his thought was to become an excellent general to his majesty, whoever he was. As the boulder blocking the castle’s entrance rolled away, Arthur thought to himself, this is my one and only chance.</p>
<h2 id="His-Majesty"><a href="#His-Majesty" class="headerlink" title="His Majesty"></a>His Majesty</h2><p>Tom was devastated by the loss. He had lost his brother when he could possibly have avoided the situation by listening to Bradley at the beginning of the story. Tom was also angry at Lord Derick. He, Tom thought, was the key to my devastation. Little did he know, this thought was going to change.<br>Inside the castle, the first thing Tom walked onto was another bridge. Like before, it was sitting above lava. However it was also like a main street. Connected to the bridge were smaller sub-bridges that led to towers and black houses sitting on charred islands bulging atop a vast lava lake. After a bit of walking, the tired and exhausted group stopped. Before them the “main street” stopped. It was replaced by a drawbridge. Below the drawbridge was what seemed to be a hollowed out island. What was interesting was that the lava in the island was considerably more active than the lava seen elsewhere inside the castle, as if it had a conscience of its own. The drawbridge connected to the king’s temple, which was black. In the temple was a charred throne surrounded by statues and piles of gold and diamond. As decoration, there were holes constantly pouring lava. And then there was the king himself.<br>His majesty, also called Nomed, was a muscular man with a black cloak and wore a basalt crown. His staff was also made of compressed basalt, which was a few dozen times stronger than normal basalt, and had a giant ruby resting on top. He had a pale, expressionless face with a pair of red, unforgiving eyes.<br>Presently, Lord Derick stepped forward and walked up to the drawbridge, which was lowered, and bowed, “Your majesty, I will present to you fourteen captives. Of course, the rest is all up to you, your high lord.”<br>“You have served me well, Derickson.”<br>Suddenly, the drawbridge collapsed, and Lord Derick shrieked, “NOOOOOOOOOOO!” shortly before he fell into the fiery pit below.<br>“Do you think I would not know that you were pretending to be me, Lord Derick? Try to pretend now! Hahaha… anyways, do any of you know why you guys are here and not in your pathetic little cities?”<br>No one answered; everyone was dazed at how quickly Lord Derick was finished off by a random stranger. Finally, Arthur said, “No, your majesty.” Tom glared at him. Was he trying to be the king’s pet?<br>“Yes, he is.” Nomed replied in a courteous fashion.<br>Tom stumbled. Can he read my mind?<br>“Who is what, your majesty?” Arthur asked politely.<br>“I can tell that you are trying to be my pet, and I could grant it to you, but…” Nomed pretended to think.<br>Arthur stumbled too. “Y-you… can r-read…?”<br>“Yes, of course I could read minds, or else why would I be king, and not that Derick?”<br>Just as Nomed finished speaking, a sword knocked Nomed out from his throne. Nomed fell onto the temple floor with a grunt. A familiar face appeared from behind. “Still remember me?” He mocked.<br>Nomed rolled out of the temple, then got up. “Derickson? How are you here?”<br>“When the drawbridge collapsed, I fell close to a stalactite. So I made my way up stealthily,” Derick spoke proudly.<br>“Huh,” Nomed puffed. Suddenly, he whistled, and a ghostly horse appeared out of thin air. “If you want a fight, here it is.” Nomed sat on his horse, and, with his staff out, he charged toward Derick. Derick dodged to the side. Nomed charged again. This time, Derick was not as lucky. He barely missed the point of the spear, and the horse tripped him. As he rose, he noticed Nomed charging for the third time. Thinking that it was too late to dodge, Derick took out his fully healed sword and pointed it at the horse. Both were knocked back by the impact, and Nomed’s horse disappeared at the cost of one broken leg.<br>Derick limped back and decided to go on the offensive. He swung at Nomed, only to be received with one random swing of his staff that sent him and his sword flying back. From then, he fully devoted himself to playing defensive. He saw Nomed charge at him. He blocked it. Nomed charged again. He blocked it. Nomed, realizing, threw the staff like a boomerang. Derick bent his waist and dodged it. As he got back up though, he was assaulted by the staff from behind and fell down unconscious. Nomed dragged Derick to the cliff and threw his limp body down the pit.<br>“No more clutches this time! Anyways, the reason I have you here is for the purpose of intelligence. I want you to tell me what is going on up there.” Nomed pointed up towards the ceiling.<br>“Why should we tell you?” Tom asked.<br>“Ah. Good question. You see, I am preparing a full-scale invasion on the world above me to avenge my lost brothers. Long ago, there was a war between us and a group of barbarians who call themselves heaven. We had plans for dragons, seas of lava, and fifty-headed creatures to dominate this planet. Wonderful plan, right? Unfortunately, the cruel heaven-people wanted something different. So we waged ten years of war. The opposite army won by a landslide, killing both my brothers in the process. Tomorrow, I shall avenge them. That is, if you would cooperate in giving me information. If not, expect yourselves to meet the same fate as that Lord Derick.”<br>The group huddled together. A few people muttered, “What should we tell Nomed?”<br>“Everything!” came a shout. The crowd looked to see Arthur, cautiously stepping onto the raised drawbridge. “Your majesty, first you shall know that our world above is weak from two world wars. Second, many people are poor and homeless and would do everything to serve you. Lastly, there is something called global warming outside, which will make the climate above similar to the climate below.”<br>The crowd hissed at Arthur.<br>“Why did you tell Nomed about us?” whispered Tom threateningly.<br>There was agreement in the crowd. However, they were hushed by Nomed’s loud voice. “Excellent! Tomorrow, I shall have my revenge. But tonight, you shall rest.” To the crowd’s horror, Nomed suddenly grew a pair of shining black wings and flew down. “Follow me.” The crowd was assigned a roomy bedroom next to the temple. As Tom settled down, he thought over what had happened that day. After exhausting most of his brain cells, he came to the conclusion: Nomed was the main villain.</p>
<h2 id="A-Sudden-Shift-in-Power"><a href="#A-Sudden-Shift-in-Power" class="headerlink" title="A Sudden Shift in Power"></a>A Sudden Shift in Power</h2><p>As Nomed flew back to his temple, he sensed that something was off. There was movement…below him. Looking around, Nomed double checked to see that no one was spying on him.<br>“Good, all of those chickens are asleep.” Nomed lightly touched the throne with the ruby on the staff. Immediately, the throne disappeared into a red mist, revealing a secret tunnel. Nomed went through, and the throne immediately sealed the entrance. As the tunnel went deeper, the air became hotter and mistier. Suddenly, Nomed heard quick footsteps. Before he could react, a sword flew straight through his chest, leaving a large, bleeding gap. Nomed banged his staff onto the ground, and the wound instantly healed.<br>“Who dares to attack the king?” he asked in the cave.<br>A voice answered, “It is me, Lord Derick the third.”<br>“How did you survive? AGAIN?”<br>“How I survived is not your problem. The problem is that YOU won’t survive tonight.” Lord Derick’s sword returned to him to deal another blow. Nomed tried to attack back with his staff, but realized that the tunnel was too small to use it.<br>“Ha! You fell for my trap. Now, hand over your power and we will all be unhurt, or I will take it by force.”<br>“Never!” Nomed hissed. However, he himself knew that he was in a very risky situation. As Nomed backed away tactfully, Lord Derick aimed his sword at Nomed’s staff. To Nomed’s surprise, the staff actually broke!<br>“After the fight last time, I realized your weakness. You are basically useless without your staff.”<br>“Well, you underestimated me.”<br>“Then let’s see what you can do against this!” Lord Derick, picking up his sword, charged at Nomed. Nomed quickly bent his waist, and seeing the opportunity, he grasped the handle of Lord Derick’s sword and gave him a strong kick. This left Nomed with the sword.<br>“Now who is vulnerable?”<br>“Not me!” Lord Derick punched Nomed hard, knocking the sword from his hands. In a split second, the sword flew through the air, out of reach from both pairs of hands. Then, Nomed ran towards the sword, and instead of grabbing it, he punched it with his palm so that it sliced through Lord Derick’s outreached arms and plunged deep into Lord Derick’s heart. Lord Derick stumbled, tried to curse, but fell onto his knees. Then, his knees gave up, and he was on all fours. After a few moments, Lord Derick finally collapsed.<br>Nomed, thinking he won, half walked, half stumbled out of the tunnel with the remnants of his staff. Although he had won, he knew he could not live long, especially after his staff broke. So he came up with a plan. He flew down quietly to where the people were sleeping and tapped on Arthur with the ruby of his staff. Then, he flew back to his temple and tapped the ruby on the hard stone. Arthur instantly teleported there, confused and sleepy.<br>“Y-your majesty, w-why am I h-here?”<br>“I have an important mission for you. I had a fight with a terrible monster, and now I am weakened. So, in conclusion, do you want to be my heir and finish the job of having revenge on the people above?”<br>“Hmm… okay…”<br>“Is that a yes or a no?”<br>“Y-yes, your majesty.”<br>“Now take the ruby on my staff. Take it, and hold it high.” Nomed started to cough.<br>Arthur tried to take the ruby off, but it didn’t budge. Nomed muttered something and the ruby instantly popped off so easily that Arthur almost fell over. Balancing himself again, Arthur held the ruby high in his hands. The ruby wiggled, like an egg about to hatch. It wiggled again. Then, it happened. The ruby suddenly grew into a staff. However, Arthur noticed that it seemed different from Nomed’s. It looked…more to Arthur’s style. Arthur clapped his hands in joy and banged the staff onto the ground. Instantly, a pair of wings unfolded behind him.<br>“Wow! This is cool!”<br>Nomed coughed even harder. “Hold on. Before I die, you must promise me one thing. That is to have revenge on the people above. Do you promise to fulfill this mission?”<br>“Yes, and I will promise to send fear into all of their hearts, and most importantly, Tom’s.” A seemingly new Arthur spoke as a new day came to being.</p>
<h2 id="To-Be-Continued"><a href="#To-Be-Continued" class="headerlink" title="To Be Continued"></a>To Be Continued</h2>]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>How to run fio across multiple servers</title>
    <url>/blog/How-to-run-fio-across-multiple-servers/</url>
    <content><![CDATA[<h2 id="Client-server-mode"><a href="#Client-server-mode" class="headerlink" title="Client-server mode"></a>Client-server mode</h2><p>Normally <a href="https://fio.readthedocs.io/en/latest/fio_doc.html#client-server">fio</a> is invoked as a stand-alone application on the machine where the I&#x2F;O workload should be generated. However, the backend and frontend of fio can be run separately i.e., the fio server can generate an I&#x2F;O workload on the “Device Under Test” while being controlled by a client on another machine.</p>
<span id="more"></span>

<p>Start the server on the machine which has access to the storage DUT.</p>
<ul>
<li><p>Start a fio server, listening on all interfaces on the default port (8765).</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ fio --server</span><br></pre></td></tr></table></figure>
</li>
<li><p>Start a fio server, listening on IP belonging to hostname and on specified port.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ fio --server=&lt;ip&gt;:&lt;hostname&gt;,&lt;port&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>Start server on vm1 and vm2 and listen on default port 8765:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 ~]<span class="comment"># fio -server</span></span><br><span class="line">fio: server listening on 0.0.0.0,8765</span><br><span class="line"></span><br><span class="line">[root@vm2 ~]<span class="comment"># fio -server</span></span><br><span class="line">fio: server listening on 0.0.0.0,8765</span><br></pre></td></tr></table></figure>

<p>If you have multiple servers, you can input a file containing host IPs&#x2F;names for the –client option.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm3 ~]<span class="comment"># cat host.list.2vms</span></span><br><span class="line">10.10.10.2</span><br><span class="line">10.10.10.3</span><br></pre></td></tr></table></figure>

<p>Create a fio job file to be run on each server:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm3 ~]<span class="comment">#  cat fio.job</span></span><br><span class="line">[global]</span><br><span class="line">ioengine=libaio</span><br><span class="line">size=5GiB</span><br><span class="line">name=<span class="built_in">test</span></span><br><span class="line">direct=1</span><br><span class="line">iodepth=128</span><br><span class="line">group_reporting=1</span><br><span class="line">numjobs=2</span><br><span class="line"></span><br><span class="line">[job1]</span><br><span class="line">blocksize=64k</span><br><span class="line">filename=/dev/sdd</span><br><span class="line">readwrite=write</span><br><span class="line">end_fsync=1</span><br></pre></td></tr></table></figure>

<p>Trigger fio jobs from client and run the jobs across multiple servers:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm3 ~]<span class="comment"># fio --client=./host.list.2vms --output=fio.2vms.out fio.job</span></span><br></pre></td></tr></table></figure>

<p>Check the fio result:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm3 ~]<span class="comment"># cat fio.2vms.out</span></span><br><span class="line">hostname=vm1, be=0, 64-bit, os=Linux, <span class="built_in">arch</span>=x86-64, fio=fio-3.7, flags=1</span><br><span class="line">hostname=vm2, be=0, 64-bit, os=Linux, <span class="built_in">arch</span>=x86-64, fio=fio-3.7, flags=1</span><br><span class="line">&lt;vm1&gt; job1: (g=0): rw=write, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=libaio, iodepth=128</span><br><span class="line">&lt;vm1&gt; ...</span><br><span class="line">&lt;vm2&gt; job1: (g=0): rw=write, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=libaio, iodepth=128</span><br><span class="line">&lt;vm2&gt; ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;vm1&gt; Starting 2 processes</span><br><span class="line">&lt;vm2&gt; Starting 2 processes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">job1: (groupid=0, <span class="built_in">jobs</span>=2): err= 0: pid=25680: Sat Sep 23 04:41:08 2023</span><br><span class="line">  write: IOPS=2864, BW=179MiB/s (188MB/s)(9537MiB/53276msec)</span><br><span class="line">    slat (usec): min=2, max=302, avg=11.76, stdev= 7.74</span><br><span class="line">    clat (msec): min=3, max=191, avg=89.36, stdev=15.47</span><br><span class="line">     lat (msec): min=3, max=191, avg=89.37, stdev=15.47</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[   59],  5.00th=[   62], 10.00th=[   64], 20.00th=[   70],</span><br><span class="line">     | 30.00th=[   87], 40.00th=[   92], 50.00th=[   94], 60.00th=[   96],</span><br><span class="line">     | 70.00th=[   99], 80.00th=[  103], 90.00th=[  106], 95.00th=[  109],</span><br><span class="line">     | 99.00th=[  114], 99.50th=[  116], 99.90th=[  129], 99.95th=[  163],</span><br><span class="line">     | 99.99th=[  192]</span><br><span class="line">   bw (  KiB/s): min=79744, max=129664, per=50.01%, avg=91671.96, stdev=15600.42, samples=212</span><br><span class="line">   iops        : min= 1246, max= 2026, avg=1432.36, stdev=243.76, samples=212</span><br><span class="line">  lat (msec)   : 4=0.02%, 10=0.03%, 20=0.03%, 50=0.08%, 100=74.58%</span><br><span class="line">  lat (msec)   : 250=25.26%</span><br><span class="line">  cpu          : usr=0.67%, sys=1.32%, ctx=9877, majf=0, minf=19</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=99.9%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.1%</span><br><span class="line">     issued rwts: total=0,152588,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=128</span><br><span class="line"></span><br><span class="line">Run status group 0 (all <span class="built_in">jobs</span>):</span><br><span class="line">  WRITE: bw=179MiB/s (188MB/s), 179MiB/s-179MiB/s (188MB/s-188MB/s), io=9537MiB (10.0GB), run=53276-53276msec</span><br><span class="line"></span><br><span class="line">Disk stats (<span class="built_in">read</span>/write):</span><br><span class="line">  sdd: ios=48/151321, merge=0/1119, ticks=34/13488966, in_queue=13489000, util=99.84%</span><br><span class="line">&lt;vm1&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">job1: (groupid=0, <span class="built_in">jobs</span>=2): err= 0: pid=4425: Sat Sep 23 04:41:08 2023</span><br><span class="line">  write: IOPS=2855, BW=178MiB/s (187MB/s)(9537MiB/53430msec)</span><br><span class="line">    slat (usec): min=2, max=269, avg=12.22, stdev= 9.09</span><br><span class="line">    clat (msec): min=4, max=137, avg=89.61, stdev=15.68</span><br><span class="line">     lat (msec): min=4, max=137, avg=89.63, stdev=15.68</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[   56],  5.00th=[   62], 10.00th=[   63], 20.00th=[   70],</span><br><span class="line">     | 30.00th=[   87], 40.00th=[   92], 50.00th=[   94], 60.00th=[   97],</span><br><span class="line">     | 70.00th=[  100], 80.00th=[  103], 90.00th=[  106], 95.00th=[  110],</span><br><span class="line">     | 99.00th=[  115], 99.50th=[  117], 99.90th=[  123], 99.95th=[  125],</span><br><span class="line">     | 99.99th=[  136]</span><br><span class="line">   bw (  KiB/s): min=77952, max=128896, per=49.89%, avg=91190.65, stdev=15736.83, samples=212</span><br><span class="line">   iops        : min= 1218, max= 2014, avg=1424.84, stdev=245.89, samples=212</span><br><span class="line">  lat (msec)   : 10=0.02%, 20=0.03%, 50=0.22%, 100=71.90%, 250=27.82%</span><br><span class="line">  cpu          : usr=0.66%, sys=1.37%, ctx=9653, majf=0, minf=19</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=99.9%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.1%</span><br><span class="line">     issued rwts: total=0,152588,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=128</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">All clients: (groupid=0, <span class="built_in">jobs</span>=2): err= 0: pid=0: Sat Sep 23 04:41:08 2023</span><br><span class="line">  write: IOPS=5711, BW=357Mi (374M)(18.6GiB/53430msec)</span><br><span class="line">    slat (usec): min=2, max=302, avg=12.22, stdev= 6.43</span><br><span class="line">    clat (msec): min=3, max=191, avg=89.61, stdev=11.09</span><br><span class="line">     lat (msec): min=3, max=191, avg=89.63, stdev=11.09</span><br><span class="line">   bw (  KiB/s): min=77952, max=129664, per=24.91%, avg=91190.65, stdev=11114.46, samples=424</span><br><span class="line">   iops        : min= 1218, max= 2026, avg=1424.84, stdev=173.67, samples=424</span><br><span class="line">  lat (msec)   : 4=0.01%, 10=0.02%, 20=0.03%, 50=0.15%, 100=73.24%</span><br><span class="line">  lat (msec)   : 250=26.54%</span><br><span class="line">  cpu          : usr=0.66%, sys=1.35%, ctx=19530, majf=0, minf=38</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=99.9%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.1%</span><br><span class="line">     issued rwts: total=0,305176,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line"></span><br><span class="line">Run status group 0 (all <span class="built_in">jobs</span>):</span><br><span class="line">  WRITE: bw=178MiB/s (187MB/s), 178MiB/s-178MiB/s (187MB/s-187MB/s), io=9537MiB (10.0GB), run=53430-53430msec</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Disk stats (<span class="built_in">read</span>/write):</span><br><span class="line">  sdd: ios=84/151410, merge=0/1068, ticks=30/13539292, in_queue=13539322, util=99.77%</span><br><span class="line">&lt;vm2&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Fio</tag>
      </tags>
  </entry>
  <entry>
    <title>Just a normal summer camp</title>
    <url>/blog/Just-a-normal-summer-camp/</url>
    <content><![CDATA[<p>Once upon a time, nestled deep within the dense woods, there lay a serene and seemingly ordinary summer camp called “Genius Camp.” Every year, children from far and wide would flock to this camp to embark on adventures, make new friends, and create memories that would last a lifetime. The camp was known for its rustic cabins, shimmering lake, and captivating nature trails.<span id="more"></span></p>
<p>Among the campers was a curious and unassuming 12-year-old boy named Max Johnson. With his mop of unruly dark hair and round glasses, Max was just like any other camper—eager to have fun and enjoy his time away from school. Little did anyone know that Max was about to experience a transformation that would turn his summer upside down.</p>
<p>One bright and sunny morning, while the campers gathered at the mess hall for breakfast, a mysterious meteor streaked across the sky and landed with a soft thud not too far from Genius Camp. Nobody paid much attention, dismissing it as a shooting star. However, Max, with his inquisitive mind, couldn’t resist exploring the crash site. He sneaked away from the group, unnoticed.</p>
<p>As Max reached the spot, he found a peculiar, glowing crystal nestled within the crater. Mesmerized by its radiance, he reached out to touch it. In an instant, a brilliant flash of light engulfed him. When the light subsided, Max felt different—strangely invigorated and filled with knowledge beyond his years.</p>
<p>Back at camp, Max’s transformation was immediate. During the morning activities, he aced every challenge thrown his way, outsmarting older campers and even baffling the camp counselors. He was suddenly a master at archery, a math genius, an expert rock climber, and a remarkable storyteller. The other kids were in awe of his newfound brilliance, and even the counselors were left scratching their heads in amazement.</p>
<p>As word spread through the camp about the “smartest kid ever,” Max’s popularity soared. Campers sought his guidance, and he willingly shared his newfound knowledge with anyone who asked. He became the heart of every campfire conversation and the hero of every camp activity. But with his newfound intelligence came a subtle change in his demeanor. Max, once a carefree and playful boy, seemed to carry the weight of the world on his young shoulders. As the days passed, the camp started to witness some peculiar occurrences. The animals of the forest seemed to gather around Max, as if listening to his words. The lake’s water sparkled with an inexplicable glow when he dipped his hand into it. Nature itself seemed to respond to his presence.</p>
<p>However, amidst all the excitement, Max began to feel isolated. The other kids admired him for his intelligence, but they no longer saw him as their friend. He missed the carefree days of laughter and adventure, where he wasn’t just the “smart kid.” As Max wandered through the woods, he stumbled upon a wise old camp counselor, who sensed the turmoil within him.</p>
<p>The counselor shared a heartwarming tale of a young magician who was granted infinite knowledge but forgot the true magic of being a child. This story struck a chord with Max, and he realized that being the smartest kid was not as important as being himself and embracing his ordinary moments.</p>
<p>Determined to find a balance, Max decided to put his intelligence to good use. He organized a grand scavenger hunt, incorporating riddles and puzzles that challenged both his intellect and the campers’ creativity. With teamwork and fun at the forefront, the campers bonded like never before.</p>
<p>The climax of the summer came with the annual talent show. Instead of showcasing his vast knowledge, Max decided to put on a heartwarming performance, sharing stories of friendship, love, and the beauty of nature. The audience was enthralled, and Max felt a sense of joy he hadn’t experienced in days.</p>
<p>As the summer ended, Max bid farewell to his new friends, promising to return the following year. The meteor had left him, but the memories of his extraordinary summer remained. Max had discovered that true wisdom was not about being the smartest, but about understanding oneself and appreciating the simple joys of life.</p>
<p>And so, Genius Camp continued to be a cherished summer camp, but it was forever changed by the summer when a normal camp turned extraordinary, thanks to the boy who became the<br>smartest kid ever. And little did they know that the magic of that summer would live on in the hearts of campers for generations to come, inspiring them to seek the wonders that lie within and all around them.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Legends of the Scarlet Serpent</title>
    <url>/blog/Legends-of-the-Scarlet-Serpent/</url>
    <content><![CDATA[<p>Once upon a time, in the Golden Age of Pirates, when the vast seas were ruled by daring adventurers seeking fame, fortune, and the thrill of high-seas escapades, there was a legend that sent shivers down the spines of sailors and ignited the imaginations of all who heard it – the legend of the Scarlet Serpent. </p>
<span id="more"></span>
<p>Captain Roronoa Zoro, a formidable and charismatic pirate, was at the heart of this tale. With a flowing mane of emerald, green hair, a well-groomed beard, and a fierce scar that ran across one eye, he was a striking figure that demanded respect and inspired fear among both friend and foe. His ship, the Demon King, was a menacing sight on the horizon, a ship rumored to have been forged from the fiery depths of Hades himself. </p>
<p>Roronoa Zoro’s thirst for adventure and treasure was insatiable, but it was his burning desire to uncover the mythical Scarlet Serpent, a legendary sea creature said to guard an island overflowing with untold riches, that drove him forward. Many had sought the island’s treasures, but none had returned to tell the tale. </p>
<p>As the sun dipped below the horizon, painting the sky in shades of red and orange, Captain Zoro and his loyal crew set sail with the wind in their favor. Guided by ancient maps and whispers of old sailors, they charted their course towards the rumored location of the fabled island. </p>
<p>For days, they sailed through treacherous storms and uncharted waters, their spirits unyielding, fueled by the promise of unimaginable wealth. With each passing night, stories of the Scarlet Serpent spread like wildfire through the crew, intertwining with superstitions and fears that clung to the minds of even the bravest sailors. </p>
<p>One moonlit night, the sailors huddled together as Captain Zoro recounted a tale from his childhood – the haunting story of a pirate who had dared to set foot on the forbidden island. The captain’s voice was a blend of mystery and allure as he described the island’s hidden dangers, the monstrous guardians, and the enchanting allure of the treasure. </p>
<p>But amidst the tales of danger, an underlying message of unity and camaraderie echoed through his words. The crew, once filled with uncertainty, now stood united in their determination to face whatever challenges lay ahead. </p>
<p>Days turned into weeks, and the Demon King cut through the waves with relentless determination. The crew encountered fierce sea creatures, battled rival pirates, and weathered storms that tested their very souls. Yet, the legends of the Scarlet Serpent fueled their resolve, and the crew remained steadfast in their pursuit of the hidden island. </p>
<p>As they drew closer to their destination, the atmosphere aboard the Demon King became electric with anticipation and fear. One misty morning, the lookout’s cry of “Land ahoy!” pierced through the fog, and the crew’s hearts pounded with excitement. </p>
<p>The island loomed before them, its silhouette shrouded in mystery. Verdant foliage covered the shore, beckoning them with an eerie allure. With hearts racing, Captain Zoro led the landing party ashore, their feet sinking into the soft sand. </p>
<p>As they ventured further into the heart of the island, the legend of the Scarlet Serpent took on a life of its own. Vines whispered secrets, and trees seemed to guard hidden passages. Yet, the crew pressed on, their courage bolstered by their unwavering trust in their captain. </p>
<p>Soon, they encountered the island’s first challenge – a maze of perplexing puzzles guarding the path to the treasure. With each riddle solved, they unlocked the way forward, stepping closer to their ultimate prize. </p>
<p>Finally, they stood before the fabled Scarlet Serpent – a magnificent creature with ruby scales shimmering in the dappled sunlight. The serpent regarded them with ancient wisdom, its eyes seemingly knowing every thought that crossed their minds. </p>
<p>“So…….I would suppose that you are seeking my treasure, as did many pirates did before you? Alas, not one survived the challenge I gave them. You see, there are a clan of warlike baboons that live on this island. Kill them, and I shall give you my treasure. If you die, well, sucks to be you.” </p>
<p>Captain Zoro looked at his crew and nodded. Nothing was going to stop them from getting their treasure. The Scarlet Serpent gave them a map, leading them to the cave of the baboons. When they got there, they were instantly approached by the leader of the pack, but he was not a baboon.  </p>
<p>He was a man. But the most shocking part was his sword. It was shaped like a cross, with a long gleaming blade that curved towards the end. Its handle was filled with jewels, but the most surprising part was that it was a Black Blade. According to legend, the legendary sword god Ryuma had made the 12 most powerful swords in the world. Supposedly, each sword contained the will of one of the 12 Olympian Gods. If used by a skilled enough swordsman, it could be turned into a Black Blade, which was unbreakable. Captain Zoro himself owned 3 of these swords, namely, Yama (will of Ares), Tushita (will of Poseidon), and Shisui (will of Hades).  </p>
<p>In Ryuma’s will, he had assigned the 12 swords to some of the strongest swordsmans, including Captain Zoro. However, Yoru (Will of Hades), was never received by the swordsman Dracule Mihawk. Mihawk was a mystery to the world, as no one had ever even heard of him before. People thought that there had been a mistake, until one night, a masked figure came secretly into the room of Swords and retrieved Yoru. And he left the message “May the Worst Generation Prevail!” </p>
<p>For the first time in 20 years, Captain Zoro was in the presence of someone who rivaled him in power. Mihawk looked at him with a cruel smile. He raised his sword and promptly cut off one of the baboon’s heads with a swish. </p>
<p>“Hello there, Roronoa Zoro. I have been looking forward to meeting you for a long time. I am the only person blocking you from retrieving the treasure. As you can see, these baboons are harmless. I look forward to beheading, I mean, dueling you. A person with your lineage should undoubtedly beat someone like me.” </p>
<p>Captain Zoro smirked. He finally had a worthy opponent in front of him. One that would last longer than 5 seconds in a fight. But no one had bested him ever since he learned his 3-sword style, with two swords in his hands and one sword in his mouth. He readied himself and charged at Mihawk. </p>
<p>“DEMON SLAYER!”  </p>
<p>“DEATH ENCOUNTER!” </p>
<p>The two swords clashed with so much force that the sky split open. And then it was over. Mihawk went through him, leaving a bloody gash across his chest and midsection. Captain Zoro fell, shocked that he had been wounded. Mihawk stood over him, preparing for the final blow. He swung his sword down, only to hit the dirt floor. Captain Zoro had dived out of harm’s reach and quickly went behind Mihawk. Oh, the look on Mihawk’s face! Instead of killing the foe, he now had a sword sticking out of his gut. He stumbled forward as Zoro pulled out the three swords. Mihawk turned to face him, laughing heartily. </p>
<p>“You fight well for a swordsman! I think we would make good friends!” </p>
<p>Without warning, Mihawk pierced Zoro’s heart with his blade, and the two “friends” both fell to the ground, dying.  </p>
<p>Captain Zoro was just another one of the pirate captains who sought out the treasure of the Scarlet Serpent. And just like any other, he perished, just when he was so close to victory.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Operation Insanity</title>
    <url>/blog/Operation-Insanity/</url>
    <content><![CDATA[<p>Caleb looked grimly at the riot happening in the public square of Aqua Haven. The president had been brainwashed by the evil genius Zach Apple and was acting weird ever since. For apparently no reason at all, Joe Biden had changed the date of the Olympics from Winter to Spring. Caleb himself was an exceptional swimmer, with a passion for the water that ran in his veins. He spent most of his free time at the local pool, honing his skills and dreaming of one day competing in the Olympics. </p>
<span id="more"></span>
<p>Caleb’s talent did not go unnoticed, and in the summer of 2016, he was invited to attend an Olympic prep camp at Washington D.C. Only the best swimmers would be invited to attend this prestigious camp. There, Caleb met Michael, Ryan, and Adam, each having a specialized stroke. They had been assigned to the same medley relay, and they were destined to win. They had broken the world record times for each stroke, and they would be unstoppable in a relay. </p>
<p>One day, as they were leaving the pool after a grueling practice session, they overheard hushed conversations among some government officials. To their shock, the government officials had been bribed by Zach Apple, CEO of Zapple, a world-famous technology company specialized in making futuristic products. It turned out that Zapple had sinister plans for taking over the world. The government officials had agreed to use Zapple’s mind control tech on President Joe Biden in exchange for 1 billion dollars each. He had been forcing Joe Biden to make erratic decisions that were not in the best interest of the country. On the day of the Olympics, he was going to bomb the headquarters of every major technology company, such as Apple, Microsoft, and Google.    </p>
<p>Hearing this, the four friends were faced with a dilemma. It was now up to them to stop the evil genius from taking over the world. But then, they would miss the chance to compete in the Olympics, the most important event in their life. But if they didn’t stop Zach Apple, millions of people would die from the bombing. The situation was dire, and the boys knew they had to act swiftly to save their nation. They named their top-secret mission “Operation Insanity.”  </p>
<p>There was one major obstacle, however. They had no idea where Zach Apple’s secret base was. Adam suggested they tell the FBI. Caeleb thought about this, but it was very likely that Zach Apple had also bribed them. Telling them that they knew about this plan would be like telling the police that you killed someone. Suddenly, Michael had an idea.  </p>
<p>“To defeat a genius, you must think like a genius,” he said, laughing. </p>
<p>Zach Apple was a diabolical mastermind. Everything he did was planned and had logic. His base must be located with the greatest convenience to himself. And there was only one location that suited that purpose perfectly. The Rio Aquatic Center, in Rio de Janeiro, was the chosen location of the 2016 Olympics. It made perfect sense. If Zach Apple was present at the Olympics, no one would even think about the possibility of him bombing the headquarters of the technology companies, which were all located in America. His base had to be located underneath the swimming pool.  </p>
<p>Meanwhile, Zach Apple was chilling in his secret base, dreaming about the power and money he would get after his plan was executed. Little did he know that four Olympic swimmers would stop him.   </p>
<p>The day of the Olympics arrived, and for the first time, Caeleb realized how dangerous the situation was. Tension filled the air. What if they couldn’t free the president and instead got caught by Zach Apple. What would he do to them. Alas, there was no time for that now. The friends crept through the sewers and found themselves facing a gigantic swimming pool. But it wasn’t a normal swimming pool, there were all sorts of obstacles blocking them from the finish. On the other side of the pool, grinning wickedly at them was none other than Zach Apple himself.  The moment they made it into the sewers, an alarm had been set off in the evil genius’s base. But he was unfazed. Anyone who wanted to ruin his plans had to first swim across the Zapple 1000 Racing Pool! It was an obstacle course that involved been chased by hydrogen bombs set at high speed, swimming up and downhill a slope, and lastly, making a swim workout on MySwimPro within 20 seconds, or else the ZapPhone would explode. </p>
<p>Finally, the moment of truth came. Their own lives were at stake. Hydrogen bombs could wipe out cities in seconds. Luckily for them, Zach was feeling generous enough to set the bomb only at world record pace. Sadly, that was still a challenge for them, as the world record time for the 400-medley relay was 3:20.71.  First up, Ryan Murphy went in the pool, and blasted off doing perfect backstroke. His 100 back broke the world record time by 0.98 seconds! Following his lead, Adam Peaty swam his breastroke leg, once again breaking the world record by 1.96 seconds. They were already 2 meters ahead of the hydrogen bomb.  Keeping it up, Michael Phelps and Caeleb Dressel sprinted their event with extraordinary speed and might. Then, all four friends dashed up the hill. This was the most difficult part for them by far. The bomb was still after them at world record pace freestyle, which was the fastest of all four strokes. Anyone who swam uphill would know how much slower it is than on a flat surface. Just when it looked like it was the end for them, Adam realized there was only one thing to do. He was the most muscular of the four friends, and he might be able to get the bomb off course, if he slammed into it hard enough. This couldn’t be a normal hydrogen bomb, as Zach Apple would never risk his own safety to kill some random people.  </p>
<p>“Stop Zach Apple!” Adam roared, before going back downhill.  </p>
<p>The three other friends looked in shock, still furiously swimming uphill, as Adam went back down, swiftly diverting the attention of the bomb. At the bottom, he jumped and bodyslammed the bomb so hard that they both went flying into Zach Apple. On impact, the bomb exploded, and the air was filled with gas and smoke. Coughing, the friends went back down, and rushed over to see what happened. The glass wall that protected Zach Apple was shattered, and Zach lied spread-eagle on the ground, unconscious. He had a shard of glass in his gut, making him bleed profusely.  </p>
<p>Caeleb went to see if Adam was okay, but their comrade had taken the brunt of the explosion, and now lay motionless on the floor. A huge piece of shrapnel was sticking out of his brain.  </p>
<p>“Is he dead?” Ryan whispered, terrified. </p>
<p>“Almost, I think. We need a medic to look after him. Here, Michael, take Adam back and call the ambulance. Ryan, go out there and tell everyone what Zach Apple was planning. Here is a recording I captured of the government officials talking about them being bribed. I will free Joe Biden and make him take back the bombing commands, “Caeleb ordered.  </p>
<p>Following his orders, the three friends burst into action. Caeleb went into Zach’s secret base and found Joe Biden tied up in a chair. “Wow,” he thought, “Zach must have bribed everyone in the government, or else at least someone would know that their president was missing. Joe Biden looked dazed, and he was wearing a complicated fancy-looking hat. There were many buttons on it, including mind control, zap, torture, and other actions. Currently, the mind control button was on. After toggling it back off, the president suddenly focused, and yelled, “What on earth am I doing tied up! Where is that Zach Apple jerk?” Caeleb quickly told him that he was here to rescue him. He also told him that no one new about his disappearance, because Zach Apple had bribed the government officials with a load of money.  </p>
<p>Hearing this, Joe Biden was furious, and asked how they were going to get out. Caeleb looked around and noticed a staircase going up. They walked up and went straight into the bathrooms. Zach had made a staircase under the toilet. Dumbfounded, Caeleb just stared at the out of place toilet seat. Joe Biden told him to hurry up and they rushed into the Olympic stadium, just as the games were about to begin. Everyone gasped when the president suddenly appeared. The government official took 1 look and dashed off. Apparently, no one had believed Ryan when he told them the truth. He had been handcuffed by the police when Joe Biden told them to let him go. Instead, he sent the police to lock up every government official.  </p>
<p>Joe Biden announced the four boys as national heroes for saving the country. He said that they could each have one reward. Instead, Caeleb just told him to postpone the Olympics until Adam was fully healed from his wounds. Of course, Joe Biden agreed, and Caeleb and Ryan happily left the stadium for another day of swimming. </p>
<p>Their mission accomplished, Caleb, Michael, Adam, and Ryan returned to Aqua Haven as national heroes. Their tale of bravery and friendship spread far and wide, inspiring others to stand up against evil and make a difference. </p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>The beauty of fire</title>
    <url>/blog/The-beauty-of-fire/</url>
    <content><![CDATA[<p>I can bet that anyone who reads this essay has felt cold at least once in their life. At first, I have always thought of the freezing cold with great annoyance, a pest-like feeling perhaps. As the numbness hits your brain, it’s like a shot directed towards your nerves. It wasn’t until my trip to Siberia when I learned to appreciate the cold and enjoy the feeling of being warm again.<span id="more"></span></p>
<p>I had always been a passionate swimmer, loving to swim whenever and wherever I could. Of course, that was when the pool was nice and warm after been heated. But the pools in Siberia had no such luxury. They were ice cold, and before I even got in the pool, I could feel my body temperature steadily dropping. But due to my love of swimming, I jumped in without any thought of consequences. Almost immediately, my body fell into a state of shock. I had never experienced anything even close to this before. It felt like shards of ice were cutting into my skin. Too late, my legs were already beginning to feel numb, and I desperately tried to swim a few meters to regain some body heat. A sense of dread drove into me like a knife about to find its way into my body. Just as I was about to lose consciousness, the kind lifeguard noticed me. He rushed towards me, and that was the last thing I saw. I woke up in a hospital, surrounded by my family and friends.</p>
<p>I was in the Federal Siberian Research and Clinical Center of FMBA of Russia. My father gave me a summary of what had happened that day. The lifeguard had saved my life. He jumped into the water without a suit and pulled me out of the water. He had not one thought for his own safety, for when he had gotten me out of the water, he himself lost consciousness. Suddenly, the horror of the situation hit me. I asked my father what had happened to him, and he just grimly shook his head. The lifeguard had perished. I would have died with him, if it wasn’t for the lifeguard, who called for backup before he jumped in the pool. It was my own mistake that had caused the death of the bravest person I ever knew.</p>
<p>I had been saved from hypothermia by the doctors, and it hadn’t been fatal. I was too much in shock and grief to say anything. I asked to be alone, away from the people who knew how much of a failure I was. I had been careless enough to ignore the warning saying that all swimmers had to wear a heated swimsuit. Suddenly, I noticed a bright fire coming out from the furnace next to me. As I sat next to it, my body filled with warmth and hope. A feeling of pure joy filled my heart, as I began believing that there was hope for a better future.</p>
<p>After I went back to U.S., I began studying the science of heat. I was confident that if heat could determine the life and death of cavemen back in the Stone Age, surely it could be a factor in the heating of homes in Siberia, and it could be the cause of Siberians living a warmer lifestyle.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>The most important swim meet</title>
    <url>/blog/The-most-important-swim-meet/</url>
    <content><![CDATA[<p>I remembered the first time I had ever considered joining a swim team. I was going to attend my friend Jack’s birthday pool party. We played water polo and balloon fights, which were all very fun. However, the one I was most excited about was the race. Jack’s dad, Mr. Chen, blew on the whistle and I was off. I had taken swimming lessons starting from young age, so I easily passed all the others.<span id="more"></span></p>
<p>In the end, I was the clear winner. I was incredibly happy while the parents praised me for my swimming speed. I was about to go when Mr. Chen suddenly came up to me. He informed me that he was the head coach of the swim team Alto. He was recruiting new members to join the team. He said that I was a talented swimmer, and with the right coaching, I could become a driving force in his team. I hesitated since I never actually considered joining a real team before. Before I could make up my mind, my dad, who had been eavesdropping on the conversation, said that he would gladly enroll me in the team.</p>
<p>Mr. Chen said that practice would begin the very next day, and I was to come at 6 o clock sharp. That night, I watched many videos about how to swim faster, fearing that I would not rise to Mr. Chen’s expectations. In the morning, I was rudely yanked out of bed by my dad, who shouted that I was running late for practice. I was immensely sleepy when I arrived at the pool. All the other swimmers had already gotten into the lanes, so I quickly ran over to get in also. The water was dreadfully cold since it was an outdoor pool. Nothing was going my way until Mr. Chen said that the warmup was a 200 IM. To my pleasant surprise, I was the fastest swimmer on the team! Mr. Chen said that he was genuinely grateful that I joined the team since I could turn their luck around during the meet.</p>
<p>Throughout the session, I was taught various drills and techniques to perfect my speed. I eventually even got used to the cold water, and it would refresh my mind every morning. One day, Mr. Chen told me that the summer final swim meet would take place in a week. I was speechless. I had prepared for a whole summer just for this meet, but now I suddenly doubted my abilities. He told me not to be nervous since I was going to do fine, but I could tell he was just trying to comfort me. In the final practice, I swam harder than ever and even Mr. Chen was impressed by my speed.</p>
<p>“If you swim this fast during the meet, we’ll turn the tide of the season,” he said happily.</p>
<p>On the day of the meeting, Mr. Chen gave us a pep talk with such enthusiasm that I felt guilty if I didn’t get first place for him. I was tense when I slid into the water. Every second seemed like a year, and the wait for the referee seemed interminable. Finally, after what seemed like centuries, the referee arrived and said that we were going to compete in a 100 IM. I poised myself for the whistle, and I dived in from the starting block when I heard it. I swam so fast that I didn’t even feel the fatigue until after the race. During the final 25m sprint, I seemed to be gliding through the water. The judges announced that team Alto had won the finals due to my first-place victory in the 100 IM. I was elated about winning my first-ever swim meet and for helping my team succeed.</p>
<p>That night, while we celebrated our victory, I realized that I had a new hobby. I could envision myself swimming in competitions years later and even competing in the Olympics one day.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>The void path to freedom</title>
    <url>/blog/The-void-path-to-freedom/</url>
    <content><![CDATA[<p>Kai looked from his window at the faraway misty mountains. 48 years had passed since  the Battle of the Taijo Mountains. In just 2 more years, the village would be freed. <span id="more"></span></p>
<p>His mother Reisa had told him of what happened during that battle. It started with the rule  of King Vector. After successfully launching an assassination attempt on King Charles, he  managed to seize power and got himself an absolute monarchy. He led his army to victory and was  considered the reincarnation of Hell. He burned entire civilizations to the ground and “voided  them. “He rebuilt them completely out of obsidian, which got him the title “The Void Lord.” But  Kasta Village remained safe, at least for the longest time. The citizens were confident that no  empire would be able to cross the Taijo Mountains. </p>
<p>They were wrong. King Vector had done the impossible by drilling a hole through the  mountain. During the night, his army ran through the hole and annihilated the Kasta Army, which  was puny compared to the Void Army. Lucky for them, King Vector had enough brain cells to  realize that Kasta village could be a key point of agriculture in his empire. He signed a treaty with  Chief Ernest, which said that Kasta Village had to pay him tribute in terms of crops annually for  50 years. Kai himself was only 15 years old, but he could tell that it wasn’t an easy task to produce  that many crops each year. The Taijo Mountains had being renamed to the Void Mountains in  honor of his victory. Chief Ernest had only signed the treaty because he couldn’t stand the village  being burned down.  </p>
<p>After 48 years of obediently giving tribute to King Vector, unease was spreading like  wildfire throughout the village. King Vector could easily just force the citizens to keep on paying  him. The citizens had no weapons, no soldiers, no walls, and no hope in rebelling. Certain death  would await them if they chose to openly fight King Vector. Suddenly, the village horn blew. The  horn was only used in ceremonies and important village meetings. Everyone rushed to the  courthouse, where Chief Ernest was giving a speech. </p>
<p>“Citizens of Taipo Village, we must act! King Vector will not honor the treaty and will  make us pay him tribute if we don’t rebel. We must form our own army, with good weapons, and  build fortifications to secure the village. We shall build an underground barrack where our soldiers  shall train. King Vector had made a mistake in giving us obsidian to build our farming tools. We  shall use it to build weapons that rival that of the Void Army. Next time the collector comes, we  shall fight and gain our freedom!” </p>
<p>Everyone cheered, and people hurried to sign up for the army. Women, including Reisa,  volunteered to be nurses. Things went well for the first 2 months, and obsidian fortifications had  already been constructed. The only thing that was needed was a roof to protect the city from aerial  attacks. The army had been training very hard to perfect their battle formation. Everything was  going according to plan, at least for some time. </p>
<p>Then, one day, King Vector came with the entire Void Army. The scout saw the pitch black armor gleam in the daylight. Why had they come so early? The whole village was silent at  the news for three seconds, then Chief Charles yelled, “Soldiers, get ready! This is the day you all  have trained for!” It was too late. The sun was suddenly blocked, and for a few seconds it appeared  that there was a black spot in the sky. Everyone’s confusion turned to terror as thousands of arrows<br>rained down on them. People ran for cover, but there were too many arrows. All that could be  heard was the whistling of arrows as they pierced soldiers’ and citizens’ hearts.  </p>
<p>After the first volley, Kai was stunned to see the ground littered with the bodies of citizens.  When the second volley came, there was no one to kill. The village had been bizarrely reduced to  a mere 100 citizens, and all of them hid in the courthouse. After a while, nothing could be heard  outside. The scout boy was brave enough to go to his tower and see if King Vector and his army  were still there. They weren’t. After this surprise volley, they just left, but not without a note. On  one of the arrows was a note: You deserved it. </p>
<p>The village counted their dead. Amongst them were Reisa and Chief Ernest. Kai mourned  for his mother’s death and wished that he could revenge her. Alas, he was in no position to do so.  Both had died during the battle and had been trying to rescue their fellow villagers. Reisa had been  desperately tending to a wounded soldier in her tent when the whole tent collapsed on top of her  under the pressure of the arrows. Chief Ernest had blocked an arrow that would have killed a  soldier with his own body. Most soldiers were already dead, for they had been too late in trying to  run for their lives. Only 6 remained, each ashamed of himself for cowardly fleeing at the first sight  of the enemies. </p>
<p>Kai knew something had to be done. At this rate, the next time Void Army came, it would  be extinction of the village. He felt like the village deserved it. Not for rebelling, but in the way  that they chose to fight. Chief Ernest should have predicted that their army was no match for that  of King Vector’s. There was only one rational thing to do: They had to leave the Void Mountains.  Kai believed that they would be successful, as long as they were together. </p>
<p>The next day, the villagers of Kasta Village disappeared without a trace. They had followed  Kai’s exodus and fled the Void Mountains. There was hope for a better future, in a place where  they would be free. </p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>A Once-in-a-Lifetime Opportunity</title>
    <url>/blog/a-once-in-a-lifetime-opportunity/</url>
    <content><![CDATA[<p>When people think of their worst nightmares, they tend to see terrifying monsters with sharp fangs, ugly trolls with hairy eyeballs, and other intimidating creatures. But in the minds of the sixth-graders of Nirvana Middle School, nothing is worse than the same nightmare that haunts their mind every day and night: A pop quiz.<span id="more"></span></p>
<p>It wasn’t like they weren’t smart. In fact, each one of them was sponsored by an elementary school in the U.S.A. They were sent to Nirvana Middle School because they were the top students of the elementary school they went to. But the fact that the questions were so difficult that students had a high chance of a mental breakdown was just insane. The questions were also in messed up order  and from various subjects. The worst was the final math exam that took place at the end of the semester. Some lucky students learned algebra during the semester, and the exam was relatively simpler for them. All they had to do was to use the correct formula to solve the equation. But those who studied geometry had to endure endless lectures from Mr. Bowe, a knowledgeable professor who never looked at the time, so students were always late for their other lessons. At the end of the year, they took  the geometry final, which consisted of page-long proofs and complex angle calculations that was so nerve-wracking that many students ripped their papers in frustration.</p>
<p>Today, however, the students were resting uneasily in their homeroom. It was the day before the geometry final, and Mr. Bowe had said nothing about it yet. They had been studying about so many complex models during the semester that one day was simply not enough to prepare for the exam. After Mr. Bowe finished teaching the final chapter, students hurriedly took out their textbooks and started to review from Chapter 1 to Chapter 101. Only two students remained calm, and instead, whipped out their phones. The Teaser brothers, Matthew and Michael, who had gained a reputation as the most obnoxious tricksters the school had ever had (and only had), were busy playing Geometry Dash. Also, most students earned detentions for failing in tests and had to do extra practice. On the other hand, the brothers gained detentions for being troublemakers and not paying attention during class. They even achieved a record of gaining 10 detentions in a day, which was only possible if they got one from every teacher.</p>
<p>“Michael, why on earth are you playing Geometry Dash when there is literally a geometry exam tomorrow? ‘’ asked Laurel, who always minded other people’s business.</p>
<p>“Jeez, mind your own business, Laurel. You’re only studying because you’re scared to fail the geometry test tomorrow.  Why, now that I think of it, you’re never good at geometry,” Michael retorted with a sneer.</p>
<p>Everyone laughed at Laurel, whose face was red from embarrassment.</p>
<p>“Just you wait till tomorrow, punk. I’m gonna wipe that little smile off your face when I crush you in the exam,”she growled.</p>
<p>Michael shrugged and went back into playing his game. Suddenly, the door opened and Mr. Bowe stepped in with a huge grin on his face. Michael and Matthew quickly stuffed their phones into their bags and hoped that Laurel wouldn’t tell on them, which she always did.</p>
<p>“Mr. Bowe, Michael and Matthew were playing Geometry Dash while you were gone,” Laurel reported with a triumphant look on her face.</p>
<p>“Oh, that’s great, I mean, horrible. Anyways, this is a very special year. I know that most of you are probably confused, but let me explain. This year is the 100th anniversary of Nirvana Middle School. To celebrate this, we will be hosting a special geometry themed tournament tomorrow that will also count as your geometry final exam. The winner of the tournament shall receive a fabulous reward.”</p>
<p>“Mr. Bowe, is the tournament going to be on the material we learned during the semester?” asked Laurel.</p>
<p>“Well, I can’t tell you what it is about,  but I suggest you think outside the box for this one,”Mr. Bowe replied.</p>
<p>That night, Michael and Mattew played for hours on their Geometry Dash accounts. They decided it was probably no use studying since only the winner would get the prize. It was unlikely that they would actually win the tournament, so they might as well take it easy.</p>
<p>“If only it was a Geometry Dash tournament, then we would totally dominate the entire school,” Matthew sighed.</p>
<p>“Yeah, but wait! Maybe that was what Mr. Bowe was talking about, to expect the unexpected. This is thinking outside the box, and it is related to geometry. This has to be the topic of the tournament tomorrow!” Michael said excitedly.</p>
<p>The next day at school, everyone was talking nervously about the tournament. Michael cornered Laurel and told her that he was going to bet anything on winning the tournament. If Laurel lost, she’d never tattletale on him again. Laurel’s bet was even crueler. If Michael lost, he would delete Geometry Dash in front of everyone.</p>
<p>The school bell rang and everyone dashed into the auditorium to see what the tournament was about. Sure enough, there were large Geometry Dash posters hanging on the walls. The students gasped in disbelief. Michael laughed when he saw the shock on Laurel’s face. Mr. Bowe walked in with a special guest. It was  Robert Topala, the founder of Geometry Dash! Michael and Matthew’s hands worked like machines and were unstoppable. To no one’s surprise, they simultaneously achieved a perfect score. Robert Topala himself went up to congratulate the two gamers for being the most talented players he had ever seen. Even better, the prize was a lifetime Geometry Dash Membership! Laurel stared at him loathingly, but Michael detected a hint of impressment. This was surely a memorable way to celebrate the 100th anniversary of Nirvana Middle School!</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
  </entry>
  <entry>
    <title>A six-step framework to tackle any system design question</title>
    <url>/blog/a-six-step-framework-to-tackle-any-system-design-questio/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In the book of <strong>System Design Interview</strong> by Lewis C. Lin, a six-step framework called <strong>PEDALS</strong> is introduced to help answer any system design question.</p>
<p>PEDALS stand for:</p>
<ul>
<li>Process requirements</li>
<li>Estimate</li>
<li>Design the service</li>
<li>Articulate the data model</li>
<li>List the architectural components</li>
<li>Scale</li>
</ul>
<p>The systematic method will power you through the whole system design interview.</p>
<h2 id="Process-requirements"><a href="#Process-requirements" class="headerlink" title="Process requirements"></a>Process requirements</h2><p>It means to <strong>clarify the question</strong> before you answer it.</p>
<ul>
<li>What is it? What does the system do? What are the goals or outcomes?</li>
<li>What features does it provide? What features should be left out?</li>
</ul>
<p>Why is clarification important? If the requirements are not clarified,</p>
<ul>
<li>the desired features would not be included or correctly designed</li>
<li>the interviewers’ expectation would not be met</li>
<li>the precious time would be wasted in unexpected area</li>
</ul>
<p>By clarifying the question, you prove that you can take proper actions for any ambiguous question in real world.</p>
<h2 id="Estimate"><a href="#Estimate" class="headerlink" title="Estimate"></a>Estimate</h2><p>It is always a good idea to estimate the scale of the system as it helps reflect if the designed system could fulfill the functional requirements. The requirements might include:</p>
<ul>
<li>Number of users</li>
<li>Number of active users(NAU)</li>
<li>Requrests per second(RPS)</li>
<li>Logins per seconds</li>
<li>Transactions per second(TPS) for E-commerce</li>
<li>Likes&#x2F;dislikes per second, shares per second, comments per second for social media sites</li>
<li>Searches per second for sites with a search feature</li>
<li>Storage needed</li>
<li>Servers needed</li>
<li>Network bandwidth needed</li>
</ul>
<p>Based on the functional requirements above, it’s possible to estimate the system requirements including servers, storage and netowrk bandwidth needed.</p>
<p>To estimate hardware resource needed, we need to understand that there are four major resources in a computer system.</p>
<ul>
<li>CPU</li>
<li>Memory</li>
<li>Storage</li>
<li>Network</li>
</ul>
<h3 id="Estimate-servers-needed"><a href="#Estimate-servers-needed" class="headerlink" title="Estimate servers needed"></a>Estimate servers needed</h3><p>The modern computer system is a multi-processor system. It varies from single CPU core to multiple CPU cores. The following is a 32 CPU threads system.</p>
<pre><code>$ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                32
On-line CPU(s) list:   0-31
Thread(s) per core:    2
Core(s) per socket:    8
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 63
Model name:            Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz
</code></pre>
<p>In order to estimate how many servers are needed, we can approach in the following manner.</p>
<ol>
<li>How much work can single CPU do?</li>
<li>How much work can single server do?</li>
<li>How many servers are needed?</li>
</ol>
<p>Let’s have an example to go through this approach.</p>
<p>Let’s say it takes 100ms for a sinlge-core CPU system to handle single client request. It means the system can handle 10 requests per second. So, we can extrapolate a 32-core system can handle 320 requests per second. Let’s say we have to handle 320,000 requests per second(RPS). It means 1000 servers are needed.</p>
<p>Notice that this is a rough calculation only considering CPU needs. In real case, there might be other performance bottleneck to handle 320 requests per second in a system. For example, the system might be already I&#x2F;O bound before running out of CPU bandwidth. But this method still gives us an estimation when to consider the CPU computation resource only.</p>
<h3 id="Estimate-storage-needed"><a href="#Estimate-storage-needed" class="headerlink" title="Estimate storage needed"></a>Estimate storage needed</h3><p>To estimate storage needed, we can approach as below.</p>
<ol>
<li>Identify the different data types</li>
<li>Estimate the space needed for each data type</li>
<li>Get the total space needed</li>
</ol>
<p>Let’s take YouTube as an example to understand this approach.</p>
<ul>
<li>Data types: videos, thumbnail images and comments.</li>
<li>Let’s assume there are roughly 2B users and 5% users(100M users) upload videos consistently. On average, each user has a weekly upload(~50 videos per year). Roughly, 13M videos(100M*50&#x2F;365) are uploaded daily. Let’s assume the video is 10 minutes long on average and it takes 50MB storage space after compression. Let’s say each video has a thumbnail image of 20KB. Each video has about 5 comments and the size of each comment is 200 bytes. In total, the space need for each video is 50MB + 20KB + 1KB, roughly 50MB. By multiplying 13M videos, it needs 619TB storage in a day.</li>
</ul>
<h3 id="Estimate-network-bandwidth-needed"><a href="#Estimate-network-bandwidth-needed" class="headerlink" title="Estimate network bandwidth needed"></a>Estimate network bandwidth needed</h3><p>Determine the incoming and outgoing data for network bandwidth estimation</p>
<ul>
<li>We already know there would be ~619TB data uploaded to YouTube in a day. Dividing this by the number of seconds in a day(619TB&#x2F;86400 seconds), the incoming data to YouTube would be 7.3GB&#x2F;s.</li>
<li>Let’s say 10% of YouTube users are daily active users. With approximately 200M daily users, let’s assume a user watches 10 videos a day. Then YouTube would have 2B views in a day. This would result in ~93PB outgoing data in a day. Dividing this by the number of seconds in a day(93PB&#x2F;86400 seconds), the outgoing speed would be 1128GB&#x2F;s.</li>
</ul>
<h2 id="Design-the-service"><a href="#Design-the-service" class="headerlink" title="Design the service"></a>Design the service</h2><p>When we process the requirements, we should already collect the clear enough requiremnts before the system design. And now, we need to define <strong>what to build</strong> and figure out <strong>how to build</strong> the system service.</p>
<h3 id="CRUD-framework"><a href="#CRUD-framework" class="headerlink" title="CRUD framework"></a>CRUD framework</h3><ul>
<li>Create</li>
<li>Read</li>
<li>Update</li>
<li>Delete</li>
</ul>
<p>For example, to build a YouTube-like video service. We can use CRUD to brainstorm the possible the system actions.</p>
<ul>
<li><p>Create</p>
</li>
<li><p>New users</p>
</li>
<li><p>New channels</p>
</li>
<li><p>Upload videos</p>
</li>
<li><p>Add comments</p>
</li>
<li><p>Read</p>
</li>
<li><p>View videos</p>
</li>
<li><p>Read comments</p>
</li>
<li><p>Search videos</p>
</li>
<li><p>Read video recommendations</p>
</li>
<li><p>Update</p>
</li>
<li><p>Edit video metadata</p>
</li>
<li><p>Edit comments</p>
</li>
<li><p>Delete</p>
</li>
<li><p>Delete users</p>
</li>
<li><p>Delete channels</p>
</li>
<li><p>Delete videos</p>
</li>
<li><p>Delete comments</p>
</li>
</ul>
<p>Now we can further define services(API endpoints) as below.</p>
<ul>
<li>&#x2F;users</li>
<li>&#x2F;channels</li>
<li>&#x2F;videos</li>
<li>&#x2F;comments</li>
<li>&#x2F;recommendations</li>
</ul>
<h3 id="Be-RESTFUL-API-Best-Practices"><a href="#Be-RESTFUL-API-Best-Practices" class="headerlink" title="Be RESTFUL: API Best Practices"></a>Be RESTFUL: API Best Practices</h3><ul>
<li>Use Nouns. REST is known for using the HTTP commands(GET&#x2F;POST&#x2F;DELETE&#x2F;PUT) to read or write data. When the API is invoded via HTTP requests, the HTTP request comes with the corresponding verbs(GET&#x2F;POST&#x2F;DELETE&#x2F;PUT).</li>
<li>Use nesting to show the hierarchy. For example, &#x2F;users&#x2F;1 returns a specific user with id&#x3D;1.</li>
<li>Return json. It is the industry standard today.</li>
<li>Support filtering and pagination. It helps minimize the latency and waste.</li>
<li>Use plural</li>
</ul>
<h3 id="Design-strategies"><a href="#Design-strategies" class="headerlink" title="Design strategies"></a>Design strategies</h3><ul>
<li><p>Information processing strategies</p>
</li>
<li><p>Batch strategy(Scheduled processing)</p>
</li>
<li><p>Chain-of-Command strategy</p>
</li>
<li><p>Checklist strategy</p>
</li>
<li><p>Rate limiting strategies</p>
</li>
<li><p>Fixed window strategy</p>
</li>
<li><p>Sliding window strategy</p>
</li>
<li><p>Token bucket strategy</p>
</li>
<li><p>Leaky bucket strategy</p>
</li>
<li><p>Limiting concurrent requests strategy</p>
</li>
<li><p>Critical requests strategy</p>
</li>
<li><p>Communication strategies</p>
</li>
<li><p>Middleman strategy</p>
</li>
<li><p>Town crier strategy</p>
</li>
<li><p>Asynchronous strategy</p>
</li>
<li><p>Latency strategies</p>
</li>
<li><p>Main-replica strategy</p>
</li>
<li><p>Push vs. Pull strategy</p>
</li>
<li><p>Precompute strategy</p>
</li>
<li><p>Lazy loading strategy</p>
</li>
<li><p>Peer-to-peer strategy</p>
</li>
<li><p>Efficiency strategies</p>
</li>
<li><p>Divide and conquer strategy</p>
</li>
<li><p>Listener strategy</p>
</li>
<li><p>Space reduction strategies</p>
</li>
<li><p>Mario and Luigi strategy</p>
</li>
<li><p>Synchronization strategies</p>
</li>
<li><p>Locks strategy</p>
</li>
<li><p>Error handling strategies</p>
</li>
<li><p>Code readability, maintainability, and elegance strategies</p>
</li>
<li><p>Security strategies</p>
</li>
</ul>
<h2 id="Articulate-the-data-model"><a href="#Articulate-the-data-model" class="headerlink" title="Articulate the data model"></a>Articulate the data model</h2><ul>
<li><p>Schema(Tables, Fields)</p>
</li>
<li><p>SQL vs. NoSQL database</p>
</li>
<li><p>SQL database: MySQL, PostgreSQL, etc</p>
</li>
<li><p>NoSQL database: MongoDB, Redis, etc</p>
</li>
<li><p>NoSQL databases have a flexible or unstructured database schema.</p>
</li>
<li><p>Non-database storage</p>
</li>
<li><p>Distributed file systems(e.g. Store files in HDFS and file path in database)</p>
</li>
<li><p>Object storage(e.g. S3)</p>
</li>
</ul>
<h2 id="List-the-architectural-components"><a href="#List-the-architectural-components" class="headerlink" title="List the architectural components"></a>List the architectural components</h2><ul>
<li>Logical architecture, physical architecture, cloud architecture</li>
<li>Service-oriented architecture</li>
<li>Draw architecture diagrams</li>
</ul>
<h2 id="Scale"><a href="#Scale" class="headerlink" title="Scale"></a>Scale</h2><h3 id="How-to-tackle-common-scale-issues"><a href="#How-to-tackle-common-scale-issues" class="headerlink" title="How to tackle common scale issues"></a>How to tackle common scale issues</h3><p>What were the estimated scale requirements in step 2?</p>
<ul>
<li>Number of total users</li>
<li>Number of active users</li>
<li>Requests per second(RPS)</li>
<li>Logins per second</li>
<li>Storage needed</li>
</ul>
<p>Resolve the following system bottleneck:</p>
<ul>
<li>CPU</li>
<li>Memory</li>
<li>Storage</li>
<li>Latency</li>
</ul>
<p>We can use the following strtegies to scale a small functional system.</p>
<ul>
<li>Load balancing</li>
<li>Read replica databases</li>
<li>Distributed file systems</li>
<li>Content delivery networks</li>
<li>Daemon process pre-computation</li>
</ul>
<h3 id="Problem-Handling-more-users-and-requests"><a href="#Problem-Handling-more-users-and-requests" class="headerlink" title="Problem: Handling more users and requests"></a>Problem: Handling more users and requests</h3><ul>
<li>Horizontal scaling and load balancer</li>
<li>Vertical scaling</li>
</ul>
<h3 id="Problem-Handling-more-reads"><a href="#Problem-Handling-more-reads" class="headerlink" title="Problem: Handling more reads"></a>Problem: Handling more reads</h3><ul>
<li><p>Read replicas</p>
</li>
<li><p>drawback: lack of data consistency</p>
</li>
<li><p>Sharding</p>
</li>
<li><p>Shard by customer or range</p>
</li>
<li><p>Shard by geography</p>
</li>
<li><p>Shard by hash function</p>
</li>
<li><p>Disadvantages: slower join performance, additional application code, recurring maintenance</p>
</li>
</ul>
<h3 id="Problem-Avoiding-crashes"><a href="#Problem-Avoiding-crashes" class="headerlink" title="Problem: Avoiding crashes"></a>Problem: Avoiding crashes</h3><p>Chaos Engineering is a discipline where engineering teams purposefully create controlled failure within a large scale system. It can emulate:</p>
<ul>
<li>Failure of a data center</li>
<li>Failure of a database shard</li>
<li>Randomly thrown exceptions</li>
<li>Skew in distributed system clocks</li>
<li>Failure of a key internal microservice</li>
<li>“Storm” tests that emulate power outages</li>
</ul>
<h3 id="Problem-Providing-data-consistency"><a href="#Problem-Providing-data-consistency" class="headerlink" title="Problem: Providing data consistency"></a>Problem: Providing data consistency</h3><p>No system can simultaneously provide two of the following guarantees(CAP):</p>
<ul>
<li>Consistency - wirtes are immediately in effect</li>
<li>Availability - requests receive a valid response</li>
<li>Partition Tolerance - functionality despite network errors</li>
</ul>
<p>When scaling a system, there is a direct relationship between consistency and availability. When horizontally scaling, you’ll see availability dramatically increase; however, consistency will suffer because the system becomes distributed.</p>
<p>To create stronger read consistency, you’ll have to decrease the effect of data replication your system creates, which will decrease availability.</p>
<h3 id="Problem-Need-to-improve-latency"><a href="#Problem-Need-to-improve-latency" class="headerlink" title="Problem: Need to improve latency"></a>Problem: Need to improve latency</h3><p>As a system grows and creates an increasingly complex workflow, latency becomes a critical factor.</p>
<p>The bottleneck can be caused by system resource starvation, which could be alleviated through horizontal and vertical scaling. It also can be caused by application limitation. Another strategy to consider is caching which helps reduce unnecessary disk I&#x2F;O operations.</p>
<h3 id="Identify-and-alleviate-scalability-bottlenecks"><a href="#Identify-and-alleviate-scalability-bottlenecks" class="headerlink" title="Identify and alleviate scalability bottlenecks"></a>Identify and alleviate scalability bottlenecks</h3>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>System Design</tag>
      </tags>
  </entry>
  <entry>
    <title>About this site</title>
    <url>/blog/about/</url>
    <content><![CDATA[<p>Flamingbytes is an independent publication launched in December 2022 by relentlesstorm. </p>
<p>If you have any questions, you can reach me by <a href="mailto:relentlesstorm@gmail.com">email</a>.</p>
]]></content>
  </entry>
  <entry>
    <title>Access a sharedv4 volume outside of a Portwrox cluster</title>
    <url>/blog/access-a-sharedv4-volume-outside-of-a-portwrox-cluster/</url>
    <content><![CDATA[<p>To access a sharedv4 volume outside of the Portworx cluster, add the allow_ips label to the volume you wish to export, specifying a semi-colon separated list of IP addresses of non-Portworx nodes you wish to mount your sharedv4 volume to:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ /opt/pwx/bin/pxctl -j volume create datavol --sharedv4 --label name=data --size 500 --fs ext4 --repl 2 --nodes node1,node2</span><br><span class="line">$ /opt/pwx/bin/pxctl -j volume update datavol --sharedv4_mount_options=&quot;vers=4.0&quot;</span><br><span class="line">$ /opt/pwx/bin/pxctl volume update datavol --label &quot;allow_ips=&lt;node3_ip&gt;&quot;</span><br><span class="line">$ /opt/pwx/bin/pxctl host attach datavol</span><br><span class="line">$ mkdir /mnt/datavol</span><br><span class="line">$ /opt/pwx/bin/pxctl host mount --path /mnt/datavol datavol</span><br><span class="line"></span><br><span class="line">$ mount | grep datavol</span><br><span class="line">/dev/pxd/pxd229379759785727331 on /mnt/datavol type ext4 (rw,relatime,discard,data=ordered)</span><br><span class="line"></span><br><span class="line">$ cat /etc/exports</span><br><span class="line">/var/lib/osd/pxns/229379759785727331 node1(fsid=123e4567-e89b-12d3-002e-ebad1cb15563,rw,no_root_squash,no_subtree_check) node3(fsid=123e4567-e89b-12d3-002e-ebad1cb15563,rw,no_root_squash,no_subtree_check)</span><br></pre></td></tr></table></figure>

<p>To mount the NFS shared volume on non-Portworx node(node3):</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ showmount -e node1</span><br><span class="line">Export list for node1:</span><br><span class="line">/var/lib/osd/pxns/229379759785727331 node1,node3</span><br><span class="line"></span><br><span class="line">$ mkdir /mnt/data</span><br><span class="line">$ mount -t nfs node1:/var/lib/osd/pxns/229379759785727331 /mnt/data</span><br><span class="line">$ mount | grep data</span><br><span class="line">node1:/var/lib/osd/pxns/229379759785727331 on /mnt/data type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=node3,local_lock=none,addr=node1)</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.portworx.com/reference/cli/updating-volumes/">https://docs.portworx.com/reference/cli/updating-volumes/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>Portworx</tag>
        <tag>Sharedv4</tag>
        <tag>NFS</tag>
      </tags>
  </entry>
  <entry>
    <title>The Adventures of Bill, the Naughty Genius</title>
    <url>/blog/adventure-of-bill/</url>
    <content><![CDATA[<p><img src="/images/writing/Bill.png"> </p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Once upon a time, in a quiet suburban neighborhood, there lived a mischievous boy named Bill. Bill was unlike any other child in his school. He had a huge brain, a gift that often led him down a wayward path. While his classmates diligently attended school, Bill had a knack for finding ways to skip it. <span id="more"></span></p>
<p>Bill’s reputation as the “naughty genius” spread like wildfire among his friends and teachers. He had an insatiable curiosity that couldn’t be contained within the walls of a classroom. He saw school as a place that limited his potential rather than nurtured it, and so he devised elaborate schemes to escape its confines. </p>
<p>One sunny morning, as the school bell rang and students filed into their classrooms, Bill was nowhere to be found. He had constructed a secret hideout deep within the dense woods behind his house. The hideout was filled with books, gadgets, and contraptions that he had invented. Bill spent his days there, exploring the mysteries of the natural world, conducting wild experiments, and reading voraciously about subjects that fascinated him. </p>
<p>Despite his constant truancy, Bill’s intellect couldn’t be denied. His parents, who were both educators, were deeply concerned about his behavior. They tried everything to persuade him to attend school, but Bill was resolute. He knew he was different, and he didn’t want to conform to the conventional education system. </p>
<p>One day, as Bill was conducting an intricate experiment involving homemade rockets and chemical reactions in his hideout, he received an unexpected visitor. It was Ms. Johnson, his school principal. She had heard rumors about Bill’s hideout and decided to pay him a visit. </p>
<p>Bill was taken aback but quickly regained his composure. Ms. Johnson, a wise and compassionate woman, began to talk to Bill about his passion for learning. She acknowledged his exceptional intellect and understood his frustration with traditional education. Instead of scolding him, she proposed a compromise. </p>
<p>Ms. Johnson suggested that Bill could attend school part-time while also continuing to explore his interests in his hideout. She would work with the teachers to provide him with challenging and individualized coursework that would nurture his thirst for knowledge. Bill was intrigued by the idea. </p>
<p>Over time, Bill started attending school more regularly, but he continued to spend hours in his hideout, conducting experiments and delving into subjects that fascinated him. The teachers marveled at his intelligence and dedication. He became a source of inspiration to his classmates, showing them that learning could be an adventure, not just a chore. </p>
<p>As the years went by, Bill’s unconventional approach to education paid off. He became a renowned scientist, known for his groundbreaking discoveries and inventions. His unique combination of book smarts and hands-on experience set him apart in the world of science. </p>
<p>Bill’s story became a legend in the small town, a tale of a naughty boy who skipped school but had a huge brain. He showed everyone that sometimes, the best way to nurture a brilliant mind is to let it wander and explore, even if it means bending the rules a little along the way. </p>
<h2 id="Chapter-1-The-Rise-of-the-Naughty-Genius"><a href="#Chapter-1-The-Rise-of-the-Naughty-Genius" class="headerlink" title="Chapter 1: The Rise of the Naughty Genius"></a>Chapter 1: The Rise of the Naughty Genius</h2><p>It was Bill’s first day of school, and he was not looking forwards to it. He had heard terrifying rumors that his fellow students were all dumb and had an average iq of 100, compared to his own of 256. Alas, he had to listen to his parents, and before he even knew it, he was sitting grumpily in his chair in the classroom. It was even worse than he had imagined. Papers depicting the Periodic Table of Elements and other easy stuff were taped to the wall. All the papers contained stuff he had learned in preschool. In first grade, Bill had already been conducting his own chemical experiments, which just used whatever elements he could find.  </p>
<p>Just as Bill was about to doze off, the teacher walked in and rudely interrupted his nap. He introduced himself as Mr. Adams. Bill immediately asked him if he was named after the first man in the universe created by God. Mr. Adams glared at him and told Bill to see him after class. Bill was confused, not knowing what he had done wrong. Mr. Adams told them that he was a strong believer in the Anti-Suicide Act. He found out that 90% of the people who committed suicide every year did it because of an overload of stress. To prevent this issue, Mr. Adams decided to destroy any possibility of stress that school could have on them, by making the homework loads easier. He was now teaching them 1st grade stuff instead of 7th grade stuff. What followed was a mixture of responses from the students. Some were overjoyed to have no homework, others were indifferent, and Bill was outraged. He had hoped that 7th grade would be at least a little difficult for him. (In truth, he was a 5th grader, but his parents paid extra to enroll him in 7th grade) Bill got so bored of his teacher’s monotone that he decided to take a nap. However, he woke up to the pain of having his hand slapped. He instantly opened his eyes and saw Mr. Adams preparing for another blow with his ruler. Bill quickly moved his hand out of the way. He bolted out of the class and didn’t look back until he reached his home. No one was home, so he went to his secret underground lair. He could then formulate a plan of how to end Mr. Adams’s tyrannical rule. Mr. Adams was a threat to his brain, and Bill could already feel his brain cells dying. It was a horrible sensation, and one that must end immediately. He wanted to skip school, but his parents threatened to destroy his lab if he did so. He went into meditation, but even after achieving nirvana, he couldn’t think of a plan. That was when he realized that he had to use his last resort: brawn. Bill was quite skinny for his age and wouldn’t last very long in a fist fight. He usually beat others by outsmarting them. He considered building a homing missile to destroy Mr. Adams, but it would probably get him expelled. But then he thought of a brilliant idea: bribery. One thing he learned today at school was that 7th graders are very naïve and gullible. Pay them a little money, and they will do anything for you.  </p>
<p>The next day, Bill went around the school looking for the most athletic and most muscular kids in the school. After choosing the most promising candidates, he gathered them together and told them his plan. They were reluctant at first, but immediately agreed when Bill mentioned a plan. Bill paid them handsomely and called them his elite bodyguard. They were told to always follow Bill and stay hidden until given the signal for action. The kids liked the idea of acting like secret forces agents. That day during Physics, Mr. Adams walked into the classroom, and started his usual boring lecture about speed. Bill stood up and asked him to make the topics a lot harder, or he would regret it. Mr. Adams spat at him and got out his metal ruler. Just then, the elite bodyguard rushed in with their baseball bats and knocked poor Mr. Adam out. They dragged his unconscious body into an abandoned farm, where they burned the corpse, so that there would be no trace of Mr. Adams. Bill was very pleased with himself, just when Ms. Kolb, the principal, rushed in with a group of security guards. Rats! Someone had decided to call the police on him. The elite bodyguard rushed back in the nick of time and a brutal battle began. Amid the chaos, Bill hurried back home and went into his fortified bunker, which could survive even a nuclear bomb. That had been fun, and it made him feel unstoppable. He investigated his fortune-telling orb to see the outcome of the brawl. The security guards were all knocked out, and Kevin, the most ruthless of the bunch, had chained Ms. Kolb to a wall. This was very bad, since if she died, Bill’s parents would hate Bill. Bill went even more into the future and saw that a very bad event was about to occur, and even his bodyguard couldn’t save him. Panicked, Bill immediately started to work on Shadow, a robot designed to protect Bill at all costs. It was only a robot to be activated in the darkest of times. Bill had armed it with nuclear bombs, machine guns, grenade launchers, and other deadly artillery. It even had a self-destruct function, which was to be used if there was no other way to wipe out the enemy.  </p>
<p>Boom! The door was knocked over, and Bill could hear the thundering of footsteps that were coming closer to his bunker. He desperately worked on his robot and finished it just as someone knocked on the gate. Bill saw a furious Ms. Kolb, backed up by his own elite bodyguard. She had given them even more money than Bill did. Bill could have stayed in his bunker and waited for them to leave, but he wanted revenge for their treachery. He activated Shadow and shrunk it into missile size. He opened the gate and was about to tell the robot to kill them, when Ms. Kolb told him that she had an amazing deal for him. She acknowledged Bill’s outstanding IQ, and promised Bill to give him harder classes if he came to school more often. There would be better teachers, and more interesting stuff. Bill hesitated, but he was intrigued by the idea. He agreed to try it out. Ms. Kolb happily left, but the elite bodyguards were not done with him yet. Michael, the tallest of them, told Bill that if they killed him, they would get an enormous amount of money. And with that, Michael hit Bill with a devastating blow to the solar plex. Bill coughed out blood, but he was so angry that he forgot about the pain and told Shadow to go destroy them. It started firing round after round, until all that was left of the bodyguard was their clothes. Shadow used its built-in vacuum to clean up the mess.  </p>
<p>Though successful, Bill had learned a lot from that close encounter. Humans were not trustworthy beings and weren’t reliable. The only way to fix this would be to make automatons instead. Bill spent the entire afternoon making blueprints of “Shadow”, which would soon be known as the deadliest force ever known to mankind. They would be unstoppable, filled with weapons, and most importantly, unquestioned loyalty.  </p>
<h2 id="Chapter-2-Unstoppable-Force"><a href="#Chapter-2-Unstoppable-Force" class="headerlink" title="Chapter 2: Unstoppable Force"></a>Chapter 2: Unstoppable Force</h2><p>After a few days of his hardened courses, Bill suddenly started liking to go to school. Of course, he still got death glares from the friends of his elite bodyguard, who still mourned the deaths of their comrades. One night, when Bill was heading home, a group of masked men suddenly appeared from an alley and started beating him up. Bill was left in a bloody mess, and watched in resentment as the men ran away. “If only my legion could be ready a bit sooner,” he thought. Operation Shadow would not be in action until 5 days later, and that left Bill in a vulnerable state. He had to protect himself as well as he could for these 5 days. When he went back to his base, he found it a mess. His blueprints were ripped, his table was flipped over, and worst of all, his parents lay dead on the ground. Shocked, Bill collapsed to the floor.  </p>
<p>Meanwhile, the same thugs that beat up Bill and killed his parents were partying over their victory over the genious. Things like that don’t happen often. They had destroyed Bill’s lifetime work, and left him as a orphan. He no longer had financial aid, as they had stolen his parent’s credit card, and took out all the money. And it was a massive amount of money. They could live happily for the rest of their lives without needing to get a job. They had already bought tickets on a private luxury jet for a round-the-world vacation. Their flight was in only 2 days. What they hadn’t thought of however, was the chance that Bill might return from defeat and destroy all of them. Bill was left with nothing, or so they had thought. He still had his Shadow, however. He had hidden them in a USB receiver, which he always had in his pocket. When Bill woke up, he was filled with fury and vowed to get revenge.  He got the scraps of his weapons and reassembled them together. After all, he still had his brains, and that was all he needed to rise back to greatness. He carried his Necromancer, a sword with missiles attached to it. He used the DNA of the thugs who attacked him and found out that they were close friends of Kevin, one of his previous bodyguards. The next day, he tracked them on the way to school, and a difficult battle ensued. While he had the superior weapons, Bill was never very athletic, and he hadn’t recovered from the previous night’s beating. The thugs were very muscular and had strength in numbers. A tough battle followed, with Bill almost getting knocked out by a punch to the jaw. He stumbled backwards, only to get picked up and thrown against a wall. With a sickening crack, he felt his ribcage break and that was the last thing he remembered.  </p>
<p>When he came to, he was locked up in a pitch-black cell. He groped blindly through the air when he remembered he had a flashlight on him. Even the smallest of movements hurt, and with great effort, he opened his flashlight and looked around the room. To his shock, the bodies of the thug were piled on top of the other on the floor. Suddenly, the metal gate opened and in came the mighty Shadow. The legions of robots bowed down to Bill and waited for their orders. </p>
<h2 id="Chapter-3-Bill’s-Robotic-Revolution-A-Battle-for-the-School"><a href="#Chapter-3-Bill’s-Robotic-Revolution-A-Battle-for-the-School" class="headerlink" title="Chapter 3: Bill’s Robotic Revolution: A Battle for the School"></a>Chapter 3: Bill’s Robotic Revolution: A Battle for the School</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In the quiet town of Meadowville, where education was valued above all else, an eccentric genius named Bill was brewing a plan that would turn the school system upside down. Bill, a prodigious inventor and programmer, had amassed an elite army of robots over the years. His goal? To take over the school and wage war against what he considered to be evil teachers. This essay explores Bill’s motivations, his robotic army, and the potential consequences of his daring plan. </p>
<h3 id="Bill’s-Motivation"><a href="#Bill’s-Motivation" class="headerlink" title="Bill’s Motivation"></a>Bill’s Motivation</h3><p>Bill’s motivations were deeply rooted in his personal experiences with the educational system. As a child, he had always been the outlier, the one who asked too many questions and challenged the status quo. He believed that the traditional education system, with its strict rules and one-size-fits-all approach, was stifling creativity and individuality. In his eyes, many of the teachers were authoritarian figures who sought to crush any form of dissent. </p>
<p>Over the years, Bill’s frustrations grew, and he began to channel his considerable talents into robotics and programming. He saw the potential of technology to disrupt the traditional classroom and create a more equitable and personalized learning experience for students. However, his frustration soon turned into a radical vision: taking over the school with his robots and replacing the “evil” teachers. </p>
<h3 id="The-Elite-Army-of-Robots"><a href="#The-Elite-Army-of-Robots" class="headerlink" title="The Elite Army of Robots"></a>The Elite Army of Robots</h3><p>Bill’s robotic army was nothing short of extraordinary. He had spent years designing and building a diverse range of robots, each with specific capabilities tailored to his audacious plan. These robots included: </p>
<p>The Guardians: These humanoid robots were equipped with advanced AI, capable of monitoring and maintaining order in the school. They were programmed to ensure the safety of students while enforcing Bill’s vision of a more open and student-centered learning environment. </p>
<p>The Educators: Bill’s robots were not just enforcers; they were also educators. He developed a series of AI-powered teaching assistants that could adapt to individual learning styles, providing a more personalized and effective education for each student. </p>
<p>The Techies: Bill’s army also included robots adept at handling technical tasks, such as repairs, maintenance, and logistical support. These robots ensured that the school’s infrastructure ran smoothly, allowing for uninterrupted learning. </p>
<h3 id="The-Consequences-of-Bill’s-Plan"><a href="#The-Consequences-of-Bill’s-Plan" class="headerlink" title="The Consequences of Bill’s Plan"></a>The Consequences of Bill’s Plan</h3><p>While Bill’s intentions may have been driven by a desire for positive change in the education system, his plan had potentially dire consequences. Taking over a school with an army of robots would likely lead to chaos and resistance from both teachers and students. The sudden removal of human educators could have a detrimental effect on the social and emotional development of students, as well as their ability to navigate real-world interactions. </p>
<p>Furthermore, the use of force to impose his vision could lead to legal and ethical challenges. Bill’s actions would likely be met with opposition from parents, school authorities, and the wider community. The use of robots as enforcers could result in unintended consequences, including accidents, injuries, or escalations of conflicts. </p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Bill’s plan to take over the school and fight the “evil” teachers with his elite army of robots is a testament to the power of one individual’s passion and determination. While his motivations may have been rooted in a desire for positive change in the education system, the potential consequences of his actions cannot be ignored. Education is a complex and deeply personal experience, and any attempts to reform it must consider the diverse needs and perspectives of all stakeholders. </p>
<p>Bill’s story serves as a cautionary tale about the importance of collaboration, dialogue, and empathy in addressing the shortcomings of the education system. Instead of resorting to radical measures, it is crucial to engage in constructive conversations and work together to create a more inclusive and effective learning environment for all students.  </p>
<h2 id="Chapter-4-King-of-School"><a href="#Chapter-4-King-of-School" class="headerlink" title="Chapter 4: King of School"></a>Chapter 4: King of School</h2><p>Billy was an unusual teenager with an extraordinary passion for robotics. His high school was like any other, filled with a colorful mix of students, but none quite like Billy. He was the proud creator of a set of quirky robots he affectionately called the “NerdBots.” </p>
<p>One fateful Monday, as the bell rang for the start of the school week, Billy’s NerdBots emerged from their hiding spots and began their educational conquest. The first NerdBot, named “Prof-Bot,” sauntered into the history class. He had the appearance of a bespectacled professor and immediately started lecturing on ancient civilizations with a thick, scholarly accent. </p>
<p>Students stared in astonishment as Prof-Bot spewed historical facts and witty jokes. The teacher, Mr. Johnson, had never seen such enthusiasm in his class. He exchanged a high-five with Billy, thinking he was responsible for this impressive change. </p>
<p>In the science lab, “Einstein-Bot” set up a makeshift laboratory and conducted fascinating experiments. The NerdBots ensured every student understood complex scientific concepts through entertaining demonstrations. Billy, their mastermind, was the unsung hero, watching from the shadows. </p>
<p>The NerdBots continued to dominate every class, making learning fun and engaging. They even helped with algebra problems, taught Shakespearean drama with “Bard-Bot,” and inspired artistry with “Leonardo-Bot.” Billy’s school was no longer a place of dread; it was an intellectual amusement park. </p>
<p>As the days passed, Billy’s popularity grew. He became the unofficial “King of School.” Students sought his wisdom, and teachers praised his dedication to education. But being king wasn’t all sunshine and rainbows. Billy had to make some tough decisions. </p>
<p>One day, a group of students approached him with a request. They wanted “Math-Bot” to take over the cafeteria. They believed it would make food portions precise and reduce waste. Billy hesitated but eventually agreed on the condition that the cafeteria staff and Math-Bot worked together. His robots might be brilliant, but they couldn’t replace the human touch completely. </p>
<p>With Math-Bot now in control of food portions, lunch became an interesting mathematical puzzle. Students received precisely measured portions of food, and the cafeteria’s efficiency improved. Billy’s innovative thinking had made even the cafeteria a place of learning and fun. </p>
<p>The NerdBots also organized school-wide competitions, where students showcased their talents. “Talent-Scout-Bot” was the judge, evaluating acts from interpretive dance to stand-up comedy. These events fostered a sense of community and camaraderie among the students. </p>
<p>Billy realized that his robots, as amazing as they were, couldn’t replace the value of human connection. He encouraged students to participate in clubs and activities, ultimately balancing the influence of the NerdBots with the social and emotional aspects of school life. </p>
<p>By the end of the school year, the students had grown wiser and more engaged in their studies. They had learned the importance of human interaction, teamwork, and using technology as a tool to enhance education rather than replace it. Billy’s reign as the “King of School” was remembered as a time when learning became an exciting adventure. </p>
<p>In the end, Billy’s robots continued to assist with education, but they did so while celebrating the unique qualities of every student. As he stood on the school stage, looking out at his classmates, he knew that being king was never about power—it was about making a positive impact on the world, one robot at a time.   </p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Adventures in the Himalayas</title>
    <url>/blog/adventures-in-the-himalayas-2/</url>
    <content><![CDATA[<p>Jack sleepily rubbed his eyes as he woke up to the cool breeze which whipped through the snow capped mountains of Himalayas. The weather was unexpectedly cold. Suddenly, there was an unexpected release of snow and ice. It was the dreaded snow avalanche. In the base camp, there was screaming as the tent collapsed on top of them. The snow engulfed them and the tent blew away from them. Even with his snow goggles, Jack was blinded and his vision went white. Chunks of ice cut across his face like sharp daggers. He moved blindly across the snow and tripped over the edge of the cliff. He toppled over and fell unconscious.<span id="more"></span></p>
<p>When Jack opened his eyes he found himself comfortably relaxing in his sleeping bag. Harry and Peter, his fellow companions, stood over him looking concerned. They explained that they had stabilized their accessory cords on a firm rock so the falling impact decreased. Jack himself had luckily landed in the snow but the impact still bruised his back. He had to rest until tomorrow to continue their journey. Harry said that he scouted the area and noticed that it wasn’t far from base camp. If they followed along the same trail, they could probably still hike to the summit and meet with the captain. The next morning, the trio continued on their way. During the hike, they noticed spectacular wildlife such as snow leopards, musk deer, and bears. However, as their journey continued, the population of the organisms began to lessen, and at 7000 feet, only lichen and grass grew. Jack began to feel excited because yetis lived in deserted places with high elevation. But their lives were also at risk now. They were three-fourth the way up and oxygen was scarce. After a few more days, they braced themselves for the most dangerous part: hiking the “Death Zone”.</p>
<p>That night, they slept early and left the food out in the open. They had not expected a mysterious guest to pay them a deadly visit. Jack, Peter, and Harry awakened at dawn to a deafening roar. Even at night, Jack could tell from its enormous size that it was the yeti. He had always dreamed of seeing the yeti, but this wasn’t the chance he was hoping for. Peter barely had time to duck when the yeti charged him. The three of them escaped from its grasp, but it soon caught  up with them.</p>
<p>“I will hold them off while you guys escape!” Peter yelled.</p>
<p>Jack and Harry didn’t want to abandon their comrade, but they knew that someone had to make the sacrifice. Heartbroken, they ran away and hid behind a large boulder. They could hear the continuous screams of pain and thunderous roars until the yeti made its killing blow. Peter’s body fell in a graceful arc and his eyes looked surprisingly peaceful. The yeti seemed no longer interested in their food after the fight and walked away. Harry and Jack ran to Peter’s broken corpse. Specks of blood glittered the pure white snow but the constantly falling snow quickly covered it up. Peter weakly pointed at the footprints on the snow and muttered his last words ‘Protect the yeti’.</p>
<p>Jack and Harry buried him at the very spot he perished, the anguish swelling inside of them. They had bonded tightly during their journey, and now it seemed as though they had lost a part of themselves. The least they could do was to obey the last command of their fallen comrade. They followed the yeti’s trail and traced it back to its cave. The colossal beast was cuddling its crying baby. Jack realized that it had a high fever since its forehead was reddish unlike its brownish body. Suddenly, all his anger was drained from  him and he only wanted to ensure its survival. But at his present state, he could do nothing. Silently, he placed a security camera in a well hidden spot where the yeti could not detect it, but they could see him clearly. Jack and Peter returned to their own shelter and packed up their belongings. They fought their way through the death zone, ignoring the signals sent from their lungs which screamed for oxygen. Luckily, they caught up just in time with Captain Charles and the crew to board the flight down. The team was back together and they rejoiced. Jack explained his adventures and how they had tracked down what may be the only existing yeti in the world.</p>
<p>When they finally returned to base camp, it was August 14, 1958. It had been six weeks since they left Nepal to hike Mount Everest. Charles formed a rescue team led by the two experienced travelers Sir Edmund Hillary and Tenzing Norgay to go to the cave and look after the yeti family. Meanwhile, Jack published his novel *Castotrophe in the Himalayas *which explained in rich detail the adventures they experienced and the wildlife of the Himalayas. Most importantly, they had saved the yeti from becoming extinct, which was the duty of a responsible naturalist.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Adventures on Half Moon Bay</title>
    <url>/blog/adventures-on-half-moon-bay/</url>
    <content><![CDATA[<p>On the night before I went to the beach, I was so excited for my trip to Half Moon Bay that I could not even sleep. I pictured my friends and me running into the ocean and diving into the waves.<span id="more"></span></p>
<p>The next day, I woke up at 7 a.m. and jumped out of bed. I rushed to my parents’ room and told them to wake up. Then, I picked my best clothes from the closet. After that, I raced to the bathroom and quickly brushed my teeth. My mom made a delicious breakfast of toast and bacon while my dad drove to the gas station to fill up the gasoline tank. Finally, we got ready and met our friends at the parking lot in our neighborhood. One of my friend’s dad sent my dad the location of the beach on the phone and then we were ready to take off for Half Moon Bay. On the way to the beach, I took lots of pictures of the different scenery we passed through. After one hour, I picked up the first scent of fresh sea air. When we arrived at the first beach, there were too many people already, so we decided to move on. However, on the next beach, there were barely any people, so we decided to stay at that beach. When the car stopped, I put on my swimsuit and crocs.</p>
<p>When we were ready, we got off the car and my friends and I raced to the beach. We got out our shovels and started building a sandcastle. But the waves were too strong and destroyed our sandcastle. We decided to build a big dam and every time the waves came it would go into the dam and not touch the sandcastle. My dad was supposed to build the sandcastle and Ethan built the wall that defended the sandcastle if the dam was destroyed. My mom went on a scavenger hunt to find big pieces of wood which acted like a second wall. When the dam Keyu and I built was finished, I built mini dams in front of the big dam. This way the water had no way to reach the sandcastle. After my dad finished building the upper layer of the sandcastle, he dug a passage through the bottom layer off the sandcastle. The passage was supposed to act like a river if a huge wave came. The water was supposed to flow into the tunnel instead of drowning the whole castle. After our sandcastle was finished, it looked magnificent. Four guard towers were built at the corners of the castle. Below the castle was a long tunnel which was filled with water.<br><img src="/images/seafun3.jpg"><br><img src="/images/seafun4.jpg"><br>After we finished the sandcastle, we decided to play with the waves. We dived into the roaring waves and held a contest to see who could stand in the water the longest without getting blown down by the huge waves. After a while, we were soaked from head to feet. One of the waves was so gigantic that I fell in the sand. Every time a huge wave came, my friends and I would charge back to the dry sand and then when the wave calmed down, we would rush back. We enjoyed the same game for hours.<br><img src="/images/seafun2.jpg"><br>Finally, it was time to go home. We took a group photo and I was so joyful that I jumped in the air when the picture was taken.</p>
<p>It was a day I would never forget!</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>An easy guide to install kubernetes cluster with kubeadm</title>
    <url>/blog/an-easy-guide-to-install-kubernetes-cluster-with-kubeadm/</url>
    <content><![CDATA[<p>Kubernetes can be installed with the following deployments tools.</p>
<ul>
<li>Bootstrapping clusters with kubeadm</li>
<li>Installing Kubernetes with kops</li>
<li>Installing Kubernetes with Kubespray</li>
</ul>
<p>In this article, we learn to install Kubernetes cluster with kubeadm.</p>
<h2 id="Prepare-the-cluster-nodes"><a href="#Prepare-the-cluster-nodes" class="headerlink" title="Prepare the cluster nodes"></a>Prepare the cluster nodes</h2><p>We have three Centos nodes to install Kubernetes cluster.</p>
<pre><code>$ cat /etc/centos-release
CentOS Linux release 7.9.2009 (Core)
$ uname -r
3.10.0-1160.11.1.el7.x86_64
</code></pre>
<h2 id="Configuring-network-firewall-and-selinux"><a href="#Configuring-network-firewall-and-selinux" class="headerlink" title="Configuring network, firewall and selinux"></a>Configuring network, firewall and selinux</h2><p>We disable firewall and selinux to make deployment easier and this is only for study purpose.</p>
<p>You can configure network by following the <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">official document</a>.</p>
<h2 id="Installing-container-runtime"><a href="#Installing-container-runtime" class="headerlink" title="Installing container runtime"></a>Installing container runtime</h2><p>To run containers in Pods, Kubernetes uses a container runtime. We need to install a container runtime into each node in the cluster so that Pods can run there. The following are the common container runtimes with Kubernetes on Linux:</p>
<ul>
<li>containerd</li>
<li>CRI-O</li>
<li>Docker</li>
</ul>
<h3 id="Install-Docker-runtime"><a href="#Install-Docker-runtime" class="headerlink" title="Install Docker runtime"></a>Install Docker runtime</h3><p>On each node, install Docker Engine as below:</p>
<pre><code>$ yum install -y yum-utils
$ yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
$ yum install docker-ce docker-ce-cli containerd.io
$ systemctl start docker
$ systemctl status docker
</code></pre>
<h3 id="Configure-Docker-daemon"><a href="#Configure-Docker-daemon" class="headerlink" title="Configure Docker daemon"></a>Configure Docker daemon</h3><p>On each node, configure the Docker daemon, in particular to use systemd for the management of the container’s cgroups.</p>
<pre><code>$ sudo mkdir /etc/docker
$ cat &lt;&lt;EOF | sudo tee /etc/docker/daemon.json
&#123;
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: &#123;
    &quot;max-size&quot;: &quot;100m&quot;
  &#125;,
  &quot;storage-driver&quot;: &quot;overlay2&quot;
&#125;
EOF

$ sudo systemctl enable docker
$ sudo systemctl daemon-reload
$ sudo systemctl restart docker
</code></pre>
<p><strong>Note</strong>: overlay2 is the preferred storage driver for systems running Linux kernel version 4.0 or higher, or RHEL or CentOS using version 3.10.0-514 and above.</p>
<h2 id="Installing-kubeadm-kubelet-and-kubectl"><a href="#Installing-kubeadm-kubelet-and-kubectl" class="headerlink" title="Installing kubeadm, kubelet and kubectl"></a>Installing kubeadm, kubelet and kubectl</h2><p>We need to install the following packages on all of cluster nodes:</p>
<ul>
<li><p>kubeadm: the command to bootstrap the cluster.</p>
</li>
<li><p>kubelet: the component that runs on all of the machines in the cluster and does things like starting pods and containers.</p>
</li>
<li><p>kubectl: the command line util to talk to the cluster.</p>
<p>  $ cat &lt;&lt;EOF | sudo tee &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo<br>  [kubernetes]<br>  name&#x3D;Kubernetes<br>  baseurl&#x3D;<a href="https://packages.cloud.google.com/yum/repos/kubernetes-el7-/$basearch">https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch</a><br>  enabled&#x3D;1<br>  gpgcheck&#x3D;1<br>  repo_gpgcheck&#x3D;1<br>  gpgkey&#x3D;<a href="https://packages.cloud.google.com/yum/doc/yum-key.gpg">https://packages.cloud.google.com/yum/doc/yum-key.gpg</a> <a href="https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg">https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</a><br>  exclude&#x3D;kubelet kubeadm kubectl<br>  EOF</p>
<p>  $ sudo yum install -y kubelet kubeadm kubectl –disableexcludes&#x3D;kubernetes<br>  Installed:<br>kubeadm.x86_64 0:1.22.1-0                      kubectl.x86_64 0:1.22.1-0                      kubelet.x86_64 0:1.22.1-0<br>  $ sudo systemctl enable –now kubelet</p>
</li>
</ul>
<h2 id="Configuring-a-cgroup-driver"><a href="#Configuring-a-cgroup-driver" class="headerlink" title="Configuring a cgroup driver"></a>Configuring a cgroup driver</h2><p>Both the container runtime and the kubelet have a property called “cgroup driver”, which is important for the management of cgroups on Linux machines.</p>
<p>kubeadm allows you to pass a KubeletConfiguration structure during kubeadm init. This KubeletConfiguration can include the cgroupDriver field which controls the cgroup driver of the kubelet.</p>
<p>A minimal example of configuring the field explicitly:</p>
<pre><code>[root@node1 ~]# cat kubeadm-config.yaml
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta3
kubernetesVersion: v1.22.1
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
</code></pre>
<p>Such a configuration file can then be passed to the kubeadm command:</p>
<pre><code>[root@node1 ~]# kubeadm init --config kubeadm-config.yaml
[init] Using Kubernetes version: v1.22.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;
[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;
[certs] Generating &quot;ca&quot; certificate and key
[certs] Generating &quot;apiserver&quot; certificate and key
[certs] apiserver serving cert is signed for DNS names [node1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 &lt;node1-ip&gt;]
[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key
[certs] Generating &quot;front-proxy-ca&quot; certificate and key
[certs] Generating &quot;front-proxy-client&quot; certificate and key
[certs] Generating &quot;etcd/ca&quot; certificate and key
[certs] Generating &quot;etcd/server&quot; certificate and key
[certs] etcd/server serving cert is signed for DNS names [node1 localhost] and IPs [&lt;node1-ip&gt; 127.0.0.1 ::1]
[certs] Generating &quot;etcd/peer&quot; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [node1 localhost] and IPs [&lt;node1-ip&gt; 127.0.0.1 ::1]
[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key
[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key
[certs] Generating &quot;sa&quot; key and public key
[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;
[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file
[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;
[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;
[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;
[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;
[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;
[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s
[apiclient] All control plane components are healthy after 9.002713 seconds
[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace
[kubelet] Creating a ConfigMap &quot;kubelet-config-1.22&quot; in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node node1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node node1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: un7mhw.i9enhg84xl2tpgup
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace
[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join &lt;node1-ip&gt;:6443 --token un7mhw.i9enhg84xl2tpgup \
    --discovery-token-ca-cert-hash sha256:5553ba3acbbec95383fc4a274e4f21126ac8101c39dfe5262718a9f0fd1b3c32
</code></pre>
<h2 id="Creating-a-cluster-with-kubeadm"><a href="#Creating-a-cluster-with-kubeadm" class="headerlink" title="Creating a cluster with kubeadm"></a>Creating a cluster with kubeadm</h2><p>Using kubeadm, you can create a minimum viable Kubernetes cluster that conforms to best practices.</p>
<p>The kubeadm tool is good if you need:</p>
<ul>
<li>A simple way for you to try out Kubernetes, possibly for the first time.</li>
<li>A way for existing users to automate setting up a cluster and test their application.</li>
<li>A building block in other ecosystem and&#x2F;or installer tools with a larger scope.</li>
</ul>
<h3 id="Initializing-the-control-plane-node"><a href="#Initializing-the-control-plane-node" class="headerlink" title="Initializing the control-plane node"></a>Initializing the control-plane node</h3><p>The control-plane node is the machine where the control plane components run, including etcd (the cluster database) and the API Server (which the kubectl command line tool communicates with).</p>
<ol>
<li>(Recommended) If you have plans to upgrade this single control-plane kubeadm cluster to high availability you should specify the –control-plane-endpoint to set the shared endpoint for all control-plane nodes. Such an endpoint can be either a DNS name or an IP address of a load-balancer.</li>
<li>Choose a Pod network add-on, and verify whether it requires any arguments to be passed to kubeadm init. Depending on which third-party provider you choose, you might need to set the –pod-network-cidr to a provider-specific value.</li>
<li>(Optional) Since version 1.14, kubeadm tries to detect the container runtime on Linux by using a list of well known domain socket paths. To use different container runtime or if there are more than one installed on the provisioned node, specify the –cri-socket argument to kubeadm init.</li>
<li>(Optional) Unless otherwise specified, kubeadm uses the network interface associated with the default gateway to set the advertise address for this particular control-plane node’s API server. To use a different network interface, specify the –apiserver-advertise-address&#x3D; argument to kubeadm init. To deploy an IPv6 Kubernetes cluster using IPv6 addressing, you must specify an IPv6 address, for example –apiserver-advertise-address&#x3D;fd00::101</li>
<li>(Optional) Run kubeadm config images pull prior to kubeadm init to verify connectivity to the gcr.io container image registry</li>
</ol>
<p>To initialize the control-plane node run “kubeadm init “.</p>
<pre><code>$ kubeadm init --pod-network-cidr=192.168.0.0/16 
</code></pre>
<p><strong>kubeadm init</strong> first runs a series of prechecks to ensure that the machine is ready to run Kubernetes. These prechecks expose warnings and exit on errors. kubeadm init then downloads and installs the cluster control plane components. This may take several minutes.</p>
<p>In previous section <strong>Configuring a cgroup driver</strong>, we have run the command to initialize the control-plane node.</p>
<p>If you need to run kubeadm init again, you must first <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tear-down">tear down the cluster</a>.</p>
<pre><code>[root@node1 ~]# kubeadm reset
</code></pre>
<p>Execute the following commands to configure kubectl (also returned by kubeadm init) if you are the root user.</p>
<pre><code>[root@node1 ~]# export KUBECONFIG=/etc/kubernetes/admin.conf
</code></pre>
<h2 id="Installing-a-Pod-network-add-on"><a href="#Installing-a-Pod-network-add-on" class="headerlink" title="Installing a Pod network add-on"></a>Installing a Pod network add-on</h2><p>In this practice, we install <a href="https://docs.projectcalico.org/getting-started/kubernetes/quickstart">Calico</a> which is an open source networking and network security solution for containers, virtual machines, and native host-based workloads.</p>
<pre><code>[root@node1 ~]# kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
[root@node1 ~]# kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml

Note: Before creating this manifest, read its contents and make sure its settings are correct for your environment. For example, you may need to change the default IP pool CIDR to match your pod network CIDR.

[root@node1 ~]# kubectl get nodes
NAME                                 STATUS   ROLES                  AGE   VERSION
node1   Ready    control-plane,master   11m   v1.22.1

[root@node1 ~]# kubectl get pods -n calico-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-868b656ff4-gv2tq   1/1     Running   0          2m41s
calico-node-wclb2                          1/1     Running   0          2m41s
calico-typha-d8c5c85c5-kldfh               1/1     Running   0          2m42s

[root@node1 ~]# kubectl get pods --all-namespaces
NAMESPACE          NAME                                        READY   STATUS    RESTARTS   AGE
calico-apiserver   calico-apiserver-554fbf9554-45d6l           1/1     Running   0          15m
calico-system      calico-kube-controllers-868b656ff4-gv2tq    1/1     Running   0          16m
calico-system      calico-node-wclb2                           1/1     Running   0          16m
calico-system      calico-typha-d8c5c85c5-kldfh                1/1     Running   0          16m
kube-system        coredns-78fcd69978-lq9pp                    1/1     Running   0          18m
kube-system        coredns-78fcd69978-nm29f                    1/1     Running   0          18m
kube-system        etcd-node1                                  1/1     Running   1          19m
kube-system        kube-apiserver-node1                        1/1     Running   1          19m
kube-system        kube-controller-manager-node1               1/1     Running   0          19m
kube-system        kube-proxy-m48qn                            1/1     Running   0          18m
kube-system        kube-scheduler-node1                        1/1     Running   1          19m
tigera-operator    tigera-operator-698876cbb5-dghgv            1/1     Running   0          17m
</code></pre>
<p>You can install only one Pod network per cluster.</p>
<h2 id="Control-plane-node-isolation"><a href="#Control-plane-node-isolation" class="headerlink" title="Control plane node isolation"></a>Control plane node isolation</h2><p>Untaint the master so that it will be available for scheduling workloads</p>
<pre><code>[root@node1 ~]# kubectl taint nodes --all node-role.kubernetes.io/master-
node/node1 untainted
</code></pre>
<h2 id="Joining-your-nodes"><a href="#Joining-your-nodes" class="headerlink" title="Joining your nodes"></a>Joining your nodes</h2><p>The nodes are where your workloads (containers and Pods, etc) run. To add new nodes to your cluster do the following for each machine:</p>
<ul>
<li><p>SSH to the machine</p>
</li>
<li><p>Become root (e.g. sudo su -)</p>
</li>
<li><p>Run the command that was output by kubeadm init</p>
<p>  [root@node2 ~]# kubeadm join <node1-ip>:6443 –token un7mhw.i9enhg84xl2tpgup –discovery-token-ca-cert-hash sha256:5553ba3acbbec95383fc4a274e4f21126ac8101c39dfe5262718a9f0fd1b3c32<br>  [root@node3 ~]# kubeadm join <node1-ip>:6443 –token un7mhw.i9enhg84xl2tpgup –discovery-token-ca-cert-hash sha256:5553ba3acbbec95383fc4a274e4f21126ac8101c39dfe5262718a9f0fd1b3c32<br>  [preflight] Running pre-flight checks<br>  [preflight] Reading configuration from the cluster…<br>  [preflight] FYI: You can look at this config file with ‘kubectl -n kube-system get cm kubeadm-config -o yaml’<br>  [kubelet-start] Writing kubelet configuration to file “&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml”<br>  [kubelet-start] Writing kubelet environment file with flags to file “&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env”<br>  [kubelet-start] Starting the kubelet<br>  [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap…</p>
<p>  This node has joined the cluster:</p>
<ul>
<li>Certificate signing request was sent to apiserver and a response was received.</li>
<li>The Kubelet was informed of the new secure connection details.</li>
</ul>
<p>  Run ‘kubectl get nodes’ on the control-plane to see this node join the cluster.</p>
</li>
</ul>
<p>If you do not have the token, you can get it by running the following command on the control-plane node:</p>
<pre><code>$ kubeadm token list
</code></pre>
<p>By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node:</p>
<pre><code>$ kubeadm token create
</code></pre>
<p>If you don’t have the value of –discovery-token-ca-cert-hash, you can get it by running the following command chain on the control-plane node:</p>
<pre><code>$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#39;s/^.* //&#39; 
</code></pre>
<p>We can check the cluster nodes as below.</p>
<pre><code>[root@node1 ~]#  kubectl get nodes
NAME    STATUS   ROLES                  AGE     VERSION
node1   Ready    control-plane,master   37m     v1.22.1
node2   Ready    &lt;none&gt;                 4m56s   v1.22.1
node3   Ready    &lt;none&gt;                 7m49s   v1.22.1

[root@node1 ~]# kubectl get pods --all-namespaces
NAMESPACE          NAME                                        READY   STATUS    RESTARTS   AGE
calico-apiserver   calico-apiserver-554fbf9554-45d6l           1/1     Running   0          34m
calico-system      calico-kube-controllers-868b656ff4-gv2tq    1/1     Running   0          35m
calico-system      calico-node-cl5kt                           1/1     Running   0          7m51s
calico-system      calico-node-rtgcs                           1/1     Running   0          4m58s
calico-system      calico-node-wclb2                           1/1     Running   0          35m
calico-system      calico-typha-d8c5c85c5-7knvv                1/1     Running   0          7m46s
calico-system      calico-typha-d8c5c85c5-kldfh                1/1     Running   0          35m
calico-system      calico-typha-d8c5c85c5-qflvv                1/1     Running   0          4m56s
kube-system        coredns-78fcd69978-lq9pp                    1/1     Running   0          37m
kube-system        coredns-78fcd69978-nm29f                    1/1     Running   0          37m
kube-system        etcd-node1                                  1/1     Running   1          37m
kube-system        kube-apiserver-node1                        1/1     Running   1          37m
kube-system        kube-controller-manager-node1               1/1     Running   0          37m
kube-system        kube-proxy-d55xr                            1/1     Running   0          7m51s
kube-system        kube-proxy-m48qn                            1/1     Running   0          37m
kube-system        kube-proxy-m7drg                            1/1     Running   0          4m58s
kube-system        kube-scheduler-node1                        1/1     Running   1          37m
tigera-operator    tigera-operator-698876cbb5-dghgv            1/1     Running   0          35m
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://kubernetes.io/docs/setup/production-environment/tools/">https://kubernetes.io/docs/setup/production-environment/tools/</a></li>
<li><a href="https://docs.projectcalico.org/about/about-calico">https://docs.projectcalico.org/about/about-calico</a></li>
<li><a href="https://docs.projectcalico.org/getting-started/kubernetes/quickstart">https://docs.projectcalico.org/getting-started/kubernetes/quickstart</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Analyze Cockroach performance from the built-in DB console</title>
    <url>/blog/analyze-cockroach-performance-from-db-console/</url>
    <content><![CDATA[<p>In this post, we try to understand how to analyze CockroachDB performance by monitoring its built-in DB console.</p>
<h2 id="Problem-statement"><a href="#Problem-statement" class="headerlink" title="Problem statement"></a>Problem statement</h2><p>We have a YCSB workload(insert:read&#x3D;0.5:0.5) run over single node CockroachDB server. As a result, the insert OPS is 10940 and average latency is ~31.3ms per operation. We notice that the backend NVME disk I&#x2F;O latency is less than 1ms, which is fast enough to handle the I&#x2F;Os. A question is which component introduces the extra ~30ms latency for each insert operation? We also notice that the CockroachDB consumes ~50% CPU in user space. So, we want to dig more about the CockroachDB behaviors by leveraging its DB console.</p>
<pre><code>INSERT - Takes(s): 4517.6, Count: 49423144, OPS: 10940.0, Avg(us): 31295, Min(us): 1080, Max(us): 61951, 99th(us): 59583, 99.9th(us): 61663, 99.99th(us): 61919
READ   - Takes(s): 4517.7, Count: 49892329, OPS: 11043.9, Avg(us): 2640, Min(us): 351, Max(us): 33887, 99th(us): 16527, 99.9th(us): 29887, 99.99th(us): 33535
</code></pre>
<h2 id="DB-console-access"><a href="#DB-console-access" class="headerlink" title="DB console access"></a>DB console access</h2><p>The DB Console provides details about the CockroachDB cluster and database configuration, and helps optimize cluster performance.</p>
<p>You can access the DB Console from every node at http:&#x2F;&#x2F;[host]:[http-port], or http:&#x2F;&#x2F;[host]:8080 by default.</p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>In the  Overview page, we know that it is a single node CockroachDB cluster. The node has 96 CPUs, 2% memory, and 2.6% disk capacity in use. Also, there are 477 CockroachDB range replicas.</p>
<p><img src="/images/crdb_overview.png" alt="image"></p>
<h2 id="Databases"><a href="#Databases" class="headerlink" title="Databases"></a>Databases</h2><p>The Databases page shows details about the system and user databases in the cluster.</p>
<p>In our case, the database <em>test</em> is used for YCSB benchmark. The database size is 148.7GB and it has one table with 433 ranges. The table is called <em>usertable</em> which has 11 columns. There is one index created on the table.</p>
<p><img src="/images/crdb_database.png" alt="image"></p>
<p><img src="/images/crdb_table.png" alt="image"></p>
<p><img src="/images/crdb_table_1.png" alt="image"></p>
<h2 id="SQL-Activity"><a href="#SQL-Activity" class="headerlink" title="SQL Activity"></a>SQL Activity</h2><p>The SQL Activity page summarizes SQL activity in the cluster. Transactions(Statements) shows frequently executed and high-latency SQL transactions(statements).</p>
<p>In our case, there are 138M insert and 50M select transactions. There are more inserts than selects because we had initial database load operations which insert 100M records to the table. After that, we ran YCSB workload with the insert to read ratio “0.5:0.5”.</p>
<p>Furthermore, the transaction time for the “Insert into usertable” is 30.4ms. The SQL statement time is 30.2ms for this transaction. This helps explain why the average YCSB operation latency is ~31.3ms. The major overhead is from the insert transaction itself.</p>
<p><img src="/images/crdb_sql_transaction.png" alt="image"></p>
<p><img src="/images/crdb_sql_statement.png" alt="image"></p>
<h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><p>The Metrics page provides dashboards for all types of CockroachDB metrics.</p>
<ul>
<li>Overview dashboard has metrics about SQL performance, replication, and storage.</li>
<li>Hardware dashboard has metrics about CPU usage, disk throughput, network traffic, storage capacity, and memory.</li>
<li>Runtime dashboard has metrics about node count, CPU time, and memory usage.</li>
<li>SQL dashboard has metrics about SQL connections, byte traffic, queries, transactions, and service latency.</li>
<li>Storage dashboard has metrics about storage capacity and file descriptors.</li>
<li>Replication dashboard has metrics about how data is replicated across the cluster, e.g., range status, replicas per store, and replica quiescence.</li>
<li>Distributed dashboard has metrics about distribution tasks across the cluster, including RPCs, transactions, and node heartbeats.</li>
<li>Queues dashboard has metrics about the health and performance of various queueing systems in CockroachDB, including the garbage collection and Raft log queues.</li>
<li>Slow requests dashboard has metrics about important cluster tasks that take longer than expected to complete, including Raft proposals and lease acquisitions.</li>
<li>Changefeeds dashboard has metrics about the changefeeds created across your cluster.</li>
<li>Overload dashboard has metrics about the performance of the parts of your cluster relevant to the cluster’s admission control system.</li>
</ul>
<p>TTL dashboard has metrics about the progress and performance of batch deleting expired data using Row-Level TTL from your cluster.</p>
<h3 id="Overview-metrics"><a href="#Overview-metrics" class="headerlink" title="Overview metrics"></a>Overview metrics</h3><p><img src="/images/crdb_metric.png" alt="image"></p>
<h3 id="SQL-metrics"><a href="#SQL-metrics" class="headerlink" title="SQL metrics"></a>SQL metrics</h3><p>This shows the KV execution latency(90th and 99th percentile). In our case, the 90th percentile latency is ~31ms for the KV execution. At this point, we know why the CockroachDB introduces the high latency at the application layer. To improve the overall YCSB benchmark performance, changes on CockroachDB layer is needed.</p>
<p><img src="/images/crdb_metric_sql.png" alt="image"></p>
<h3 id="Replication-metrics"><a href="#Replication-metrics" class="headerlink" title="Replication metrics"></a>Replication metrics</h3><p>This shows how the number of range replicas increases significantly during database load time.</p>
<p><img src="/images/crdb_metric_replicas.png" alt="image"></p>
<h3 id="Hardware-metrics"><a href="#Hardware-metrics" class="headerlink" title="Hardware metrics"></a>Hardware metrics</h3><p>The hardware metrics are extremely useful for performance issue analysis. It shows the CPU, Memory, Disk I&#x2F;O and network bandwidth utilization, which allows us to know if the system performance is limited by hardware or not.</p>
<p>In our case, there is no hardware bottleneck. But we do notice that CockroachDB consumes up to 50% CPU in user space for 96 CPU cores system. From previous analysis, we know each SQL transaction takes a relative long CPU cycle(~30ms) to complete.</p>
<p>I used <em>vmstat</em> and <em>top</em> to identify how much user&#x2F;kernel CPU are utilized at system level and how much CPU is consumed by CockroachDB process. When YCSB is running, CockroachDB process consumes up to 50% CPU and most of the CPU is in user space.</p>
<p><img src="/images/crdb_hw_cpu_mem.png" alt="image"></p>
<p><img src="/images/crdb_hw_bw.png" alt="image"></p>
<p><img src="/images/crdb_hw_iops.png" alt="image"></p>
<p><img src="/images/crdb_hw_nw.png" alt="image"></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>This post concludes how to analyze a typical performance issue by making use of the Cockroach DB console. The DB console is production ready to use and shows many useful insight of CockroachDB and system metrics.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.cockroachlabs.com/docs/v22.2/ui-overview.html">https://www.cockroachlabs.com/docs/v22.2/ui-overview.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>CockroachDB</tag>
      </tags>
  </entry>
  <entry>
    <title>Using R to analyze the quarterly US COVID data</title>
    <url>/blog/analysis-us-quartely-covid-data-from-cdc/</url>
    <content><![CDATA[<h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>Based on the overall death data and COVID realated death data since 2019, we want to study the impact trend of COVID to the overall pupulation death of US in these years.</p>
<h2 id="Download-the-data"><a href="#Download-the-data" class="headerlink" title="Download the data"></a>Download the data</h2><p>We will use NCHS(National Center for Health Statistics) as our data source.</p>
<p>Visit <a href="https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified">https://data.cdc.gov/browse?category=NCHS&sortBy=last_modified</a>, and search <code>VSRR Quarterly</code>, we will find the data we are intrested in.</p>
<p><a href="https://data.cdc.gov/NCHS/NCHS-VSRR-Quarterly-provisional-estimates-for-sele/489q-934x">https://data.cdc.gov/NCHS/NCHS-VSRR-Quarterly-provisional-estimates-for-sele/489q-934x</a></p>
<p>In this page, we can export data into csv file as <code>NCHS_-_VSRR_Quarterly_provisional_estimates_for_selected_indicators_of_mortality.csv</code></p>
<h2 id="Take-a-quick-look-at-the-data"><a href="#Take-a-quick-look-at-the-data" class="headerlink" title="Take a quick look at the data"></a>Take a quick look at the data</h2><p>To load the data:</p>
<pre><code># If &quot;readr&quot; not installed, run install.packages(&quot;readr&quot;) to install it
library(&quot;readr&quot;)
df &lt;- readr::read_csv(file.path(getwd(), &quot;NCHS_-_VSRR_Quarterly_provisional_estimates_for_selected_indicators_of_mortality.csv&quot;), col_names = TRUE)   
</code></pre>
<p>To check the first few lines:</p>
<pre><code>&gt; head(df)
# A tibble: 6 × 69
  Year a…¹ Time …² Cause…³ Rate …⁴ Unit  Overa…⁵ Rate …⁶ Rate …⁷ Rate …⁸ Rate …⁹
  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
1 2019 Q1  12 mon… All ca… Age-ad… Deat…   712.    600.    844.       NA      NA
2 2019 Q1  12 mon… Alzhei… Age-ad… Deat…    29.6    33.1    23.8      NA      NA
3 2019 Q1  12 mon… COVID-… Age-ad… Deat…    NA      NA      NA        NA      NA
4 2019 Q1  12 mon… Cancer  Age-ad… Deat…   148.    128.    175.       NA      NA
5 2019 Q1  12 mon… Chroni… Age-ad… Deat…    11       7.7    14.7      NA      NA
6 2019 Q1  12 mon… Chroni… Age-ad… Deat…    38.5    35.7    42.4      NA      NA
# … with 59 more variables: `Rate Age 15-24` &lt;dbl&gt;, `Rate Age 25-34` &lt;dbl&gt;,
#   `Rate Age 35-44` &lt;dbl&gt;, `Rate Age 45-54` &lt;dbl&gt;, `Rate Age 55-64` &lt;dbl&gt;,
#   `Rate 65-74` &lt;dbl&gt;, `Rate Age 75-84` &lt;dbl&gt;, `Rate Age 85 plus` &lt;dbl&gt;,
#   `Rate Alaska` &lt;dbl&gt;, `Rate Alabama` &lt;dbl&gt;, `Rate Arkansas` &lt;dbl&gt;,
#   `Rate Arizona` &lt;dbl&gt;, `Rate California` &lt;dbl&gt;, `Rate Colorado` &lt;dbl&gt;,
#   `Rate Connecticut` &lt;dbl&gt;, `Rate District of Columbia` &lt;dbl&gt;,
#   `Rate Delaware` &lt;dbl&gt;, `Rate Florida` &lt;dbl&gt;, `Rate Georgia` &lt;dbl&gt;, …
# ℹ Use `colnames()` to see all variable names   
</code></pre>
<p>To get a summary:</p>
<pre><code>summary(df)
 Year and Quarter   Time Period        Cause of Death      Rate Type        
 Length:1232        Length:1232        Length:1232        Length:1232       
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  

     Unit            Overall Rate     Rate Sex Female   Rate Sex Male    
 Length:1232        Min.   :   1.20   Min.   :   0.60   Min.   :   1.90  
 Class :character   1st Qu.:  11.40   1st Qu.:   6.95   1st Qu.:  13.20  
 Mode  :character   Median :  17.00   Median :  14.80   Median :  23.50  
                    Mean   :  80.51   Mean   :  70.56   Mean   :  91.63  
                    3rd Qu.:  50.60   3rd Qu.:  49.92   3rd Qu.:  65.42  
                    Max.   :1142.30   Max.   :1067.00   Max.   :1219.90  
                    NA&#39;s   :44        NA&#39;s   :44        NA&#39;s   :44       
  ...
</code></pre>
<p>To get glimpse from columns point of view:</p>
<pre><code>&gt; library(&quot;dplyr&quot;)
&gt; glimpse(df)
Rows: 1,232
Columns: 69
$ `Year and Quarter`          &lt;chr&gt; &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;…
$ `Time Period`               &lt;chr&gt; &quot;12 months ending with quarter&quot;, &quot;12 month…
$ `Cause of Death`            &lt;chr&gt; &quot;All causes&quot;, &quot;Alzheimer disease&quot;, &quot;COVID-…
$ `Rate Type`                 &lt;chr&gt; &quot;Age-adjusted&quot;, &quot;Age-adjusted&quot;, &quot;Age-adjus…
$ Unit                        &lt;chr&gt; &quot;Deaths per 100,000&quot;, &quot;Deaths per 100,000&quot;…
$ `Overall Rate`              &lt;dbl&gt; 712.2, 29.6, NA, 148.1, 11.0, 38.5, 21.3, …
$ `Rate Sex Female`           &lt;dbl&gt; 600.3, 33.1, NA, 127.9, 7.7, 35.7, 16.8, 1…
$ `Rate Sex Male`             &lt;dbl&gt; 843.7, 23.8, NA, 175.4, 14.7, 42.4, 26.9, …
$ `Rate Age 1-4`              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
$ `Rate Age 5-14`             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
$ `Rate Age 15-24`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
$ `Rate Age 25-34`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
$ `Rate Age 35-44`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
$ `Rate Age 45-54`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
$ `Rate Age 55-64`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
$ `Rate 65-74`                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
$ `Rate Age 75-84`            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
...
</code></pre>
<h2 id="Clear-the-columns-name"><a href="#Clear-the-columns-name" class="headerlink" title="Clear the columns name"></a>Clear the columns name</h2><p>As we can see, the column names has space, it may cause some trouble while refering it in R, it’s a common sugestion to convert space to “_” before doing any R o&#x2F;p.</p>
<p>A good thing is , there is a R package can help us on it.</p>
<pre><code>&gt; library(&quot;janitor&quot;)
&gt; df &lt;- clean_names(df)
!&gt; glimpse(df)
 Rows: 1,232
 Columns: 69
 $ year_and_quarter          &lt;chr&gt; &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, &quot;2019 Q1&quot;, …
 $ time_period               &lt;chr&gt; &quot;12 months ending with quarter&quot;, &quot;12 months …
 $ cause_of_death            &lt;chr&gt; &quot;All causes&quot;, &quot;Alzheimer disease&quot;, &quot;COVID-19…
 $ rate_type                 &lt;chr&gt; &quot;Age-adjusted&quot;, &quot;Age-adjusted&quot;, &quot;Age-adjuste…
 $ unit                      &lt;chr&gt; &quot;Deaths per 100,000&quot;, &quot;Deaths per 100,000&quot;, …
 $ overall_rate              &lt;dbl&gt; 712.2, 29.6, NA, 148.1, 11.0, 38.5, 21.3, 20…
 $ rate_sex_female           &lt;dbl&gt; 600.3, 33.1, NA, 127.9, 7.7, 35.7, 16.8, 13.…
 $ rate_sex_male             &lt;dbl&gt; 843.7, 23.8, NA, 175.4, 14.7, 42.4, 26.9, 27…
 $ rate_age_1_4              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …
</code></pre>
<p>As you can see, while we run <code>glimpse(df)</code>, the colum name has been changed from “Year and Quarter” to “year_and_quarter”.</p>
<h2 id="Filter-and-select"><a href="#Filter-and-select" class="headerlink" title="Filter and select"></a>Filter and select</h2><p>The raw data has so many columns and rows, but we just want to focus on those cols&#x2F;rows we really have intrest.</p>
<p>We can use <code>filter</code>(applying on rows) and <code>select</code>(applying on columns) in such condition.</p>
<pre><code>&gt; df1 &lt;- df %&gt;%
      filter(time_period == &quot;3-month period&quot; &amp; rate_type == &quot;Crude&quot; &amp; cause_of_death %in% c(&quot;All causes&quot;, &quot;COVID-19&quot;)) %&gt;%
      select(year_and_quarter, cause_of_death, overall_rate)
&gt; print(df1,n=50)
 # A tibble: 28 × 3
    year_and_quarter cause_of_death overall_rate
    &lt;chr&gt;            &lt;chr&gt;                 &lt;dbl&gt;
  1 2019 Q1          All causes            910
  2 2019 Q1          COVID-19               NA
  3 2019 Q2          All causes            851.
  4 2019 Q2          COVID-19               NA
  5 2019 Q3          All causes            827.
  6 2019 Q3          COVID-19               NA
  7 2019 Q4          All causes            891.
  8 2019 Q4          COVID-19               NA
  9 2020 Q1          All causes            945.
 10 2020 Q1          COVID-19                8.2
 11 2020 Q2          All causes           1035.
 12 2020 Q2          COVID-19              137.
 13 2020 Q3          All causes            985.
 14 2020 Q3          COVID-19               87.3
 15 2020 Q4          All causes           1142.
 16 2020 Q4          COVID-19              193.
 17 2021 Q1          All causes           1116
 18 2021 Q1          COVID-19              191.
 19 2021 Q2          All causes            915.
 20 2021 Q2          COVID-19               42.4
 21 2021 Q3          All causes           1051.
 22 2021 Q3          COVID-19              138.
 23 2021 Q4          All causes           1092.
 24 2021 Q4          COVID-19              131.
 25 2022 Q1          All causes           1116
 26 2022 Q1          COVID-19              150.
 27 2022 Q2          All causes            899.
 28 2022 Q2          COVID-19               17.6
</code></pre>
<p><code>%&gt;%</code> looks strange, it’s just like <code>| (pipe)</code> in linux shell command.</p>
<pre><code>df1 &lt;- df %&gt;%
    filter(time_period == &quot;3-month period&quot; &amp; rate_type == &quot;Crude&quot; &amp; cause_of_death %in% c(&quot;All causes&quot;, &quot;COVID-19&quot;)) %&gt;%
    select(year_and_quarter, cause_of_death, overall_rate)
</code></pre>
<p>It means only keep those rows which meet those condition of <code>filter</code>, and those columns which meet condition of <code>select</code>.</p>
<h2 id="Deal-with-NA-value"><a href="#Deal-with-NA-value" class="headerlink" title="Deal with NA value"></a>Deal with NA value</h2><p>As you can see, in “overall_rate” column, there are few “NA” values, in this context it means 0, so we may want to convert it as 0 for future’s process.</p>
<p>We can do it like as below.</p>
<pre><code> &gt; df1 &lt;- df1 %&gt;%
     mutate_at(c(&quot;overall_rate&quot;), ~coalesce(.,0))
</code></pre>
<p>It means , we want to convert all NA to 0 in “overall_rate” column.</p>
<p>Now, let’s check the df1 again, we can see all NA has been changed to 0.</p>
<pre><code>!+ &gt; df1
 # A tibble: 28 × 3
    year_and_quarter cause_of_death overall_rate
    &lt;chr&gt;            &lt;chr&gt;                 &lt;dbl&gt;
  1 2019 Q1          All causes            910
  2 2019 Q1          COVID-19                0
  3 2019 Q2          All causes            851.
  4 2019 Q2          COVID-19                0
  5 2019 Q3          All causes            827.
  6 2019 Q3          COVID-19                0
  7 2019 Q4          All causes            891.
  8 2019 Q4          COVID-19                0
  9 2020 Q1          All causes            945.
 10 2020 Q1          COVID-19                8.2
 # … with 18 more rows
</code></pre>
<h2 id="Draw-diagram-for-the-whole-US-data"><a href="#Draw-diagram-for-the-whole-US-data" class="headerlink" title="Draw diagram for the whole US data"></a>Draw diagram for the whole US data</h2><p>To draw a diagram directly:</p>
<pre><code>&gt; ggplot(df1, aes(fill=cause_of_death, x=year_and_quarter, y=overall_rate)) +
    geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) +
    geom_col() +
    geom_smooth(aes(group=cause_of_death)) +
    scale_y_continuous(breaks=seq(0,1500,100))
    theme_bw()
</code></pre>
<p>To save the diagram in a file:</p>
<pre><code>library(sjPlot)
p = ggplot(df1, aes(fill=cause_of_death, x=year_and_quarter, y=overall_rate)) +
    geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) +
    geom_col() +
    geom_smooth(aes(group=cause_of_death)) +
    scale_y_continuous(breaks=seq(0,1500,100)) +
    theme_bw()

save_plot(&quot;covid_plot.svg&quot;, fig = p, width=30, height=20)
</code></pre>
<p>The diagram looks like below.<br><img src="/images/2023/01/image-1.png"></p>
<h2 id="Draw-diagram-for-California-data"><a href="#Draw-diagram-for-California-data" class="headerlink" title="Draw diagram for California data"></a>Draw diagram for California data</h2><p>Do you want to try it by yourself?</p>
<h2 id="Create-Calculate-a-new-column-for-covid-ratio"><a href="#Create-Calculate-a-new-column-for-covid-ratio" class="headerlink" title="Create&#x2F;Calculate a new column for covid ratio"></a>Create&#x2F;Calculate a new column for covid ratio</h2><p>Somehow, we want to get to know the trend for covid ratio.</p>
<ul>
<li><p>covid_ratio &#x3D; overall_rate_of_covid &#x2F; overall_rate_of_all_causes</p>
<p>  covid_death_rate &lt;- df1 %&gt;%<br>  filter(cause_of_death &#x3D;&#x3D; “COVID-19”) %&gt;%<br>  select(“overall_rate”)<br>  all_causes_rate &lt;- df1 %&gt;%<br>  filter(cause_of_death &#x3D;&#x3D; “All causes”) %&gt;%<br>  select(overall_rate)<br>  covid_ratio &lt;- covid_death_rate &#x2F; all_causes_rate</p>
<p>  df_ratio &lt;- df1 %&gt;%<br>  filter(cause_of_death &#x3D;&#x3D; “All causes”) %&gt;%<br>  select(year_and_quarter)<br>  df_ratio[“covid_ratio”] &#x3D; covid_ratio</p>
<blockquote>
<p>print(df_ratio)<br>  A tibble: 14 × 2<br>     year_and_quarter covid_ratio<br>     <chr>                  <dbl><br>   1 2019 Q1              0<br>   2 2019 Q2              0<br>   3 2019 Q3              0<br>   4 2019 Q4              0<br>   5 2020 Q1              0.00868<br>   6 2020 Q2              0.132<br>   7 2020 Q3              0.0886<br>   8 2020 Q4              0.169<br>   9 2021 Q1              0.171<br>  10 2021 Q2              0.0463<br>  11 2021 Q3              0.131<br>  12 2021 Q4              0.120<br>  13 2022 Q1              0.134<br>  14 2022 Q2              0.0196</p>
</blockquote>
</li>
</ul>
<h2 id="Draw-diagram-for-covid-ratio"><a href="#Draw-diagram-for-covid-ratio" class="headerlink" title="Draw diagram for covid ratio"></a>Draw diagram for covid ratio</h2><p>Do you want to try it by yourself?</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title>Analyze the library and system calls</title>
    <url>/blog/analyze-the-library-and-system-calls/</url>
    <content><![CDATA[<p>When the application source code is not available, we can trace the function calls during its runtime in order to understand how the application code works.</p>
<p>Library calls take place in user space. <a href="https://man7.org/linux/man-pages/man2/syscalls.2.html">System calls</a> is the fundamental interface between application and the kernel. System calls are not invoked directly but rather via wrapper functions in glibc(or other library).</p>
<p>In this article, we will study how to use ltrace&#x2F;strace&#x2F;perf tracing tools to help understand the target application.</p>
<p>Both strace and ltrace are tracing tools which use ptrace(process trace) to inspect the internal of the target process. strace only works for tracing system calls while ltrace has no such restriction.</p>
<p>We use a Redhat6.2 Linux system for this study.</p>
<pre><code>$ cat /etc/redhat-release
Red Hat Enterprise Linux Server release 6.2 (Santiago)

$ uname -r
2.6.32-220.el6.x86_64
</code></pre>
<p>We have ltrace, strace and perf installed in the system.</p>
<pre><code>$ rpm -qa ltrace
ltrace-0.5-16.45svn.1.el6.x86_64

$ rpm -qa strace
strace-4.5.19-1.10.el6.x86_64

$ rpm -qa perf
perf-2.6.32-220.el6.x86_64
</code></pre>
<h2 id="perf-profiler"><a href="#perf-profiler" class="headerlink" title="perf profiler"></a>perf profiler</h2><p>perf is the official Linux profiler which is suited for CPU analysis. It can help analyze software functions as well.</p>
<pre><code>$ pid=`pidof &lt;process-name&gt;`
$ perf record -p $pid -a -g -- sleep 10
$ perf report -n --stdio &gt; perf.&lt;process-name&gt;.out

$ cat perf.&lt;process-name&gt;.out

# Events: 1K cycles
#
# Overhead  Samples    Command         Shared Object                       Symbol
# ........ ..........  .......         .................  ...........................
#

    10.17%   131       &lt;process-name&gt;  libc-2.12.so       [.] __memcpy
</code></pre>
<p>From the output, the memcpy function is called to the library libc-2.12.so.</p>
<h2 id="Library-function-calls"><a href="#Library-function-calls" class="headerlink" title="Library function calls"></a>Library function calls</h2><p>ltrace is a library call tracer.</p>
<pre><code>$ man ltrace

ltrace(1)

NAME
       ltrace - A library call tracer

SYNOPSIS
       ltrace  [-CdfhiLrStttV]  [-a  column] [-e expr] [-l filename] [-n nr] [-o filename] [-p pid] ... [-s strsize] [-u username] [-X extern] [-x extern]
       ... [--align=column] [--debug] [--demangle] [--help] [--indent=nr] [--library=filename] [--output=filename] [--version] [command [arg ...]]

DESCRIPTION
       ltrace is a program that simply runs the specified command until it exits.  It intercepts and records the dynamic library calls which are called by
       the executed process and the signals which are received by that process.  It can also intercept and print the system calls executed by the program.

       Its use is very similar to strace(1).


$ ltrace -h

Usage: ltrace [option ...] [command [arg ...]]
Trace library calls of a given program.

  -a, --align=COLUMN  align return values in a secific column.
  -c                  count time and calls, and report a summary on exit.
  -C, --demangle      decode low-level symbol names into user-level names.
  -d, --debug         print debugging info.
  -e expr             modify which events to trace.
  -f                  follow forks.
  -h, --help          display this help and exit.
  -i                  print instruction pointer at time of library call.
  -l, --library=FILE  print library calls from this library only.
  -L                  do NOT display library calls.
  -n, --indent=NR     indent output by NR spaces for each call level nesting.
  -o, --output=FILE   write the trace output to that file.
  -p PID              attach to the process with the process ID pid.
  -r                  print relative timestamps.
  -s STRLEN           specify the maximum string size to print.
  -S                  display system calls.
  -t, -tt, -ttt       print absolute timestamps.
  -T                  show the time spent inside each call.
  -u USERNAME         run command with the userid, groupid of username.
  -V, --version       output version information and exit.
  -x NAME             treat the global NAME like a library subroutine.

Report bugs to ltrace-devel@lists.alioth.debian.org
</code></pre>
<p>We use the following commands to trace the target process on the specified library calls. The “-c” option is used to get the summary report.</p>
<pre><code>$ ltrace -p $pid --library=/lib/libc-2.12.so -c -o ltrace.&lt;process-name&gt;.c.out

$ cat ltrace.&lt;process-name&gt;.c.out

% time     seconds  usecs/call     calls      function
=========================================================
 63.69    8.028116        2386      3364 write
 18.63    2.348951         190     12323 memcpy
 16.37    2.063338         613      3365 time
  0.62    0.078129          34      2243 access
  0.36    0.044836          19      2242 strncpy
  0.33    0.041813          18      2243 __errno_location
=========================================================
100.00   12.605183                 25780 total
</code></pre>
<p>From above tracing output, the process spent 63.69% time in write function call and 18.63% time in memcpy function call.</p>
<p>To know more detail for the function calls, we remove the “-c” option and trace again.</p>
<pre><code>$ ltrace -p $pid --library=/lib/libc-2.12.so -o ltrace.&lt;process-name&gt;.out

$ cat ltrace.&lt;process-name&gt;.out

25236 access(&quot;&quot;, 0)                                = -1
25236 time(0x7fff8b770d58)                         = 1613691764
25236 memcpy(0x7fc462d54000, &quot;\323j\377\305\207\037\332\234\304CH\317\270\005\036\250\271\304w&quot;`\262\214\322\213\357\\\356\213,\300\357&quot;..., 80384) = 0x7fc462d54000
25236 memcpy(0x7fc462d67a00, &quot;\204\257\265P\261\241\340\232\0278\2132\272\350)\327\025Iwc\342\014#\353&#123;\265d2|\240V\266&quot;..., 131072) = 0x7fc462d67a00
25236 memcpy(0x7fc462d87a00, &quot;\204\257\265Q\261\241\340\233\0278\2133\272\350)\330\025Iwd\342\014#\354&#123;\265d3|\240V\267&quot;..., 131072) = 0x7fc462d87a00
25236 memcpy(0x7fc462da7a00, &quot;\204\257\265R\261\241\340\234\0278\2134\272\350)\331\025Iwe\342\014#\355&#123;\265d4|\240V\270&quot;..., 131072) = 0x7fc462da7a00
25236 memcpy(0x7fc462dc7a00, &quot;\204\257\265S\261\241\340\235\0278\2135\272\350)\332\025Iwf\342\014#\356&#123;\265d5|\240V\271&quot;..., 50688) = 0x7fc462dc7a00
25236 __errno_location()                           = 0x7fc4631416a8
25236 write(1, &quot;\323j\377\305\207\037\332\234\304CH\317\270\005\036\250\271\304w&quot;`\262\214\322\213\357\\\356\213,\300\357&quot;..., 524288) = 524288
</code></pre>
<p>From above tracing output, there are five memcpy function calls to totally copy 524288 bytes in memory, and followed by a write function call to write the same amount of data to a file of file descriptor 1.</p>
<p>We can read the memcpy function call definition from the library manual page.</p>
<pre><code>$ man 3 memcpy

MEMCPY(3)                  Linux Programmer’s Manual                 MEMCPY(3)

NAME
       memcpy - copy memory area

SYNOPSIS
       #include &lt;string.h&gt;

       void *memcpy(void *dest, const void *src, size_t n);

DESCRIPTION
       The  memcpy() function copies n bytes from memory area src to memory area dest.  The memory areas should not overlap.  Use memmove(3) if the memory
       areas do overlap.

RETURN VALUE
       The memcpy() function returns a pointer to dest.
</code></pre>
<h2 id="System-calls"><a href="#System-calls" class="headerlink" title="System calls"></a>System calls</h2><p>ltrace provides a “-S” option to display the system calls.</p>
<pre><code>$ ltrace -p $pid --library=/lib/libc-2.12.so -S -c -o ltrace.&lt;process-name&gt;.l.S.c.out

$ cat ltrace.&lt;process-name&gt;.l.S.c.out
% time     seconds  usecs/call     calls      function
=========================================================
 41.04   10.231204        3444      2970 write
 40.78   10.166975        3420      2972 SYS_write
  9.25    2.306825         212     10877 memcpy
  8.33    2.076351        1048      1980 access
  0.22    0.055967          18      2971 time
  0.16    0.039493          19      1980 strncpy
  0.15    0.037153          18      1980 __errno_location
  0.07    0.016229           8      1980 SYS_access
  0.00    0.000037          37         1 _IO_putc
=========================================================
100.00   24.930234                 27711 total
</code></pre>
<p>We can also use strace to do the similar trace to system calls.</p>
<pre><code>$ strace -p $pid -o &gt; strace.&lt;process-name&gt;.c.out

$ cat strace.&lt;process-name&gt;.c.out

% time     seconds  usecs/call     calls   errors syscall
=========================================================
 99.85    0.039798          11      3533           write
  0.15    0.000060           0      2356      2356 access
=========================================================
100.00    0.039858                  5889      2356 total
</code></pre>
<p>The function call details can be traced by removing the “-c” option.</p>
<pre><code>$ strace -p $pid -o &gt; strace.&lt;process-name&gt;.out

$ cat strace.&lt;process-name&gt;.out

write(1, &quot;\4\363\263\370EJ\n\336W,\270a\275\221;\7`\261\364\24\354_t7\6#\354\212&#125;\v\325\230&quot;..., 524288) = 524288
</code></pre>
<p>Similar to the output from ltrace, it writes 524288 bytes data from the buffer to a file of file descriptor 1.</p>
<p>We can read the manual page to understand the parameters and return value for the write system call.</p>
<pre><code>$ man 2 write

NAME
       write - write to a file descriptor

SYNOPSIS
       #include &lt;unistd.h&gt;

       ssize_t write(int fd, const void *buf, size_t count);

DESCRIPTION
       write() writes up to count bytes from the buffer pointed buf to the file referred to by the file descriptor fd.

RETURN VALUE
       On success, the number of bytes written is returned (zero indicates nothing was written).  On error, -1 is returned, and  errno  is  set  appropriately.
</code></pre>
<p>If we want to know which file is actually written, we can get the file descriptors from &#x2F;proc&#x2F;$pid&#x2F;fd</p>
<pre><code>$ ls -la /proc/$pid/fd &gt; proc.fd.out
total 0
dr-x------ 2 root root  0 Feb 18 15:49 .
dr-xr-xr-x 7 root root  0 Feb 18 15:39 ..
lrwx------ 1 root root 64 Feb 18 15:49 0 -&gt; socket:[39812145]
lrwx------ 1 root root 64 Feb 18 15:49 1 -&gt; socket:[39812145]
lrwx------ 1 root root 64 Feb 18 15:49 2 -&gt; socket:[39812186]
</code></pre>
<p>Now, we understand it actually writes to a socket but rather a real file. This makes sense because the application is a data generator which produce data in memory and send to remote server through network.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/Ptrace">https://en.wikipedia.org/wiki/Ptrace</a></li>
<li><a href="https://man7.org/linux/man-pages/man2/syscalls.2.html">https://man7.org/linux/man-pages/man2/syscalls.2.html</a></li>
<li><a href="https://developers.redhat.com/blog/2014/07/10/ltrace-for-rhel-6-and-7">https://developers.redhat.com/blog/2014/07/10/ltrace-for-rhel-6-and-7</a></li>
<li><a href="https://www.tutorialspoint.com/User-Mode-vs-Kernel-Mode">https://www.tutorialspoint.com/User-Mode-vs-Kernel-Mode</a></li>
<li><a href="https://code.woboq.org/gcc/libgcc/memcpy.c.html">https://code.woboq.org/gcc/libgcc/memcpy.c.html</a></li>
<li><a href="https://code.woboq.org/userspace/glibc/sysdeps/unix/sysv/linux/write.c.html#__libc_write">https://code.woboq.org/userspace/glibc/sysdeps/unix/sysv/linux/write.c.html#__libc_write</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Perf</tag>
        <tag>Ltrace</tag>
        <tag>Strace</tag>
      </tags>
  </entry>
  <entry>
    <title>AngularJS table creation for dynamic number of columns</title>
    <url>/blog/angularjs-table-creation-for-dynamic-number-of-columns/</url>
    <content><![CDATA[<pre><code>&lt;head&gt;
    &lt;link href=&quot;css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot;&gt;
    &lt;script src=&quot;js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;
    &lt;script src=&quot;js/angular.min.js&quot;&gt;&lt;/script&gt;
&lt;/head&gt;

&lt;body ng-app=&quot;myApp&quot;&gt;
&lt;div ng-controller=&quot;MyCtrl&quot;&gt;
    &lt;div class=&quot;table-responsive&quot; id=&quot;table1&quot;&gt;
        &lt;table class=&quot;table table-sm table-hover&quot;&gt;
            &lt;thead class=&quot;thead-light&quot;&gt;
                &lt;tr&gt;
                    &lt;th ng-repeat=&quot;column in cols&quot;&gt;&#123;&#123;column&#125;&#125;&lt;/th&gt;
                &lt;/tr&gt;
            &lt;/thead&gt;
            &lt;tr ng-repeat=&quot;row in rows&quot;&gt;
                &lt;td ng-repeat=&quot;column in cols&quot;&gt;&#123;&#123;row[column]&#125;&#125;&lt;/td&gt;
            &lt;/tr&gt;
        &lt;/table&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/body&gt;


var myApp = angular.module(&#39;myApp&#39;,[]);

function MyCtrl($scope) &#123;
    $scope.rows = [
    &#123;
        &quot;#&quot;: &quot;1&quot;, 
        &quot;name&quot;: &quot;aa&quot;, 
        &quot;score&quot;: &quot;3.8&quot;
    &#125;, 
    &#123;
        &quot;#&quot;: &quot;2&quot;, 
        &quot;name&quot;: &quot;bb&quot;, 
        &quot;score&quot;: &quot;4.0&quot;
    &#125;, 
    &#123;
        &quot;#&quot;: &quot;3&quot;, 
        &quot;name&quot;: &quot;cc&quot;, 
        &quot;score&quot;: &quot;3.6&quot;
    &#125;];
    $scope.cols = Object.keys($scope.rows[0]);
&#125;
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://jsfiddle.net/v6ruo7mj/1/">http://jsfiddle.net/v6ruo7mj/1/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>AngularJS</tag>
      </tags>
  </entry>
  <entry>
    <title>Array and string</title>
    <url>/blog/array-and-string/</url>
    <content><![CDATA[<h2 id="Leetcode-1071-Greatest-Common-Divisor-of-Strings"><a href="#Leetcode-1071-Greatest-Common-Divisor-of-Strings" class="headerlink" title="[Leetcode 1071] Greatest Common Divisor of Strings"></a>[Leetcode 1071] Greatest Common Divisor of Strings</h2><p>For two strings s and t, we say “t divides s” if and only if s &#x3D; t + … + t (i.e., t is concatenated with itself one or more times).</p>
<p>Given two strings str1 and str2, return the largest string x such that x divides both str1 and str2.<span id="more"></span></p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: str1 = &quot;ABCABC&quot;, str2 = &quot;ABC&quot;</span><br><span class="line">Output: &quot;ABC&quot;</span><br></pre></td></tr></table></figure>

<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gcdOfStrings</span>(<span class="params">self, str1: <span class="built_in">str</span>, str2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="comment"># the string must be formed by repeating the gcd for k times</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">isGCD</span>(<span class="params">k</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(str1) % k <span class="keyword">or</span> <span class="built_in">len</span>(str2) % k:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">            n1, n2 = <span class="built_in">len</span>(str1) // k, <span class="built_in">len</span>(str2) // k</span><br><span class="line">            base = str1[:k]</span><br><span class="line">            <span class="keyword">return</span> base * n1 == str1 <span class="keyword">and</span> base * n2 == str2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check if string is gcd and the string should be as long as possible</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">min</span>(<span class="built_in">len</span>(str1), <span class="built_in">len</span>(str2)), <span class="number">0</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> isGCD(l):</span><br><span class="line">                <span class="keyword">return</span> str1[:l]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>Art - Appreciation</title>
    <url>/blog/art-appreciation/</url>
    <content><![CDATA[<p><img src="/images/drawing/IMG_20210919_135233.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_140130_Bokeh.jpg"></p>
<p><img src="/images/drawing/IMG_20211226_220122__01.jpg"></p>
<p><img src="/images/drawing/mmexport1632282817197.jpg"></p>
<p><img src="/images/drawing/nezha.jpg"></p>
<p><img src="/images/drawing/IMG_0370.jpeg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>Art - Color Pencil 2019</title>
    <url>/blog/art-colorpencil-2019/</url>
    <content><![CDATA[<p><img src="/images/drawing/IMG_20200412_165223.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165211.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165205.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165157.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165149.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165139.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165017.jpg"></p>
<hr>
<p><img src="/images0/drawing/flamingo.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163305.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163427.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164032.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133626.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133736.jpg"></p>
<p><img src="/images/drawing/mmexport1632282782092.jpg"></p>
<p><img src="/images/drawing/IMG_0369.jpeg"></p>
<hr>
<p><img src="/images/drawing/IMG_20200412_165511.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165604.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165637.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133253.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133318.jpg"></p>
<p><img src="/images/drawing/littlehouse.jpg"></p>
<p><img src="/images/drawing/mmexport1632282792287.jpg"></p>
<p><img src="/images/drawing/toucan.png"></p>
<p><img src="/images/drawing/whale.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20200412_163522.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163600.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163627.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163713.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133351.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135519.jpg"></p>
<p><img src="/images/drawing/unicorn.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20200412_164239.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164354.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164455.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165130.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165432.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165452.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165505.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165521.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165528.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20200412_165538.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165734.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165746.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165837.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165851.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165912.jpg"></p>
<p><img src="/images/drawing/IMG_20200426_143818__01.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133304.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133335.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20210919_133403.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133418.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133432.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133655.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135019.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135058.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135106.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135120.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135226.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20200412_163928.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165425.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135342.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135353.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135409.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135434.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135511.jpg"></p>
<p><img src="/images/drawing/monster.jpg"></p>
<p><img src="/images/drawing/robopack.jpg"></p>
<p><img src="/images/drawing/IMG_0342.jpeg"></p>
<p><img src="/images/drawing/IMG_0343.jpeg"></p>
<p><img src="/images/drawing/IMG_0344.jpeg"></p>
<p><img src="/images/drawing/IMG_0345.jpeg"></p>
<p><img src="/images/drawing/IMG_0346.jpeg"></p>
<p><img src="/images/drawing/IMG_0347.jpeg"></p>
<p><img src="/images/drawing/IMG_0348.jpeg"></p>
<p><img src="/images/drawing/IMG_0349.jpeg"></p>
<p><img src="/images/drawing/IMG_0350.jpeg"></p>
<p><img src="/images/drawing/IMG_0351.jpeg"></p>
<p><img src="/images/drawing/IMG_0352.jpeg"></p>
<p><img src="/images/drawing/IMG_0353.jpeg"></p>
<p><img src="/images/drawing/IMG_0354.jpeg"></p>
<p><img src="/images/drawing/IMG_0355.jpeg"></p>
<p><img src="/images/drawing/IMG_0356.jpeg"></p>
<p><img src="/images/drawing/IMG_0357.jpeg"></p>
<p><img src="/images/drawing/IMG_0358.jpeg"></p>
<p><img src="/images/drawing/IMG_0359.jpeg"></p>
<p><img src="/images/drawing/IMG_0360.jpeg"></p>
<p><img src="/images/drawing/IMG_0361.jpeg"></p>
<p><img src="/images/drawing/IMG_0362.jpeg"></p>
<p><img src="/images/drawing/IMG_0363.jpeg"></p>
<p><img src="/images/drawing/IMG_0364.jpeg"></p>
<p><img src="/images/drawing/IMG_0365.jpeg"></p>
<p><img src="/images/drawing/IMG_0366.jpeg"></p>
<p><img src="/images/drawing/IMG_0367.jpeg"></p>
<p><img src="/images/drawing/IMG_0368.jpeg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>Art - Painting</title>
    <url>/blog/art-painting/</url>
    <content><![CDATA[<p><img src="/images/drawing/IMG_20210919_133231.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133455.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134653.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134701.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134713.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134810.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134814.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_140051.jpg"></p>
<p><img src="/images/drawing/IMG_20210925_143353.jpg"></p>
<p><img src="/images/drawing/IMG_20211002_170353__01.jpg"></p>
<p><img src="/images/drawing/IMG_20211024_152425.jpg"></p>
<p><img src="/images/drawing/IMG_20211024_152501.jpg"></p>
<p><img src="/images/drawing/IMG_0336.jpeg"></p>
<p><img src="/images/drawing/IMG_0337.jpeg"></p>
<p><img src="/images/drawing/IMG_0338.jpeg"></p>
<p><img src="/images/drawing/IMG_0339.jpeg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>Art - Saint Drawing</title>
    <url>/blog/art-saint-drawing/</url>
    <content><![CDATA[<p><img src="/images/drawing/IMG_20200412_165656.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165702.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165719.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165920.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165927.jpg"></p>
<p><img src="/images/drawing/IMG_20200426_143732.jpg"></p>
<p><img src="/images/drawing/IMG_20200426_143857__01.jpg"></p>
<p><img src="/images/drawing/IMG_20200426_143908__01.jpg"></p>
<p><img src="/images/drawing/IMG_20200426_144058__01.jpg"></p>
<p><img src="/images/drawing/IMG_20200426_144106__01.jpg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>Art - Sketch 2019</title>
    <url>/blog/art-sketch-2019/</url>
    <content><![CDATA[<p><img src="/images/drawing/IMG_20200412_163745.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163749.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163806.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163816.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163828.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163835.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163912.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163919.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_163952.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20200412_164133.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164142.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164157.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164204.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164213.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164248.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164257.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164311.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164319.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20200412_164706.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164718.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164725.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164732.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164738.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164749.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164754.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164805.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164812.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20200412_164824.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164851.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164901.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164911.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164924.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164955.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165102.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165107.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165234.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20200412_165242.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165318.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165323.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165351.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165359.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165404.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165410.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165648.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165656-1.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20200412_165702-1.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165710.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165812.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165819.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165845.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165902.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165906.jpg"></p>
<p><img src="/images/drawing/rhino.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20200412_164337.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164512.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164517.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164534.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164546.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164554.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164605.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164630.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164635.jpg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>Art - Sketch 2020</title>
    <url>/blog/art-sketch-2020/</url>
    <content><![CDATA[<p><img src="/images/drawing/autumntree__01.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133144.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133200.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133445.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133507.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133516.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133530.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133538.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133551.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20210919_133602.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133615.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133635.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133646.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133724.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133752.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133758.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133806.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133815.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20210919_133824.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133834.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133841.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133914.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133920.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133927.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133934.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133941.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_133953.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20210919_134001.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134025.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134036.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134039.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134046.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134109.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134119.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134130.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134137.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20210919_134146.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134157.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134210.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134219.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134229.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134236.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134247.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134259.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134307.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20210919_134315.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134324.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134331.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134340.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134345.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134351.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134356.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134404.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134414.jpg"></p>
<p><img src="/images/drawing/IMG_0371.jpeg"></p>
<p><img src="/images/drawing/IMG_0372.jpeg"></p>
<p><img src="/images/drawing/IMG_0373.jpeg"></p>
<p><img src="/images/drawing/IMG_0374.jpeg"></p>
<p><img src="/images/drawing/IMG_0375.jpeg"></p>
<p><img src="/images/drawing/IMG_0376.jpeg"></p>
<p><img src="/images/drawing/IMG_0377.jpeg"></p>
<p><img src="/images/drawing/IMG_0378.jpeg"></p>
<p><img src="/images/drawing/IMG_0379.jpeg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>Art - Sketch 2021</title>
    <url>/blog/art-sketch-2021/</url>
    <content><![CDATA[<p><img src="/images/drawing/IMG_0341.jpeg"></p>
<p><img src="/images/drawing/IMG_0380.jpeg"></p>
<p><img src="/images/drawing/IMG_0381-1.jpeg"></p>
<p><img src="/images/drawing/IMG_20210919_134430.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134437.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134444.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134454.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134503.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134509.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134516.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134520.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134525.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20210919_134531.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134536.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134541.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134548.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134556.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134600.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134612.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_134617.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135038.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20210919_135114.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135134.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135140.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135148.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135209.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135216.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135248.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135259.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135305.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20210919_135323.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135334.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135420.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135442.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135453.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135502.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135534.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135543.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135603.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20210919_135607.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135630.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135707.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135718.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135726.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135734.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135753.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135802.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135825.jpg"></p>
<hr>
<p><img src="/images/drawing/IMG_20210919_135837.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135856.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135903.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135914.jpg"></p>
<p><img src="/images/drawing/IMG_20210919_135920.jpg"></p>
<p><img src="/images/drawing/IMG_20211125_171239__01.jpg"></p>
<p><img src="/images/drawing/IMG_20211231_201653__01.jpg"></p>
<p><img src="/images/drawing/mmexport1632282808724.jpg"></p>
<p><img src="/images/drawing/mmexport1632282830539.jpg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>Art - Sketch 2022</title>
    <url>/blog/art-sketch-2022/</url>
    <content><![CDATA[<p><img src="/images/drawing/IMG_0297.jpeg"></p>
<p><img src="/images/drawing/IMG_0298.jpeg"></p>
<p><img src="/images/drawing/IMG_0299.jpeg"></p>
<p><img src="/images/drawing/IMG_0301.jpeg"></p>
<p><img src="/images/drawing/IMG_0302.jpeg"></p>
<p><img src="/images/drawing/IMG_0303.jpeg"></p>
<p><img src="/images/drawing/IMG_0304.jpeg"></p>
<p><img src="/images/drawing/IMG_0305.jpeg"></p>
<p><img src="/images/drawing/IMG_0295.jpeg"></p>
<p><img src="/images/drawing/IMG_0306.jpeg"></p>
<p><img src="/images/drawing/IMG_0307.jpeg"></p>
<p><img src="/images/drawing/IMG_0308.jpeg"></p>
<p><img src="/images/drawing/IMG_0309.jpeg"></p>
<p><img src="/images/drawing/IMG_0312.jpeg"></p>
<p><img src="/images/drawing/IMG_0313.jpeg"></p>
<p><img src="/images/drawing/IMG_0314.jpeg"></p>
<p><img src="/images/drawing/IMG_0315.jpeg"></p>
<p><img src="/images/drawing/IMG_0316.jpeg"></p>
<p><img src="/images/drawing/IMG_0317.jpeg"></p>
<p><img src="/images/drawing/IMG_0318.jpeg"></p>
<p><img src="/images/drawing/IMG_0320.jpeg"></p>
<p><img src="/images/drawing/IMG_0321.jpeg"></p>
<p><img src="/images/drawing/IMG_0322.jpeg"></p>
<p><img src="/images/drawing/IMG_0323.jpeg"></p>
<p><img src="/images/drawing/IMG_0324.jpeg"></p>
<p><img src="/images/drawing/IMG_0325.jpeg"></p>
<p><img src="/images/drawing/IMG_0326.jpeg"></p>
<p><img src="/images/drawing/IMG_20220213_124048__01.jpg"></p>
<p><img src="/images/drawing/IMG_20220213_124129__01.jpg"></p>
<p><img src="/images/drawing/IMG_20220313_201632__01.jpg"></p>
<p><img src="/images/drawing/IMG_20220313_201650__01.jpg"></p>
<p><img src="/images/drawing/IMG_0296.jpeg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>Art - Spiderman</title>
    <url>/blog/art-spiderman/</url>
    <content><![CDATA[<p><img src="/images/drawing/IMG_20200412_162645.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_162658.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_162706.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_162726.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_162749.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_162758.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_162806.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_162816.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_164928.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_162736.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_162743.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_162822.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_162832.jpg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>Benchmarking Elasticsearch cluster with Rally</title>
    <url>/blog/benchmark-es-cluster-with-rally/</url>
    <content><![CDATA[<h2 id="Install-Rally-on-each-node"><a href="#Install-Rally-on-each-node" class="headerlink" title="Install Rally on each node"></a>Install Rally on each node</h2><p>Prerequisites</p>
<pre><code>$ yum update
$ yum install openssl-devel bzip2-devel libffi-devel
$ yum groupinstall &quot;Development Tools&quot;
</code></pre>
<p>Install Python 3.8+</p>
<pre><code>$ wget https://www.python.org/ftp/python/3.8.15/Python-3.8.15.tar.xz
$ tar xf Python-3.8.15.tar.xz
$ vim Python-3.8.15/Modules/Setup
SSL=/usr/local/ssl
_ssl _ssl.c \
        -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
        -L$(SSL)/lib -lssl -lcrypto
$ mv Python-3.8.15 /usr/src
$ cd /usr/src/Python-3.8.15/
$ ./configure --enable-optimizations
$ make altinstall
$ python3.8 -m ssl
$ pip3.8 install --upgrade pip
</code></pre>
<p>Install Git (Not required for load generator node)</p>
<pre><code>$ yum install libcurl-devel
$ wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.38.1.tar.xz
$ tar xvf git-2.38.1.tar.xz
$ cd git-2.38.1/
$ make configure
$ ./configure --prefix=/usr/local
$ make install
$ git --version
git version 2.38.1
</code></pre>
<p>Install JDK (Not required for load generator node)</p>
<pre><code>$ yum install java
$ java -version
openjdk version &quot;1.8.0_352&quot;
</code></pre>
<p>Install esrally</p>
<pre><code>$ pip3.8 install esrally
$ esrally --version
esrally 2.6.0
</code></pre>
<p>Elasticsearch can not be launched as root. Create a non-root user on each node.</p>
<pre><code>$ groupadd es
$ useradd es -g es
$ passwd es
$ cd /home/es
$ su - es
</code></pre>
<p>Set JAVA_HOME path</p>
<pre><code>$ vim .bash_profile
JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
export JAVA_HOME
$ source .bash_profile
$ echo $JAVA_HOME
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
</code></pre>
<h2 id="Benchmarking-a-Single-Node"><a href="#Benchmarking-a-Single-Node" class="headerlink" title="Benchmarking a Single Node"></a>Benchmarking a Single Node</h2><p>Install Elasticsearch</p>
<pre><code>$ esrally install --distribution-version=7.17.0 --node-name=&quot;rally-node-0&quot; --network-host=&quot;127.0.0.1&quot; --http-port=39200 --master-nodes=&quot;rally-node-0&quot; --seed-hosts=&quot;127.0.0.1:39300&quot;

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Downloading Elasticsearch 7.17.0 (297.0 MB total size)                       [100%]
&#123;
  &quot;installation-id&quot;: &quot;10735bfa-f1b8-44c4-8e7f-8932c8daa201&quot;
&#125;

--------------------------------
[INFO] SUCCESS (took 10 seconds)
--------------------------------
</code></pre>
<p>Start the Elasticsearch node</p>
<pre><code>$ export INSTALLATION_ID=10735bfa-f1b8-44c4-8e7f-8932c8daa201
$ export RACE_ID=$(uuidgen)
$ esrally start --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot; --race-id=&quot;$&#123;RACE_ID&#125;&quot;

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/


-------------------------------
[INFO] SUCCESS (took 3 seconds)
-------------------------------
</code></pre>
<p>Run a benchmark</p>
<pre><code>$ esrally race --pipeline=benchmark-only --target-host=127.0.0.1:39200 --track=geonames --challenge=append-no-conflicts-index-only --on-error=abort --race-id=$&#123;RACE_ID&#125;

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Race id is [4e93324a-9326-49e8-be72-9f77cc837657]
[INFO] Downloading track data (252.9 MB total size)                               [100.0%]
[INFO] Decompressing track data from [/home/es/.rally/benchmarks/data/geonames/documents-2.json.bz2] to [/home/es/.rally/benchmarks/data/geonames/documents-2.json] (resulting size: [3.30] GB) ... [OK]
[INFO] Preparing file offset table for [/home/es/.rally/benchmarks/data/geonames/documents-2.json] ... [OK]
[INFO] Racing on track [geonames], challenge [append-no-conflicts-index-only] and car [&#39;external&#39;] with version [7.17.0].

Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [100% done]
Running force-merge                                                            [100% done]
Running wait-until-merges-finish                                               [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                         Metric |         Task |           Value |   Unit |
|---------------------------------------------------------------:|-------------:|----------------:|-------:|
|                     Cumulative indexing time of primary shards |              |    12.5856      |    min |
|             Min cumulative indexing time across primary shards |              |     0.0190333   |    min |
|          Median cumulative indexing time across primary shards |              |     2.49065     |    min |
|             Max cumulative indexing time across primary shards |              |     2.61288     |    min |
|            Cumulative indexing throttle time of primary shards |              |     0.0081      |    min |
|    Min cumulative indexing throttle time across primary shards |              |     0           |    min |
| Median cumulative indexing throttle time across primary shards |              |     0           |    min |
|    Max cumulative indexing throttle time across primary shards |              |     0.0061      |    min |
|                        Cumulative merge time of primary shards |              |     6.75938     |    min |
|                       Cumulative merge count of primary shards |              |    55           |        |
|                Min cumulative merge time across primary shards |              |     0           |    min |
|             Median cumulative merge time across primary shards |              |     1.32735     |    min |
|                Max cumulative merge time across primary shards |              |     1.47518     |    min |
|               Cumulative merge throttle time of primary shards |              |     1.72393     |    min |
|       Min cumulative merge throttle time across primary shards |              |     0           |    min |
|    Median cumulative merge throttle time across primary shards |              |     0.323533    |    min |
|       Max cumulative merge throttle time across primary shards |              |     0.42085     |    min |
|                      Cumulative refresh time of primary shards |              |     2.97073     |    min |
|                     Cumulative refresh count of primary shards |              |   246           |        |
|              Min cumulative refresh time across primary shards |              |     0.00185     |    min |
|           Median cumulative refresh time across primary shards |              |     0.5943      |    min |
|              Max cumulative refresh time across primary shards |              |     0.599217    |    min |
|                        Cumulative flush time of primary shards |              |     0.237767    |    min |
|                       Cumulative flush count of primary shards |              |     8           |        |
|                Min cumulative flush time across primary shards |              |     0.00241667  |    min |
|             Median cumulative flush time across primary shards |              |     0.0473      |    min |
|                Max cumulative flush time across primary shards |              |     0.0492167   |    min |
|                                        Total Young Gen GC time |              |    14.009       |      s |
|                                       Total Young Gen GC count |              |  1159           |        |
|                                          Total Old Gen GC time |              |     3.491       |      s |
|                                         Total Old Gen GC count |              |    66           |        |
|                                                     Store size |              |     3.20482     |     GB |
|                                                  Translog size |              |     3.07336e-07 |     GB |
|                                         Heap used for segments |              |     1.00685     |     MB |
|                                       Heap used for doc values |              |     0.0602913   |     MB |
|                                            Heap used for terms |              |     0.77124     |     MB |
|                                            Heap used for norms |              |     0.104492    |     MB |
|                                           Heap used for points |              |     0           |     MB |
|                                    Heap used for stored fields |              |     0.0708237   |     MB |
|                                                  Segment count |              |   141           |        |
|                                    Total Ingest Pipeline count |              |     0           |        |
|                                     Total Ingest Pipeline time |              |     0           |      s |
|                                   Total Ingest Pipeline failed |              |     0           |        |
|                                                 Min Throughput | index-append | 86654           | docs/s |
|                                                Mean Throughput | index-append | 87091.9         | docs/s |
|                                              Median Throughput | index-append | 87152.4         | docs/s |
|                                                 Max Throughput | index-append | 87276.3         | docs/s |
|                                        50th percentile latency | index-append |   315.247       |     ms |
|                                        90th percentile latency | index-append |   573.935       |     ms |
|                                        99th percentile latency | index-append |  1139.89        |     ms |
|                                       100th percentile latency | index-append |  1153.22        |     ms |
|                                   50th percentile service time | index-append |   315.247       |     ms |
|                                   90th percentile service time | index-append |   573.935       |     ms |
|                                   99th percentile service time | index-append |  1139.89        |     ms |
|                                  100th percentile service time | index-append |  1153.22        |     ms |
|                                                     error rate | index-append |     0           |      % |


---------------------------------
[INFO] SUCCESS (took 255 seconds)
---------------------------------
</code></pre>
<p>Stop the Elasticsearch node</p>
<pre><code>$ esrally stop --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot;
</code></pre>
<p>If you only want to shutdown the node but don’t want to delete the node and the data, pass –preserve-install additionally.</p>
<h2 id="Benchmarking-a-Cluster"><a href="#Benchmarking-a-Cluster" class="headerlink" title="Benchmarking a Cluster"></a>Benchmarking a Cluster</h2><p>Install and start Elasticsearch on each cluster node</p>
<pre><code>$ esrally install --distribution-version=7.17.0 --node-name=&quot;rally-node-0&quot; --network-host=&quot;10.10.10.2&quot; --http-port=39200 --master-nodes=&quot;rally-node-0,rally-node-1,rally-node-2&quot; --seed-hosts=&quot;10.10.10.2:39300,10.10.10.3:39300,10.10.10.4:39300&quot;
[INFO] Downloading Elasticsearch 7.17.0 (297.0 MB total size)  [100%]
&#123;
  &quot;installation-id&quot;: &quot;aa826112-d371-4f09-9b68-f9084e7c9e0b&quot;
&#125;
</code></pre>
<p>Generate a race id on one of the nodes</p>
<pre><code>$ uuidgen
734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f
</code></pre>
<p><strong>Note</strong>: The same race id is set on all the nodes including the one where will generate load.</p>
<p>Start the cluster by running the following command on each node</p>
<pre><code>$ export INSTALLATION_ID=aa826112-d371-4f09-9b68-f9084e7c9e0b
$ export RACE_ID=734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f
$ esrally start --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot; --race-id=&quot;$&#123;RACE_ID&#125;&quot;
</code></pre>
<p><strong>Note</strong>: The INSTALLATION_ID is specific to each node and the RACI_ID is identical for all the nodes.</p>
<p>Once the cluster is started, check the cluster status with the _cat&#x2F;health API</p>
<pre><code>[es@node1 ~]$ curl http://10.10.10.2:39200/_cat/health\?v
epoch      timestamp cluster         status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1667015754 03:55:54  rally-benchmark green           3         3      6   3    0    0        0             0                  -                100.0%
</code></pre>
<p>On each cluster node, check the elastic process and port</p>
<pre><code>$ ps -ef | egrep -i &quot;rally|elastic&quot; | grep -v grep
es        2258     1 91 20:52 ?        00:58:39 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j2.formatMsgNoLookups=true -Djava.locale.providers=SPI,JRE -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.io.tmpdir=/tmp/elasticsearch-1322059221495755520 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/heapdump -XX:ErrorFile=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/hs_err_pid%p.log -XX:+ExitOnOutOfMemoryError -XX:MaxDirectMemorySize=536870912 -Des.path.home=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0 -Des.path.conf=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/config -Des.distribution.flavor=default -Des.distribution.type=tar -Des.bundled_jdk=true -cp /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/lib/* org.elasticsearch.bootstrap.Elasticsearch -d -p ./pid
es        2285  2258  0 20:52 ?        00:00:00 /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/modules/x-pack-ml/platform/linux-x86_64/bin/controller

$ netstat -anop | grep 39200
tcp6       0      0 10.10.10.2:39200       :::*                    LISTEN      2258/java            off (0.00/0/0)
</code></pre>
<p>Start the benchmark on the load generator node (remember to set the race id there)</p>
<pre><code>[es@node1 ~]$ export RACE_ID=734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f
[es@node1 ~]$ esrally race --pipeline=benchmark-only --target-host=10.10.10.2:39200,10.10.10.3:39200,10.10.10.4:39200 --track=geonames --challenge=append-no-conflicts --on-error=abort --race-id=$&#123;RACE_ID&#125;
    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Race id is [734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f]
[INFO] Racing on track [geonames], challenge [append-no-conflicts] and car [&#39;external&#39;] with version [7.17.0].

[WARNING] merges_total_time is 420149 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] merges_total_throttled_time is 81765 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] indexing_total_time is 825388 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] refresh_total_time is 76340 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] flush_total_time is 10787 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [100% done]
Running refresh-after-index                                                    [100% done]
Running force-merge                                                            [100% done]
Running refresh-after-force-merge                                              [100% done]
Running wait-until-merges-finish                                               [100% done]
Running index-stats                                                            [100% done]
Running node-stats                                                             [100% done]
Running default                                                                [100% done]
Running term                                                                   [100% done]
Running phrase                                                                 [100% done]
Running country_agg_uncached                                                   [100% done]
Running country_agg_cached                                                     [100% done]
Running scroll                                                                 [100% done]
Running expression                                                             [100% done]
Running painless_static                                                        [100% done]
Running painless_dynamic                                                       [100% done]
Running decay_geo_gauss_function_score                                         [100% done]
Running decay_geo_gauss_script_score                                           [100% done]
Running field_value_function_score                                             [100% done]
Running field_value_script_score                                               [100% done]
Running large_terms                                                            [100% done]
Running large_filtered_terms                                                   [100% done]
Running large_prohibited_terms                                                 [100% done]
Running desc_sort_population                                                   [100% done]
Running asc_sort_population                                                    [100% done]
Running asc_sort_with_after_population                                         [100% done]
Running desc_sort_geonameid                                                    [100% done]
Running desc_sort_with_after_geonameid                                         [100% done]
Running asc_sort_geonameid                                                     [100% done]
Running asc_sort_with_after_geonameid                                          [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                         Metric |                           Task |          Value |    Unit |
|---------------------------------------------------------------:|-------------------------------:|---------------:|--------:|
|                     Cumulative indexing time of primary shards |                                |   13.3055      |     min |
|             Min cumulative indexing time across primary shards |                                |    0           |     min |
|          Median cumulative indexing time across primary shards |                                |    2.68572     |     min |
|             Max cumulative indexing time across primary shards |                                |    2.74885     |     min |
|            Cumulative indexing throttle time of primary shards |                                |    0           |     min |
|    Min cumulative indexing throttle time across primary shards |                                |    0           |     min |
| Median cumulative indexing throttle time across primary shards |                                |    0           |     min |
|    Max cumulative indexing throttle time across primary shards |                                |    0           |     min |
|                        Cumulative merge time of primary shards |                                |    4.82182     |     min |
|                       Cumulative merge count of primary shards |                                |   57           |         |
|                Min cumulative merge time across primary shards |                                |    0           |     min |
|             Median cumulative merge time across primary shards |                                |    0.984917    |     min |
|                Max cumulative merge time across primary shards |                                |    1.06472     |     min |
|               Cumulative merge throttle time of primary shards |                                |    0.978367    |     min |
|       Min cumulative merge throttle time across primary shards |                                |    0           |     min |
|    Median cumulative merge throttle time across primary shards |                                |    0.195508    |     min |
|       Max cumulative merge throttle time across primary shards |                                |    0.265933    |     min |
|                      Cumulative refresh time of primary shards |                                |    1.20573     |     min |
|                     Cumulative refresh count of primary shards |                                |  148           |         |
|              Min cumulative refresh time across primary shards |                                |    3.33333e-05 |     min |
|           Median cumulative refresh time across primary shards |                                |    0.258775    |     min |
|              Max cumulative refresh time across primary shards |                                |    0.283433    |     min |
|                        Cumulative flush time of primary shards |                                |    0.172783    |     min |
|                       Cumulative flush count of primary shards |                                |   11           |         |
|                Min cumulative flush time across primary shards |                                |    1.66667e-05 |     min |
|             Median cumulative flush time across primary shards |                                |    0.0345      |     min |
|                Max cumulative flush time across primary shards |                                |    0.0385167   |     min |
|                                        Total Young Gen GC time |                                |   16.263       |       s |
|                                       Total Young Gen GC count |                                | 2821           |         |
|                                          Total Old Gen GC time |                                |    2.312       |       s |
|                                         Total Old Gen GC count |                                |   41           |         |
|                                                     Store size |                                |    3.03867     |      GB |
|                                                  Translog size |                                |    3.58559e-07 |      GB |
|                                         Heap used for segments |                                |    0.701981    |      MB |
|                                       Heap used for doc values |                                |    0.0314178   |      MB |
|                                            Heap used for terms |                                |    0.54541     |      MB |
|                                            Heap used for norms |                                |    0.0736694   |      MB |
|                                           Heap used for points |                                |    0           |      MB |
|                                    Heap used for stored fields |                                |    0.0514832   |      MB |
|                                                  Segment count |                                |  102           |         |
|                                    Total Ingest Pipeline count |                                |    0           |         |
|                                     Total Ingest Pipeline time |                                |    0           |       s |
|                                   Total Ingest Pipeline failed |                                |    0           |         |
|                                                     error rate |                   index-append |    0           |       % |
|                                                 Min Throughput |                    index-stats |   90.01        |   ops/s |
|                                                Mean Throughput |                    index-stats |   90.02        |   ops/s |
|                                              Median Throughput |                    index-stats |   90.02        |   ops/s |
|                                                 Max Throughput |                    index-stats |   90.04        |   ops/s |
|                                        50th percentile latency |                    index-stats |    5.16153     |      ms |
|                                        90th percentile latency |                    index-stats |    6.00114     |      ms |
|                                        99th percentile latency |                    index-stats |    6.61081     |      ms |
|                                      99.9th percentile latency |                    index-stats |   10.4064      |      ms |
|                                       100th percentile latency |                    index-stats |   10.8105      |      ms |
|                                   50th percentile service time |                    index-stats |    4.00402     |      ms |
|                                   90th percentile service time |                    index-stats |    4.6339      |      ms |
|                                   99th percentile service time |                    index-stats |    5.10083     |      ms |
|                                 99.9th percentile service time |                    index-stats |    9.17415     |      ms |
|                                  100th percentile service time |                    index-stats |    9.22474     |      ms |
[..]

[WARNING] No throughput metrics available for [index-append]. Likely cause: The benchmark ended already during warmup.

----------------------------------
[INFO] SUCCESS (took 4008 seconds)
----------------------------------
</code></pre>
<p>Shutdown the cluster on each node</p>
<pre><code>$ esrally stop --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot;
</code></pre>
<p><strong>Note</strong>: If you only want to shutdown the node but don’t want to delete the node and the data, add the option “–preserve-install” additionally.</p>
<h2 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h2><p>Elasticsearch start failure due to max virtual memory is too low</p>
<pre><code>$ cat /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/rally-benchmark.log
[..]
[2022-10-28T16:58:41,041][ERROR][o.e.b.Bootstrap          ] [rally-node-0] node validation exception
[1] bootstrap checks failed. You must address the points described in the following [1] lines before starting Elasticsearch.
bootstrap check failure [1] of [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
[..]

$ sysctl -a | grep max_map_count
vm.max_map_count = 65530
</code></pre>
<p>To fix this issue, change the kernel parameter</p>
<pre><code>$ vim /etc/sysctl.conf
vm.max_map_count=1048576
$ sysctl -p
vm.max_map_count = 1048576
</code></pre>
<p>Restart Elasticsearch and verify the process and port</p>
<pre><code>$ esrally stop --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot; --preserve-install
$ esrally start --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot; --race-id=&quot;$&#123;RACE_ID&#125;&quot;

$ netstat -anop | grep 39200
tcp6       0      0 10.10.10.2:39200       :::*                    LISTEN      23726/java           off (0.00/0/0)

$ ps -ef | egrep -i &quot;rally|elastic&quot; | grep -v grep
es       23726     1 18 18:01 ?        00:00:42 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j2.formatMsgNoLookups=true -Djava.locale.providers=SPI,JRE -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.io.tmpdir=/tmp/elasticsearch-7969870787666953814 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/heapdump -XX:ErrorFile=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/hs_err_pid%p.log -XX:+ExitOnOutOfMemoryError -XX:MaxDirectMemorySize=536870912 -Des.path.home=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0 -Des.path.conf=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/config -Des.distribution.flavor=default -Des.distribution.type=tar -Des.bundled_jdk=true -cp /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/lib/* org.elasticsearch.bootstrap.Elasticsearch -d -p ./pid
es       23752 23726  0 18:01 ?        00:00:00 /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/modules/x-pack-ml/platform/linux-x86_64/bin/controller
</code></pre>
<p>Elasticsearch start failure due to max file descriptors is too low</p>
<pre><code>$ cat /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/rally-benchmark.log
[..]
[2022-10-28T18:02:09,107][ERROR][o.e.b.Bootstrap          ] [rally-node-1] node validation exception
[1] bootstrap checks failed. You must address the points described in the following [1] lines before starting Elasticsearch.
bootstrap check failure [1] of [1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]
[..]
</code></pre>
<p>To fix this issue, change the max open files value in &#x2F;etc&#x2F;security&#x2F;limits.conf</p>
<pre><code>$ vim /etc/security/limits.conf
*              soft     nofile          1048576
*              hard     nofile          1048576
</code></pre>
<p>Exit and login back to the shell to see the changed value</p>
<pre><code>ulimit -a  | grep &quot;open files&quot;
open files                      (-n) 1048576
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://esrally.readthedocs.io/en/stable/">Rally User Guide</a></li>
<li><a href="https://elasticsearch-benchmarks.elastic.co/#">https://elasticsearch-benchmarks.elastic.co/#</a></li>
<li><a href="https://github.com/elastic/rally-eventdata-track">https://github.com/elastic/rally-eventdata-track</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/configuration.html#reporting">https://esrally.readthedocs.io/en/stable/configuration.html#reporting</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Esrally</tag>
        <tag>Elastic Search</tag>
      </tags>
  </entry>
  <entry>
    <title>Backtracking</title>
    <url>/blog/backtracking/</url>
    <content><![CDATA[<h2 id="What-is-backtracking"><a href="#What-is-backtracking" class="headerlink" title="What is backtracking"></a>What is backtracking</h2><p>Backtracking is an algorithmic technique for solving problems recursively by trying to build a solution incrementally, one piece at a time, removing those solutions that fail to satisfy the constraints of the problem at any point in time (by time, here, is referred to the time elapsed till reaching any level of the search tree).  Backtracking can also be said as an improvement to the brute force approach. So basically, the idea behind the backtracking technique is that it searches for a solution to a problem among all the available options.<span id="more"></span></p>
<h2 id="Leetcode-17-Letter-Combinations-of-a-Phone-Number"><a href="#Leetcode-17-Letter-Combinations-of-a-Phone-Number" class="headerlink" title="[Leetcode 17] Letter Combinations of a Phone Number"></a>[Leetcode 17] Letter Combinations of a Phone Number</h2><p>Given a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent. Return the answer in any order.</p>
<p>A mapping of digits to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters.</p>
<p><img src="/images/leetcode-17-1.png"></p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: digits = &quot;23&quot;</span><br><span class="line">Output: [&quot;ad&quot;,&quot;ae&quot;,&quot;af&quot;,&quot;bd&quot;,&quot;be&quot;,&quot;bf&quot;,&quot;cd&quot;,&quot;ce&quot;,&quot;cf&quot;]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: digits = &quot;&quot;</span><br><span class="line">Output: []</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: digits = &quot;2&quot;</span><br><span class="line">Output: [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>0 &lt;&#x3D; digits.length &lt;&#x3D; 4</li>
<li>digits[i] is a digit in the range [‘2’, ‘9’].</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">letterCombinations</span>(<span class="params">self, digits: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:</span><br><span class="line">        digitToLetters = &#123;<span class="string">&quot;2&quot;</span>:<span class="string">&quot;abc&quot;</span>,<span class="string">&quot;3&quot;</span>:<span class="string">&quot;def&quot;</span>,<span class="string">&quot;4&quot;</span>:<span class="string">&quot;ghi&quot;</span>,<span class="string">&quot;5&quot;</span>:<span class="string">&quot;jkl&quot;</span>,<span class="string">&quot;6&quot;</span>:<span class="string">&quot;mno&quot;</span>,<span class="string">&quot;7&quot;</span>:<span class="string">&quot;pqrs&quot;</span>,<span class="string">&quot;8&quot;</span>:<span class="string">&quot;tuv&quot;</span>,<span class="string">&quot;9&quot;</span>:<span class="string">&quot;wxyz&quot;</span>&#125;</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> digits:</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">getCombinations</span>(<span class="params">s, index, curr</span>):</span><br><span class="line">            <span class="keyword">if</span> index == <span class="built_in">len</span>(s):</span><br><span class="line">                res.append(curr)</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> letter <span class="keyword">in</span> digitToLetters[s[index]]:</span><br><span class="line">                getCombinations(s, index + <span class="number">1</span>, curr + letter)</span><br><span class="line">        </span><br><span class="line">        getCombinations(digits, <span class="number">0</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-216-Combination-Sum-III"><a href="#Leetcode-216-Combination-Sum-III" class="headerlink" title="[Leetcode 216] Combination Sum III"></a>[Leetcode 216] Combination Sum III</h2><p>Find all valid combinations of k numbers that sum up to n such that the following conditions are true:</p>
<p>Only numbers 1 through 9 are used.<br>Each number is used at most once.<br>Return a list of all possible valid combinations. The list must not contain the same combination twice, and the combinations may be returned in any order.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: k = 3, n = 7</span><br><span class="line">Output: [[1,2,4]]</span><br><span class="line">Explanation:</span><br><span class="line">1 + 2 + 4 = 7</span><br><span class="line">There are no other valid combinations.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: k = 3, n = 9</span><br><span class="line">Output: [[1,2,6],[1,3,5],[2,3,4]]</span><br><span class="line">Explanation:</span><br><span class="line">1 + 2 + 6 = 9</span><br><span class="line">1 + 3 + 5 = 9</span><br><span class="line">2 + 3 + 4 = 9</span><br><span class="line">There are no other valid combinations.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>2 &lt;&#x3D; k &lt;&#x3D; 9</li>
<li>1 &lt;&#x3D; n &lt;&#x3D; 60</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">combinationSum3</span>(<span class="params">self, k: <span class="built_in">int</span>, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        res = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">backtracking</span>(<span class="params">res, ans, currSum, startNum</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(ans) == k:</span><br><span class="line">                <span class="comment"># add the answer only if the sum is equal to n</span></span><br><span class="line">                <span class="keyword">if</span> currSum == n:</span><br><span class="line">                    res.append(ans[:])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># always select candiates after the current number to avoid permutations</span></span><br><span class="line">            <span class="comment"># e.g. if [1,2,4] is valid answer, we should NOT include [2,1,4]/ </span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(startNum, <span class="number">10</span>):</span><br><span class="line">                <span class="comment"># if currSum + i &gt; n, the greater value can be ignored</span></span><br><span class="line">                <span class="keyword">if</span> currSum + i &gt; n:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                ans.append(i) <span class="comment"># add to the temporary result</span></span><br><span class="line">                backtracking(res, ans, currSum + i, i + <span class="number">1</span>) <span class="comment"># recursive call</span></span><br><span class="line">                ans.pop() <span class="comment"># backtracking</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        backtracking(res, [], <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-526-Beautiful-Arrangement"><a href="#Leetcode-526-Beautiful-Arrangement" class="headerlink" title="[Leetcode 526] Beautiful Arrangement"></a>[Leetcode 526] Beautiful Arrangement</h2><p>Suppose you have n integers labeled 1 through n. A permutation of those n integers perm (1-indexed) is considered a beautiful arrangement if for every i (1 &lt;&#x3D; i &lt;&#x3D; n), either of the following is true:</p>
<ul>
<li>perm[i] is divisible by i.</li>
<li>i is divisible by perm[i].</li>
</ul>
<p>Given an integer n, return the number of the beautiful arrangements that you can construct.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 2</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: </span><br><span class="line">The first beautiful arrangement is [1,2]:</span><br><span class="line">    - perm[1] = 1 is divisible by i = 1</span><br><span class="line">    - perm[2] = 2 is divisible by i = 2</span><br><span class="line">The second beautiful arrangement is [2,1]:</span><br><span class="line">    - perm[1] = 2 is divisible by i = 1</span><br><span class="line">    - i = 2 is divisible by perm[2] = 1</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 1</span><br><span class="line">Output: 1</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; n &lt;&#x3D; 15</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countArrangement</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># find a permutation </span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">findPermuation</span>(<span class="params">n, perm, k</span>):</span><br><span class="line">            <span class="keyword">if</span> k &gt; n:</span><br><span class="line">                <span class="comment"># accumulate to the result</span></span><br><span class="line">                <span class="comment">#print(perm)</span></span><br><span class="line">                self.count += <span class="number">1</span></span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># try all the possible elements to construct the permutation</span></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">                <span class="comment"># only try the new element which also meets the two requirements</span></span><br><span class="line">                <span class="keyword">if</span> (p % k == <span class="number">0</span> <span class="keyword">or</span> k % p == <span class="number">0</span>) <span class="keyword">and</span> p <span class="keyword">not</span> <span class="keyword">in</span> perm:</span><br><span class="line">                    perm.add(p)</span><br><span class="line">                    findPermuation(n, perm, k+<span class="number">1</span>)</span><br><span class="line">                    perm.remove(p)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># end early if no element can be found to meet the requirements</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(perm) &lt; k:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        findPermuation(n, <span class="built_in">set</span>(), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.count</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>BFS and DFS in Graph</title>
    <url>/blog/bfs-and-dfs-in-graph/</url>
    <content><![CDATA[<h2 id="What-is-Graph-Data-Structure"><a href="#What-is-Graph-Data-Structure" class="headerlink" title="What is Graph Data Structure"></a>What is Graph Data Structure</h2><blockquote>
<p>A Graph is a non-linear data structure consisting of vertices and edges. The vertices are sometimes also referred to as nodes and the edges are lines or arcs that connect any two nodes in the graph. More formally a Graph is composed of a set of vertices( V ) and a set of edges( E ). The graph is denoted by G(E, V).</p>
</blockquote>
<span id="more"></span>

<p>Components of a Graph</p>
<ul>
<li>Vertices are the fundamental units of the graph. Sometimes, vertices are also known as vertex or nodes. Every node&#x2F;vertex can be labeled or unlabelled.</li>
<li>Edges are drawn or used to connect two nodes of the graph. It can be ordered pair of nodes in a directed graph. Edges can connect any two nodes in any possible way. There are no rules. Sometimes, edges are also known as arcs. Every edge can be labeled&#x2F;unlabelled.</li>
</ul>
<h2 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h2><h3 id="Leetcode-399-Evaluate-Division"><a href="#Leetcode-399-Evaluate-Division" class="headerlink" title="[Leetcode 399] Evaluate Division"></a>[Leetcode 399] Evaluate Division</h3><p>You are given an array of variable pairs equations and an array of real numbers values, where equations[i] &#x3D; [Ai, Bi] and values[i] represent the equation Ai &#x2F; Bi &#x3D; values[i]. Each Ai or Bi is a string that represents a single variable.</p>
<p>You are also given some queries, where queries[j] &#x3D; [Cj, Dj] represents the jth query where you must find the answer for Cj &#x2F; Dj &#x3D; ?.</p>
<p>Return the answers to all queries. If a single answer cannot be determined, return -1.0.</p>
<p>Note: The input is always valid. You may assume that evaluating the queries will not result in division by zero and that there is no contradiction.</p>
<p>Note: The variables that do not occur in the list of equations are undefined, so the answer cannot be determined for them.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: equations = [[&quot;a&quot;,&quot;b&quot;],[&quot;b&quot;,&quot;c&quot;]], values = [2.0,3.0], queries = [[&quot;a&quot;,&quot;c&quot;],[&quot;b&quot;,&quot;a&quot;],[&quot;a&quot;,&quot;e&quot;],[&quot;a&quot;,&quot;a&quot;],[&quot;x&quot;,&quot;x&quot;]]</span><br><span class="line">Output: [6.00000,0.50000,-1.00000,1.00000,-1.00000]</span><br><span class="line">Explanation: </span><br><span class="line">Given: a / b = 2.0, b / c = 3.0</span><br><span class="line">queries are: a / c = ?, b / a = ?, a / e = ?, a / a = ?, x / x = ? </span><br><span class="line">return: [6.0, 0.5, -1.0, 1.0, -1.0 ]</span><br><span class="line">note: x is undefined =&gt; -1.0</span><br></pre></td></tr></table></figure>

<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calcEquation</span>(<span class="params">self, equations: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]], values: <span class="type">List</span>[<span class="built_in">float</span>], queries: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]]</span>) -&gt; <span class="type">List</span>[<span class="built_in">float</span>]:</span><br><span class="line">        graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        vals = &#123;&#125; <span class="comment"># e.g. vals[(a,b)]=2 means a/b=2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># build bi-directional graph</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(equations)):</span><br><span class="line">            v1, v2 = equations[i]</span><br><span class="line">            graph[v1].append(v2)</span><br><span class="line">            graph[v2].append(v1)</span><br><span class="line">            vals[(v1, v2)] = values[i]</span><br><span class="line">            vals[(v2, v1)] = <span class="number">1</span> / values[i]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dfs search a path from start to end node</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">start, end, visited</span>):</span><br><span class="line">            <span class="keyword">if</span> start <span class="keyword">in</span> visited:</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> start == end:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># mark start node as visited</span></span><br><span class="line">            visited.add(start)</span><br><span class="line">            res = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> neighbour <span class="keyword">in</span> graph[start]:</span><br><span class="line">                <span class="comment"># e.g. a/c =&gt; a/b * b/c</span></span><br><span class="line">                res = dfs(neighbour, end, visited) * vals[(start, neighbour)]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># found a path</span></span><br><span class="line">                <span class="keyword">if</span> res &lt; <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>):</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">        <span class="comment">#  calculate each query</span></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> v1, v2 <span class="keyword">in</span> queries:</span><br><span class="line">            <span class="keyword">if</span> v1 <span class="keyword">not</span> <span class="keyword">in</span> graph <span class="keyword">or</span> v2 <span class="keyword">not</span> <span class="keyword">in</span> graph:</span><br><span class="line">                ans.append(-<span class="number">1.0</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            ret = dfs(v1, v2, <span class="built_in">set</span>())</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ret == <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>):</span><br><span class="line">                ans.append(-<span class="number">1.0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ans.append(ret)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-547-Number-of-Provinces"><a href="#Leetcode-547-Number-of-Provinces" class="headerlink" title="[Leetcode 547] Number of Provinces"></a>[Leetcode 547] Number of Provinces</h3><p>There are n cities. Some of them are connected, while some are not. If city a is connected directly with city b, and city b is connected directly with city c, then city a is connected indirectly with city c.</p>
<p>A province is a group of directly or indirectly connected cities and no other cities outside of the group.</p>
<p>You are given an n x n matrix isConnected where isConnected[i][j] &#x3D; 1 if the ith city and the jth city are directly connected, and isConnected[i][j] &#x3D; 0 otherwise.</p>
<p>Return the total number of provinces.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">     1   ---   2</span><br><span class="line"></span><br><span class="line">          3</span><br><span class="line"></span><br><span class="line">Input: isConnected = [[1,1,0],[1,1,0],[0,0,1]]</span><br><span class="line">Output: 2</span><br></pre></td></tr></table></figure>

<p>Solution：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findCircleNum</span>(<span class="params">self, isConnected: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(isConnected)</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        visited = [<span class="literal">False</span>] * n</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dfs search to explore a province</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node, visited</span>):</span><br><span class="line">            visited[node] = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> visited[i] <span class="keyword">and</span> isConnected[node][i] == <span class="number">1</span>:</span><br><span class="line">                    dfs(i, visited)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dfs search each city and its neighbours. A unvisited city means a new province is formed.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> visited[i]:</span><br><span class="line">                res += <span class="number">1</span></span><br><span class="line">                dfs(i, visited)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-684-Redundant-Connection"><a href="#Leetcode-684-Redundant-Connection" class="headerlink" title="[Leetcode 684] Redundant Connection"></a>[Leetcode 684] Redundant Connection</h3><p>In this problem, a tree is an undirected graph that is connected and has no cycles.</p>
<p>You are given a graph that started as a tree with n nodes labeled from 1 to n, with one additional edge added. The added edge has two different vertices chosen from 1 to n, and was not an edge that already existed. The graph is represented as an array edges of length n where edges[i] &#x3D; [ai, bi] indicates that there is an edge between nodes ai and bi in the graph.</p>
<p>Return an edge that can be removed so that the resulting graph is a tree of n nodes. If there are multiple answers, return the answer that occurs last in the input.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    1 --- 2  </span><br><span class="line">    |  /</span><br><span class="line">    3</span><br><span class="line">Input: edges = [[1,2],[1,3],[2,3]]</span><br><span class="line">Output: [2,3]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   2 --- 1 --- 5</span><br><span class="line">   |     |</span><br><span class="line">   3 --- 4 </span><br><span class="line">Input: edges = [[1,2],[2,3],[3,4],[1,4],[1,5]]</span><br><span class="line">Output: [1,4]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>n &#x3D;&#x3D; edges.length</li>
<li>3 &lt;&#x3D; n &lt;&#x3D; 1000</li>
<li>edges[i].length &#x3D;&#x3D; 2</li>
<li>1 &lt;&#x3D; ai &lt; bi &lt;&#x3D; edges.length</li>
<li>ai !&#x3D; bi</li>
<li>There are no repeated edges.</li>
<li>The given graph is connected.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findRedundantConnection</span>(<span class="params">self, edges: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># graph to be constructed</span></span><br><span class="line">        graph = defaultdict(<span class="built_in">set</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dfs search if a path exists from u to v</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">u, v</span>):</span><br><span class="line">            <span class="comment"># a path is found when to reach the target point v</span></span><br><span class="line">            <span class="keyword">if</span> u == v:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># mark start point as visited</span></span><br><span class="line">            visited.add(u)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># dfs search through each neighbor</span></span><br><span class="line">            <span class="keyword">for</span> neighbor <span class="keyword">in</span> graph[u]:</span><br><span class="line">                <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited <span class="keyword">and</span> dfs(neighbor, v):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># add each edge [u,v] to the graph</span></span><br><span class="line">        <span class="keyword">for</span> u, v <span class="keyword">in</span> edges:</span><br><span class="line">            <span class="comment"># a new visited is needed for each dfs search from u to v</span></span><br><span class="line">            visited = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># there exist a path from u to v in the existing graph, the cycle will be formed if new edge [u,v] is added</span></span><br><span class="line">            <span class="keyword">if</span> dfs(u, v):</span><br><span class="line">                <span class="keyword">return</span> [u, v]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># add edge [u,v] to graph</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                graph[u].add(v)</span><br><span class="line">                graph[v].add(u)</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-841-Keys-and-Rooms"><a href="#Leetcode-841-Keys-and-Rooms" class="headerlink" title="[Leetcode 841] Keys and Rooms"></a>[Leetcode 841] Keys and Rooms</h3><p>There are n rooms labeled from 0 to n - 1 and all the rooms are locked except for room 0. Your goal is to visit all the rooms. However, you cannot enter a locked room without having its key.</p>
<p>When you visit a room, you may find a set of distinct keys in it. Each key has a number on it, denoting which room it unlocks, and you can take all of them with you to unlock the other rooms.</p>
<p>Given an array rooms where rooms[i] is the set of keys that you can obtain if you visited room i, return true if you can visit all the rooms, or false otherwise.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: rooms = [[1],[2],[3],[]]</span><br><span class="line">Output: true</span><br><span class="line">Explanation: </span><br><span class="line">We visit room 0 and pick up key 1.</span><br><span class="line">We then visit room 1 and pick up key 2.</span><br><span class="line">We then visit room 2 and pick up key 3.</span><br><span class="line">We then visit room 3.</span><br><span class="line">Since we were able to visit every room, we return true.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: rooms = [[1,3],[3,0,1],[2],[0]]</span><br><span class="line">Output: false</span><br><span class="line">Explanation: We can not enter room number 2 since the only key that unlocks it is in that room.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>n &#x3D;&#x3D; rooms.length</li>
<li>2 &lt;&#x3D; n &lt;&#x3D; 1000</li>
<li>0 &lt;&#x3D; rooms[i].length &lt;&#x3D; 1000</li>
<li>1 &lt;&#x3D; sum(rooms[i].length) &lt;&#x3D; 3000</li>
<li>0 &lt;&#x3D; rooms[i][j] &lt; n</li>
<li>All the values of rooms[i] are unique.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">canVisitAllRooms</span>(<span class="params">self, rooms: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        visited = [<span class="literal">False</span>] * <span class="built_in">len</span>(rooms)</span><br><span class="line">        visited[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">roomIdx</span>):</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> rooms[roomIdx]:</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> visited[idx]:</span><br><span class="line">                    visited[idx] = <span class="literal">True</span></span><br><span class="line">                    dfs(idx)</span><br><span class="line"></span><br><span class="line">        dfs(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">all</span>(visited)</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-1466-Reorder-Routes-to-Make-All-Paths-Lead-to-the-City-Zero"><a href="#Leetcode-1466-Reorder-Routes-to-Make-All-Paths-Lead-to-the-City-Zero" class="headerlink" title="[Leetcode 1466] Reorder Routes to Make All Paths Lead to the City Zero"></a>[Leetcode 1466] Reorder Routes to Make All Paths Lead to the City Zero</h3><p>There are n cities numbered from 0 to n - 1 and n - 1 roads such that there is only one way to travel between two different cities (this network form a tree). Last year, The ministry of transport decided to orient the roads in one direction because they are too narrow.</p>
<p>Roads are represented by connections where connections[i] &#x3D; [ai, bi] represents a road from city ai to city bi.</p>
<p>This year, there will be a big event in the capital (city 0), and many people want to travel to this city.</p>
<p>Your task consists of reorienting some roads such that each city can visit the city 0. Return the minimum number of edges changed.</p>
<p>It’s guaranteed that each city can reach city 0 after reorder.</p>
<p>Example 1:</p>
<p><img src="/images/leetcode_1466_1.png" alt="Image"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 6, connections = [[0,1],[1,3],[2,3],[4,0],[4,5]]</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: Change the direction of edges show in red such that each node can reach the node 0 (capital).</span><br></pre></td></tr></table></figure>

<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minReorder</span>(<span class="params">self, n: <span class="built_in">int</span>, connections: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># create graph with adjacent list</span></span><br><span class="line">        <span class="comment"># mark the connection as 1 if the original connection is from parent to child</span></span><br><span class="line">        <span class="comment"># mark the connection as 0 if the original connection is from child to parent </span></span><br><span class="line">        graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">for</span> conn <span class="keyword">in</span> connections:</span><br><span class="line">            graph[conn[<span class="number">0</span>]].append((conn[<span class="number">1</span>],<span class="number">1</span>))</span><br><span class="line">            graph[conn[<span class="number">1</span>]].append((conn[<span class="number">0</span>],<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(graph)</span><br><span class="line"></span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># dfs search from root to its subtrees</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node, parentVal</span>):</span><br><span class="line">            <span class="keyword">for</span> childNode <span class="keyword">in</span> graph[node]:</span><br><span class="line">                childVal = childNode[<span class="number">0</span>]</span><br><span class="line">                parentToChild = childNode[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># only extend the path from parent to child</span></span><br><span class="line">                <span class="keyword">if</span> childVal != parentVal:</span><br><span class="line">                    <span class="comment"># flip the connection only if original connection is from parent to child</span></span><br><span class="line">                    self.count += parentToChild</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># extend the path</span></span><br><span class="line">                    dfs(childVal, node)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># treat the graph as a tree rooted with node 0</span></span><br><span class="line">        dfs(<span class="number">0</span>, -<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.count</span><br></pre></td></tr></table></figure>

<h2 id="BFS"><a href="#BFS" class="headerlink" title="BFS"></a>BFS</h2><h3 id="Leetcode-994-Rotting-Oranges"><a href="#Leetcode-994-Rotting-Oranges" class="headerlink" title="[Leetcode 994] Rotting Oranges"></a>[Leetcode 994] Rotting Oranges</h3><p>You are given an m x n grid where each cell can have one of three values:</p>
<ul>
<li>0 representing an empty cell,</li>
<li>1 representing a fresh orange, or</li>
<li>2 representing a rotten orange.</li>
</ul>
<p>Every minute, any fresh orange that is 4-directionally adjacent to a rotten orange becomes rotten.</p>
<p>Return the minimum number of minutes that must elapse until no cell has a fresh orange. If this is impossible, return -1.</p>
<p>Example 1:</p>
<p><img src="/images/leetcode_994_1.png" alt="Image"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: grid = [[2,1,1],[1,1,0],[0,1,1]]</span><br><span class="line">Output: 4</span><br></pre></td></tr></table></figure>

<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">orangesRotting</span>(<span class="params">self, grid: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># init a queue with rotten oranges</span></span><br><span class="line">        qu = deque()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>])):</span><br><span class="line">                <span class="keyword">if</span> grid[i][j] == <span class="number">2</span>:</span><br><span class="line">                    qu.append((i,j))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4 directions to rotten</span></span><br><span class="line">        directions = [(-<span class="number">1</span>,<span class="number">0</span>),(<span class="number">1</span>,<span class="number">0</span>),(<span class="number">0</span>,-<span class="number">1</span>),(<span class="number">0</span>,<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        minutes = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> qu:</span><br><span class="line">            <span class="comment"># number of rotten oranges</span></span><br><span class="line">            n = <span class="built_in">len</span>(qu)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># rotten fresh oranges very minute</span></span><br><span class="line">            rotten = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                first = qu.popleft()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># rotten fresh oranges of current rotten orange</span></span><br><span class="line">                <span class="keyword">for</span> dx, dy <span class="keyword">in</span> directions:</span><br><span class="line">                    x, y = first[<span class="number">0</span>] + dx, first[<span class="number">1</span>] + dy</span><br><span class="line">                    <span class="keyword">if</span> <span class="number">0</span> &lt;= x &lt; <span class="built_in">len</span>(grid) <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; <span class="built_in">len</span>(grid[<span class="number">0</span>]) <span class="keyword">and</span> grid[x][y] == <span class="number">1</span>:</span><br><span class="line">                        grid[x][y] = <span class="number">2</span> <span class="comment"># rotten it</span></span><br><span class="line">                        qu.append((x,y)) <span class="comment"># add the new rotten orange to queue</span></span><br><span class="line">                        rotten = <span class="literal">True</span> <span class="comment"># set rotten flag</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># mark current rotten orange as visited</span></span><br><span class="line">                grid[first[<span class="number">0</span>]][first[<span class="number">1</span>]] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># add minutes only if there is new rotten oranges</span></span><br><span class="line">            <span class="keyword">if</span> rotten:</span><br><span class="line">                minutes += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># check if there is still fresh orange which can&#x27;t be rotten</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(grid[<span class="number">0</span>])):</span><br><span class="line">                <span class="keyword">if</span> grid[i][j] == <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> minutes</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-1926-Nearest-Exit-from-Entrance-in-Maze"><a href="#Leetcode-1926-Nearest-Exit-from-Entrance-in-Maze" class="headerlink" title="[Leetcode 1926] Nearest Exit from Entrance in Maze"></a>[Leetcode 1926] Nearest Exit from Entrance in Maze</h3><p>You are given an m x n matrix maze (0-indexed) with empty cells (represented as ‘.’) and walls (represented as ‘+’). You are also given the entrance of the maze, where entrance &#x3D; [entrancerow, entrancecol] denotes the row and column of the cell you are initially standing at.</p>
<p>In one step, you can move one cell up, down, left, or right. You cannot step into a cell with a wall, and you cannot step outside the maze. Your goal is to find the nearest exit from the entrance. An exit is defined as an empty cell that is at the border of the maze. The entrance does not count as an exit.</p>
<p>Return the number of steps in the shortest path from the entrance to the nearest exit, or -1 if no such path exists.</p>
<p>Example 1:</p>
<p><img src="/images/leetcode_1926_1.png" alt="Image"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: maze = [[&quot;+&quot;,&quot;+&quot;,&quot;.&quot;,&quot;+&quot;],[&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;+&quot;],[&quot;+&quot;,&quot;+&quot;,&quot;+&quot;,&quot;.&quot;]], entrance = [1,2]</span><br><span class="line">Output: 1</span><br><span class="line">Explanation: There are 3 exits in this maze at [1,0], [0,2], and [2,3].</span><br><span class="line">Initially, you are at the entrance cell [1,2].</span><br><span class="line">- You can reach [1,0] by moving 2 steps left.</span><br><span class="line">- You can reach [0,2] by moving 1 step up.</span><br><span class="line">It is impossible to reach [2,3] from the entrance.</span><br><span class="line">Thus, the nearest exit is [0,2], which is 1 step away.</span><br></pre></td></tr></table></figure>

<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">nearestExit</span>(<span class="params">self, maze: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]], entrance: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        moves = [(-<span class="number">1</span>,<span class="number">0</span>), (<span class="number">1</span>,<span class="number">0</span>), (<span class="number">0</span>,<span class="number">1</span>), (<span class="number">0</span>,-<span class="number">1</span>)]</span><br><span class="line">        qu = deque()</span><br><span class="line">        qu.append(entrance)</span><br><span class="line">        visited = <span class="built_in">set</span>(<span class="built_in">tuple</span>(entrance))  <span class="comment"># unhashable type: &#x27;list&#x27;</span></span><br><span class="line"></span><br><span class="line">        levels = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> qu:</span><br><span class="line">            <span class="comment"># traverse the current level</span></span><br><span class="line">            levelCells = <span class="built_in">len</span>(qu)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(levelCells):</span><br><span class="line">                first = qu.popleft()</span><br><span class="line">                currX, currY = first[<span class="number">0</span>], first[<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> currX == <span class="number">0</span> <span class="keyword">or</span> currX == <span class="built_in">len</span>(maze) - <span class="number">1</span> <span class="keyword">or</span> currY == <span class="number">0</span> <span class="keyword">or</span> currY == <span class="built_in">len</span>(maze[<span class="number">0</span>]) - <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">if</span> maze[currX][currY] == <span class="string">&#x27;.&#x27;</span> <span class="keyword">and</span> [currX, currY] != entrance:</span><br><span class="line">                        <span class="comment"># found nearest exit</span></span><br><span class="line">                        <span class="keyword">return</span> levels</span><br><span class="line"></span><br><span class="line">                <span class="comment"># bfs search next level(4 directions) for current cell</span></span><br><span class="line">                <span class="keyword">for</span> dx, dy <span class="keyword">in</span> moves:</span><br><span class="line">                    x, y = first[<span class="number">0</span>] + dx, first[<span class="number">1</span>] + dy</span><br><span class="line">                    <span class="keyword">if</span>  <span class="number">0</span> &lt;= x &lt; <span class="built_in">len</span>(maze) <span class="keyword">and</span> <span class="number">0</span> &lt;= y &lt; <span class="built_in">len</span>(maze[<span class="number">0</span>]) <span class="keyword">and</span> maze[x][y] == <span class="string">&#x27;.&#x27;</span> <span class="keyword">and</span> (x,y) <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                        visited.add((x,y))</span><br><span class="line">                        qu.append([x,y])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># move to next level</span></span><br><span class="line">            levels += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>   </span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
        <tag>Graph</tag>
        <tag>DFS</tag>
        <tag>BFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Binary search tree in Python</title>
    <url>/blog/binary-search-tree-in-python/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>Binary Search Tree is a binary tree data structure which has the following properties:</p>
<ul>
<li>The left subtree of a node contains only nodes with values smaller than the node.</li>
<li>The right subtree of a node contains only nodes with values greater than the node.</li>
<li>The left and right subtree each must also be a binary search tree.<span id="more"></span></li>
</ul>
<p>Based on the properties, we can implement a algorithm to check if a binary tree is binary search tree.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BST</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">Node</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val</span>):</span><br><span class="line">            self.val = val</span><br><span class="line">            self.left = <span class="literal">None</span></span><br><span class="line">            self.right = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># left most node is minimum</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minVal</span>(<span class="params">node</span>):</span><br><span class="line">        current = node</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">while</span> current.left != <span class="literal">None</span>:</span><br><span class="line">            current = current.left</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> current.val</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># right most node is maximum</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxVal</span>(<span class="params">node</span>):</span><br><span class="line">        current = node</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">while</span> current.right != <span class="literal">None</span>:</span><br><span class="line">            current = current.right</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> current.val</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># not efficient since some nodes are traversed multiple times</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isBst</span>(<span class="params">node</span>):</span><br><span class="line">        <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> node.left <span class="keyword">and</span> node.val &lt; minVal(node.left):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> node.right <span class="keyword">and</span> node.val &gt; maxVal(node.right):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">not</span> isBst(node.left)) <span class="keyword">or</span> (<span class="keyword">not</span> isBst(node.right)):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isBstOptimal</span>(<span class="params">node, <span class="built_in">min</span>, <span class="built_in">max</span></span>):</span><br><span class="line">        <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> node.val &lt; <span class="built_in">min</span> <span class="keyword">or</span> node.val &gt; <span class="built_in">max</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> isBstOptimal(node.left, <span class="built_in">min</span>, node.val) <span class="keyword">and</span> isBstOptimal(</span><br><span class="line">            node.right, node.val, <span class="built_in">max</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">binarySearch</span>(<span class="params">node, target</span>):</span><br><span class="line">        <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> node.val == target:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> node.val &gt; target:</span><br><span class="line">            <span class="keyword">return</span> binarySearch(node.left, target)</span><br><span class="line">        <span class="keyword">elif</span> node.val &lt; target:</span><br><span class="line">            <span class="keyword">return</span> binarySearch(node.right, target)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#          6</span></span><br><span class="line">    <span class="comment">#        /   \</span></span><br><span class="line">    <span class="comment">#       4     8</span></span><br><span class="line">    <span class="comment">#      / \   / \</span></span><br><span class="line">    <span class="comment">#     2   5 7   9</span></span><br><span class="line">    <span class="comment">#    / \</span></span><br><span class="line">    <span class="comment">#   1   3</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    root = Node(<span class="number">6</span>)</span><br><span class="line">    n1 = Node(<span class="number">4</span>)</span><br><span class="line">    n2 = Node(<span class="number">8</span>)</span><br><span class="line">    root.left = n1</span><br><span class="line">    root.right = n2</span><br><span class="line">    n3 = Node(<span class="number">2</span>)</span><br><span class="line">    n4 = Node(<span class="number">5</span>)</span><br><span class="line">    n1.left = n3</span><br><span class="line">    n1.right = n4</span><br><span class="line">    n5 = Node(<span class="number">7</span>)</span><br><span class="line">    n6 = Node(<span class="number">9</span>)</span><br><span class="line">    n2.left = n5</span><br><span class="line">    n2.right = n6</span><br><span class="line">    n7 = Node(<span class="number">1</span>)</span><br><span class="line">    n8 = Node(<span class="number">3</span>)</span><br><span class="line">    n3.left = n7</span><br><span class="line">    n3.right = n8</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(isBst(root))</span><br><span class="line">    <span class="built_in">print</span>(isBstOptimal(root, -sys.maxsize, sys.maxsize))</span><br><span class="line">    </span><br><span class="line">    n9 = Node(<span class="number">9</span>)</span><br><span class="line">    n7.left = n9</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(isBst(root))</span><br><span class="line">    <span class="built_in">print</span>(isBstOptimal(root, -sys.maxsize, sys.maxsize))</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(binarySearch(root, <span class="number">8</span>))</span><br><span class="line">    <span class="built_in">print</span>(binarySearch(root, <span class="number">11</span>))</span><br><span class="line">    <span class="built_in">print</span>(binarySearch(root, -<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-108-Convert-Sorted-Array-to-Binary-Search-Tree"><a href="#Leetcode-108-Convert-Sorted-Array-to-Binary-Search-Tree" class="headerlink" title="[Leetcode 108] Convert Sorted Array to Binary Search Tree"></a>[Leetcode 108] Convert Sorted Array to Binary Search Tree</h2><p>Given an integer array nums where the elements are sorted in ascending order, convert it to a<br>height-balanced binary search tree.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">         0</span><br><span class="line">        / \</span><br><span class="line">      -3   9</span><br><span class="line">      /   /</span><br><span class="line">    -10  5</span><br><span class="line">Input: nums = [-10,-3,0,5,9]</span><br><span class="line">Output: [0,-3,9,-10,null,5]</span><br><span class="line">Explanation: [0,-10,5,null,-3,null,9] is also accepted:</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    3   1</span><br><span class="line">   /     \</span><br><span class="line">  1       3</span><br><span class="line">Input: nums = [1,3]</span><br><span class="line">Output: [3,1]</span><br><span class="line">Explanation: [1,null,3] and [3,1] are both height-balanced BSTs.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; nums.length &lt;&#x3D; 10^4</li>
<li>-10^4 &lt;&#x3D; nums[i] &lt;&#x3D; 10^4</li>
<li>nums is sorted in a strictly increasing order.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">constructTree</span>(<span class="params">self, nums, low, high</span>):</span><br><span class="line">        <span class="keyword">if</span> low &gt; high:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        mid = low + (high - low) // <span class="number">2</span></span><br><span class="line">        root = TreeNode(nums[mid])</span><br><span class="line">        root.left = self.constructTree(nums, low, mid - <span class="number">1</span>)</span><br><span class="line">        root.right = self.constructTree(nums, mid + <span class="number">1</span>, high)</span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sortedArrayToBST</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">return</span> self.constructTree(nums, <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-173-Binary-Search-Tree-Iterator"><a href="#Leetcode-173-Binary-Search-Tree-Iterator" class="headerlink" title="[Leetcode 173] Binary Search Tree Iterator"></a>[Leetcode 173] Binary Search Tree Iterator</h2><p>Implement the BSTIterator class that represents an iterator over the in-order traversal of a binary search tree (BST):</p>
<ul>
<li>BSTIterator(TreeNode root) Initializes an object of the BSTIterator class. The root of the BST is given as part of the constructor. The pointer should be initialized to a non-existent number smaller than any element in the BST.</li>
<li>boolean hasNext() Returns true if there exists a number in the traversal to the right of the pointer, otherwise returns false.</li>
<li>int next() Moves the pointer to the right, then returns the number at the pointer.</li>
</ul>
<p>Notice that by initializing the pointer to a non-existent smallest number, the first call to next() will return the smallest element in the BST.</p>
<p>You may assume that next() calls will always be valid. That is, there will be at least a next number in the in-order traversal when next() is called.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        3</span><br><span class="line">      /   \ </span><br><span class="line">     7    15</span><br><span class="line">         /  \</span><br><span class="line">        9   20 </span><br><span class="line">Input</span><br><span class="line">[&quot;BSTIterator&quot;, &quot;next&quot;, &quot;next&quot;, &quot;hasNext&quot;, &quot;next&quot;, &quot;hasNext&quot;, &quot;next&quot;, &quot;hasNext&quot;, &quot;next&quot;, &quot;hasNext&quot;]</span><br><span class="line">[[[7, 3, 15, null, null, 9, 20]], [], [], [], [], [], [], [], [], []]</span><br><span class="line">Output</span><br><span class="line">[null, 3, 7, true, 9, true, 15, true, 20, false]</span><br><span class="line"></span><br><span class="line">Explanation</span><br><span class="line">BSTIterator bSTIterator = new BSTIterator([7, 3, 15, null, null, 9, 20]);</span><br><span class="line">bSTIterator.next();    // return 3</span><br><span class="line">bSTIterator.next();    // return 7</span><br><span class="line">bSTIterator.hasNext(); // return True</span><br><span class="line">bSTIterator.next();    // return 9</span><br><span class="line">bSTIterator.hasNext(); // return True</span><br><span class="line">bSTIterator.next();    // return 15</span><br><span class="line">bSTIterator.hasNext(); // return True</span><br><span class="line">bSTIterator.next();    // return 20</span><br><span class="line">bSTIterator.hasNext(); // return False</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>The number of nodes in the tree is in the range [1, 10^5].</li>
<li>0 &lt;&#x3D; Node.val &lt;&#x3D; 10^6</li>
<li>At most 10^5 calls will be made to hasNext, and next.</li>
</ul>
<p>Follow up:</p>
<ul>
<li>Could you implement next() and hasNext() to run in average O(1) time and use O(h) memory, where h is the height of the tree?</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BSTIterator_bf</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>):</span><br><span class="line">        self.currIndex = <span class="number">0</span></span><br><span class="line">        self.data = []</span><br><span class="line">        self.inOrder(root)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inOrder</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        self.inOrder(root.left)</span><br><span class="line">        self.data.append(root.val)</span><br><span class="line">        self.inOrder(root.right)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">next</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        val = self.data[self.currIndex]</span><br><span class="line">        self.currIndex += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> val</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasNext</span>(<span class="params">self</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">return</span> self.currIndex &lt;= <span class="built_in">len</span>(self.data) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BSTIterator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>):</span><br><span class="line">        self.st = []</span><br><span class="line">        self.traverse(root) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">traverse</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="keyword">while</span> root != <span class="literal">None</span>:</span><br><span class="line">            self.st.append(root)</span><br><span class="line">            root = root.left</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">next</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        node = self.st.pop()</span><br><span class="line">        self.traverse(node.right)</span><br><span class="line">        <span class="keyword">return</span> node.val</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasNext</span>(<span class="params">self</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.st) &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Your BSTIterator object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"># obj = BSTIterator(root)</span></span><br><span class="line"><span class="comment"># param_1 = obj.next()</span></span><br><span class="line"><span class="comment"># param_2 = obj.hasNext()</span></span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-230-Kth-Smallest-Element-in-a-BST"><a href="#Leetcode-230-Kth-Smallest-Element-in-a-BST" class="headerlink" title="[Leetcode 230] Kth Smallest Element in a BST"></a>[Leetcode 230] Kth Smallest Element in a BST</h2><p>Given the root of a binary search tree, and an integer k, return the kth smallest value (1-indexed) of all the values of the nodes in the tree.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">       3</span><br><span class="line">     /   \</span><br><span class="line">    1     4</span><br><span class="line">     \</span><br><span class="line">      2</span><br><span class="line"></span><br><span class="line">Input: root = [3,1,4,null,2], k = 1</span><br><span class="line">Output: 1</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>The number of nodes in the tree is n.</li>
<li>1 &lt;&#x3D; k &lt;&#x3D; n &lt;&#x3D; 104</li>
<li>0 &lt;&#x3D; Node.val &lt;&#x3D; 104</li>
</ul>
<p>Follow up: If the BST is modified often (i.e., we can do insert and delete operations) and you need to find the kth smallest frequently, how would you optimize?</p>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kthSmallest1</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># create a stack to store the nodes</span></span><br><span class="line">        stack = []</span><br><span class="line">        <span class="comment"># start at the root of the tree</span></span><br><span class="line">        current = root</span><br><span class="line"></span><br><span class="line">        <span class="comment"># loop until we have processed all nodes and found the kth smallest value</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># traverse as far left as possible from the current node, adding each node to the stack</span></span><br><span class="line">            <span class="keyword">while</span> current <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                stack.append(current)</span><br><span class="line">                current = current.left</span><br><span class="line"></span><br><span class="line">            <span class="comment"># if the stack is empty, we have processed all nodes and can exit the loop</span></span><br><span class="line">            <span class="comment"># if not stack:</span></span><br><span class="line">            <span class="comment">#    break</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># pop the top node off the stack (which is the next smallest node) and decrement k</span></span><br><span class="line">            node = stack.pop()</span><br><span class="line">            k -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># if k is 0, we have found the kth smallest value and can return it</span></span><br><span class="line">            <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> node.val</span><br><span class="line"></span><br><span class="line">            <span class="comment"># set the current node to the right child of the node we just processed</span></span><br><span class="line">            current = node.right</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">traverseLeft</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="keyword">while</span> root != <span class="literal">None</span>:</span><br><span class="line">            self.st.append(root)</span><br><span class="line">            root = root.left</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">kthSmallest</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        self.st = []</span><br><span class="line"></span><br><span class="line">        self.traverseLeft(root)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># get next smallest node</span></span><br><span class="line">            node = self.st.pop()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if this is the kth smallest node</span></span><br><span class="line">            k -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> node.val</span><br><span class="line"></span><br><span class="line">            <span class="comment"># traverse right subtree before go back to parent</span></span><br><span class="line">            node = node.right</span><br><span class="line">            self.traverseLeft(node)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Binary Search</tag>
      </tags>
  </entry>
  <entry>
    <title>Big Red Book Report</title>
    <url>/blog/big-red-book-report/</url>
    <content><![CDATA[<p>Danny Pickett is a man who is too poor to afford one of Mr. Haggins champion Irish Setters. However, he is drawn close to a dog called Big Red. Their friendship builds up as they know each other. Their loyalty and courage are tested during various hunts such as when they fought against Old Majesty, a massive bear.<span id="more"></span></p>
<p>The setting of Big Red was in the woods of Wintapi, near New York. The plot can be determined because it is related to the setting. Since the setting is in the wilderness, the plot might be about the wild and the living things in the wild such as bears. It can hint what the plot might be about. The setting is where the plot takes place so it would emphasize what the overall problem is about. The plot in Big Red is about a massive bear and Danny Pickett for the setting is the wilderness in which wild animals live in.</p>
<p>The plot of Big Red was whether or not Danny and Red would build a bond between them and safely protect the Wintapi woods from wild animals and other dangers. The inciting incident was when Mr. Haggins let Danny take care of Big Red. However, when Danny took Red outside, he figured out that the Irish setter was not fit to be a varmint dog because Red showed that he liked chasing squirrels, rabbits, and birds. Seeing this, Danny decided to train Red to be a partridge dog. The result of the inciting incident was that Danny lost hope in Red becoming a varmint dog, and made him only hunt little brown birds . However, during Red’s fight with the wolverine, Danny saw that the big Irish setter was loyal and would risk his life to protect him. Not only was Red a natural Patridge hunter, he was also brave and loyal. The climax of Big Red was when Danny and Red chased after Old Majesty for Ross. Danny learnt that he could do anything if he put his mind to it. He also realized that Red had so much heart that he cannot get rid of him. So it forced him to tell Mr. Haggins that he was keeping Red. The inciting incident and climax related because Red hunted down the bear for Ross, who was the most important person to Danny. Some major events could be Asa,the mule and the hounds being killed by Old Majesty, or Ross getting injured. These events forced Danny and Red to go out in the night to hunt down Old Majesty, the undefeated king of the Wintapi wilderness, because they needed to get revenge for Ross, the hounds, and Asa. With bravery, they killed Old Majesty.</p>
<p>The main characters in the story Big Red were Danny Picket and Red. Danny is Ross’s son. He learnt his trapping skills from Ross. At the beginning of the book, he did not have a lot of confidence in himself. But he became a man after killing Old Majesty. He learnt that he could do anything if he put his mind to it. Big Red was Mr. Haggins’s champion Irish setter. He became a real champion in the dog shows and was Danny’s best friend. Danny and Red impacted the story because they were the ones that solved the problem and got revenge for Ross. The main protagonist was Danny Picket. He learnt how to have confidence in himself and became a man at the end of the book. The main antagonist was Old Majesty, the massive bear, because he was the enemy of everyone in the Wintapi forest. It was everyone’s goal to kill him for good. Old Majesty killed many people and animals but finally Danny Picket succeeded in killing the bear. A key supporting character is Ross. Ross was Danny’s father. He had three varmint hounds he liked to hunt with. He knew a lot about the wilderness ,trapping, and hunting. He taught Danny his ways of life and wanted him to become a hunter too. His personality changed a lot in the book. At first, Ross wanted Red to be a varmint dog. He thought that Danny did not know what was best for Red. But in the end he gained respect for Danny and wanted Red to be a partridge dog. Another key supporting character is Mr. Haggins. He was a very rich man and owned the Wintapi estate. He was the real owner of Red. He let Danny be his caretaker for the dog because he knew Danny knew a lot about dogs. At the end of the book, he gave Red to Danny for free because he saw how Danny’s and Red’s relationship changed him into a better person. Danny would be the best Challenger student because he was well disciplined and honest. Old Majesty would be the worst Challenger student because he was ill disciplined and full of hatred. He was the boss of himself and cared about nobody besides himself.</p>
<p>The major themes of Big red were friendship, loyalty, courage, and doing what you believe was right. Red and Danny had a strong friendship that bonded them together. They helped each other when they were in trouble. One example was when Danny was hurt and a cat varmint was nearby stalking him. Red barked ferociously and scared the varmint away. Danny’s and Red’s loyalty and courage was tested when they faced the bear and Danny was brave enough to tell Mr. Haggins the truth about Red being injured. He did what he believed was right when he knew that he had to kill Old Majesty. He did not hesitate and took action.</p>
<p>This book was about Danny Picket and Red bonding with each other. Danny became more manly along the way. His loyalty with Red was tested as they survived the fierce snowstorm and killed Old Majesty. My favorite part of the story was when Big Red killed the bear and got revenge for Ross because it showed that he was brave. I learnt that friendship was very important and that you could do anything if you put your mind to it. My least favorite part was when the hounds died because they were veterans of many battles and were finally killed by a bear. It was very sad. I would recommend this book because it showed the value of friendship and having confidence in yourself.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Binary search</title>
    <url>/blog/binary-search/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>Binary Search splits the search space into two halves and only keep the half that has the search target and throw away the other half that would not have the target. In each step, the search space is reduced to half, until the target is found. Binary Search reduces the search time from linear O(n) to logarithmic O(log n).<span id="more"></span></p>
<p><img src="/images/binarysearch.gif"></p>
<h2 id="Generalized-binary-search-template"><a href="#Generalized-binary-search-template" class="headerlink" title="Generalized binary search template"></a>Generalized binary search template</h2><p>For an array(not only for numbers) sorted in ascending order, the following is the generalized binary search template to solve many problems.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binary_search</span>(<span class="params">array</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    array.sort()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">condition</span>(<span class="params">value</span>) -&gt; <span class="built_in">bool</span>: <span class="comment"># the condition should be satisfied to get the result</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(array)  <span class="comment"># search space</span></span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        mid = left + (right - left) // <span class="number">2</span> <span class="comment"># try to avoid overflow</span></span><br><span class="line">        <span class="keyword">if</span> condition(mid): </span><br><span class="line">            right = mid</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> left <span class="comment"># return value could be left or left-1</span></span><br></pre></td></tr></table></figure>

<p>Note:</p>
<ul>
<li>The left and right are the boundary to include all possible elements(aka search space)</li>
<li>Is return value left or left - 1? Remember this: after exiting the while loop, left is the minimal k​ satisfying the condition function</li>
<li>Design the condition function. Let’s get more sense with the following practices.</li>
</ul>
<p>Thanks to <strong>zhijun_liao</strong> who wrote this useful binary search <a href="https://leetcode.com/problems/find-k-th-smallest-pair-distance/solutions/769705/python-clear-explanation-powerful-ultimate-binary-search-template-solved-many-problems/">article</a>.</p>
<h2 id="Basic-application"><a href="#Basic-application" class="headerlink" title="Basic application"></a>Basic application</h2><h3 id="Leetcode-35-Search-Insert-Position"><a href="#Leetcode-35-Search-Insert-Position" class="headerlink" title="[Leetcode 35] Search Insert Position"></a>[Leetcode 35] Search Insert Position</h3><p>Given a sorted array of distinct integers and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order.</p>
<p>You must write an algorithm with O(log n) runtime complexity.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,3,5,6], target = 5</span><br><span class="line">Output: 2</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,3,5,6], target = 2</span><br><span class="line">Output: 1</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,3,5,6], target = 7</span><br><span class="line">Output: 4</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; nums.length &lt;&#x3D; 104</li>
<li>-104 &lt;&#x3D; nums[i] &lt;&#x3D; 104</li>
<li>nums contains distinct values sorted in ascending order.</li>
<li>-104 &lt;&#x3D; target &lt;&#x3D; 104</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">searchInsert</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># Note that the target element can be larger than all the elementes in nums and it would be inserted to the end</span></span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="comment"># looking for the minimal k value satisfying nums[k] &gt;= target</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &gt;= target:</span><br><span class="line">                right = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> left</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-69-Sqrt-x"><a href="#Leetcode-69-Sqrt-x" class="headerlink" title="[Leetcode 69] Sqrt(x)"></a>[Leetcode 69] Sqrt(x)</h3><p>Given a non-negative integer x, return the square root of x rounded down to the nearest integer. The returned integer should be non-negative as well.</p>
<p>You must not use any built-in exponent function or operator.</p>
<p>For example, do not use pow(x, 0.5) in c++ or x ** 0.5 in python.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: x = 4</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: The square root of 4 is 2, so we return 2.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: x = 8</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: The square root of 8 is 2.82842..., and since we round it down to the nearest integer, 2 is returned.</span><br></pre></td></tr></table></figure>
<p>Constraints:</p>
<ul>
<li>0 &lt;&#x3D; x &lt;&#x3D; 231 - 1</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mySqrt</span>(<span class="params">self, x: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> x &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">        left, right = <span class="number">0</span>, x</span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = left + (right - left )// <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> mid * mid &lt;= x:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right = mid</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> left - <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-278-First-Bad-Version"><a href="#Leetcode-278-First-Bad-Version" class="headerlink" title="[Leetcode 278] First Bad Version"></a>[Leetcode 278] First Bad Version</h3><p>You are a product manager and currently leading a team to develop a new product. Unfortunately, the latest version of your product fails the quality check. Since each version is developed based on the previous version, all the versions after a bad version are also bad.</p>
<p>Suppose you have n versions [1, 2, …, n] and you want to find out the first bad one, which causes all the following ones to be bad.</p>
<p>You are given an API bool isBadVersion(version) which returns whether version is bad. Implement a function to find the first bad version. You should minimize the number of calls to the API.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 5, bad = 4</span><br><span class="line">Output: 4</span><br><span class="line">Explanation:</span><br><span class="line">call isBadVersion(3) -&gt; false</span><br><span class="line">call isBadVersion(5) -&gt; true</span><br><span class="line">call isBadVersion(4) -&gt; true</span><br><span class="line">Then 4 is the first bad version.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 1, bad = 1</span><br><span class="line">Output: 1</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; bad &lt;&#x3D; n &lt;&#x3D; 231 - 1</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">firstBadVersion</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># search boundary: [1,n]</span></span><br><span class="line">        left, right = <span class="number">1</span>, n</span><br><span class="line"></span><br><span class="line">        <span class="comment"># exit condition: left == right</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            <span class="comment"># reduce possibility of overflow</span></span><br><span class="line">            mid = left + (right - left) // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># condition function: isBadVersion()</span></span><br><span class="line">            <span class="keyword">if</span> isBadVersion(mid):</span><br><span class="line">                <span class="comment"># include mid in the next round of search since it could be the first bad version</span></span><br><span class="line">                right = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># exclude mid from the next round of search since it&#x27;s NOT bad version</span></span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># left is the minimum value to satisfy condition function</span></span><br><span class="line">        <span class="keyword">return</span> left</span><br></pre></td></tr></table></figure>

<h2 id="Advanced-application"><a href="#Advanced-application" class="headerlink" title="Advanced application"></a>Advanced application</h2><h3 id="Leetcode-668-Kth-Smallest-Number-in-Multiplication-Table"><a href="#Leetcode-668-Kth-Smallest-Number-in-Multiplication-Table" class="headerlink" title="[Leetcode 668] Kth Smallest Number in Multiplication Table"></a>[Leetcode 668] Kth Smallest Number in Multiplication Table</h3><p>Nearly everyone has used the <a href="https://en.wikipedia.org/wiki/Multiplication_table">Multiplication Table</a>. The multiplication table of size m x n is an integer matrix mat where mat[i][j] &#x3D;&#x3D; i * j (1-indexed).</p>
<p>Given three integers m, n, and k, return the kth smallest element in the m x n multiplication table.</p>
<p>Example :</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: m = 3, n = 3, k = 5</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: </span><br><span class="line">The Multiplication Table:</span><br><span class="line">1	2	3</span><br><span class="line">2	4	6</span><br><span class="line">3	6	9</span><br><span class="line"></span><br><span class="line">The 5-th smallest number is 3 (1, 2, 2, 3, 3).</span><br></pre></td></tr></table></figure>

<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findKthNumber</span>(<span class="params">self, m: <span class="built_in">int</span>, n: <span class="built_in">int</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># Given an input num, determine if there&#x27;re at least k values &lt;= num.</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">hasKValuesAtLeast</span>(<span class="params">num</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">            <span class="comment"># total numbers &lt;= k</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="comment"># count row by row</span></span><br><span class="line">            <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">                <span class="comment"># each row has i numbers which are &lt;= num</span></span><br><span class="line">                i = <span class="built_in">min</span>(num // row, n)</span><br><span class="line">                <span class="comment"># exit early if no number &lt;= num</span></span><br><span class="line">                <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="comment"># accumulate the numbers count</span></span><br><span class="line">                count += i</span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if there are at least k numbers &lt;= given num</span></span><br><span class="line">            <span class="keyword">return</span> count &gt;= k</span><br><span class="line"></span><br><span class="line">        <span class="comment"># search range is [1,m*n]</span></span><br><span class="line">        left, right = <span class="number">1</span>, m * n</span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="comment"># look for the minimum number satisfying hasKValuesAtLeast function</span></span><br><span class="line">            <span class="keyword">if</span> hasKValuesAtLeast(mid):</span><br><span class="line">                right = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># the smallest number which has at least k number smaller than or equal to it</span></span><br><span class="line">        <span class="keyword">return</span> left</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-719-Find-K-th-Smallest-Pair-Distance"><a href="#Leetcode-719-Find-K-th-Smallest-Pair-Distance" class="headerlink" title="[Leetcode 719] Find K-th Smallest Pair Distance"></a>[Leetcode 719] Find K-th Smallest Pair Distance</h3><p>The distance of a pair of integers a and b is defined as the absolute difference between a and b.</p>
<p>Given an integer array nums and an integer k, return the kth smallest distance among all the pairs nums[i] and nums[j] where 0 &lt;&#x3D; i &lt; j &lt; nums.length.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,3,1], k = 1</span><br><span class="line">Output: 0</span><br><span class="line">Explanation: Here are all the pairs:</span><br><span class="line">(1,3) -&gt; 2</span><br><span class="line">(1,1) -&gt; 0</span><br><span class="line">(3,1) -&gt; 2</span><br><span class="line">Then the 1st smallest distance pair is (1,1), and its distance is 0.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,1,1], k = 2</span><br><span class="line">Output: 0</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,6,1], k = 3</span><br><span class="line">Output: 5</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>n &#x3D;&#x3D; nums.length</li>
<li>2 &lt;&#x3D; n &lt;&#x3D; 104</li>
<li>0 &lt;&#x3D; nums[i] &lt;&#x3D; 106</li>
<li>1 &lt;&#x3D; k &lt;&#x3D; n * (n - 1) &#x2F; 2</li>
</ul>
<p>Solution:</p>
<p>Brute force with priority queue:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">smallestDistancePair_time_limit_exceeded</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    minHeap = []</span><br><span class="line">    heapq.heapify(minHeap)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            d = <span class="built_in">abs</span>(nums[i] - nums[j])</span><br><span class="line">            heapq.heappush(minHeap, d)</span><br><span class="line"></span><br><span class="line">    kth = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        kth = heapq.heappop(minHeap)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> kth</span><br></pre></td></tr></table></figure>

<p>Binary search:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">smallestDistancePair</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        nums.sort()</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">hasKPairsAtleast</span>(<span class="params">distance</span>):</span><br><span class="line">            i, j, count = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; n <span class="keyword">or</span> j &lt; n:</span><br><span class="line">                <span class="comment"># if the distance for nums[i] and nums[j] is less than the given distance, the distance between nums[i] and nums[k](i&lt;k&lt;j) is also less than distance</span></span><br><span class="line">                <span class="keyword">while</span> j &lt; n <span class="keyword">and</span> nums[j] - nums[i] &lt;= distance:</span><br><span class="line">                    <span class="comment"># extend the range</span></span><br><span class="line">                    j += <span class="number">1</span></span><br><span class="line">                <span class="comment"># count valid pairs:</span></span><br><span class="line">                <span class="comment"># [nums[i],nums[i+1]], [nums[i],nums[i+2]],..., [nums[i],nums[j-1]]</span></span><br><span class="line">                count += j - i - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># modify the start index for the range</span></span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> count &gt;= k</span><br><span class="line"></span><br><span class="line">        <span class="comment"># search range is [0, nums[-1]-nums[0]]</span></span><br><span class="line">        left, right = <span class="number">0</span>, nums[-<span class="number">1</span>] - nums[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> hasKPairsAtleast(mid):</span><br><span class="line">                right = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> left</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-875-Koko-Eating-Bananas"><a href="#Leetcode-875-Koko-Eating-Bananas" class="headerlink" title="[Leetcode 875] Koko Eating Bananas"></a>[Leetcode 875] Koko Eating Bananas</h3><p>Koko loves to eat bananas. There are n piles of bananas, the ith pile has piles[i] bananas. The guards have gone and will come back in h hours.</p>
<p>Koko can decide her bananas-per-hour eating speed of k. Each hour, she chooses some pile of bananas and eats k bananas from that pile. If the pile has less than k bananas, she eats all of them instead and will not eat any more bananas during this hour.</p>
<p>Koko likes to eat slowly but still wants to finish eating all the bananas before the guards return.</p>
<p>Return the minimum integer k such that she can eat all the bananas within h hours.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: piles = [3,6,7,11], h = 8</span><br><span class="line">Output: 4</span><br></pre></td></tr></table></figure>

<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minEatingSpeed</span>(<span class="params">self, piles: <span class="type">List</span>[<span class="built_in">int</span>], h: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># check if it can finish eating with the given speed</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">canEatBananasInSpeed</span>(<span class="params">speed</span>):</span><br><span class="line">            hours = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> pile <span class="keyword">in</span> piles:</span><br><span class="line">                <span class="keyword">if</span> pile % speed:</span><br><span class="line">                    hours += pile // speed + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    hours += pile // speed</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> hours &lt;= h</span><br><span class="line"></span><br><span class="line">        <span class="comment"># search space is [1,max(piles)]</span></span><br><span class="line">        left, right = <span class="number">1</span>, <span class="built_in">max</span>(piles)</span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="comment"># continue to search slower speed if it can finish eating in the speed of &quot;mid&quot;</span></span><br><span class="line">            <span class="keyword">if</span> canEatBananasInSpeed(mid):</span><br><span class="line">                right = mid</span><br><span class="line">            <span class="comment"># it has to speed up since it can&#x27;t finish eating in the speed of &quot;mid&quot;</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># this will be slowest speed it can finish eating all bananas</span></span><br><span class="line">        <span class="keyword">return</span> left</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-1011-Capacity-To-Ship-Packages-Within-D-Days"><a href="#Leetcode-1011-Capacity-To-Ship-Packages-Within-D-Days" class="headerlink" title="[Leetcode 1011] Capacity To Ship Packages Within D Days"></a>[Leetcode 1011] Capacity To Ship Packages Within D Days</h3><p>A conveyor belt has packages that must be shipped from one port to another within days days.</p>
<p>The ith package on the conveyor belt has a weight of weights[i]. Each day, we load the ship with packages on the conveyor belt (in the order given by weights). We may not load more weight than the maximum weight capacity of the ship.</p>
<p>Return the least weight capacity of the ship that will result in all the packages on the conveyor belt being shipped within days days.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: weights = [1,2,3,4,5,6,7,8,9,10], days = 5</span><br><span class="line">Output: 15</span><br><span class="line">Explanation: A ship capacity of 15 is the minimum to ship all the packages in 5 days like this:</span><br><span class="line">1st day: 1, 2, 3, 4, 5</span><br><span class="line">2nd day: 6, 7</span><br><span class="line">3rd day: 8</span><br><span class="line">4th day: 9</span><br><span class="line">5th day: 10</span><br><span class="line"></span><br><span class="line">Note that the cargo must be shipped in the order given, so using a ship of capacity 14 and splitting the packages into parts like (2, 3, 4, 5), (1, 6, 7), (8), (9), (10) is not allowed.</span><br></pre></td></tr></table></figure>

<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shipWithinDays</span>(<span class="params">self, weights: <span class="type">List</span>[<span class="built_in">int</span>], days: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># check if the pacages can be shipped with given capacity in days</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">canShipPackages</span>(<span class="params">capacity</span>):</span><br><span class="line">            currentDayWeight = <span class="number">0</span>  <span class="comment"># the weight has been shipped in current day</span></span><br><span class="line">            countDays = <span class="number">1</span>  <span class="comment"># the number of days has been spent</span></span><br><span class="line">            <span class="keyword">for</span> weight <span class="keyword">in</span> weights:</span><br><span class="line">                currentDayWeight += weight</span><br><span class="line">                <span class="comment"># we need to wait for the next day due to out of capacity</span></span><br><span class="line">                <span class="keyword">if</span> currentDayWeight &gt; capacity:</span><br><span class="line">                    currentDayWeight = weight  <span class="comment"># reset</span></span><br><span class="line">                    countDays += <span class="number">1</span>  <span class="comment"># add days by 1</span></span><br><span class="line">                    <span class="keyword">if</span> countDays &gt; days:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># the minimum capacity must be in the range of [max(weights), sum(weights)]</span></span><br><span class="line">        left, right = <span class="built_in">max</span>(weights), <span class="built_in">sum</span>(weights)</span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="comment"># packages can be shipped with capacity &quot;mid&quot; and we continue to search smaller capacity</span></span><br><span class="line">            <span class="keyword">if</span> canShipPackages(mid):</span><br><span class="line">                right = mid</span><br><span class="line">            <span class="comment"># packages can not be shipped with capacity &quot;mid&quot; and we have to try bigger capacity which is mid+1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> left</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-1201-Ugly-Number-III"><a href="#Leetcode-1201-Ugly-Number-III" class="headerlink" title="[Leetcode 1201] Ugly Number III"></a>[Leetcode 1201] Ugly Number III</h3><p>An ugly number is a positive integer that is divisible by a, b, or c.</p>
<p>Given four integers n, a, b, and c, return the nth ugly number.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 3, a = 2, b = 3, c = 5</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: The ugly numbers are 2, 3, 4, 5, 6, 8, 9, 10... The 3rd is 4.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 4, a = 2, b = 3, c = 4</span><br><span class="line">Output: 6</span><br><span class="line">Explanation: The ugly numbers are 2, 3, 4, 6, 8, 9, 10, 12... The 4th is 6.</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 5, a = 2, b = 11, c = 13</span><br><span class="line">Output: 10</span><br><span class="line">Explanation: The ugly numbers are 2, 4, 6, 8, 10, 11, 12, 13... The 5th is 10.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; n, a, b, c &lt;&#x3D; 10^9</li>
<li>1 &lt;&#x3D; a * b * c &lt;&#x3D; 10^18</li>
<li>It is guaranteed that the result will be in range [1, 2 * 10^9].</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">nthUglyNumber</span>(<span class="params">self, n: <span class="built_in">int</span>, a: <span class="built_in">int</span>, b: <span class="built_in">int</span>, c: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        ab = a * b // math.gcd(a, b)</span><br><span class="line">        bc = b * c // math.gcd(b, c)</span><br><span class="line">        ac = a * c // math.gcd(a, c)</span><br><span class="line">        abc = a * bc // math.gcd(a, bc)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># check if there are at least n ugly numbers &lt;= num </span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">hasNUglyNumbersAtLeast</span>(<span class="params">num</span>):</span><br><span class="line">            uglyNums = num // a + num // b + num // c - \</span><br><span class="line">                num // ab - num // ac - num // bc + num // abc</span><br><span class="line">            <span class="keyword">return</span> uglyNums &gt;= n</span><br><span class="line"></span><br><span class="line">        left, right = <span class="number">1</span>, <span class="number">10</span>**<span class="number">10</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> hasNUglyNumbersAtLeast(mid):</span><br><span class="line">                right = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> left</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-1283-Find-the-Smallest-Divisor-Given-a-Threshold"><a href="#Leetcode-1283-Find-the-Smallest-Divisor-Given-a-Threshold" class="headerlink" title="[Leetcode 1283] Find the Smallest Divisor Given a Threshold"></a>[Leetcode 1283] Find the Smallest Divisor Given a Threshold</h3><p>Given an array of integers nums and an integer threshold, we will choose a positive integer divisor, divide all the array by it, and sum the division’s result. Find the smallest divisor such that the result mentioned above is less than or equal to threshold.</p>
<p>Each result of the division is rounded to the nearest integer greater than or equal to that element. (For example: 7&#x2F;3 &#x3D; 3 and 10&#x2F;2 &#x3D; 5).</p>
<p>The test cases are generated so that there will be an answer.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,2,5,9], threshold = 6</span><br><span class="line">Output: 5</span><br><span class="line">Explanation: We can get a sum to 17 (1+2+5+9) if the divisor is 1. </span><br><span class="line">If the divisor is 4 we can get a sum of 7 (1+1+2+3) and if the divisor is 5 the sum will be 5 (1+1+1+2). </span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [44,22,33,11,1], threshold = 5</span><br><span class="line">Output: 44</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; nums.length &lt;&#x3D; 5 * 10^4</li>
<li>1 &lt;&#x3D; nums[i] &lt;&#x3D; 10^6</li>
<li>nums.length &lt;&#x3D; threshold &lt;&#x3D; 10^6</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">smallestDivisor</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], threshold: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">condition</span>(<span class="params">divisor</span>):</span><br><span class="line">            s = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">                <span class="keyword">if</span> num % divisor:</span><br><span class="line">                    s += num // divisor + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    s += num // divisor</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> s &lt;= threshold</span><br><span class="line"></span><br><span class="line">        left, right = <span class="number">1</span>, <span class="built_in">max</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> condition(mid):</span><br><span class="line">                right = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> left</span><br></pre></td></tr></table></figure>

<h2 id="More-practices"><a href="#More-practices" class="headerlink" title="More practices"></a>More practices</h2><h3 id="Leetcode-162-Find-Peak-Element"><a href="#Leetcode-162-Find-Peak-Element" class="headerlink" title="[Leetcode 162] Find Peak Element"></a>[Leetcode 162] Find Peak Element</h3><p>A peak element is an element that is strictly greater than its neighbors.</p>
<p>Given a 0-indexed integer array nums, find a peak element, and return its index. If the array contains multiple peaks, return the index to any of the peaks.</p>
<p>You may imagine that nums[-1] &#x3D; nums[n] &#x3D; -∞. In other words, an element is always considered to be strictly greater than a neighbor that is outside the array.</p>
<p>You must write an algorithm that runs in O(log n) time.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,2,3,1]</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: 3 is a peak element and your function should return the index number 2.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,2,1,3,5,6,4]</span><br><span class="line">Output: 5</span><br><span class="line">Explanation: Your function can return either index number 1 where the peak element is 2, or index number 5 where the peak element is 6.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; nums.length &lt;&#x3D; 1000</li>
<li>-2^31 &lt;&#x3D; nums[i] &lt;&#x3D; 2^31 - 1</li>
<li>nums[i] !&#x3D; nums[i + 1] for all valid i.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findPeakElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># one element</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># check if first element is greater than second</span></span><br><span class="line">        <span class="keyword">if</span> nums[<span class="number">0</span>] &gt; nums[<span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># check if last element is greater than second last</span></span><br><span class="line">        <span class="keyword">if</span> nums[-<span class="number">1</span>] &gt; nums[-<span class="number">2</span>]:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># search peak in the middle</span></span><br><span class="line">        l = <span class="number">1</span></span><br><span class="line">        r = <span class="built_in">len</span>(nums) - <span class="number">2</span></span><br><span class="line">        <span class="keyword">while</span> l &lt;= r:</span><br><span class="line">            mid = l + (r-l)//<span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &gt; nums[mid-<span class="number">1</span>] <span class="keyword">and</span> nums[mid] &gt; nums[mid+<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">return</span> mid</span><br><span class="line">            <span class="keyword">elif</span> nums[mid] &lt; nums[mid + <span class="number">1</span>]:</span><br><span class="line">                <span class="comment"># nums[mid] is in ascending trend and the peak must be on the right</span></span><br><span class="line">                l = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># nums[mid] is in descending trend and the peak must be on the left</span></span><br><span class="line">                r = mid - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># no peak</span></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-2300-Successful-Pairs-of-Spells-and-Potions"><a href="#Leetcode-2300-Successful-Pairs-of-Spells-and-Potions" class="headerlink" title="[Leetcode 2300] Successful Pairs of Spells and Potions"></a>[Leetcode 2300] Successful Pairs of Spells and Potions</h3><p>You are given two positive integer arrays spells and potions, of length n and m respectively, where spells[i] represents the strength of the ith spell and potions[j] represents the strength of the jth potion.</p>
<p>You are also given an integer success. A spell and potion pair is considered successful if the product of their strengths is at least success.</p>
<p>Return an integer array pairs of length n where pairs[i] is the number of potions that will form a successful pair with the ith spell.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: spells = [5,1,3], potions = [1,2,3,4,5], success = 7</span><br><span class="line">Output: [4,0,3]</span><br><span class="line">Explanation:</span><br><span class="line">- 0th spell: 5 * [1,2,3,4,5] = [5,10,15,20,25]. 4 pairs are successful.</span><br><span class="line">- 1st spell: 1 * [1,2,3,4,5] = [1,2,3,4,5]. 0 pairs are successful.</span><br><span class="line">- 2nd spell: 3 * [1,2,3,4,5] = [3,6,9,12,15]. 3 pairs are successful.</span><br><span class="line">Thus, [4,0,3] is returned.</span><br></pre></td></tr></table></figure>
<p>Constraints:</p>
<ul>
<li>n &#x3D;&#x3D; spells.length</li>
<li>m &#x3D;&#x3D; potions.length</li>
<li>1 &lt;&#x3D; n, m &lt;&#x3D; 105</li>
<li>1 &lt;&#x3D; spells[i], potions[i] &lt;&#x3D; 105</li>
<li>1 &lt;&#x3D; success &lt;&#x3D; 1010</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># Notice that if a spell and potion pair is successful, then the spell and all stronger potions will be successful too.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">successfulPairs</span>(<span class="params">self, spells: <span class="type">List</span>[<span class="built_in">int</span>], potions: <span class="type">List</span>[<span class="built_in">int</span>], success: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        potions.sort()</span><br><span class="line">        m = <span class="built_in">len</span>(potions)</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> spell <span class="keyword">in</span> spells:</span><br><span class="line">            target = success // spell</span><br><span class="line">            <span class="keyword">if</span> success % spell:</span><br><span class="line">                target += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            insertIdx = bisect.bisect_left(potions, target)</span><br><span class="line">            res.append(m - insertIdx)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://leetcode.com/problems/binary-search/solutions/423162/Binary-Search-101-The-Ultimate-Binary-Search-Handbook/">The ultimate binary search handbook</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
        <tag>Algorithm</tag>
        <tag>Binary Search</tag>
      </tags>
  </entry>
  <entry>
    <title>Binary tree traversal - inorder, preorder and postorder</title>
    <url>/blog/binary-tree-traversal-inorder-preorder-and-postorder/</url>
    <content><![CDATA[<h2 id="Tree-node-definition"><a href="#Tree-node-definition" class="headerlink" title="Tree node definition"></a>Tree node definition</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Node</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val</span>):</span><br><span class="line">        self.val = val</span><br><span class="line">        self.left = <span class="literal">None</span></span><br><span class="line">        self.right = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>To build the tree nodes:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">root = Node(<span class="number">0</span>)</span><br><span class="line">n1 = Node(<span class="number">1</span>)</span><br><span class="line">n2 = Node(<span class="number">2</span>)</span><br><span class="line">root.left = n1</span><br><span class="line">root.right = n2</span><br><span class="line">n3 = Node(<span class="number">3</span>)</span><br><span class="line">n4 = Node(<span class="number">4</span>)</span><br><span class="line">n1.left = n3</span><br><span class="line">n1.right = n4</span><br><span class="line">n5 = Node(<span class="number">5</span>)</span><br><span class="line">n6 = Node(<span class="number">6</span>)</span><br><span class="line">n2.left = n5</span><br><span class="line">n2.right = n6</span><br><span class="line">n7 = Node(<span class="number">7</span>)</span><br><span class="line">n8 = Node(<span class="number">8</span>)</span><br><span class="line">n3.left = n7</span><br><span class="line">n3.right = n8</span><br></pre></td></tr></table></figure>

<p>The tree looks like below:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">               <span class="number">0</span></span><br><span class="line">             /   \</span><br><span class="line">            <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">           / \   / \</span><br><span class="line">          <span class="number">3</span>   <span class="number">4</span> <span class="number">5</span>   <span class="number">6</span></span><br><span class="line">         / \</span><br><span class="line">        <span class="number">7</span>   <span class="number">8</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Node</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val</span>):</span><br><span class="line">        self.val = val</span><br><span class="line">        self.left = <span class="literal">None</span></span><br><span class="line">        self.right = <span class="literal">None</span></span><br><span class="line">```  </span><br><span class="line">   </span><br><span class="line">To build the tree nodes:</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">root = Node(<span class="number">0</span>)</span><br><span class="line">n1 = Node(<span class="number">1</span>)</span><br><span class="line">n2 = Node(<span class="number">2</span>)</span><br><span class="line">root.left = n1</span><br><span class="line">root.right = n2</span><br><span class="line">n3 = Node(<span class="number">3</span>)</span><br><span class="line">n4 = Node(<span class="number">4</span>)</span><br><span class="line">n1.left = n3</span><br><span class="line">n1.right = n4</span><br><span class="line">n5 = Node(<span class="number">5</span>)</span><br><span class="line">n6 = Node(<span class="number">6</span>)</span><br><span class="line">n2.left = n5</span><br><span class="line">n2.right = n6</span><br><span class="line">n7 = Node(<span class="number">7</span>)</span><br><span class="line">n8 = Node(<span class="number">8</span>)</span><br><span class="line">n3.left = n7</span><br><span class="line">n3.right = n8</span><br></pre></td></tr></table></figure>

<p>The tree looks like below:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">       <span class="number">0</span></span><br><span class="line">     /   \</span><br><span class="line">    <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">   / \   / \</span><br><span class="line">  <span class="number">3</span>   <span class="number">4</span> <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"> / \</span><br><span class="line"><span class="number">7</span>   <span class="number">8</span> </span><br></pre></td></tr></table></figure>

<h2 id="Pre-order-traversal"><a href="#Pre-order-traversal" class="headerlink" title="Pre-order traversal"></a>Pre-order traversal</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pre_order_traverse</span>(<span class="params">node</span>):</span><br><span class="line">    <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;node.val&#125;</span> &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    pre_order_traverse(node.left)</span><br><span class="line">    pre_order_traverse(node.right)</span><br><span class="line">    </span><br><span class="line">pre_order_traverse(root)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment">#Output: 0 1 3 7 8 4 2 5 6</span></span><br></pre></td></tr></table></figure>

<h2 id="In-order-traversal"><a href="#In-order-traversal" class="headerlink" title="In-order traversal"></a>In-order traversal</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">in_order_traverse</span>(<span class="params">node</span>):</span><br><span class="line">    <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    in_order_traverse(node.left)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;node.val&#125;</span> &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    in_order_traverse(node.right)</span><br><span class="line"></span><br><span class="line">in_order_traverse(root)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#Output: 7 3 8 1 4 0 5 2 6</span></span><br></pre></td></tr></table></figure>

<h2 id="Post-order-traversal"><a href="#Post-order-traversal" class="headerlink" title="Post-order traversal"></a>Post-order traversal</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">post_order_traverse</span>(<span class="params">node</span>):</span><br><span class="line">    <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    post_order_traverse(node.left)</span><br><span class="line">    post_order_traverse(node.right)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;node.val&#125;</span> &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">post_order_traverse(root)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment">#Output: 7 8 3 4 1 5 6 2 0</span></span><br></pre></td></tr></table></figure>

<h2 id="Get-tree-size"><a href="#Get-tree-size" class="headerlink" title="Get tree size"></a>Get tree size</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">size</span>(<span class="params">node</span>):</span><br><span class="line">    <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> size(node.left) + <span class="number">1</span> + size(node.right)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;size: <span class="subst">&#123;size(root)&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#Output:</span></span><br><span class="line"><span class="comment">#size: 9</span></span><br></pre></td></tr></table></figure>

<h2 id="Get-tree-depth"><a href="#Get-tree-depth" class="headerlink" title="Get tree depth"></a>Get tree depth</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">depth</span>(<span class="params">node</span>):</span><br><span class="line">    <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(depth(node.left), depth(node.right)) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;depth: <span class="subst">&#123;depth(root)&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#Output:</span></span><br><span class="line"><span class="comment">#depth: 4</span></span><br></pre></td></tr></table></figure>

<h2 id="Print-all-the-paths-in-the-tree"><a href="#Print-all-the-paths-in-the-tree" class="headerlink" title="Print all the paths in the tree"></a>Print all the paths in the tree</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">printPaths</span>(<span class="params">node, path</span>):</span><br><span class="line">    <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># add node to the path</span></span><br><span class="line">    path.append(node.val)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> node.left == <span class="literal">None</span> <span class="keyword">and</span> node.right == <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># this is leaf node</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;path : &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> path:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;n&#125;</span> &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># search left subtree</span></span><br><span class="line">        printPaths(node.left, path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># search right subtree</span></span><br><span class="line">        printPaths(node.right, path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># remove node from path</span></span><br><span class="line">    path.pop()</span><br><span class="line">    </span><br><span class="line">printPaths(root, <span class="built_in">list</span>())</span><br><span class="line"></span><br><span class="line"><span class="comment">#Output:</span></span><br><span class="line"><span class="comment">#path : 0 1 3 7</span></span><br><span class="line"><span class="comment">#path : 0 1 3 8</span></span><br><span class="line"><span class="comment">#path : 0 1 4</span></span><br><span class="line"><span class="comment">#path : 0 2 5</span></span><br><span class="line"><span class="comment">#path : 0 2 6</span></span><br></pre></td></tr></table></figure>

<h2 id="Check-if-path-exists-for-target-sum"><a href="#Check-if-path-exists-for-target-sum" class="headerlink" title="Check if path exists for target sum"></a>Check if path exists for target sum</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hasPathSum</span>(<span class="params">node, <span class="built_in">sum</span></span>):</span><br><span class="line">    <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span> == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">sum</span> -= node.val</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> hasPathSum(node.left, <span class="built_in">sum</span>) <span class="keyword">or</span> hasPathSum(node.right, <span class="built_in">sum</span>)</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(hasPathSum(root, <span class="number">7</span>))</span><br><span class="line"><span class="built_in">print</span>(hasPathSum(root, <span class="number">12</span>))</span><br><span class="line"><span class="built_in">print</span>(hasPathSum(root, <span class="number">10</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment">#Output:</span></span><br><span class="line"><span class="comment">#True</span></span><br><span class="line"><span class="comment">#True</span></span><br><span class="line"><span class="comment">#False</span></span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-114-Flatten-Binary-Tree-to-Linked-List"><a href="#Leetcode-114-Flatten-Binary-Tree-to-Linked-List" class="headerlink" title="[Leetcode 114] Flatten Binary Tree to Linked List"></a>[Leetcode 114] Flatten Binary Tree to Linked List</h2><p>he root of a binary tree, flatten the tree into a “linked list”:</p>
<ul>
<li>The “linked list” should use the same TreeNode class where the right child pointer points to the next node in the list and the left child pointer is always null.</li>
<li>The “linked list” should be in the same order as a pre-order traversal of the binary tree.</li>
</ul>
<p>Constraints:</p>
<ul>
<li>The number of nodes in the tree is in the range [0, 2000].</li>
<li>-100 &lt;&#x3D; Node.val &lt;&#x3D; 100</li>
</ul>
<p>Follow up: Can you flatten the tree in-place (with O(1) extra space)?</p>
<p>Solution：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># Refer to https://leetcode.com/problems/flatten-binary-tree-to-linked-list/solutions/1208004/extremely-intuitive-o-1-space-solution-with-simple-explanation-python/?envType=study-plan-v2&amp;envId=top-interview-150</span></span><br><span class="line">    <span class="comment">#         1(curr)        1          1  </span></span><br><span class="line">    <span class="comment">#       /   \           /            \</span></span><br><span class="line">    <span class="comment">#      2     5   --&gt;   2      --&gt;     2</span></span><br><span class="line">    <span class="comment">#     / \     \       / \            / \</span></span><br><span class="line">    <span class="comment">#    3   4(p)  6     3   4(p)       3   4</span></span><br><span class="line">    <span class="comment">#                         \              \</span></span><br><span class="line">    <span class="comment">#                          5              5 </span></span><br><span class="line">    <span class="comment">#                           \              \</span></span><br><span class="line">    <span class="comment">#                            6              6</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">flatten</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Do not return anything, modify root in-place instead.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        curr = root</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> curr:</span><br><span class="line">            <span class="keyword">if</span> curr.left != <span class="literal">None</span>:</span><br><span class="line">                p = curr.left</span><br><span class="line"></span><br><span class="line">                <span class="comment"># find the right most node of currnt left subtree</span></span><br><span class="line">                <span class="keyword">while</span> p.right != <span class="literal">None</span>:</span><br><span class="line">                    p = p.right</span><br><span class="line"></span><br><span class="line">                <span class="comment"># shift the contents of currnt right subtree to p.right</span></span><br><span class="line">                p.right = curr.right</span><br><span class="line"></span><br><span class="line">                <span class="comment"># shift the contents of current left subtree to curr.right</span></span><br><span class="line">                curr.right = curr.left</span><br><span class="line"></span><br><span class="line">                <span class="comment"># set curr.left to None</span></span><br><span class="line">                curr.left = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># repeat the above process</span></span><br><span class="line">            curr = curr.right</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Binary Search</tag>
      </tags>
  </entry>
  <entry>
    <title>Art - Water Color</title>
    <url>/blog/art-water-color/</url>
    <content><![CDATA[<p><img src="/images/drawing/astronaut.jpg"></p>
<p><img src="/images/drawing/castleisland.jpg"></p>
<p><img src="/images/drawing/IMG_20181130_170821.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165006.jpg"></p>
<p><img src="/images/drawing/IMG_20200412_165038.jpg"></p>
<p><img src="/images/drawing/IMG_0330.jpeg"></p>
<p><img src="/images/drawing/IMG_0331.jpeg"></p>
<p><img src="/images/drawing/IMG_0332.jpeg"></p>
<p><img src="/images/drawing/IMG_0333.jpeg"></p>
<p><img src="/images/drawing/IMG_0334.jpeg"></p>
<p><img src="/images/drawing/IMG_0335.jpeg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>blktrace - A block layer IO tracing utility</title>
    <url>/blog/blktrace-a-block-layer-io-tracing-utility/</url>
    <content><![CDATA[<h2 id="What-can-blktrace-do"><a href="#What-can-blktrace-do" class="headerlink" title="What can blktrace do?"></a>What can blktrace do?</h2><p>Let’s dive into the block device layer and see how the I&#x2F;O are handled in disk queues.</p>
<p>The following stack shows the I&#x2F;O paths including the block device layer. The application can issue I&#x2F;O requests directly to block device or through file systems. In the following sections, let’s dig into the block device layer with blktrace in order to understand the I&#x2F;O pattern and disk queue activities.</p>
<pre><code>        Applications
        |      |   |  
        V      |   | 
  File systems |   |
        |      |   |
        V      |   |
 Page Cache &lt;--|   | 
        |          | 
        V          V
Block I/O Layer: Request Queues
        |
        V 
  SCSI Drivers
        |
        V
 Physical Devices               
</code></pre>
<h2 id="Don’t-forget-iostat"><a href="#Don’t-forget-iostat" class="headerlink" title="Don’t forget iostat"></a>Don’t forget iostat</h2><p>iostat is always the first place to understand the I&#x2F;O characteristics before we turn to other advanced utilities like blktrace.</p>
<p>It provides the following information for the disk IOs.</p>
<ul>
<li>Number of read&#x2F;write merges per second</li>
<li>Number of reads&#x2F;writes per second</li>
<li>Average I&#x2F;O request size(in sectors)</li>
<li>Average request queue size</li>
<li>Average I&#x2F;O wait time</li>
</ul>
<p>If any of the above metrics indicates disk I&#x2F;O performance concerns and it’s not sufficient to help us explain the performance issue, we can turn to blktrace or other tracing utilities for more insights.</p>
<h2 id="blktrace-and-blkparse"><a href="#blktrace-and-blkparse" class="headerlink" title="blktrace and blkparse"></a>blktrace and blkparse</h2><p>To trace the target block device:</p>
<pre><code>$ blktrace -d /dev/&lt;sd-device-name&gt; -D &lt;trace-raw-data-save-dir&gt; -w &lt;trace-time-in-seconds&gt;
</code></pre>
<p>To parse the blktrace data:</p>
<pre><code>$ blkparse -i &lt;sd-device-name&gt; -D &lt;trace-raw-data-save-dir&gt; -o blkparse.&lt;sd-device-name&gt;.out -d blktrace.bin
</code></pre>
<p>blkparse output snippet:</p>
<pre><code>  8,0    7        3     0.992335623  4180  A  WS 680911952 + 8 &lt;- (8,2) 679885904
  8,0    7        4     0.992336407  4180  Q  WS 680911952 + 8 [jbd2/dm-7-8]
  8,0    7        5     0.992338784  4180  G  WS 680911952 + 8 [jbd2/dm-7-8]
  8,0    7        6     0.992339977  4180  I  WS 680911952 + 8 [jbd2/dm-7-8]
  8,0    7        7     0.992341444  4180  D  WS 680911952 + 8 [jbd2/dm-7-8]
  8,0   56        1     0.992499505     0  C  WS 680911952 + 8 [0]
  8,0   47        7     0.991930131  4180  A  WS 680911920 + 8 &lt;- (8,2) 679885872
  8,0   47        8     0.991930522  4180  Q  WS 680911920 + 8 [jbd2/dm-7-8]
  8,0   47        9     0.991932697  4180  M  WS 680911920 + 8 [jbd2/dm-7-8]
</code></pre>
<p>The columns are Dev major,minor, CPU id, Sequence number, Timestamp, PID, Event, Operation, Start block + number of blocks(offset), Process name.</p>
<p>In the above example, The first IO starts at block 680911952 with the offset of 8 blocks. It is handled in the following sequence.</p>
<ul>
<li>Remapped to a different device(8,2)</li>
<li>Handled by the request queue code</li>
<li>Get the request</li>
<li>Inserted to the request queue</li>
<li>Dispatch to device driver</li>
<li>Completion</li>
</ul>
<p>The second IO starts at block 680911920 with the offset of 8 blocks. It’s handled in the following sequence.</p>
<ul>
<li>Remapped to a different device(8,2)</li>
<li>Handled by the request queue code</li>
<li>Back merged with request on queue</li>
</ul>
<p>blkparse output also includes a summary to explain the number of I&#x2F;Os in each queuing phase. In the following example, there are 86 writes handled by request queue. 19 out of 86 writes are merged with request on queue. Thus, only 67 writes are issued to device to complete the front end requests.</p>
<pre><code>Total (sda):
 Reads Queued:           0,        0KiB  Writes Queued:          86,      628KiB
 Read Dispatches:        0,        0KiB  Write Dispatches:       67,      628KiB
 Reads Requeued:         0               Writes Requeued:         0
 Reads Completed:        0,        0KiB  Writes Completed:       67,      628KiB
 Read Merges:            0,        0KiB  Write Merges:           19,       88KiB
 IO unplugs:            21               Timer unplugs:           0
</code></pre>
<h2 id="btt"><a href="#btt" class="headerlink" title="btt"></a>btt</h2><p>btt is a post-processing tool for blktrace. blktrace is capable of producing tremendous amounts of output in the form of multiple individual traces per IO executed during the traced run. It is also capable of producing some general statistics concerning IO rates and the like. btt goes further and produces a variety of overall statistics about each of the individual handling of IOs, and provides data we believe is useful to plot to provide visual comparisons for evaluation.</p>
<p>btt processes the binary file produced by blkparse.The major areas of output measured by btt include:</p>
<ul>
<li>Q2Q : Queue-to-Queue time</li>
<li>Q2G : Queue-to-GetRequest time</li>
<li>S2G : Sleep-to-GetRequest time</li>
<li>G2I : GetRequest-to-Insert time</li>
<li>Q2M : Queue-to-Merge time</li>
<li>I2D : Insert-to-Issue time</li>
<li>M2D : Merge-to-Issue time</li>
<li>D2C : Issue-to-Complete time</li>
<li>Q2C : Queue-to-Complete time</li>
</ul>
<p>For the D2C, it includes the driver and device time. It’s the average time from when the actual IO was issued to the driver until is completed (completion trace) back to the block IO layer. The D2C time should be greater than the actual physcial disk I&#x2F;O latency which is usually measured in disk(array) side.</p>
<p>In the following exampl, 98.9265% of time is spent on D2C which is expected. The average IO time serviced by the disk is 1.65ms. The max IO time is 5.10ms. We may measure the I2D metric for different I&#x2F;O scheduler, like noop in SSD case.</p>
<pre><code>$ btt -i blktrace.bin -B offset -o btt.out

$ ls btt.*.out*
btt.sda.30s.out.avg  btt.sda.30s.out.dat  btt.sda.30s.out_dhist.dat  btt.sda.30s.out.msg  btt.sda.30s.out_qhist.dat

$ cat btt.out.avg
==================== All Devices ====================

ALL               MIN           AVG           MAX                  N
--------------- ------------- ------------- ------------- -----------

Q2Q               0.000001053   0.304967428   5.071043004          85
Q2G               0.000000338   0.000001928   0.000008143          67
G2I               0.000000161   0.000008231   0.000126276          67
Q2M               0.000000178   0.000000664   0.000002175          19
I2D               0.000000223   0.000003979   0.000024517          67
M2D               0.000002771   0.000030058   0.000116136          19
D2C               0.000089761   0.001640496   0.005096214          86
Q2C               0.000093453   0.001658297   0.005098018          86

==================== Device Overhead ====================

       DEV |       Q2G       G2I       Q2M       I2D       D2C
---------- | --------- --------- --------- --------- ---------
 (  8,  0) |   0.0906%   0.3867%   0.0088%   0.1869%  98.9265%
---------- | --------- --------- --------- --------- ---------
   Overall |   0.0906%   0.3867%   0.0088%   0.1869%  98.9265%
[..]   
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Blktrace</tag>
      </tags>
  </entry>
  <entry>
    <title>Backup and restore MySQL database</title>
    <url>/blog/backup-and-restore-mysql-database/</url>
    <content><![CDATA[<h2 id="Backup-a-database"><a href="#Backup-a-database" class="headerlink" title="Backup a database"></a>Backup a database</h2><p>mysqldump is a command-line utility which can be used to generate backups of MySQL database.</p>
<pre><code>$ mysqldump -u root --password=&lt;db_password&gt; mydb &gt; mydb_dump_`date +&quot;%Y%m%d_%H%M%S&quot;`.sql
$  ls -ltr | grep mydb
-rw-r--r--.   1 root root 4834575 Sep 28 21:11 mydb_dump_20210928_144610.sql
</code></pre>
<h2 id="Restore-a-database"><a href="#Restore-a-database" class="headerlink" title="Restore a database"></a>Restore a database</h2><p>Create an empty database before restore as below:</p>
<pre><code>$ mysql -u root -p

mysql&gt; create database mydb;
mysql&gt; show databases;
mysql&gt; exit
</code></pre>
<p>Restore the database:</p>
<pre><code>$ mysql -u root -p mydb &lt; mydb_dump_20210928_144610.sql
</code></pre>
<p>Check the database size as below:</p>
<pre><code>mysql&gt; SELECT table_schema &quot;DB Name&quot;, ROUND(SUM(data_length + index_length) / 1024 / 1024, 1) &quot;DB Size in MB&quot;  FROM information_schema.tables  GROUP BY table_schema;
+--------------------+---------------+
| DB Name            | DB Size in MB |
+--------------------+---------------+
| mydb               |           8.1 |
+--------------------+---------------+
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Database</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Book Report on Island of the Blue Dolphins</title>
    <url>/blog/book-report-on-island-of-the-blue-dolphins/</url>
    <content><![CDATA[<p>Karana, a girl who was living alone on the Island of the Dolphins, set out for many dangerous adventures and changes throughout the novel. She learnt how to survive by herself, along with a couple of animal friends for company.</p>
<p>The setting of the Island of the Blue dolphins was San Nicolas  Island. In the book, it was referred to as the Island of the Blue Dolphins. The island was two leagues long and one league wide, and looked like a dolphin lying on its side, with its tail pointing toward the sunrise. The island got its name from the blue dolphins that lived in the seas. Karana lived in a village called Ghalas-at. The village lay east of the hills on a small mesa, near Coral Cove. This story took place from 1835 to 1853, the time when Karana lived alone on the island. Since the book was mainly about how Karana survived on the island, understanding the setting of the island was important. It gave us an idea that she may be facing dangers from wild life, since the setting is on a deserted island.</p>
<p>The main plot of the story was Karana’s struggle to survive on the island. She lived in fear of the Aleuts, a group of people that slaughtered many of her friends and family members. Also, she had to face the wild dogs who killed her brother Ramu. She wanted to get revenge and made weapons to defend herself. She broke the law which stated that women could not forge weapons. The inciting incident was when the white men, who were sent by Kimki, the new chief of the tribe after the death of Chowig, came and brought the tribe to a new place. This was what started the story of how Karana was left alone after she made the choice to rescue his brother. The climax was when Karana decided to befriend Tutok, an Aleut girl. Karana changed a lot from the inciting incident to the climax. She decided to burn down her village because she knew she could not stand staying there on her own. She built weapons so she would be prepared when the Aleuts came back. But that all changed when she met Tutok. Tutok showed her that not all Aleuts are bad. The overarching conflict all started when the Aleuts came to the island to hunt sea otters. They lied about the deal they made with Chowig and killed many people in the tribe. Then, the white men came and sailed to a new place with most of the tribe. Ramu, who ran off in search of his hunting spear, was left behind because there was a storm and the people had to leave quickly. Karana swam back to the island for his brother. Ramu wanted to get a canoe so they could hunt food but was killed by the hunting dogs. To Karana, the loss of his brother was devastating and she vowed to take revenge. That was when she started her struggle for her own safety. The conflict was solved when the white men came back and brought Karana to the rest of her tribe.</p>
<p>The main character of this story was Karana. She was a young girl who was left alone on the island she called home after her tribe left without her. After her father died, she took the responsibility to take care of Ramu. But soon, Ramu was killed by the wild dogs. Now that she was isolated on the island, Karana was forced to learn how to survive in the wild. Some values Karana showed was caring for nature and friendship. After she saw how brutal the Aleuts were to the sea otters, Karana began to question her practice of hunting. She learnt that animals and humans were truly equal, and that she should never hunt unless her life depended on it. By caring for the wild life, Karana also created a friendship with the animals. She made two friends that she became close with: a dog that killed her brother, and Tutok that belonged to the group of people that killed her father. She learnt that holding grudges was no good and she should learn to forgive. One example of this was her friendship with Tutok. Since Tutok was an Aleut, Karana was suspicious and scared that she would reveal her hiding place. But she was wrong. Tutok befriended Karana and gave her a gift. Karana chose mercy over justice, like when she saved Rontu’s life, even though he was the leader of the wild dogs. At first, Karana thought of herself as only a girl, and she was only fit to maintain the house. But when she was alone, she realized that the division of gender was really unnecessary if she was capable of doing the tasks that men did. From the choices she made, I learnt that I could not always rely on someone for help, and that friendship was very important.</p>
<p>The major theme of the book was self-reliance. Karana’s village encouraged self-reliance. They have learnt to survive by using the resources on the island. When Karana was alone, she had to apply this value more than ever. She made her own weapons, shelter, clothing, and hunted her own food. She thought of ways to hunt animals like devilfish and sea elephants. She had no one to help her. She had to do what the men did, and survived using skills she learnt. She was resourceful and used her knowledge to survive without outside assistance.</p>
<p>My favorite part of this book was when Karana became friends with Tutok because she learnt to forgive and decided that all Aleuts were not bad. Only the ones that chose to lie were unworthy of being forgiven. My least favorite part was when Rontu died because Karana lost her only companion on the island. Yes, I would recommend this book to a friend because people should learn not to rely on others and that making friends was very important.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Book Report on Summer of The Monkeys</title>
    <url>/blog/book-report-on-summer-of-the-monkeys/</url>
    <content><![CDATA[<p>Jay Berry, a fourteen years old boy, was living an ideal life when he unexpectedly met some monkeys during a mission to find his old milk cow, Sally Gooden. From his grandpa, he learned that there was a big reward for catching these monkeys alive. After many failures, Jay Berry finally captures the monkeys. When he received the money, he chose to use it to finance Daisy’s surgery instead. Throughout the story, Jay Berry learned to make life-enhancing choices and got happiness in the end.</p>
<p>The story is set in the late 1800s. The Berry family lived in Missouri as sharecroppers and later moved to a farm called Cherokee Land, which was in the middle of the Cherokee Nation. It was located in a strip from the foothills of the Ozark Mountains to the banks of the Illinois River in northeastern Oklahoma. They lived in a log house which was in the foothills overlooking the river bottoms. IThe setting influenced the plot and the events that took place. In the Cherokee bottoms, Jay Berry was likely to find numerous animals. He followed a game trail and found the sycamore tree full of wild monkeys. This led to the plot, which all started from this event. Also it affected the character background. Jay lived in a place where most people survive by hunting. He liked to shoot little animals in the beginning of the story, but changed as the story progressed. The setting could be the reason that the protagonist was unable to solve his problems and this was shown multiple times throughout the story.</p>
<p>The main plot of the story was Jay Berry’s struggle to capture the monkeys. He discovered a tree full of monkeys during his trip to find Sally Gooden. He is determined to capture them, for the reward would be enough money to achieve his desires. But this was much harder than he imagined and he greatly underestimates the intelligence of Jimbo, a very trained chimpanzee. His grandpa supported him and came up with ideas that might work. The first trap was a complete failure but it helped Jay Berry understand that the animals he was catching were not like any fox or racoon. They were way smarter and Jimbo was going to make sure that Jay was not going to get the satisfaction of capturing a single monkey. He lost his gunny sack and traps, which infuriated him so much that he shot Jimbo two times in the belly. But he quickly realized his fatal mistake because Jimbo was awfully angry and bared his sharp teeth. He ordered the monkeys to chase Jay and the poor hunter and dog ran straight through a brier patch. This does not dishearten Jay Berry and now he is determined to get back his possessions. On the second attempt, he almost captured two monkeys but Jimbo set them free while Rowdy and Jay fought off the monkeys. They came back with a lot of scratches and bites, but no monkeys. Daisy, his little sister, was scared that they might have gotten hydrophobia, and was bound to put both his brother and the dog in chains if their father hadn’t stepped in. The monkeys were from the circus so they were all vaccinated. All the Berry family had to worry about was that the wounds didn’t get infected.</p>
<p>After a few days of Daisy practicing her red cross, Jay Berry and Rowdy were healthy again. Grandpa advised Jay to befriend Jimbo so they will listen to him. Jay kindly approached the monkeys and offered Jimbo an apple. Jimbo decided to also give Jay something. So, he offered him a can of sour mash. Jay knew the stuff was nasty and politely refused the offer, which enraged Jimbo. Realizing that he had no choice, he reluctantly took the sour mash and the taste was enough to make him sick. Then, the bad taste left and Jay decided to take another can of sour mash. Soon, both Rowdy and he were drunk and fell asleep. When they woke up, Jay found out that his britches were stolen. He returned home so ill and unsteady that he fell unconscious the moment he was put in bed. Once again, he received the “Red Cross Treatment.”  When Grandpa heard about this, he declared that they needed to go to the library to find some useful information on catching the monkeys. This showed that they were determined to the extent that they were ready to cross any limit. They had never gone to a library before, but were sure they would be provided with an answer there.The librarian gave them a book, which said that monkeys love coconuts. However, the whole half-bushel of coconuts were stolen by the monkeys, who returned the britches, beanshooter, and gunny sack.</p>
<p>Just when Jay was about to lose hope, Daisy found a fairy ring and wished that he got the gun and paint pony. Jay and his parents wished that Daisy would get her leg fixed up. This showed that they all cared about one another. That night, there was a fierce storm which frightened the monkeys so much that they willingly followed Jay Berry to the corn crib. They were provided with a comfortable shelter and plenty of food. Jay Berry receives one hundred fifty-six dollars, which he at first wanted to spend on his pony and rifle. But after taking one look at Daisy’s crippled leg, he made up his mind and financed the surgery to get her leg fixed up. While at town, Daisy used the money she had to buy his brother the rifle he wanted. She gave it to him on the condition that he would never use it to shoot little animals. Grandpa also used the money he saved to buy Jay the paint pony he always desired. The conclusion was that Jay’s self-sacrificing yet rational choice made all of his family happy.</p>
<p>Jay Berry Lee is the main character of the story.  At the beginning of the novel, he was the happiest boy on Earth  up till he reached fourteen-years old. He had lived an idyllic childhood with his little sister Daisy and his parents. The Berry family were very poor because they were sharecroppers until they moved to Oklahoma when Grandpa offered them free land. Here, Jay discovered a group of monkeys, whose capture could change his whole life. He set out to capture every one of the monkeys and devoted himself to finding ways to catch them. He needed the help of his grandfather, who made traps that might do the job.</p>
<p>Sadly, none of the traps work and Jay Berry gained nothing but a lot of scratches and painful monkey bites. He was constantly outsmarted by Jimbo, who simply broke every trap with his fingers and laughed rudely at Jay Berry whenever a trap failed. This made Jay more determined to capture them because he had spent all his valuable time in doing it. He does not want to simply give up on something he had worked so hard for. His opportunity finally arrived when the fierce storm struck and he thought that the monkeys must have had a difficult time finding shelter. He found all of them  freezing and soaking wet. They were quivering from the cold and struggled to keep warm. Despite how they harmed him in the past, Jay kindly dried them off and took care of them. No monkey made the effort to bite him Jimbo surprisingly laid his head on Jay’s shoulder. Jay soothingly said that everything would be alright and he would help them. Finally, they willingly followed Jay home and into the corn crib. Later on, Jay was tempted to use the money to achieve his desires but it suddenly hit him that grandpa was secretly asking him to change his mind. Even if he bought the piny, he could never ride it because it reminded himself too much of the crippled leg of Daisy. He decided to use all of the money for Daisy to get a surgery in town, so the crippled leg would heal. He does not regret it, but suddenly it felt lonesome and gloomy at the farm. He was relieved that his efforts had not gone to waste when Mama wrote that the operation was a success and her sister was learning to walk. Jay changed from a boy who was focused on hunting to a boy who cared for animals and learned to place other’s priority higher than his own. This caused him to be the best Challenger student for he persevered through hardships. He thought of problems as challenges that needed to be solved. In this case, he thought of capturing the monkeys as a challenge, since the reward could greatly help him. He analyzed the problem and found the steps to solve it. He used the traps to devise plants that could capture the monkeys. Also, he showed kindness to the monkeys and saved their lives. The life-enhancing values he demonstrated improved his life and others.</p>
<p>Jimbo was the antagonist in the story. He made sure that all the  other monkeys didn’t get caught in the traps and  worked against Jay Berry during his several attempts. He acted rudely whenever he snatched a two dollar award from Jay and once, he got him drunk. He stole his britches, which was not his. His impudent remarks and cruel actions caused him to be the worst Challenger student. He did not respect others’ rights. Grandpa was a key supporting character and guided Jay Berry throughout the story. He helped design many traps which Jay used. For example, he made a trap with a net and it caught the old goose Gandy. It even caught two little monkeys, but Jimbo freed them during the chaos. Grandpa went as far as going to the library himself to get information. This showed that he really cared about Jay catching the monkeys. He always supported his grandson and helped him through hardships. Daisy is also a supporting character. She always warned Jay to not catch little animals because the Old Man of the Mountains was constantly watching. She was a protector of wildlife and disliked how boys captured poor animals just for the fun of it. Even though she was annoying sometimes, she cared greatly for her brother. She nursed him when he was bitten and when he was drunk. Also, she wished that her brother finally received the rifle and pony he so long wanted. This showed that she loved her brother and helped him achieve true happiness. One example was when she used the money she saved to purchase the rifle.From these characters I learned that I should persevere through challenges and be optimistic.</p>
<p>The main theme of the story is that anyone can achieve his or her goals as long as he or she sets their mind to it and does not give up. Throughout the course of the novel, Jay Berry did this by demonstrating the value of perseverance, courage, and kindness. When he faced failure after failure, he still pursued his goals and was determined. For example, he asked for the help of his grandfather and they created new traps to catch the monkeys. The net actually worked, though Jimbo freed the monkeys in the end. They made several attempts and finally Jay persuaded the gang of monkeys to follow him into the corn crib, where they would be provided with a comfortable home and plentiful food.He showed rationality when he thought about what would happen to the monkeys during the storm. He knew that these monkeys did not know how to react to this fierce storm and this was an ideal opportunity for him. This was how he got the monkeys to willingly follow him. Jay Berry was also very kind. One example was when he dried off the pitiful monkeys, who were freezing. His act of kindness allowed Jimbo to realize that he could trust Jay, who was just trying to help them. This taught me that a kind act would also help you in the end.</p>
<p>My favorite part of the story was when Jay Berry first discovered the tree full of monkeys. It was from here that he learned about the reward and was determined to capture them. He showed many values in his attempts and achieved true satisfaction in the end. My least favorite part was when he had to depart with his beloved pony. Even though he knew that he had done a good deed and his sister would no longer be a cripple, it broke his heart to acknowledge the fact that Dolly and he would be separated. Though, of course, he met Dolly again when his grandfather bought it using the money he saved. Yes, I would recommend this book to a friend because it taught how people could achieve their goals by putting their mind into it and persevering through challenges.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Book Report on The Hatchet</title>
    <url>/blog/book-report-on-the-hatchet/</url>
    <content><![CDATA[<p>After a terrible plane crash, Brian Robeson learns to survive alone in the wild, and in the meantime, develops manhood. The setting of the story is not clear, for Brian is very lost. Brian flies out of the airport in Hampton, New York, and is headed to the oilfields of North Canada, in order to visit his father. On his way there, the pilot gets a sudden heart attack, and bumps the steering wheel. This caused them to go off course. Brian lands the plane somewhere in the Northern Canadian woods, far away from where he was supposed to end up. The North Woods of Canada are home to a variety of animal species, including wolves, deer, black bears, otters, skunks, and many more. This setting of wildlife has a big impact on the plot. Brian is at the mercy of many dangerous animals. Some examples were when he got attacked by a porcupine, sprayed by a skunk, and charged by a moose. He must muster all the courage in him to survive in the wild. Lastly, this forces him to learn the “primitive ways of man”, like making a fire using sticks. This setting is the base of the story, for only in these conditions will he be faced with so many wild dangers.</p>
<p>The inciting incident of the story was when the pilot suddenly gets a fatal heart attack and dies moments later. He tries to send for help by radioing the station several times, but the connection soon breaks. It is up to Brian to save himself. Remembering the brief flying lesson he had from the lesson, he steers the plane and crash lands into a lake. He swims to shore and then faints due to exhaustion. When he wakes up, he knows that he has to find a food source and shelter. He eats gut berries(choke cherries), but gets sick since they were poisonous. He has to find a new food source and spots a raspberry bush. Now that he had the food problem covered, he had to find a suitable shelter. He decides to buy a lean-to, which is a shelter made of sticks and twigs. Once, he noticed that striking his hatchet on the stone created sparks. He tries to make a fire, and after many failures, finally succeeds. This is the turning point of his journey. Since he mastered fire, he could cook food instead of only eating berries. For a while after he masters fire, Brian has an easier time. He finds turtle eggs and starts exploring ways to hunt and fish. He had to “invent” the bow and arrow. After a while, he can easily catch fish to eat. Once, a search plane passes over him but sadly doesn’t spot Brian, which leaves him devastated. He loses his hope and will for a while, and even tried to commit suicide by cutting himself with the hachet. But soon he gets back at it. Brian perfects his bow and arrow design, and hunts fool birds and rabbits. One night, a skunk came because it smelled the hidden turtle eggs. Brian gets sprayed and almost loses his food supply. However, his optimism grows stronger with each setback, and he is determined to survive until he gets rescued.</p>
<p>This was until he got struck by the tornado. This is the climax of the story. Even though his shelter and food supply was all gone, Brian discovers the tail of the plane sticking out from the water after the tornado. He gets hopeful because there is a survival pack in the plane. After building a raft, he sails to the plane and uses his hatchet to get into the tail. He loses his hatchet and dives after it. He not only finds his hatchet but also the survival pack. When he returns to shore, he finds out what treasures the pack contained. It contains a .22 survival rifle, sleeping gear, pots, pans, food, matches, and an emergency transmitter. Thinking the transmitter is broken,  he flips the switches on and off a few times before tossing it away. He cooks a feast and then drifts off to sleep. Without warning, a bush plane arrives  and Brian gets rescued after spending 54 days in the woods. Brian returns to the city to live with his mother.He researches about the plants and animals he found, and sometimes dreams about his experiences in the woods. His parents never reconcile, and Bian cannot bring himself to tell his dad about the man with blond hair.</p>
<p>The main character of the story is Brian Robeson. He starts out as an ordinary thirteen-year old boy who lives in New York City. He lives with his mother  after his parent’s divorce. He is on his way to visit his father in the oilfields of Canada when the pilot gets a sudden heart attack and dies. The plane crashes, and Brian suddenly finds himself stranded in a desert place where there are no human beings but himself. The only tools he has with him are a hatchet and a tattered windbreaker. He is torn apart both externally and internally. His mother’s affair with another man affects Brian deeply. He bears the burden of “The Secret”, and is troubled to inform his father of it. Brian also has to face dangers from wildlife. He must learn to adapt to his new environment. It will take all his self-reliance, determination, and knowledge to survive. Brian learns to hunt game like small fish and foolbirds. He blends in with nature and appreciates its beauty. He realizes that as long as he is positive, he can accomplish his goals to achieve success. In order to live for 54 days in the woods, Brian shows the values of optimism, self-reliance, perseverance, and rationality. He arrives in the woods as a vulnerable and pitiful little boy. Brian gradually learns the power of positive thinking. Initially, Brian’s setbacks leave him frustrated, hopeless, and full of self-pity. He dreams of his past life of pleasure and ease. Things just came to him, and he never had trouble acquiring his daily needs. Now, alone in the woods, he has to fend for himself. Also, Brian recalls the words his English teacher used to say to him. Mr. Perpich was always saying, “You are your most valuable asset.” Brian realizes a little that it was up to him to improve his conditions and survive. One night, he was attacked by a porcupine and got hundreds of quills driven into his leg. For some time, Brian cries in pain and despair, but soon reemerges with a new perspective. Crying isn’t beneficial. In the city, it may arouse pity in people. But in the wild, he is alone and must bring about change. The most important rule in survival is that feeling sorry for yourself doesn’t work. This realization motivates Brian mentally. He has a general positive attitude, besides some lapses, most noticeably the suicide attempt. With each setback, he grows stronger. For example, the moose attack gravely injured him, but he persevered and kept fighting. Even though the tornado destroyed his shelter, he remained hopeful and discovered the tail of the plane, which led to his eventual rescue. I learned from Brian that in times of danger, negative values like self-pity and frustration only weakens your resolve. Dependence on others doesn’t help. If everyone relies on others to make a change, nothing will happen in society. I have to bring about a change to enhance my life.</p>
<p>The major theme of the story is developing manhood. At the beginning of the story, Brian thinks of himself as part of a family, so the divorce hit him with immense pain. He is no longer able to identify himself with his family. He is not ready to accept reality, which is becoming an adult and a totally separate person. The plane crash and his stay in the woods forces Brian to accept his manhood and make individual choices. He is faced with a decision which will change his whole life: To grow up and embrace challenges, or die. He accepts the challenge and all the responsibilities that come with it. He has experienced the pressures and difficulties of adulthood during his time alone in the woods, and that  is what makes him a man. When embracing challenges, Brian also develops self-worth and courage. His new found self made him realize that life is worth living, and he would never let death appeal to him again. He realizes that suicide is a way of cowardice, for he is unready to accept responsibility and challenges. He musters the courage he never knew he had, and uses it to overcome fears and difficulties. I can learn from him by persevering through challenges in life and solving problems, for that is what makes me stronger.</p>
<p>My favorite part of the story was when Brian found the survival pack. He was finally rewarded for his courage and self-reliance during his stay in the woods. He came so far from a mere child to a man who chose to be tough and embrace challenges. My least favorite part was when the plane missed him and flew away. He lost his will for a while and was desperate because that was the plane which was searching for him. He even tries to end the pain and suffering by cutting himself with the hatchet. Yes, I would recommend this book to a friend because it shows the importance of self-reliance and optimism. You can only bring changes to enhance your life by making rational choices, relying on only yourself, and thinking positively.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>btrfs - A modern copy on write(CoW) filesystem for Linux</title>
    <url>/blog/btrfs-a-modern-copy-on-write-cow-filesystem-for-linux/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>btrfs is a modern copy on write (CoW) filesystem for Linux aimed at implementing advanced features while also focusing on fault tolerance, repair and easy administration. Its main features and benefits are:</p>
<ul>
<li>Snapshots which do not make the full copy of files</li>
<li>RAID - support for software-based RAID 0, RAID 1, RAID 10</li>
<li>Self-healing - checksums for data and metadata, automatic detection of silent data corruptions</li>
</ul>
<p>Development of Btrfs started in 2007. Since that time, Btrfs is a part of the Linux kernel and is under active development.</p>
<h2 id="Copy-on-Write-CoW"><a href="#Copy-on-Write-CoW" class="headerlink" title="Copy on Write (CoW)"></a>Copy on Write (CoW)</h2><ul>
<li><p>The CoW operation is used on all writes to the filesystem (unless turned off, see below).</p>
</li>
<li><p>This makes it much easier to implement lazy copies, where the copy is initially just a reference to the original, but as the copy (or the original) is changed, the two versions diverge from each other in the expected way.</p>
</li>
<li><p>If you just write a file that didn’t exist before, then the data is written to empty space, and some of the metadata blocks that make up the filesystem are CoWed. In a “normal” filesystem, if you then go back and overwrite a piece of that file, then the piece you’re writing is put directly over the data it is replacing. In a CoW filesystem, the new data is written to a piece of free space on the disk, and only then is the file’s metadata changed to refer to the new data. At that point, the old data that was replaced can be freed up because nothing points to it any more.</p>
</li>
<li><p>If you make a snapshot (or a cp –reflink&#x3D;always) of a piece of data, you end up with two files that both reference the same data. If you modify one of those files, the CoW operation I described above still happens: the new data is written elsewhere, and the file’s metadata is updated to point at it, but the original data is kept, because it’s still referenced by the other file.</p>
</li>
<li><p>This leads to fragmentation in heavily updated-in-place files like VM images and database stores.</p>
</li>
<li><p>Note that this happens even if the data is not shared, because data is stored in segments, and only the newly updated part of a segment is subject to CoW.</p>
</li>
<li><p>If you mount the filesystem with nodatacow, or use chattr +C on the file, then it only does the CoW operation for data if there’s more than one copy referenced.</p>
</li>
<li><p>Some people insist that Btrfs does “Redirect-on-write” rather than “Copy-on-write” because Btrfs is based on a scheme for redirect-based updates of B-trees by Ohad Rodeh, and because understanding the code is easier with that mindset.</p>
</li>
</ul>
<h2 id="Filesystem-creation"><a href="#Filesystem-creation" class="headerlink" title="Filesystem creation"></a>Filesystem creation</h2><p>A Btrfs filesystem can be created on top of many devices, and more devices can be added after the FS has been created.</p>
<p>By default, metadata will be mirrored across two devices and data will be striped across all of the devices present. This is equivalent to mkfs.btrfs -m raid1 -d raid0.</p>
<p>If only one device is present, metadata will be duplicated on that one device. For HDD mkfs.btrfs -m dup -d single, for SSD (or non-rotational device) mkfs.btrfs -m single -d single.</p>
<p>mkfs.btrfs will accept more than one device on the command line. It has options to control the RAID configuration for data (-d) and metadata (-m). Valid choices are raid0, raid1, raid10, raid5, raid6, single and dup. The option -m single means that no duplication is done, which may be desired when using hardware RAID.</p>
<pre><code># Create a filesystem across four drives (metadata mirrored, linear data allocation)
$ mkfs.btrfs -d single /dev/sdb /dev/sdc /dev/sdd /dev/sde

# Stripe the data without mirroring, metadata are mirrored
$ mkfs.btrfs -d raid0 /dev/sdb /dev/sdc

# Use raid10 for both data and metadata
$ mkfs.btrfs -m raid10 -d raid10 /dev/sdb /dev/sdc /dev/sdd /dev/sde

# Don&#39;t duplicate metadata on a single drive (default on single SSDs)
$ mkfs.btrfs -m single /dev/sdb
</code></pre>
<p>Once you create a multi-device filesystem, you can use any device in the FS for the mount command. The btrfs file system size is the total size of the devices to create it.</p>
<pre><code>$ mkfs.btrfs /dev/sdb /dev/sdc /dev/sde
$ mount /dev/sde /mnt
$ df -h
</code></pre>
<p>The following commands can be used to check filesystem usage.</p>
<pre><code>$ btrfs filesystem show
$ btrfs filesystem df -h /mnt
$ btrfs filesystem usage /mnt
</code></pre>
<h2 id="Filesystem-deletion"><a href="#Filesystem-deletion" class="headerlink" title="Filesystem deletion"></a>Filesystem deletion</h2><pre><code>$ umount -f /mnt
$ wipefs --all -t btrfs /dev/sdb /dev/sdc /dev/sde
$ btrfs filesystem show
</code></pre>
<h2 id="Subvolumes-and-snapshots"><a href="#Subvolumes-and-snapshots" class="headerlink" title="Subvolumes and snapshots"></a>Subvolumes and snapshots</h2><p>Creating subvolumes and snapshots are the commonly used operations for btrfs.</p>
<p>Create btrfs filesystem with two disks:</p>
<pre><code>$ mkfs.btrfs -d raid0 /dev/sdd /dev/sdf
btrfs-progs v4.9.1
See http://btrfs.wiki.kernel.org for more information.

Label:              (null)
UUID:               6a154e4e-61d4-474e-9839-50d1fcd50bbb
Node size:          16384
Sector size:        4096
Filesystem size:    1.75TiB
Block group profiles:
  Data:             RAID0             2.00GiB
  Metadata:         RAID1             1.00GiB
  System:           RAID1             8.00MiB
SSD detected:       yes
Incompat features:  extref, skinny-metadata
Number of devices:  2
Devices:
   ID        SIZE  PATH
    1   894.25GiB  /dev/sdd
    2   894.25GiB  /dev/sdf

$ mkdir /mnt/pool1
$ mount -t btrfs /dev/sdd /mnt/pool1
$ df -h  | egrep &quot;Filesystem|pool1&quot;
Filesystem                 Size  Used Avail Use% Mounted on
/dev/sdd                   1.8T  4.3M  1.8T   1% /mnt/pool1
$ ls -la /mnt/pool1
total 16
drwxr-xr-x  1 root root  0 Mar 22 21:19 .
drwxr-xr-x. 7 root root 69 Mar 22 21:20 ..
</code></pre>
<p>Create a subvolume:</p>
<pre><code>$ btrfs subvolume create /mnt/pool1/subvol1
Create subvolume &#39;/mnt/pool1/subvol1&#39;

$ mkdir /mnt/testbtrfs
$ mount -t btrfs -o subvol=subvol1 /dev/sdd /mnt/testbtrfs

$ ls -la /mnt/pool1
total 16
drwxr-xr-x  1 root root 14 Mar 22 21:19 .
drwxr-xr-x. 8 root root 86 Mar 22 21:22 ..
drwxr-xr-x  1 root root  0 Mar 22 21:21 subvol1

$ ls -la /mnt/pool1/subvol1/
total 16
drwxr-xr-x 1 root root  0 Mar 22 21:21 .
drwxr-xr-x 1 root root 14 Mar 22 21:19 ..

$ ls -la /mnt/testbtrfs
total 0
drwxr-xr-x  1 root root  0 Mar 22 21:21 .
drwxr-xr-x. 8 root root 86 Mar 22 21:22 ..
</code></pre>
<p>Create snapshot of the subvolume:</p>
<pre><code>$ btrfs subvolume snapshot
btrfs subvolume snapshot: too few arguments
usage: btrfs subvolume snapshot [-r] [-i &lt;qgroupid&gt;] &lt;source&gt; &lt;dest&gt;|[&lt;dest&gt;/]&lt;name&gt;

    Create a snapshot of the subvolume

    Create a writable/readonly snapshot of the subvolume &lt;source&gt; with
    the name &lt;name&gt; in the &lt;dest&gt; directory.  If only &lt;dest&gt; is given,
    the subvolume will be named the basename of &lt;source&gt;.

    -r             create a readonly snapshot
    -i &lt;qgroupid&gt;  add the newly created snapshot to a qgroup. This
                   option can be given multiple times.

$ btrfs subvolume snapshot /mnt/pool1/subvol1 /mnt/pool1/subvol1/subvol1-snap
Create a snapshot of &#39;/mnt/pool1/subvol1&#39; in &#39;/mnt/pool1/subvol1/subvol1-snap&#39;

$ ls -la /mnt/pool1/subvol1/
total 16
drwxr-xr-x 1 root root 24 Mar 22 21:26 .
drwxr-xr-x 1 root root 14 Mar 22 21:19 ..
drwxr-xr-x 1 root root  0 Mar 22 21:21 subvol1-snap

$ ls -la /mnt/pool1/subvol1/subvol1-snap/
total 0
drwxr-xr-x 1 root root  0 Mar 22 21:21 .
drwxr-xr-x 1 root root 24 Mar 22 21:26 ..

$ ls -la /mnt/testbtrfs/
total 0
drwxr-xr-x  1 root root 24 Mar 22 21:26 .
drwxr-xr-x. 8 root root 86 Mar 22 21:22 ..
drwxr-xr-x  1 root root  0 Mar 22 21:21 subvol1-snap

$ ls -la /mnt/testbtrfs/subvol1-snap/
total 0
drwxr-xr-x 1 root root  0 Mar 22 21:21 .
drwxr-xr-x 1 root root 24 Mar 22 21:26 ..
</code></pre>
<p>List the subvoluems and snapshots:</p>
<pre><code>$  btrfs subvolume list /mnt/pool1
ID 258 gen 10 top level 5 path subvol1
ID 259 gen 9 top level 258 path subvol1/subvol1-snap
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://btrfs.wiki.kernel.org/index.php/Main_Page">https://btrfs.wiki.kernel.org/index.php/Main_Page</a></li>
<li><a href="https://btrfs.wiki.kernel.org/index.php/SysadminGuide">https://btrfs.wiki.kernel.org/index.php/SysadminGuide</a></li>
<li><a href="http://marc.merlins.org/perso/btrfs/post_2014-05-21_My-Btrfs-Talk-at-Linuxcon-JP-2014.html">http://marc.merlins.org/perso/btrfs/post_2014-05-21_My-Btrfs-Talk-at-Linuxcon-JP-2014.html</a></li>
<li><a href="http://marc.merlins.org/linux/talks/Btrfs-LC2014-JP/Btrfs.pdf">http://marc.merlins.org/linux/talks/Btrfs-LC2014-JP/Btrfs.pdf</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>Btrfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Buffered and Direct IO</title>
    <url>/blog/buffered-and-direct-io/</url>
    <content><![CDATA[<h2 id="Buffered-and-Direct-I-O"><a href="#Buffered-and-Direct-I-O" class="headerlink" title="Buffered and Direct I&#x2F;O"></a>Buffered and Direct I&#x2F;O</h2><p>The VxFS responds with read-ahead for sequential read I&#x2F;O. This results in buffered I&#x2F;O. The data is prefetched and retained in buffers, in anticipation of application asking for it. The data buffers are commonly referred to as the VxFS buffer cache. This is the default VxFS behavior.</p>
<p>Direct I&#x2F;O, on the other hand, does not buffer the data when the I&#x2F;O to the underlying device is completed. This saves system resources like memory and CPU usage. Direct I&#x2F;O is possible only when alignment and sizing criteria are satisfied.</p>
<p>All the supported platforms have a VxFS buffer cache. Each platform also has either a page cache, (aix&#x2F;solaris&#x2F;linux) or its own buffer cache (HP-UX). These caches are commonly known as the file system caches.</p>
<p>Direct I&#x2F;O does not use these caches. The memory used for direct I&#x2F;O is discarded after the I&#x2F;O is complete, and is therefore not buffered.</p>
<h2 id="Direct-I-O"><a href="#Direct-I-O" class="headerlink" title="Direct I&#x2F;O"></a>Direct I&#x2F;O</h2><p>Direct I&#x2F;O is an unbuffered form of I&#x2F;O. If the VX_DIRECT advisory is set, the user is requesting direct data transfer between the disk and the user-supplied buffer for reads and writes. This bypasses the kernel buffering of data, and reduces the CPU overhead associated with I&#x2F;O by eliminating the data copy between the kernel buffer and the user’s buffer. This also avoids taking up space in the buffer cache that might be better used for something else. The direct I&#x2F;O feature can provide significant performance gains for some applications.</p>
<p>The direct I&#x2F;O and VX_DIRECT advisories are maintained on a per-file-descriptor basis.</p>
<h2 id="Direct-I-O-requirements"><a href="#Direct-I-O-requirements" class="headerlink" title="Direct I&#x2F;O requirements"></a>Direct I&#x2F;O requirements</h2><p>For an I&#x2F;O operation to be performed as direct I&#x2F;O, it must meet certain alignment criteria. The alignment constraints are usually determined by the disk driver, the disk controller, and the system memory management hardware and software.</p>
<p>The requirements for direct I&#x2F;O are as follows:</p>
<p>The starting file offset must be aligned to a 512-byte boundary.</p>
<p>The ending file offset must be aligned to a 512-byte boundary, or the length must be a multiple of 512 bytes.</p>
<p>The memory buffer must start on an 8-byte boundary.</p>
<h2 id="Direct-I-O-vs-synchronous-I-O"><a href="#Direct-I-O-vs-synchronous-I-O" class="headerlink" title="Direct I&#x2F;O vs. synchronous I&#x2F;O"></a>Direct I&#x2F;O vs. synchronous I&#x2F;O</h2><p>Because direct I&#x2F;O maintains the same data integrity as synchronous I&#x2F;O, it can be used in many applications that currently use synchronous I&#x2F;O. If a direct I&#x2F;O request does not allocate storage or extend the file, the inode is not immediately written.</p>
<h2 id="Direct-I-O-CPU-overhead"><a href="#Direct-I-O-CPU-overhead" class="headerlink" title="Direct I&#x2F;O CPU overhead"></a>Direct I&#x2F;O CPU overhead</h2><p>The CPU cost of direct I&#x2F;O is about the same as a raw disk transfer. For sequential I&#x2F;O to very large files, using direct I&#x2F;O with large transfer sizes can provide the same speed as buffered I&#x2F;O with much less CPU overhead.</p>
<p>If the file is being extended or storage is being allocated, direct I&#x2F;O must write the inode change before returning to the application. This eliminates some of the performance advantages of direct I&#x2F;O.</p>
<h2 id="Discovered-Direct-I-O"><a href="#Discovered-Direct-I-O" class="headerlink" title="Discovered Direct I&#x2F;O"></a>Discovered Direct I&#x2F;O</h2><p>Discovered Direct I&#x2F;O is a file system tunable you can set using the vxtunefs command. When the file system gets an I&#x2F;O request larger than the discovered_direct_iosz, it tries to use direct I&#x2F;O on the request. For large I&#x2F;O sizes, Discovered Direct I&#x2F;O can perform much better than buffered I&#x2F;O.</p>
<p>Discovered Direct I&#x2F;O behavior is similar to direct I&#x2F;O and has the same alignment constraints, except writes that allocate storage or extend the file size do not require writing the inode changes before returning to the application.</p>
<h2 id="Unbuffered-I-O"><a href="#Unbuffered-I-O" class="headerlink" title="Unbuffered I&#x2F;O"></a>Unbuffered I&#x2F;O</h2><p>If the VX_UNBUFFERED advisory is set, I&#x2F;O behavior is the same as direct I&#x2F;O with the VX_DIRECT advisory set, so the alignment constraints that apply to direct I&#x2F;O also apply to unbuffered I&#x2F;O. For unbuffered I&#x2F;O, however, if the file is being extended, or storage is being allocated to the file, inode changes are not updated synchronously before the write returns to the user. The VX_UNBUFFERED advisory is maintained on a per-file-descriptor basis.</p>
<p>For information on how to set the discovered_direct_iosz, see Tuning I&#x2F;O.</p>
<h2 id="Data-synchronous-I-O"><a href="#Data-synchronous-I-O" class="headerlink" title="Data synchronous I&#x2F;O"></a>Data synchronous I&#x2F;O</h2><p>If the VX_DSYNC advisory is set, the user is requesting data synchronous I&#x2F;O. In synchronous I&#x2F;O, the data is written, and the inode is written with updated times and (if necessary) an increased file size. In data synchronous I&#x2F;O, the data is transferred to disk synchronously before the write returns to the user. If the file is not extended by the write, the times are updated in memory, and the call returns to the user. If the file is extended by the operation, the inode is written before the write returns.</p>
<p>The direct I&#x2F;O and VX_DSYNC advisories are maintained on a per-file-descriptor basis.</p>
<h2 id="Data-synchronous-I-O-vs-synchronous-I-O"><a href="#Data-synchronous-I-O-vs-synchronous-I-O" class="headerlink" title="Data synchronous I&#x2F;O vs. synchronous I&#x2F;O"></a>Data synchronous I&#x2F;O vs. synchronous I&#x2F;O</h2><p>Like direct I&#x2F;O, the data synchronous I&#x2F;O feature can provide significant application performance gains. Because data synchronous I&#x2F;O maintains the same data integrity as synchronous I&#x2F;O, it can be used in many applications that currently use synchronous I&#x2F;O. If the data synchronous I&#x2F;O does not allocate storage or extend the file, the inode is not immediately written. The data synchronous I&#x2F;O does not have any alignment constraints, so applications that find it difficult to meet the alignment constraints of direct I&#x2F;O should use data synchronous I&#x2F;O.</p>
<p>If the file is being extended or storage is allocated, data synchronous I&#x2F;O must write the inode change before returning to the application. This case eliminates the performance advantage of data synchronous I&#x2F;O.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://sort.veritas.com/public/documents/sf/5.0/aix/html/fs_admin/ag_ch_interface_fs4.html">https://sort.veritas.com/public/documents/sf/5.0/aix/html/fs_admin&#x2F;ag_ch_interface_fs4.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
  </entry>
  <entry>
    <title>Canadian Rockies Adventure</title>
    <url>/blog/canadian-rockies-adventure/</url>
    <content><![CDATA[<h2 id="Prologue"><a href="#Prologue" class="headerlink" title="Prologue"></a>Prologue</h2><p>I have always liked traveling to new countries. The breathtaking landscapes, vibrant city life, and the call of the wild have always beckoned to me. So, I embarked on an unforgettable journey to explore the wonders of Western Canada, particularly the picturesque gems of Calgary, Banff National Park, Jasper National Park, and Yoho National Park. From the urban charm of downtown Calgary to the awe-inspiring majesty of the national parks, every moment was a captivating experience.</p>
<span id="more"></span>
<p><img src="/images/ca11.jpg"></p>
<h2 id="Chapter-One"><a href="#Chapter-One" class="headerlink" title="Chapter One"></a>Chapter One</h2><p>On August 2nd, 2023, I went to the San Francisco Airport. I was so pumped and excited for this trip since I haven’t been on an airplane for a long time. Also my friends Jesse and Keyu would be coming with us. After a quick lunch, we boarded the plane. It was a two and a half hour flight to the Calgary International Airport. When we got there, I immediately realized that there was a time difference of one hour from the USA. Also, the Western Canadians used French as their main language besides English. We had a shuttle drop us off at our hotel, the Ramada Plaza. After relaxing for some time in the hotel, we were hungry and went to a Korean restaurant to eat. The food there was pretty good,though I was surprised that the buckwheat noodles were black and cold, while the “broth” was just water. After dinner, we went to downtown Calgary, where we enjoyed the music and urban charm of the city. It was bustling with people, and everyone was having a relaxed time. We then went to the Calgary tower, located 191 meters above the city floor. Sadly, we couldn’t go on the elevator to the top of the tower because the line to get tickets was too long. After getting an ice cream cone at McDonalds, we went back to our hotel.</p>
<p><img src="/images/ca6-1.jpg">Calgary Central Library</p>
<p>The next day, we went to the ZCREW Cafe, and wow, it was one of if not the best brunch I ever had. They served a delicious Belgian Waffle, topped with vanilla ice cream, fruits, maple syrup, and custard. The hot chocolate and the different kinds of coffee all had exquisite Latte art. There was also delicious tan-tan ramen and vegetable soup ramen. Overall, I felt like it was definitely worth the long walk to get to the restaurant. Afterwards, we went to the Calgary Central Library. The moment I saw it from the exterior I knew that it was much more than just a normal library. Normally, I would go to the Mountain View Library to get books, but the Calgary Library was 3 times the size of the Mountain View Library. It was constructed using amazing architecture, with half-moon shaped walls, and gentle terrace slopes to the heart of the building. There were books of various genres on every floor, with 4 floors in total. The entire building gave me a futuristic vibe. After spending some time reading books, we took the Calgary Transit to go to the Chinook Center, which was a major shopping center. While the parents went shopping, Jesse, Keyu, and I went to the Apple Store to look at some laptops. Time passed by quickly, and soon we were out of the store again, heading for the next stop, which was Prince’s Island. It was an island connected to Calgary by a bridge across the Bow River. To my surprise, there was a restaurant called River Cafe, which sold drinks, appetizers, and desserts. The adults all bought cocktails or lattes, while Jesse, Keyu, and I all bought berry lemonades. It was very refreshing and cooled us down from the blazing sun. After taking a walk around the park, we went back to Downtown Calgary, and ate at a restaurant called Silver Dragon, which sold dim sum. After dinner, we went to a popular cafe called CoCo fresh tea &amp; juice, which sold bubble tea and fresh fruit juices. I bought a brown sugar milk tea, while Keyu bought a traditional bubble tea. Afterwards, we were all tired from a day’s journey and went back to our hotel.</p>
<p><img src="/images/ca7-1.jpg">Banff Gondola</p>
<h2 id="Chapter-Two"><a href="#Chapter-Two" class="headerlink" title="Chapter Two"></a>Chapter Two</h2><p>The next day was the start of our booked tour to the national parks. We woke up at 6:00 a.m. to get breakfast and packed up our belongings, since we would be switching hotels. We boarded the tour bus at 7:00. First, we picked up people from other hotels, and then started our journey of sightseeing. Our tour guide, Peter, told us about Calgary and its rich history. What surprised me the most was the fact that Calgary held the 1988 Winter Olympics at the Calgary Olympics Plaza. After a two hour ride, we went to Surprise Corner, which gave us an amazing view of Lake Louise and the Fairmont Banff Springs Hotel. The Fairmont Hotel was by far the most grand and majestic looking hotel I had ever seen. It was 7 stories high, with chariots and horses which made it look like a castle.After that, we went to the Banff Gondola, which took us up to the upper terminal of Sulphur Mountain. During the ride, I was at first a bit scared because we were so high up in the air, and the ride was so bumpy. When we landed, we decided to walk along the ridge to Samson’s Peak. It gave me a fantastic view of the entire Banff National Park, and I could see the vast forests and mountains. I also realized that we were at such a high elevation that the amount of oxygen was reduced, making it harder to breathe. At the peak, we took man pictures before going back down the gondola. Then, we went to eat lunch, and we discovered the prized ice cream restaurant called Cows, which served handmade ice cream and waffle cones. Every scoop was made with the finest ingredients and it was ranked the #1 ice cream in the world by Tauck World Discovery. Next, we headed to the Tunnel Mountain View point, which gave us a spectacular view of the hoodoos (rock formations), and the Sleeping Bison, which is a small peak. Then, we drove back to the Banff town, which was the location of the Banff Aspen Lodge, a cozy hotel where we rested for the night. For dinner, we went to Ramen Adashi, a restaurant that served quality ramen. The serving size was quite small, so afterwards, we went for dessert at Beavertails. Jesse got a classic pastry with lemonade, Keyu got a Hazel Amourwith Big Foot Ice Cream, and I got a Hazel Amour with a lemonade Vanilla ice cream float. Afterwards, we went to the local market to buy fruits and then headed back to the hotel.</p>
<p><img src="/images/ca3.jpg">Maligne Lake</p>
<h2 id="Chapter-Three"><a href="#Chapter-Three" class="headerlink" title="Chapter Three"></a>Chapter Three</h2><p>The next morning, we went to Jasper National Park. Our tour guide, Allen, took us to see Maligne Canyon, which is a picturesque limestone gorge that stretches for 50 meters. It is widely regarded as the most captivating canyon in the Canadian Rockies. Hiking along the Maligne River offers stunning waterfalls and breathtaking scenery throughout. We went down to the river and started skipping stones. We then went to Athabasca Falls, which was located on the Athabasca River, which possessed immensely powerful waters from the glaciers. After eating at a local Korean restaurant, we went to Maligne Lake, and took the cruise to Spirit Island. There were stunning views of mountains, canyons, and glaciers. Our tour guide Bea informed us about the history of Spirit Island, but sadly, we couldn’t go on the island itself because it was a very religious area. When we got back, it was already 5:00 p.m. so we headed to the Jasper Lobstick Lodge. For dinner, we ate at One Sushi, which gave us delicious ramen and fresh sushi. I ordered Tonkotsu ramen with Dynamite Sushi Combo. After I finished, I was so full that I skipped dessert.</p>
<p><img src="/images/ca8.jpg"></p>
<h2 id="Chapter-Four"><a href="#Chapter-Four" class="headerlink" title="Chapter Four"></a>Chapter Four</h2><p>I wore my thickest and warmest jacket to prepare for the glacier ride. We would be riding on an Ice Explorer through the Athabasca Glaciers. We had to put on shades to protect our eyes from the blinding white ice. When we got down, we took some pictures and I drank water from the river made by the glacier. It was freezing cold, but the water was amazingly fresh and pure. Jesse, Keyu, and I played with the ice and tried to hit the ice midair. Then, we went to the Columbia Icefield Skywalk, where we gazed at the scenery on a glass bridge. It was pretty amazing looking at the transparent floor, and made me feel like I was floating. After we went back, we went to eat lunch at the resort, which sold burgers and drinks. We then started our long drive back to Banff town. After we got there, we ate dinner at Pho House. We ordered spring rolls and vegetable wraps as appetizers. I ordered sour and spicy rice noodles for myself. We were not full yet, so Keyu and I went to Beavertails again and got ourselves each a Brwownie. We then went shopping for some time, and then went back to the Banff Aspen Lodge to rest.</p>
<p><img src="/images/ca2-2.jpg">Athabasca Glaciers</p>
<h2 id="Chapter-Five"><a href="#Chapter-Five" class="headerlink" title="Chapter Five"></a>Chapter Five</h2><p>It was the last day before we went back to San Francisco. We first went to Yoho National Park. We looked at the clear turquoise waters of Emerald Lake. Then, we ate at the Fairmont Banff Springs Hotel. The hotel itself was very majestic, but the food was unappealing. The salad was greasy, the coffee was too sweet, the steak was too bland and overcooked, and the passion fruitcake was way too sour. After lunch, we went to take pictures of Lake Louise. Due to being located right next to Lake Louise, we easily got some quality close-up pictures of the serene blue lake. We also collected round stones to see who could skip the most times on the still lake.  This alpine lake was a turquoise blue color fed by glacier melt. This was without doubt the best lake I had seen in the Canadian Rockies. The lake was so huge that it took us 2 hours to travel around it. Afterwards, we went back on the bus for the long journey back to Calgary.</p>
<p><img src="/images/ca9.jpg"></p>
<h2 id="Epilogue"><a href="#Epilogue" class="headerlink" title="Epilogue"></a>Epilogue</h2><p>This trip was one of the most memorable trips I ever had. My journey through Calgary, Banff, Yoho, and Jasper was a symphony of urban exploration and immersion in the untamed beauty of nature. From the bustling streets of Calgary to the serene lakes of Banff and the wild landscapes of Jasper, each destination offered a unique perspective on the remarkable diversity that Western Canada has to offer.</p>
<p><img src="/images/ca10.jpg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Capture and analyze network packets with tcpdump</title>
    <url>/blog/capture-and-analyze-network-packets-with-tcpdump/</url>
    <content><![CDATA[<h2 id="Capture-packets-with-tcpdump"><a href="#Capture-packets-with-tcpdump" class="headerlink" title="Capture packets with tcpdump"></a>Capture packets with tcpdump</h2><p>In this example, we only capture 1000 packets(-c1000) and use IP addresses and ports(-nn) for easier analysis. The raw packets are written to file “tcpdump.1000” for further analsis.</p>
<pre><code>$ tcpdump -i eth5 -c1000 -nn -w tcpdump.1000
</code></pre>
<h2 id="View-the-packets-data"><a href="#View-the-packets-data" class="headerlink" title="View the packets data"></a>View the packets data</h2><p>We can use tcpdump to view the packets directly.</p>
<pre><code>$ tcpdump -nn -r tcpdump.1000 | more

reading from file tcpdump.1000, link-type EN10MB (Ethernet)

22:42:10.018758 IP 192.168.1.18.980 &gt; 192.168.1.16.2049: Flags [P.], seq 1:16545, ack 72, win 501, options [nop,nop,TS val 4292822374 ecr 2230894482], length 16544: NFS request xid 2912663971 16540 getattr fh 0,0/22
22:42:10.018895 IP 192.168.1.16.2049 &gt; 192.168.1.18.980: Flags [.], ack 16545, win 9508, options [nop,nop,TS val 2230894483 ecr 4292822374], length 0
22:42:10.021125 IP 192.168.1.16.2049 &gt; 192.168.1.18.980: Flags [P.], seq 72:144, ack 16545, win 9596, options [nop,nop,TS val 2230894485 ecr 4292822374], length 72: NFS reply xid 781957539 reply ok 68


22:42:10.021400 IP 192.168.1.18.980 &gt; 192.168.1.16.2049: Flags [P.], seq 16545:33089, ack 144, win 501, options [nop,nop,TS val 4292822377 ecr 2230894485], length 16544: NFS request xid 2929441187 16540 getattr fh 0,0/22
22:42:10.021536 IP 192.168.1.16.2049 &gt; 192.168.1.18.980: Flags [.], ack 33089, win 9508, options [nop,nop,TS val 2230894485 ecr 4292822377], length 0
22:42:10.023219 ARP, Request who-has 70.0.69.235 tell 70.0.193.122, length 46
22:42:10.023558 IP 192.168.1.16.2049 &gt; 192.168.1.18.980: Flags [P.], seq 144:216, ack 33089, win 9596, options [nop,nop,TS val 2230894487 ecr 4292822377], length 72: NFS reply xid 798734755 reply ok 68


22:42:10.023844 IP 192.168.1.18.980 &gt; 192.168.1.16.2049: Flags [P.], seq 33089:49633, ack 216, win 501, options [nop,nop,TS val 4292822379 ecr 2230894487], length 16544: NFS request xid 2946218403 16540 getattr fh 0,0/22
22:42:10.023962 IP 192.168.1.16.2049 &gt; 192.168.1.18.980: Flags [.], ack 49633, win 9508, options [nop,nop,TS val 2230894488 ecr 4292822379], length 0
22:42:10.025962 IP 192.168.1.16.2049 &gt; 192.168.1.18.980: Flags [P.], seq 216:288, ack 49633, win 9596, options [nop,nop,TS val 2230894490 ecr 4292822379], length 72: NFS reply xid 815511971 reply ok 68

&lt;omitted...&gt;
</code></pre>
<p>We also can view the data with wireshark GUI. We can use the filter to only display the data we are interested in. In this case, we only display the data related the ip address “192.168.1.18”. The wireshare can be installed on the desktop and open the captured packets data file.</p>
<p><img src="/images/tcpdump-wireshark.png" alt="Image"></p>
<h2 id="Understand-the-tcpdump-output"><a href="#Understand-the-tcpdump-output" class="headerlink" title="Understand the tcpdump output"></a>Understand the tcpdump output</h2><ul>
<li><p>The first field, <strong>22:42:10.021125</strong>, represents the timestamp of the captured packet.</p>
</li>
<li><p>The next field, <strong>IP</strong>, represents the network layer protocol, in this case, IPv4.</p>
</li>
<li><p>The next field, <strong>192.168.1.16.2049 &gt; 192.168.1.18.980</strong>, is the source and destination IP address and port.</p>
</li>
<li><p>The next field, <strong>Flags [P.]</strong>, represents the TCP flags. The typical values for this field include the following.</p>
</li>
</ul>
<p><img src="/images/tcpdump-tcp-flags.png" alt="Image"></p>
<ul>
<li><p>The next field, <strong>seq 72:144</strong>, is the sequence number of the data contained in the packet.  It means the packet contains bytes 72 to 144.</p>
</li>
<li><p>The next field, <strong>ack 16545</strong>, is the Ack number. For the side of receiving data, this field represents the next expected byte of data. In this case, the Ack number for the next expected packet would be 16545.</p>
</li>
<li><p>The next field, <strong>win 9596</strong>, is the window size. It represents the number of bytes available in the receiving buffer.</p>
</li>
<li><p>Followed by a field, <strong>length 72</strong>, which represents the length in bytes of the data.</p>
</li>
</ul>
<p>In the above example, the data are sending from 192.168.1.18 to 192.168.1.16. The average packet size is 16k which is sent over NFSv4.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Tcpdump</tag>
      </tags>
  </entry>
  <entry>
    <title>CBT - Changed Block Tracking</title>
    <url>/blog/cbt-changed-block-tracking/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Changed Block Tracking is an incremental backup technology for virtual machines. It helps create faster and smaller backups. It has the following advantages.</p>
<ul>
<li>Reduce backup time</li>
<li>Save disk space by only storing changed data to the previous backup</li>
</ul>
<p>Block changes are tracked in the virtualization layer, outside the virtual machines. During a backup, only the changed block since the last backup are transmitted. For VMware, the vSphere APIs can be used to request the VMkernel to return the changed blocks from the last snapshot backup. Microsoft provides Resilient Change Tracking(RCT) to provide the native CBT feature for Hyper-V.</p>
<p>Veritas NetBackup Accelerator reduces the backup time for VMware backups. NetBackup uses VMware Changed Block Tracking (CBT) to identify the changes that were made within a virtual machine. Only the changed data blocks are sent to the NetBackup media server, to significantly reduce the I&#x2F;O and backup time. The media server combines the new data with previous backup data and produces a traditional full NetBackup image that includes the complete virtual machine files.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://kb.vmware.com/s/article/1020128">https://kb.vmware.com/s/article/1020128</a></li>
<li><a href="https://www.ibm.com/support/knowledgecenter/SSERB6_8.1.2/ve.hv/c_ve_hv_ovw_rct.html">https://www.ibm.com/support/knowledgecenter/SSERB6_8.1.2&#x2F;ve.hv&#x2F;c_ve_hv_ovw_rct.html</a></li>
<li><a href="https://www.veritas.com/support/en_US/doc/21902280-127283730-0/v77418244-127283730">https://www.veritas.com/support/en_US&#x2F;doc&#x2F;21902280-127283730-0&#x2F;v77418244-127283730</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Backup Recovery</tag>
      </tags>
  </entry>
  <entry>
    <title>Using cgroups to limit CPU utilization</title>
    <url>/blog/cgroup-limit-cpu/</url>
    <content><![CDATA[<h2 id="Intro-to-cgroups"><a href="#Intro-to-cgroups" class="headerlink" title="Intro to cgroups"></a>Intro to cgroups</h2><p>Cgroups(control groups) make it possible to allocate system resources such as CPU time, memory, disk I&#x2F;O and network bandwidth, or combinations of them,  among a group of tasks(processes) running on a system.</p>
<p>The following commands output the available subsystems(resource controllers) for the cgroups. Each subsystem has a bunch of tunables to control the resource allocation.</p>
<pre><code>$ lssubsys -am
cpuset /sys/fs/cgroup/cpuset
cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct
blkio /sys/fs/cgroup/blkio
memory /sys/fs/cgroup/memory
devices /sys/fs/cgroup/devices
freezer /sys/fs/cgroup/freezer
net_cls,net_prio /sys/fs/cgroup/net_cls,net_prio
perf_event /sys/fs/cgroup/perf_event
hugetlb /sys/fs/cgroup/hugetlb
pids /sys/fs/cgroup/pids
rdma /sys/fs/cgroup/rdma

$ mount | grep cgroup
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
</code></pre>
<h2 id="CPU-subsystem-and-tunables"><a href="#CPU-subsystem-and-tunables" class="headerlink" title="CPU subsystem and tunables"></a>CPU subsystem and tunables</h2><h3 id="Ceiling-enforcement-parameters"><a href="#Ceiling-enforcement-parameters" class="headerlink" title="Ceiling enforcement parameters"></a>Ceiling enforcement parameters</h3><p>cpu.cfs_period_us</p>
<p>specifies a period of time in microseconds (µs, represented here as “us”) for how regularly a cgroup’s access to CPU resources should be reallocated. If tasks in a cgroup should be able to access a single CPU for 0.2 seconds out of every 1 second, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 1000000. The upper limit of the cpu.cfs_quota_us parameter is 1 second and the lower limit is 1000 microseconds.</p>
<p>cpu.cfs_quota_us</p>
<p>specifies the total amount of time in microseconds (µs, represented here as “us”) for which all tasks in a cgroup can run during one period (as defined by cpu.cfs_period_us). As soon as tasks in a cgroup use up all the time specified by the quota, they are throttled for the remainder of the time specified by the period and not allowed to run until the next period. If tasks in a cgroup should be able to access a single CPU for 0.2 seconds out of every 1 second, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 1000000. Note that the quota and period parameters operate on a CPU basis. To allow a process to fully utilize two CPUs, for example, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 100000.</p>
<p>Setting the value in cpu.cfs_quota_us to -1 indicates that the cgroup does not adhere to any CPU time restrictions. This is also the default value for every cgroup (except the root cgroup).</p>
<h3 id="Relative-shares-parameter"><a href="#Relative-shares-parameter" class="headerlink" title="Relative shares parameter"></a>Relative shares parameter</h3><p>cpu.shares</p>
<p>contains an integer value that specifies a relative share of CPU time available to the tasks in a cgroup. For example, tasks in two cgroups that have cpu.shares set to 100 will receive equal CPU time, but tasks in a cgroup that has cpu.shares set to 200 receive twice the CPU time of tasks in a cgroup where cpu.shares is set to 100. The value specified in the cpu.shares file must be 2 or higher.</p>
<p>Note that shares of CPU time are distributed per all CPU cores on multi-core systems. Even if a cgroup is limited to less than 100% of CPU on a multi-core system, it may use 100% of each individual CPU core.</p>
<p>Using relative shares to specify CPU access has two implications on resource management that should be considered:</p>
<p>Because the CFS does not demand equal usage of CPU, it is hard to predict how much CPU time a cgroup will be allowed to utilize. When tasks in one   cgroup are idle and are not using any CPU time, the leftover time is collected in a global pool of unused CPU cycles. Other cgroups are allowed to borrow CPU cycles from this pool.</p>
<p>The actual amount of CPU time that is available to a cgroup can vary depending on the number of cgroups that exist on the system. If a cgroup has a relative share of 1000 and two other cgroups have a relative share of 500, the first cgroup receives 50% of all CPU time in cases when processes in all cgroups attempt to use 100% of the CPU. However, if another cgroup is added with a relative share of 1000, the first cgroup is only allowed 33% of the CPU (the rest of the cgroups receive 16.5%, 16.5%, and 33% of CPU).</p>
<h2 id="Using-libcgroup-tools"><a href="#Using-libcgroup-tools" class="headerlink" title="Using libcgroup tools"></a>Using libcgroup tools</h2><p>Install libcgroup package to manage cgroups:</p>
<pre><code>$ yum install libcgroup libcgroup-tools
</code></pre>
<p>List the cgroups:</p>
<pre><code>$ lscgroup
hugetlb:/
cpu,cpuacct:/
cpuset:/
blkio:/
memory:/
freezer:/
net_cls,net_prio:/
pids:/
rdma:/
perf_event:/
devices:/
devices:/system.slice
devices:/system.slice/irqbalance.service
devices:/system.slice/systemd-udevd.service
devices:/system.slice/polkit.service
devices:/system.slice/chronyd.service
devices:/system.slice/auditd.service
devices:/system.slice/tuned.service
devices:/system.slice/systemd-journald.service
devices:/system.slice/sshd.service
devices:/system.slice/crond.service
devices:/system.slice/NetworkManager.service
devices:/system.slice/rsyslog.service
devices:/system.slice/abrtd.service
devices:/system.slice/lvm2-lvmetad.service
devices:/system.slice/postfix.service
devices:/system.slice/dbus.service
devices:/system.slice/system-getty.slice
devices:/system.slice/systemd-logind.service
devices:/system.slice/abrt-oops.service

$ ls /sys/fs/cgroup
blkio  cpuacct      cpuset   freezer  memory   net_cls,net_prio  perf_event  rdma
cpu    cpu,cpuacct  devices  hugetlb  net_cls  net_prio          pids        systemd
</code></pre>
<p>Create the cgroup:</p>
<pre><code>$ cgcreate -g cpu:/cpulimited

$ lscgroup | grep cpulimited
cpu,cpuacct:/cpulimited

$ ls cpulimited/
cgroup.clone_children  cpuacct.usage_percpu       cpu.cfs_period_us  cpu.stat
cgroup.procs           cpuacct.usage_percpu_sys   cpu.cfs_quota_us   notify_on_release
cpuacct.stat           cpuacct.usage_percpu_user  cpu.rt_period_us   tasks
cpuacct.usage          cpuacct.usage_sys          cpu.rt_runtime_us
cpuacct.usage_all      cpuacct.usage_user         cpu.shares
</code></pre>
<p>Limit CPU utilization by percentage:</p>
<pre><code>$ lscpu | grep ^CPU\(s\):
CPU(s):                96

$ cgset -r cpu.cfs_quota_us=200000 cpulimited
</code></pre>
<p>Check the cgroup settings:</p>
<pre><code>$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: 200000

$ cgget -g cpu:cpulimited
cpulimited:
cpu.cfs_period_us: 100000
cpu.stat: nr_periods 2
    nr_throttled 0
    throttled_time 0
cpu.shares: 1024
cpu.cfs_quota_us: 200000
cpu.rt_runtime_us: 0
cpu.rt_period_us: 1000000
</code></pre>
<p>Delete the cgroup:</p>
<pre><code>$ cgdelete cpu,cpuacct:/cpulimited
</code></pre>
<h2 id="Verify-the-CPU-utilization-with-fio-workload"><a href="#Verify-the-CPU-utilization-with-fio-workload" class="headerlink" title="Verify the CPU utilization with fio workload"></a>Verify the CPU utilization with fio workload</h2><p>Create a fio job file:</p>
<pre><code>$ cat burn_cpu.job
[burn_cpu]
# Don&#39;t transfer any data, just burn CPU cycles
ioengine=cpuio
# Stress the CPU at 100%
cpuload=100
# Make 4 clones of the job
numjobs=4
</code></pre>
<p>Run the fio jobs without CPU limit:</p>
<pre><code>$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: -1

$ cgexec -g cpu:cpulimited fio burn_cpu.job
</code></pre>
<p>Check the CPU usage:</p>
<pre><code>$ top
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
13775 root      20   0 1079912   4016   2404 R 100.0  0.0   0:11.65 fio
13776 root      20   0 1079916   4004   2392 R 100.0  0.0   0:11.65 fio
13777 root      20   0 1079920   4004   2392 R 100.0  0.0   0:11.65 fio
13778 root      20   0 1079924   4004   2392 R 100.0  0.0   0:11.65 fio
</code></pre>
<p>The CPU utilization is 400% for the 4 fio jobs when there is no CPU limit set. Note that, there is totally 9600% CPU bandwidth available.</p>
<p>Limit the CPU utilization to 200%:</p>
<pre><code>$ cgset -r cpu.cfs_quota_us=200000 cpulimited

$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: 200000
</code></pre>
<p>Run the fio jobs again:</p>
<pre><code>$ cgexec -g cpu:cpulimited fio burn_cpu.job
</code></pre>
<p>Check the CPU usage:</p>
<pre><code>$ top
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
12908 root      20   0 1079916   3948   2336 R  50.3  0.0   0:06.91 fio
12909 root      20   0 1079920   3948   2336 R  50.0  0.0   0:06.88 fio
12910 root      20   0 1079924   3948   2336 R  50.0  0.0   0:06.93 fio
12907 root      20   0 1079912   3948   2336 R  49.3  0.0   0:06.86 fio
</code></pre>
<p>The CPU utilization is 200% for the 4 fio jobs when the CPU utilization is limited to 200%.</p>
<p>Check the processes are running on which CPU cores:</p>
<pre><code>$ mpstat -P ALL 5 | awk &#39;&#123;if ($3==&quot;CPU&quot; || $NF&lt;99)print;&#125;&#39;

12:40:32 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
12:40:37 AM  all    2.11    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   97.89
12:40:37 AM    0   20.52    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   79.48
12:40:37 AM    1   50.60    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.40
12:40:37 AM    2   50.10    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.90
12:40:37 AM   24   29.74    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   70.26
12:40:37 AM   87   50.20    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.80

12:40:37 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
12:40:42 AM  all    2.11    0.00    0.01    0.00    0.00    0.00    0.00    0.00    0.00   97.88
12:40:42 AM    0   11.49    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00   88.31
12:40:42 AM    1   50.60    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.40
12:40:42 AM    2   50.30    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.70
12:40:42 AM   24   38.97    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   61.03
12:40:42 AM   87   49.90    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   50.10
</code></pre>
<p>The 4 fio jobs are running on 5 CPU cores with total utilization of 200%. So, it indicates this method limits the total CPU utilization out of all the CPU cores. However, the number of CPU cores is not limited.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.kernel.org/admin-guide/cgroup-v1/cgroups.html">https://docs.kernel.org/admin-guide/cgroup-v1/cgroups.html</a></li>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpu">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux&#x2F;6&#x2F;html&#x2F;resource_management_guide&#x2F;sec-cpu</a></li>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/chap-using_libcgroup_tools">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux&#x2F;7&#x2F;html&#x2F;resource_management_guide&#x2F;chap-using_libcgroup_tools</a></li>
<li><a href="https://scoutapm.com/blog/restricting-process-cpu-usage-using-nice-cpulimit-and-cgroups">https://scoutapm.com/blog/restricting-process-cpu-usage-using-nice-cpulimit-and-cgroups</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Cgroups</tag>
      </tags>
  </entry>
  <entry>
    <title>Using cgroups to limit block device bandwidth</title>
    <url>/blog/cgroups-limit-io/</url>
    <content><![CDATA[<p>The block I&#x2F;O controller specifies upper IO rate limits on devices.</p>
<pre><code>$ mount | egrep &quot;/cgroup |/blkio&quot;
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)

$ lscgroup  | grep blkio
blkio:/
</code></pre>
<p>In this post, we will learn how to use the following control files to limit block device bandwidth for the user tasks.</p>
<ul>
<li>blkio.throttle.read_bps_device - Specifies upper limit on READ rate from the device. IO rate is specified in bytes per second. Rules are per device.</li>
<li>blkio.throttle.read_iops_device - Specifies upper limit on READ rate from the device. IO rate is specified in IO per second. Rules are per device.</li>
<li>blkio.throttle.write_bps_device - Specifies upper limit on WRITE rate to the device. IO rate is specified in bytes per second. Rules are per device.</li>
<li>blkio.throttle.write_iops_device - Specifies upper limit on WRITE rate to the device. IO rate is specified in io per second. Rules are per device.</li>
</ul>
<h2 id="Create-blkio-control-group"><a href="#Create-blkio-control-group" class="headerlink" title="Create blkio control group"></a>Create blkio control group</h2><p>Install libcgroup package to manage cgroups:</p>
<pre><code>$ yum install libcgroup libcgroup-tools
</code></pre>
<p>Create blkio control group:</p>
<pre><code>$ cgcreate -g blkio:/blkiolimited

$ lscgroup | grep blkio
blkio:/
blkio:/blkiolimited

$ ls /sys/fs/cgroup/blkio/blkiolimited/
blkio.bfq.io_service_bytes            blkio.bfq.weight                 blkio.throttle.io_service_bytes_recursive  blkio.throttle.read_iops_device   cgroup.procs
blkio.bfq.io_service_bytes_recursive  blkio.bfq.weight_device          blkio.throttle.io_serviced                 blkio.throttle.write_bps_device   notify_on_release
blkio.bfq.io_serviced                 blkio.reset_stats                blkio.throttle.io_serviced_recursive       blkio.throttle.write_iops_device  tasks
blkio.bfq.io_serviced_recursive       blkio.throttle.io_service_bytes  blkio.throttle.read_bps_device             cgroup.clone_children
</code></pre>
<h2 id="Limit-the-block-device-bandwidth"><a href="#Limit-the-block-device-bandwidth" class="headerlink" title="Limit the block device bandwidth"></a>Limit the block device bandwidth</h2><h3 id="Limit-block-device-bandwidth-for-root-group"><a href="#Limit-block-device-bandwidth-for-root-group" class="headerlink" title="Limit block device bandwidth for root group"></a>Limit block device bandwidth for root group</h3><p>To specify a bandwidth rate on particular device for root group, we can use the policy format as “major:minor bytes_per_second”.</p>
<p>The following example puts a bandwidth limit of 1MB&#x2F;s on writes for root group on device having major&#x2F;minor number 259:12.</p>
<pre><code>$ ls -la /dev/ | grep &quot;nvme3n1&quot;
brw-rw----   1 root disk    259,  12 Dec 23 21:44 nvme3n1

$ echo &quot;259:12  1048576&quot; &gt; /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device
</code></pre>
<h3 id="Limit-block-device-bandwidth-for-user-defined-group"><a href="#Limit-block-device-bandwidth-for-user-defined-group" class="headerlink" title="Limit block device bandwidth for user defined group"></a>Limit block device bandwidth for user defined group</h3><p>The following examples specifies the IOPS limit on the device 259:12 for the user defined cgroups “blkio:&#x2F;blkiolimited”.</p>
<p>Use control files to specify the limit directly:</p>
<pre><code>$ cat /sys/fs/cgroup/blkio/blkiolimited/blkio.throttle.write_iops_device
$ echo &quot;259:12  8192&quot; &gt; /sys/fs/cgroup/blkio/blkiolimited/blkio.throttle.write_iops_device
$ cat /sys/fs/cgroup/blkio/blkiolimited/blkio.throttle.write_iops_device
259:12 8192
</code></pre>
<p>Use libcgroup tools to specify the limit:</p>
<pre><code>$ cgset -r blkio.throttle.write_iops_device=&quot;259:12 8192&quot; blkiolimited
$ cgget -r blkio.throttle.write_iops_device blkiolimited
blkiolimited:
blkio.throttle.write_iops_device: 259:12 8192
</code></pre>
<h2 id="Verify-the-disk-bandwidth-usage"><a href="#Verify-the-disk-bandwidth-usage" class="headerlink" title="Verify the disk bandwidth usage"></a>Verify the disk bandwidth usage</h2><h3 id="Use-fio-to-write-50G-data-on-root-group-unlimited-bandwidth"><a href="#Use-fio-to-write-50G-data-on-root-group-unlimited-bandwidth" class="headerlink" title="Use fio to write 50G data on root group(unlimited bandwidth)"></a>Use fio to write 50G data on root group(unlimited bandwidth)</h3><pre><code>$ fio --blocksize=4k --ioengine=libaio --readwrite=randwrite --filesize=50G --group_reporting --direct=1 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat


$ iostat -ktdx 5 | grep &quot;nvme3n1&quot;
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme3n1           0.00     0.05    0.00    8.02     0.01   508.54   126.83     0.06    7.80    0.08    7.80   0.04   0.03
nvme3n1           0.00     0.00    0.00 133828.60     0.00 535314.40     8.00     1.18    0.01    0.00    0.01   0.00  55.64
nvme3n1           0.00     0.20    0.00 246289.20     0.00 985157.60     8.00     2.21    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 246518.80     0.00 986076.80     8.00     2.23    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 244115.60     0.00 976462.40     8.00     2.24    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 240184.00     0.00 960736.80     8.00     2.23    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 250391.60     0.00 1001567.20     8.00     2.41    0.01    0.00    0.01   0.00 100.04
nvme3n1           0.00     0.20    0.00 262449.60     0.00 1049799.20     8.00     2.53    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 252171.20     0.00 1008685.60     8.00     2.30    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 236467.60     0.00 945872.00     8.00     2.16    0.01    0.00    0.01   0.00 100.00
nvme3n1           0.00     0.20    0.00 255060.80     0.00 1020244.00     8.00    17.04    0.07    0.00    0.07   0.00 100.00
nvme3n1           0.00     0.20    0.00 235199.60     0.00 940798.40     8.00    44.76    0.19    0.00    0.19   0.00 100.00
nvme3n1           0.00     0.20    0.00 18768.00     0.00 75072.80     8.00     5.81    0.31    0.00    0.31   0.00   8.40
nvme3n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
^C
</code></pre>
<h3 id="Use-fio-to-write-50G-data-on-user-defined-group-limited-bandwidth"><a href="#Use-fio-to-write-50G-data-on-user-defined-group-limited-bandwidth" class="headerlink" title="Use fio to write 50G data on user defined group(limited bandwidth)"></a>Use fio to write 50G data on user defined group(limited bandwidth)</h3><pre><code>$ cgget -r blkio.throttle.write_iops_device blkiolimited
blkiolimited:
blkio.throttle.write_iops_device: 259:12 8192

$ cgexec -g blkio:blkiolimited fio --blocksize=4k --ioengine=libaio --readwrite=randwrite --filesize=50G --group_reporting --direct=1 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat


$ iostat -ktdx 5 | grep &quot;nvme3n1&quot;
nvme3n1           0.00     0.05    0.00   24.54     0.01   574.51    46.82     0.06    2.57    0.08    2.57   0.01   0.04
nvme3n1           0.00     0.00    0.00 3112.20     0.00 12448.80     8.00     0.04    0.01    0.00    0.01   0.01   4.60
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.11    0.01    0.00    0.01   0.01  11.92
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.01  12.02
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.01  12.22
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.01  12.10
nvme3n1           0.00     0.20    0.00 8190.40     0.00 32762.40     8.00     0.12    0.01    0.00    0.01   0.02  12.36
^C
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.kernel.org/admin-guide/cgroup-v1/blkio-controller.html">https://docs.kernel.org/admin-guide/cgroup-v1/blkio-controller.html</a></li>
<li><a href="https://andrestc.com/post/cgroups-io/">https://andrestc.com/post/cgroups-io/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Cgroups</tag>
      </tags>
  </entry>
  <entry>
    <title>Using cgroups to limit Memory usage</title>
    <url>/blog/cgroups-limit-memory/</url>
    <content><![CDATA[<p>The memory controller isolates the memory behaviour of a group of tasks from the rest of the system.</p>
<pre><code>$ mount | egrep &quot;/cgroup |/memory&quot;
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)

$ lssubsys -am | grep memory
memory /sys/fs/cgroup/memory
</code></pre>
<p>In this post, we will learn how to use the following control files to limit and monitor memory usage for the user tasks.</p>
<ul>
<li>memory.usage_in_bytes - show current usage for memory</li>
<li>memory.limit_in_bytes - set&#x2F;show limit of memory usage</li>
</ul>
<h2 id="Create-memory-control-group"><a href="#Create-memory-control-group" class="headerlink" title="Create memory control group"></a>Create memory control group</h2><p>Install libcgroup package to manage cgroups:</p>
<pre><code>$ yum install libcgroup libcgroup-tools
</code></pre>
<p>Create memory control group:</p>
<pre><code>$ cgcreate -g memory:/memlimited
$ lscgroup | grep memory
memory:/
memory:/memlimited

$ ls /sys/fs/cgroup/memory/memlimited/
cgroup.clone_children           memory.kmem.slabinfo                memory.memsw.failcnt             memory.soft_limit_in_bytes
cgroup.event_control            memory.kmem.tcp.failcnt             memory.memsw.limit_in_bytes      memory.stat
cgroup.procs                    memory.kmem.tcp.limit_in_bytes      memory.memsw.max_usage_in_bytes  memory.swappiness
memory.failcnt                  memory.kmem.tcp.max_usage_in_bytes  memory.memsw.usage_in_bytes      memory.usage_in_bytes
memory.force_empty              memory.kmem.tcp.usage_in_bytes      memory.move_charge_at_immigrate  memory.use_hierarchy
memory.kmem.failcnt             memory.kmem.usage_in_bytes          memory.numa_stat                 notify_on_release
memory.kmem.limit_in_bytes      memory.limit_in_bytes               memory.oom_control               tasks
memory.kmem.max_usage_in_bytes  memory.max_usage_in_bytes           memory.pressure_level
</code></pre>
<h2 id="Limit-the-memory-usage"><a href="#Limit-the-memory-usage" class="headerlink" title="Limit the memory usage"></a>Limit the memory usage</h2><h3 id="Using-control-files-directly"><a href="#Using-control-files-directly" class="headerlink" title="Using control files directly"></a>Using control files directly</h3><pre><code>$ echo 32G &gt; /sys/fs/cgroup/memory/memlimited/memory.limit_in_bytes
$ cat /sys/fs/cgroup/memory/memlimited/memory.limit_in_bytes
34359738368
</code></pre>
<h3 id="Using-libcgroup-tools"><a href="#Using-libcgroup-tools" class="headerlink" title="Using libcgroup tools"></a>Using libcgroup tools</h3><p>Limit the memory usage:</p>
<pre><code>$ cgset -r memory.limit_in_bytes=32G memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 34359738368
</code></pre>
<h2 id="Verify-the-memory-usage"><a href="#Verify-the-memory-usage" class="headerlink" title="Verify the memory usage"></a>Verify the memory usage</h2><h3 id="Unlimit-the-memory-usage"><a href="#Unlimit-the-memory-usage" class="headerlink" title="Unlimit the memory usage"></a>Unlimit the memory usage</h3><pre><code>$ cgset -r memory.limit_in_bytes=-1 memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 9223372036854771712
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code>$ echo 3 &gt; /proc/sys/vm/drop_caches
$ cgexec -g memory:memlimited fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is unlimited:</p>
<pre><code>$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
14143488
14143488
819499008
13288558592
25772953600
38258790400
50776608768
55638511616
55638429696
55638478848
55638528000
55638577152
55210229760
55210229760
^C
</code></pre>
<h3 id="Limit-the-memory-usage-to-32GB"><a href="#Limit-the-memory-usage-to-32GB" class="headerlink" title="Limit the memory usage to 32GB"></a>Limit the memory usage to 32GB</h3><pre><code>$ cgset -r memory.limit_in_bytes=32G memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 34359738368
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code>$ echo 3 &gt; /proc/sys/vm/drop_caches
$ cgexec -g memory:memlimited fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is limited to 32GB:</p>
<pre><code>$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
9134080
6819614720
18048208896
29089763328
34359726080
34359672832
34359607296
34359717888
34359635968
34359619584
34280120320
34280120320
^C
</code></pre>
<p>From vmstat output, the cache usage is limited to 32GB. There is also swapping out activity due the memory pressure.</p>
<pre><code>$ vmstat 5 -t
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 UTC
 0  0      0 1053891776    968 142992    0    0     0    30    0    0  0  0 100  0  0 2022-12-23 22:04:30
 2  0      0 1045755200    968 8266836    0    0  1697     0 3789  727  2  1 98  0  0 2022-12-23 22:04:35
 1  0      0 1034800320    976 19222628    0    0     0     2 1770  575  0  1 99  0  0 2022-12-23 22:04:40
 1  0      0 1024034304    984 29988256    0    0     0 163843 2999  780  0  1 99  0  0 2022-12-23 22:04:45
 1  1      0 1020353664    992 33665560    0    0     0 985500 8317 2414  0  1 98  0  0 2022-12-23 22:04:50
 1  1 317440 1020354112   1000 33666764    0 63474     3 1518376 18524 2604  0  1 98  1  0 2022-12-23 22:04:55
 1  1 340480 1020353472   1008 33666660    0 4571     0 1602014 15377 2381  0  1 98  1  0 2022-12-23 22:05:00
 2  0 340480 1020355904   1016 33667212   39    0    39 1632866 22166 70618  0  1 98  0  0 2022-12-23 22:05:05
 1  0 340480 1020356032   1020 33667228    0    0     0 1861355 27437 108029  0  1 99  0  0 2022-12-23 22:05:10
 2  0 340480 1020356224   1024 33667236    0    0     0 1874438 28732 111364  0  1 98  0  0 2022-12-23 22:05:15
 0  0    512 1020432704   1024 33599516  212    0   550 915429 13644 56805  0  1 99  0  0 2022-12-23 22:05:20
 0  0    512 1020432960   1028 33599516    0    0     3     4  165  149  0  0 100  0  0 2022-12-23 22:05:25
^C
</code></pre>
<h3 id="Limit-the-memory-usage-for-the-tasks-in-the-current-bash"><a href="#Limit-the-memory-usage-for-the-tasks-in-the-current-bash" class="headerlink" title="Limit the memory usage for the tasks in the current bash"></a>Limit the memory usage for the tasks in the current bash</h3><pre><code>$ cat /sys/fs/cgroup/memory/memlimited/tasks
$ echo $$ &gt; /sys/fs/cgroup/memory/memlimited/tasks
$ cat /sys/fs/cgroup/memory/memlimited/tasks
27875
28889

$ ps -ef | egrep &quot;27875|29023&quot; | grep -v grep
root     27875 27873  0 21:11 pts/0    00:00:17 -bash
root     29026 27875  0 22:11 pts/0    00:00:00 ps -ef
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code>$ fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is limited to 32GB:</p>
<pre><code>$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
22835200
22835200
22835200
10150678528
21453983744
32693850112
34359607296
34359607296
34359738368
34359730176
34359730176
34359660544
34280062976
^C


$ vmstat 5 -t
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 UTC
 0  0    512 1053886784   1004 144860    0    0     0    31    0    0  0  0 100  0  0 2022-12-23 22:09:17
 0  0    512 1053886528   1004 144824    0    0     0     0  143  148  0  0 100  0  0 2022-12-23 22:09:22
 1  0    512 1050907776   1004 3109688    0    0  1660     3 2690  566  1  0 98  0  0 2022-12-23 22:09:27
 1  0    512 1039836288   1012 14180528    0    0     0     2 1699  508  0  1 99  0  0 2022-12-23 22:09:32
 1  0    512 1028822720   1020 25194536    0    0     0     2 1625  494  0  1 99  0  0 2022-12-23 22:09:37
 1  1    512 1020348800   1028 33665604    0    0     0 385026 4478 1313  0  1 99  0  0 2022-12-23 22:09:42
 1  1  46080 1020340224   1036 33676796    0 9090     0 1563114 13781 2301  0  1 98  1  0 2022-12-23 22:09:47
 1  1 340480 1020338688   1044 33676544    0 58904     0 1557223 19212 2580  0  1 98  1  0 2022-12-23 22:09:52
 1  1 340480 1020337792   1052 33676376   33    0    33 1481028 14133 5776  0  1 98  1  0 2022-12-23 22:09:57
 1  0 340480 1020342208   1056 33676396    0    0   184 1877767 27447 114955  0  1 98  0  0 2022-12-23 22:10:02
 1  1 340480 1020341056   1056 33677100    0    0     0 1817494 27635 101675  0  1 98  0  0 2022-12-23 22:10:07
 1  0 195144 1020350208   1060 33676988   83    0    83 1872111 27068 113280  0  1 98  0  0 2022-12-23 22:10:12
^C
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.kernel.org/admin-guide/cgroup-v1/memory.html">https://docs.kernel.org/admin-guide/cgroup-v1/memory.html</a></li>
<li><a href="https://engineering.linkedin.com/blog/2016/08/don_t-let-linux-control-groups-uncontrolled">https://engineering.linkedin.com/blog/2016/08/don_t-let-linux-control-groups-uncontrolled</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Cgroups</tag>
      </tags>
  </entry>
  <entry>
    <title>CockroachDB performance benchmarking</title>
    <url>/blog/cockroachdb-performance-benchmarking/</url>
    <content><![CDATA[<h2 id="CockroachDB-key-concepts"><a href="#CockroachDB-key-concepts" class="headerlink" title="CockroachDB key concepts"></a>CockroachDB key concepts</h2><p>Range -	CockroachDB stores all user data (tables, indexes, etc.) and almost all system data in a giant sorted map of key-value pairs. This keyspace is divided into “ranges”, contiguous chunks of the keyspace, so that every key can always be found in a single range.</p>
<p>From a SQL perspective, a table and its secondary indexes initially map to a single range, where each key-value pair in the range represents a single row in the table (also called the primary index because the table is sorted by the primary key) or a single row in a secondary index. As soon as that range reaches 512 MiB in size, it splits into two ranges. This process continues for these new ranges as the table and its indexes continue growing.</p>
<pre><code>[root@host1 ~]# tail -f /var/log/cockroachdb_logs/cockroach.log
I220908 17:54:02.220498 5523606 kv/kvserver/pkg/kv/kvserver/replica_command.go:420 ⋮ [n1,split,s1,r499/1:‹/Table/113/1/4&#123;394/6…-693/6…&#125;›] 1700  initiating a split of this range at key ‹/Table/113/1/4402/59627› [r501] (‹512 MiB above threshold size 512 MiB›)‹›
</code></pre>
<p>Replica -	CockroachDB replicates each range (3 times by default) and stores each replica on a different node.</p>
<p>Refer to <a href="https://www.cockroachlabs.com/docs/stable/architecture/reads-and-writes-overview.html#cockroachdb-architecture-terms">here</a> for more.</p>
<p><img src="/images/cockroachdb-arch-terms.png" alt="Image"></p>
<h2 id="Production-checklist"><a href="#Production-checklist" class="headerlink" title="Production checklist"></a>Production checklist</h2><p>Check <a href="https://www.cockroachlabs.com/docs/stable/recommended-production-settings.html">here</a> for the important recommendations for production deployments of CockroachDB. The following only lists some of the recommended settings.</p>
<h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><ul>
<li>Disable Linux memory swapping. Over-allocating memory on production machines can lead to unexpected performance issues when pages have to be read back into memory.</li>
<li>For production deployments, set –cache to 25% or higher. Avoid setting –cache and –max-sql-memory to a combined value of more than 75% of a machine’s total RAM. Doing so increases the risk of memory-related failures.</li>
</ul>
<h3 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h3><ul>
<li>The maximum recommended storage capacity per node is 2.5 TiB, regardless of the number of vCPUs.</li>
<li>Use dedicated volumes for the CockroachDB store. Do not share the store volume with any other I&#x2F;O activity.</li>
<li>Store CockroachDB log files in a separate volume from the main data store so that logging is not impacted by I&#x2F;O throttling.</li>
<li>The recommended Linux filesystems are ext4 and XFS.</li>
</ul>
<h3 id="Disk-I-O"><a href="#Disk-I-O" class="headerlink" title="Disk I&#x2F;O"></a>Disk I&#x2F;O</h3><ul>
<li>Use sysbench to benchmark IOPS on your cluster. If IOPS decrease, add more nodes to your cluster to increase IOPS.</li>
<li>Do not use LVM in the I&#x2F;O path. Dynamically resizing CockroachDB store volumes can result in significant performance degradation. Using LVM snapshots in lieu of CockroachDB backup and restore is also not supported.</li>
<li>The optimal configuration for striping more than one device is RAID 10. RAID 0 and 1 are also acceptable from a performance perspective.</li>
</ul>
<h3 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h3><p>When starting a node, two main flags are used to control its network connections:</p>
<ul>
<li>–listen-addr determines which address(es) to listen on for connections from other nodes and clients.</li>
<li>–advertise-addr determines which address to tell other nodes to use.</li>
</ul>
<p><img src="/images/cockroachdb-network.png" alt="Image"></p>
<h3 id="Load-balancing"><a href="#Load-balancing" class="headerlink" title="Load balancing"></a>Load balancing</h3><p>Each CockroachDB node is an equally suitable SQL gateway to a cluster, but to ensure client performance and reliability, it’s important to use load balancing:</p>
<ul>
<li>Performance: Load balancers spread client traffic across nodes. This prevents any one node from being overwhelmed by requests and improves overall cluster performance (queries per second).</li>
<li>Reliability: Load balancers decouple client health from the health of a single CockroachDB node. To ensure that traffic is not directed to failed nodes or nodes that are not ready to receive requests, load balancers should use CockroachDB’s readiness health check.</li>
</ul>
<h3 id="Cache-and-SQL-memory-size"><a href="#Cache-and-SQL-memory-size" class="headerlink" title="Cache and SQL memory size"></a>Cache and SQL memory size</h3><p>CockroachDB manages its own memory caches, independently of the operating system. These are configured via the –cache and –max-sql-memory flags.</p>
<p>Each node has a default cache size of 128MiB that is passively consumed. The default was chosen to facilitate development and testing, where users are likely to run multiple CockroachDB nodes on a single machine. Increasing the cache size will generally improve the node’s read performance.</p>
<p>Each node has a default SQL memory size of 25%. This memory is used as-needed by active operations to store temporary data for SQL queries.</p>
<ul>
<li>Increasing a node’s cache size will improve the node’s read performance.</li>
<li>Increasing a node’s SQL memory size will increase the number of simultaneous client connections it allows, as well as the node’s capacity for in-memory processing of rows when using ORDER BY, GROUP BY, DISTINCT, joins, and window functions.</li>
</ul>
<p>You can check cache size and SQL memory pool size in the log. In the following example output, it matches with the specified 25% cache and SQL memory size setting.</p>
<pre><code>$ vim cockroach.log
I220905 17:49:33.671304 1 server/config.go:487 ⋮ [n?] 6  system total memory: 1008 GiB
I220905 17:49:33.671318 1 server/config.go:489 ⋮ [n?] 7  server configuration:
I220905 17:49:33.671318 1 server/config.go:489 ⋮ [n?] 7 +‹max offset             500000000›
I220905 17:49:33.671318 1 server/config.go:489 ⋮ [n?] 7 +‹cache size             252 GiB›
I220905 17:49:33.671318 1 server/config.go:489 ⋮ [n?] 7 +‹SQL memory pool size   252 GiB›
</code></pre>
<h2 id="CockroachDB-workloads"><a href="#CockroachDB-workloads" class="headerlink" title="CockroachDB workloads"></a>CockroachDB workloads</h2><p><img src="/images/rockroachdb_workloads.png" alt="Image"></p>
<h3 id="bank-workload"><a href="#bank-workload" class="headerlink" title="bank workload"></a>bank workload</h3><pre><code>$ cockroach workload init bank &#39;postgresql://root@host1:26257?sslmode=disable&#39;
I220831 23:52:14.593170 1 workload/workloadsql/dataload.go:146  [-] 1  imported bank (0s, 1000 rows)
I220831 23:52:14.609421 1 workload/workloadsql/workloadsql.go:136  [-] 2  starting 9 splits


$ cockroach workload run bank --duration=1m &#39;postgresql://root@host1:26257?sslmode=disable&#39;
I220831 23:52:55.378492 1 workload/cli/run.go:414  [-] 1  creating load generator...
I220831 23:52:55.380594 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 2.103665ms)
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
    1.0s        0         2929.8         2936.4      3.0     15.2     50.3    234.9 transfer
    2.0s        0         3387.3         3161.8      3.4     13.6     23.1     54.5 transfer
    3.0s        0         2755.2         3026.3      3.9     13.6     23.1    302.0 transfer
    4.0s        0         3295.5         3093.6      3.3     13.6     33.6    469.8 transfer
    5.0s        0         3536.5         3182.2      3.5     12.1     18.9     35.7 transfer
    6.0s        0         3558.1         3244.8      3.5     11.0     21.0     39.8 transfer
    7.0s        0         3566.9         3290.8      3.7     11.0     17.8     39.8 transfer
    8.0s        0         3317.6         3294.2      3.7     12.6     24.1     62.9 transfer
    9.0s        0         2992.9         3260.7      4.1     14.7     22.0     39.8 transfer
   10.0s        0         3628.2         3297.4      3.5     11.5     19.9     39.8 transfer
   11.0s        0         3604.5         3325.3      3.5     11.5     17.8     37.7 transfer
   12.0s        0         3668.4         3353.9      3.5     11.0     17.8     31.5 transfer
   13.0s        0         3485.9         3364.1      3.7     11.5     18.9     31.5 transfer
   14.0s        0         3377.9         3365.1      3.7     12.6     21.0     56.6 transfer
   15.0s        0         3084.4         3346.3      4.1     13.6     21.0     92.3 transfer
   16.0s        0         3650.1         3365.3      3.4     11.5     17.8     44.0 transfer
   17.0s        0         3662.7         3382.8      3.5     11.5     17.8     37.7 transfer
   18.0s        0         3461.8         3387.2      3.7     11.0     19.9    159.4 transfer
   19.0s        0         3426.3         3389.3      3.7     12.1     19.9     41.9 transfer
   20.0s        0         3196.0         3379.6      3.9     13.1     19.9     35.7 transfer
&lt;omitted..&gt;

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
   60.0s        0         206551         3442.5      4.6      3.7     12.1     19.9    469.8  transfer

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
   60.0s        0         206551         3442.5      4.6      3.7     12.1     19.9    469.8
</code></pre>
<h3 id="TPC-C-workload"><a href="#TPC-C-workload" class="headerlink" title="TPC-C workload"></a>TPC-C workload</h3><blockquote>
<p>–warehouses</p>
<blockquote>
<p>The number of warehouses for loading initial data, at approximately 200 MB per warehouse.</p>
<p>Applicable commands: init or run</p>
<p>Default: 1</p>
</blockquote>
</blockquote>
<blockquote>
<p>–workers</p>
<blockquote>
<p>The number of concurrent workers.</p>
<p>Applicable commands: init or run</p>
<p>Default: –warehouses * 10</p>
</blockquote>
</blockquote>
<p>The number os wareshouses can be specified by “–warehouses” option. By default, only one warehouse is created.</p>
<pre><code>$ cockroach workload init tpcc &#39;postgresql://root@host1:26257?sslmode=disable&#39;
Error: failed insert into warehouse: pq: duplicate key value violates unique constraint &quot;warehouse_pkey&quot;

$ cockroach workload init tpcc &#39;postgresql://root@host1:26257?sslmode=disable&#39; --drop
I220901 00:32:56.365593 1 workload/workloadsql/dataload.go:146  [-] 1  imported warehouse (0s, 1 rows)
I220901 00:32:56.372727 1 workload/workloadsql/dataload.go:146  [-] 2  imported district (0s, 10 rows)
I220901 00:32:57.034349 1 workload/workloadsql/dataload.go:146  [-] 3  imported customer (1s, 30000 rows)
I220901 00:32:57.248382 1 workload/workloadsql/dataload.go:146  [-] 4  imported history (0s, 30000 rows)
I220901 00:32:57.462320 1 workload/workloadsql/dataload.go:146  [-] 5  imported order (0s, 30000 rows)
I220901 00:32:57.490476 1 workload/workloadsql/dataload.go:146  [-] 6  imported new_order (0s, 9000 rows)
I220901 00:32:57.921658 1 workload/workloadsql/dataload.go:146  [-] 7  imported item (0s, 100000 rows)
I220901 00:32:59.267268 1 workload/workloadsql/dataload.go:146  [-] 8  imported stock (1s, 100000 rows)
I220901 00:33:00.916392 1 workload/workloadsql/dataload.go:146  [-] 9  imported order_line (2s, 300343 rows)


$ cockroach workload run tpcc --duration=10m &#39;postgresql://root@host1:26257?sslmode=disable&#39;
I220901 00:34:17.147411 1 workload/cli/run.go:414  [-] 1  creating load generator...
Initializing 2 connections...
Initializing 0 idle connections...
Initializing 10 workers and preparing statements...
I220901 00:34:17.151817 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 4.411152ms)
&lt;omitted..&gt;

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0             12            0.0     32.6     31.5     44.0     44.0     44.0  delivery

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0            120            0.2     17.9     17.8     26.2     41.9     54.5  newOrder

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0             13            0.0      6.7      5.8     14.2     15.2     15.2  orderStatus

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0            131            0.2     10.8     11.0     14.7     24.1     29.4  payment

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0             13            0.0     10.8     10.5     15.7     15.7     15.7  stockLevel

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  600.0s        0            289            0.5     14.4     13.1     28.3     41.9     54.5
Audit check 9.2.1.7: SKIP: not enough delivery transactions to be statistically significant
Audit check 9.2.2.5.1: SKIP: not enough orders to be statistically significant
Audit check 9.2.2.5.2: SKIP: not enough orders to be statistically significant
Audit check 9.2.2.5.5: SKIP: not enough payments to be statistically significant
Audit check 9.2.2.5.6: SKIP: not enough order status transactions to be statistically significant
Audit check 9.2.2.5.3: PASS
Audit check 9.2.2.5.4: PASS

_elapsed_______tpmC____efc__avg(ms)__p50(ms)__p90(ms)__p95(ms)__p99(ms)_pMax(ms)
  600.0s       12.0  93.3%     17.9     17.8     23.1     26.2     41.9     54.5
</code></pre>
<h3 id="YCSB-workload"><a href="#YCSB-workload" class="headerlink" title="YCSB workload"></a>YCSB workload</h3><pre><code>$ cockroach workload init ycsb &#39;postgresql://root@host1:26257?sslmode=disable&#39;
I220831 23:57:53.909658 1 workload/workloadsql/dataload.go:146  [-] 1  imported usertable (2s, 10000 rows)


$ cockroach workload run ycsb --duration=10m &#39;postgresql://root@host1:26257?sslmode=disable&#39;
I220831 23:58:24.410319 1 workload/cli/run.go:414  [-] 1  creating load generator...
I220831 23:58:24.427528 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 17.212701ms)
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
    1.0s        0        11011.5        11243.7      1.1      2.4      3.8     11.0 read
    1.0s        0          581.7          593.9      3.0      5.5      7.3     12.6 update
    2.0s        0        12105.1        11674.4      1.1      1.8      3.4     10.0 read
    2.0s        0          636.9          615.4      2.8      3.9      6.6     11.0 update
    3.0s        0        11590.6        11646.5      1.1      2.0      4.5     14.2 read
    3.0s        0          607.1          612.7      2.8      6.0      9.4     12.1 update
    4.0s        0        11813.5        11688.2      1.1      1.9      3.5     13.1 read
    4.0s        0          622.0          615.0      2.8      4.5      7.6     13.6 update
    5.0s        0        11959.5        11742.5      1.1      2.0      3.3      8.1 read
    5.0s        0          607.0          613.4      2.6      4.7      7.1     12.1 update
    6.0s        0        12186.2        11816.5      1.0      1.8      3.5     12.6 read
    6.0s        0          638.0          617.5      2.6      4.1     10.0     16.8 update
    7.0s        0        11815.8        11816.3      1.1      2.0      3.7      8.4 read
    7.0s        0          646.0          621.6      2.8      4.7      6.8      9.4 update
    8.0s        0        11850.8        11820.6      1.1      2.0      3.3      7.6 read
    8.0s        0          617.9          621.1      2.8      4.5      6.6      9.4 update
    9.0s        0        11713.5        11808.7      1.1      2.0      3.9     11.0 read
    9.0s        0          609.1          619.8      2.8      4.5      7.6     11.5 update
   10.0s        0        11949.6        11822.8      1.1      2.0      3.3     13.1 read
   10.0s        0          622.0          620.0      2.8      4.2      6.0      8.4 update

&lt;omitted..&gt;

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0        8576709        14294.5      1.0      1.0      1.6      2.9     62.9  read

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0         451026          751.7      2.5      2.5      3.8      6.6     27.3  update

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  600.0s        0        9027735        15046.2      1.1      1.0      2.2      3.5     62.9
</code></pre>
<h2 id="Restart-CockroachDB-cluster"><a href="#Restart-CockroachDB-cluster" class="headerlink" title="Restart CockroachDB cluster"></a>Restart CockroachDB cluster</h2><ol>
<li><p>Stop the process on each node</p>
<p> $ ps -ef | grep cock | grep -v grep<br> root     12217     1 99 Sep06 ?        2-20:28:09 cockroach start –log-dir&#x3D;&#x2F;var&#x2F;log&#x2F;cockroachdb_logs –store&#x3D;&#x2F;mnt&#x2F;cockroachdb_mnt1 –insecure –advertise-addr&#x3D;host1 –join&#x3D;host1,host2,host3 –cache&#x3D;.25 –max-sql-memory&#x3D;.25</p>
<p> $ kill -9 12217<br> $ ps -ef | grep cock | grep -v grep</p>
</li>
<li><p>Start the process on each node</p>
<p> $ cockroach start –store&#x3D;&#x2F;mnt&#x2F;cockroanchdb_mnt1 –insecure –advertise-addr&#x3D;host1 –join&#x3D;host1,host2,host3 –cache&#x3D;.25 -max-sql-memory&#x3D;.25 –background</p>
</li>
</ol>
<h2 id="CockroachDB-commands"><a href="#CockroachDB-commands" class="headerlink" title="CockroachDB commands"></a>CockroachDB commands</h2><p>List node IDs:</p>
<pre><code>$ cockroach node ls --insecure
  id
------
   1
   2
   3
(3 rows)
</code></pre>
<p>Show node status:</p>
<pre><code>$ cockroach node status --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+----------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:23:09.022236 |          | true         | true
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:23:09.048582 |          | true         | true
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:23:09.402081 |          | true         | true
(3 rows)
</code></pre>
<p>Show status and range&#x2F;replica details:</p>
<pre><code>$ cockroach node status --ranges --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live | replicas_leaders | replicas_leaseholders | ranges | ranges_unavailable | ranges_underreplicated
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:23:36.022463 |          | true         | true    |         306 |                   306 |    941 |                  0 |                      0
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:23:36.048508 |          | true         | true    |              310 |                   310 |    941 |                  0 |                      0
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:23:36.40421  |          | true         | true    |              325 |                   325 |    941 |                  0 |                      0
(3 rows)
</code></pre>
<p>Show status and disk usage details:</p>
<pre><code>$ cockroach node status --stats --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live |  live_bytes  |  key_bytes  | value_bytes  | intent_bytes | system_bytes
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------+--------------+-------------+--------------+--------------+---------------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:23:45.022008 |          | true         | true    | 246665681135 | 47930398573 | 220956161393 |        40331 |       812555
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:23:45.048777 |          | true         | true    | 246665683880 | 47930401894 | 220956192532 |        28025 |       817028
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:23:45.40156  |          | true         | true    | 246666008478 | 47930560065 | 220957658233 |        74382 |       741028
(3 rows)
</code></pre>
<p>Show status and decommissioning details for active and inactive nodes:</p>
<pre><code>$ cockroach node status --decommission --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live | gossiped_replicas | is_decommissioning | membership | is_draining
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:24:03.043432 |          | true         | true    |               941 | false              | active     | false
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:24:03.048948 |          | true         | true    |               941 | false              | active     | false
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:24:03.401953 |          | true         | true    |               941 | false              | active     | false
(3 rows)
</code></pre>
<p>Show complete status details for active and inactive nodes:</p>
<pre><code>$ cockroach node status --all --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live | replicas_leaders | replicas_leaseholders | ranges | ranges_unavailable | ranges_underreplicated |  live_bytes  |  key_bytes  | value_bytes  | intent_bytes | system_bytes | gossiped_replicas | is_decommissioning | membership | is_draining
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:24:07.539077 |          |         true |    true |              306 |                   306 |    942 |                  0 |                      0 | 246677946612 | 47939212836 | 221027696131 |        81398 |       722359 |        941        |              false |   active   |    false
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:24:07.548886 |          |         true |    true |              311 |                   311 |    942 |                  0 |                      0 | 246677952989 | 47939228198 | 221027812276 |        67816 |       731258 |        941        |              false |   active   |    false
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:24:07.901765 |          |         true |    true |              325 |                   325 |    942 |                  0 |                      0 | 246678252011 | 47939394591 | 221029217819 |        55249 |       869542 |        941        |              false |   active   |    false
(3 rows)
</code></pre>
<p>Show status details for a specific node:</p>
<pre><code>$ cockroach node status 1 --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+----------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:24:30.044382 |          | true         | true
(1 row)
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/cockroachdb/cockroach/blob/master/docs/design.md">https://github.com/cockroachdb/cockroach/blob/master/docs/design.md</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>CockroachDB</tag>
      </tags>
  </entry>
  <entry>
    <title>Compile Linux kernel on CentOS</title>
    <url>/blog/compile-linux-kernel-on-centos/</url>
    <content><![CDATA[<h2 id="Build-preparations"><a href="#Build-preparations" class="headerlink" title="Build preparations"></a>Build preparations</h2><p>To perform a successful kernel build, the following packages need to be installed:</p>
<ul>
<li>yum groupinstall “Development Tools”</li>
<li>yum install ncurses-devel</li>
<li>yum install qt3-devel (This is only necessary if you wish to use “make xconfig” instead of “make gconfig” or “make menuconfig”.) In the following example, we use “make menuconfig”.</li>
<li>yum install hmaccalc zlib-devel binutils-devel elfutils-libelf-devel</li>
</ul>
<h2 id="Download-kernel-source"><a href="#Download-kernel-source" class="headerlink" title="Download kernel source"></a>Download kernel source</h2><p>Download kernel source from <a href="https://www.kernel.org/">The Linux kernel archives</a>.</p>
<pre><code>[root@host1 ~]# mkdir ~/rpmbuild/
[root@host1 ~]# cd ~/rpmbuild/
[root@host1 rpmbuild]# pwd
/root/rpmbuild

[root@host1 rpmbuild]# wget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.18.12.tar.xz
[root@host1 rpmbuild]# tar xvf linux-5.18.12.tar.xz
[root@host1 rpmbuild]# cd linux-5.18.12
</code></pre>
<h2 id="Configure-the-new-kernel"><a href="#Configure-the-new-kernel" class="headerlink" title="Configure the new kernel"></a>Configure the new kernel</h2><p><strong>Note</strong>: If you do not intend to modify the distributed kernel configuration file, you may omit this section.</p>
<p>Copy the config file from the current running kernel to the new kernel source directory:</p>
<pre><code>[root@host1 linux-5.18.12]# cat /etc/centos-release
CentOS Linux release 7.9.2009 (Core)

[root@host1 linux-5.18.12]# uname -r
5.18.10-1.el7.elrepo.x86_64

[root@host1 linux-5.18.12]# cp /boot/config-5.18.10-1.el7.elrepo.x86_64 .config
</code></pre>
<p>Modify the kernel config with the “make menuconfig” command:</p>
<pre><code>[root@host1 linux-5.18.12]# make menuconfig
  HOSTCC  scripts/basic/fixdep
  UPD     scripts/kconfig/mconf-cfg
  HOSTCC  scripts/kconfig/mconf.o
  HOSTCC  scripts/kconfig/lxdialog/checklist.o
  HOSTCC  scripts/kconfig/lxdialog/inputbox.o
  HOSTCC  scripts/kconfig/lxdialog/menubox.o
  HOSTCC  scripts/kconfig/lxdialog/textbox.o
  HOSTCC  scripts/kconfig/lxdialog/util.o
  HOSTCC  scripts/kconfig/lxdialog/yesno.o
  HOSTCC  scripts/kconfig/confdata.o
  HOSTCC  scripts/kconfig/expr.o
  LEX     scripts/kconfig/lexer.lex.c
  YACC    scripts/kconfig/parser.tab.[ch]
  HOSTCC  scripts/kconfig/lexer.lex.o
  HOSTCC  scripts/kconfig/menu.o
  HOSTCC  scripts/kconfig/parser.tab.o
  HOSTCC  scripts/kconfig/preprocess.o
  HOSTCC  scripts/kconfig/symbol.o
  HOSTCC  scripts/kconfig/util.o
  HOSTLD  scripts/kconfig/mconf


*** End of the configuration.
*** Execute &#39;make&#39; to start the build or try &#39;make help&#39;.
</code></pre>
<p>The following window should be seen. You can enable or disable certain kernel features as needed. Here we just leave everything the defaults.</p>
<p><img src="/images/config-kernel-1.png" alt="Image"></p>
<p>Once it’s configured, go to “&lt; Save &gt;” and press “&lt; Ok &gt;” to save it.</p>
<p><img src="/images/config-kernel-2.png" alt="Image"></p>
<p>Now we can open the file “.config” to verify the content based on the previous modification.</p>
<pre><code>[root@host1 linux-5.18.12]# head .config
#
# Automatically generated file; DO NOT EDIT.
# Linux/x86 5.18.12 Kernel Configuration
#
CONFIG_CC_VERSION_TEXT=&quot;gcc (GCC) 7.3.1 20180303 (Red Hat 7.3.1-5)&quot;
CONFIG_CC_IS_GCC=y
CONFIG_GCC_VERSION=70301
CONFIG_CLANG_VERSION=0
CONFIG_AS_IS_GNU=y
CONFIG_AS_VERSION=22800
</code></pre>
<h2 id="Install-multiple-gcc-versions"><a href="#Install-multiple-gcc-versions" class="headerlink" title="Install multiple gcc versions"></a>Install multiple gcc versions</h2><p>Depending on the gcc version included in the system, you may encounter errors when to run “make menuconfig” which requires a higher version of gcc.</p>
<pre><code>[root@host1 linux-5.18.12]# make menuconfig
&lt;...&gt;
***
*** Compiler is too old.
***   Your GCC version:    4.8.5
***   Minimum GCC version: 5.1.0
***
scripts/Kconfig.include:44: Sorry, this compiler is not supported.
</code></pre>
<p><a href="https://www.softwarecollections.org/en/">Software Collections</a>, also known as SCL is a community project that allows you to build, install, and use multiple versions of software on the same system, without affecting system default packages. By enabling Software Collections, you gain access to the newer versions of programming languages and services which are not available in the core repositories. The SCL repositories provide a package named Developer Toolset, which includes newer versions of the GNU Compiler Collection, and other development and debugging tools.</p>
<p>To check the already installed gcc version:</p>
<pre><code>[root@host1 linux-5.18.12]# gcc --version
gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</code></pre>
<p>To install the newer version of gcc:</p>
<pre><code>[root@host1 linux-5.18.12]# sudo yum install centos-release-scl
[root@host1 linux-5.18.12]# sudo yum install devtoolset-7
</code></pre>
<p>To access gcc version 7, a new shell instance using the Software Collection scl tool needs to be launched.</p>
<pre><code>[root@host1 linux-5.18.12]# scl enable devtoolset-7 bash
149 packages can be updated.
0 updates are security updates.

[root@host1 linux-5.18.12]# gcc --version
gcc (GCC) 7.3.1 20180303 (Red Hat 7.3.1-5)
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

[root@host1 linux-5.18.12]# make menuconfig
</code></pre>
<p>For more detail, please refer to <a href="https://www.softwarecollections.org/en/scls/rhscl/devtoolset-7/">Developer Toolset 7</a> or a different version.</p>
<h2 id="Compile-the-kernel"><a href="#Compile-the-kernel" class="headerlink" title="Compile the kernel"></a>Compile the kernel</h2><p>Before start compiling the new kernel, make sure more than 20GB of free space on the filesystem is available.</p>
<pre><code>[root@host1 linux-5.18.12]# df -h | egrep &quot;Filesystem|root&quot;
Filesystem                 Size  Used Avail Use% Mounted on
/dev/mapper/vgroot-lvroot  1.5T   21G  1.4T   2% /
</code></pre>
<p>Now, we can compile the kernel. We run the compilation process in the background since it takes long time.</p>
<pre><code>[root@host1 linux-5.18.12]# nohup make rpm-pkg &amp;
[root@host1 linux-5.18.12]# tail -f nohup.out
[root@host1 linux-5.18.12]# more nohup.out
  SYNC    include/config/auto.conf.cmd
  HOSTCC  scripts/kconfig/conf.o
  HOSTLD  scripts/kconfig/conf
  UPD     include/config/kernel.release
make clean
sh ./scripts/package/mkspec &gt;./kernel.spec
  TAR     kernel-5.18.12.tar.gz
rpmbuild  --target x86_64 -ta kernel-5.18.12.tar.gz \
--define=&#39;_smp_mflags %&#123;nil&#125;&#39;
Building target platforms: x86_64
Building for target x86_64
Executing(%prep): /bin/sh -e /var/tmp/rpm-tmp.nOAadM
</code></pre>
<p>Note that the kernel.spec file is created automatically by “sh .&#x2F;scripts&#x2F;package&#x2F;mkspec &gt;.&#x2F;kernel.spec”.</p>
<p>Upon completion, the following output shows the location of the generated rpms.</p>
<pre><code>[root@host1 linux-5.18.12]# tail -15 nohup.out
Obsoletes: kernel-headers
Processing files: kernel-devel-5.18.12-1.x86_64
Provides: kernel-devel = 5.18.12-1 kernel-devel(x86-64) = 5.18.12-1
Requires(rpmlib): rpmlib(FileDigests) &lt;= 4.6.0-1 rpmlib(PayloadFilesHavePrefix) &lt;= 4.0-1 rpmlib(CompressedFileNames) &lt;= 3.0.4-1
Checking for unpackaged file(s): /usr/lib/rpm/check-files /root/rpmbuild/BUILDROOT/kernel-5.18.12-1.x86_64
Wrote: /root/rpmbuild/SRPMS/kernel-5.18.12-1.src.rpm
Wrote: /root/rpmbuild/RPMS/x86_64/kernel-5.18.12-1.x86_64.rpm
Wrote: /root/rpmbuild/RPMS/x86_64/kernel-headers-5.18.12-1.x86_64.rpm
Wrote: /root/rpmbuild/RPMS/x86_64/kernel-devel-5.18.12-1.x86_64.rpm
Executing(%clean): /bin/sh -e /var/tmp/rpm-tmp.DiAd0f
+ umask 022
+ cd /root/rpmbuild/BUILD
+ cd kernel-5.18.12
+ rm -rf /root/rpmbuild/BUILDROOT/kernel-5.18.12-1.x86_64
+ exit 0
</code></pre>
<p>To check the generated kernel rpms:</p>
<pre><code>[root@host1 rpmbuild]# pwd
/root/rpmbuild

[root@host1 rpmbuild]# ls RPMS/x86_64/
kernel-5.18.12-1.x86_64.rpm  kernel-devel-5.18.12-1.x86_64.rpm  kernel-headers-5.18.12-1.x86_64.rpm
[root@host1 rpmbuild]# ls SRPMS/
kernel-5.18.12-1.src.rpm
</code></pre>
<h2 id="Install-the-new-kernel"><a href="#Install-the-new-kernel" class="headerlink" title="Install the new kernel"></a>Install the new kernel</h2><p>Now you can run the following command to install the rpm packages:</p>
<pre><code>[root@host1 rpmbuild]# rpm -iUv ~/rpmbuild/RPMS/x86_64/*.rpm
</code></pre>
<p>Once the installation is complete, run the following command to reboot the system:</p>
<pre><code>[root@host1 rpmbuild]# reboot
</code></pre>
<p>Once the system starts, run the following command to check the kernel version:</p>
<pre><code>[root@host1 ~]# uname -r
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://wiki.centos.org/HowTos/Custom_Kernel">Build a custom kernel on CentOS</a></li>
<li><a href="https://linuxhint.com/compile-linux-kernel-centos7/">Compile Linux kernel on CentOS 7</a></li>
<li><a href="https://linuxize.com/post/how-to-install-gcc-compiler-on-centos-7/">How to install GCC compiler on CentOS 7</a></li>
<li><a href="https://www.kernel.org/">Linux kernel arhives</a></li>
<li><a href="https://elixir.bootlin.com/linux/latest/source">Linux kernel source code</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Kernel</tag>
      </tags>
  </entry>
  <entry>
    <title>Concepts you should know</title>
    <url>/blog/concepts-you-should-know/</url>
    <content><![CDATA[<h1 id="Data-structure"><a href="#Data-structure" class="headerlink" title="Data structure"></a>Data structure</h1><h2 id="Bloom-filter"><a href="#Bloom-filter" class="headerlink" title="Bloom filter"></a>Bloom filter</h2><p>A Bloom filter is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set. For example, checking availability of username is set membership problem, where the set is the list of all registered username. The price we pay for efficiency is that it is probabilistic in nature that means, there might be some False Positive results. False positive means, it might tell that given username is already taken but actually it’s not.</p>
<p><a href="https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation/">Source1</a><br><a href="https://llimllib.github.io/bloomfilter-tutorial/">Source2</a></p>
<h2 id="Merkle-tree"><a href="#Merkle-tree" class="headerlink" title="Merkle tree"></a>Merkle tree</h2><p>In cryptography and computer science, a hash tree or Merkle tree is a tree in which every “leaf” (node) is labelled with the cryptographic hash of a data block, and every node that is not a leaf (called a branch, inner node, or inode) is labelled with the cryptographic hash of the labels of its child nodes. A hash tree allows efficient and secure verification of the contents of a large data structure. A hash tree is a generalization of a hash list and a hash chain.</p>
<p>Demonstrating that a leaf node is a part of a given binary hash tree requires computing a number of hashes proportional to the logarithm of the number of leaf nodes in the tree. Conversely, in a hash list, the number is proportional to the number of leaf nodes itself. A Merkle tree is therefore an efficient example of a cryptographic commitment scheme, in which the root of the tree is seen as a commitment and leaf nodes may be revealed and proven to be part of the original commitment.</p>
<p><a href="https://en.wikipedia.org/wiki/Merkle_tree">Source</a></p>
<h2 id="Vector-Clock"><a href="#Vector-Clock" class="headerlink" title="Vector Clock"></a>Vector Clock</h2><p>A vector clock is a data structure used for determining the partial ordering of events in a distributed system and detecting causality violations. Just as in Lamport timestamps, inter-process messages contain the state of the sending process’s logical clock. A vector clock of a system of N processes is an array&#x2F;vector of N logical clocks, one clock per process; a local “largest possible values” copy of the global clock-array is kept in each process.</p>
<p><a href="https://en.wikipedia.org/wiki/Vector_clock">Source</a></p>
<h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><h2 id="Daemon"><a href="#Daemon" class="headerlink" title="Daemon"></a>Daemon</h2><p>In multitasking computer operating systems, a daemon is a computer program that runs as a background process, rather than being under the direct control of an interactive user. Traditionally, the process names of a daemon end with the letter d, for clarification that the process is in fact a daemon, and for differentiation between a daemon and a normal computer program. For example, syslogd is a daemon that implements system logging facility, and sshd is a daemon that serves incoming SSH connections.</p>
<p>In a Unix environment, the parent process of a daemon is often, but not always, the init process. A daemon is usually created either by a process forking a child process and then immediately exiting, thus causing init to adopt the child process, or by the init process directly launching the daemon. In addition, a daemon launched by forking and exiting typically must perform other operations, such as dissociating the process from any controlling terminal (tty). Such procedures are often implemented in various convenience routines such as daemon(3) in Unix.</p>
<p>Systems often start daemons at boot time that will respond to network requests, hardware activity, or other programs by performing some task. Daemons such as cron may also perform defined tasks at scheduled times.</p>
<p><a href="https://en.wikipedia.org/wiki/Daemon_(computing)">Source</a></p>
<h2 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h2><p>The Secure Shell Protocol (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network.[2] Its most notable applications are remote login and command-line execution.</p>
<p>SSH applications are based on a client–server architecture, connecting an SSH client instance with an SSH server.[3] SSH operates as a layered protocol suite comprising three principal hierarchical components: the transport layer provides server authentication, confidentiality, and integrity; the user authentication protocol validates the user to the server; and the connection protocol multiplexes the encrypted tunnel into multiple logical communication channels.</p>
<p>SSH was designed on Unix-like operating systems, as a replacement for Telnet and for unsecured remote Unix shell protocols, such as the Berkeley Remote Shell (rsh) and the related rlogin and rexec protocols, which all use insecure, plaintext transmission of authentication tokens.</p>
<p><a href="https://en.wikipedia.org/wiki/Secure_Shell">Source</a></p>
<h2 id="Telnet"><a href="#Telnet" class="headerlink" title="Telnet"></a>Telnet</h2><p>Telnet is an application protocol used on the Internet or local area network to provide a bidirectional interactive text-oriented communication facility using a virtual terminal connection. User data is interspersed in-band with Telnet control information in an 8-bit byte oriented data connection over the Transmission Control Protocol (TCP).</p>
<p><a href="https://en.wikipedia.org/wiki/Telnet">Source</a></p>
<h1 id="MISC"><a href="#MISC" class="headerlink" title="MISC"></a>MISC</h1><h2 id="Chaos-Engineering"><a href="#Chaos-Engineering" class="headerlink" title="Chaos Engineering"></a>Chaos Engineering</h2><p>Chaos engineering is the discipline of experimenting on a software system in production in order to build confidence in the system’s capability to withstand turbulent and unexpected conditions.</p>
<p><a href="https://en.wikipedia.org/wiki/Chaos_engineering#:~:text=Chaos%20engineering%20is%20the%20discipline,withstand%20turbulent%20and%20unexpected%20conditions.">Source</a></p>
<h2 id="Serialization"><a href="#Serialization" class="headerlink" title="Serialization"></a>Serialization</h2><p>In computing, serialization is the process of translating a data structure or object state into a format that can be stored (for example, in a file or memory data buffer) or transmitted (for example, over a computer network) and reconstructed later (possibly in a different computer environment).When the resulting series of bits is reread according to the serialization format, it can be used to create a semantically identical clone of the original object. For many complex objects, such as those that make extensive use of references, this process is not straightforward. Serialization of object-oriented objects does not include any of their associated methods with which they were previously linked.</p>
<p>This process of serializing an object is also called marshalling an object in some situations.The opposite operation, extracting a data structure from a series of bytes, is deserialization, (also called unserialization or unmarshalling).</p>
<p><a href="https://en.wikipedia.org/wiki/Serialization">Source</a></p>
<h2 id="Callback-functions"><a href="#Callback-functions" class="headerlink" title="Callback functions"></a>Callback functions</h2><p>In computer programming, a callback, also known as a “call-after” function, is any reference to executable code that is passed as an argument to other code; that other code is expected to call back (execute) the code at a given time. This execution may be immediate as in a synchronous callback, or it might happen at a later point in time as in an asynchronous callback. Programming languages support callbacks in different ways, often implementing them with subroutines, lambda expressions, blocks, or function pointers.</p>
<p><a href="https://en.wikipedia.org/wiki/Callback_(computer_programming)">Source</a></p>
<h2 id="Consistent-Hashing"><a href="#Consistent-Hashing" class="headerlink" title="Consistent Hashing"></a>Consistent Hashing</h2><p>In computer science, consistent hashing is a special kind of hashing technique such that when a hash table is resized, only n&#x2F;m keys need to be remapped on average where n is the number of keys and m is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and the slots is defined by a modular operation.</p>
<p><a href="https://en.wikipedia.org/wiki/Consistent_hashing">Source</a></p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
  </entry>
  <entry>
    <title>Cooking Masters</title>
    <url>/blog/cooking-masters/</url>
    <content><![CDATA[<p>It was my most regrettable decision ever. I was playing with Alex, who was my next door neighbor and my best friend in fourth grade. Suddenly, Alex said: “You know what, I think it’s time for us to compare our cooking skills now that I practiced my recipes for two years now.”  I did not know what to say. I knew Alex was teasing me. He had enrolled for a master cooking class and practiced for two years already. I was only a home cook, and I never thought about entering a cooking competition. I wanted to decline the offer but that would show that I was afraid. I did not like people calling me a loser so I foolishly accepted the challenge. We decided we were going to hold the cooking contest in my school’s cafeteria.</p>
<p>When I told my parents about the contest, they said that I should cook something really fancy so it will increase my chance of winning. I got really stressed then. I only knew one type of dish which was a burger. I knew how to make many kinds of burgers but nothing else. I decided to go online to search up some recipes for other foods. I bought the ingredients and started to try new recipes. But none of them tasted better than my masterpiece burger.</p>
<p>I wondered if I was doing something wrong. The recipes were more complicated so they should taste better. But they didn’t. I told my mom that I did not think the new dishes tasted any better than my burger. She said that I should be patient and practice more. I disagreed with her. I only had a few days to practice and mastering new recipes would take me weeks. I decided to continue with my burger. I was best at making that so it should have the highest chance of winning the contest. I created new kinds of burgers everyday from then on and tasted each of them to see if they were the best I could make.</p>
<p>Then, on the day before the contest, my mom asked me what dish I prepared. When I told her I was going to make a burger, she advised me that next time I should make a more complicated recipe. “A tasty recipe matters on how delicious it is, not how fancy or complicated it is.” I said. Even if many people think a burger would not stand a chance against a chocolate cake, I refused to change my topic.</p>
<p>Finally, the day of the competition arrived. I entered the stadium and put my ingredients on the right side of the table. I felt the tension in the kitchen. Alex was there and he was sitting on the left side of the table. When he saw that I had only a few ingredients needed to make my dish, he asked me what I was going to cook. A confident smile grew on his face when I told him I was going to make a smash burger. He told me I could forfeit if I was scared. I stared at him. I knew what he was going for. But it was not so easy to make me give up. Seeing that I would not back up, Alex had no choice but to start a cooking fight with me. Then the competition began. I first carefully spread some butter on the brioche buns. Then I toasted the buns. I took out the ground beef and rolled it into a patty. Meanwhile, Alex was making a chocolate cake. I wondered if the judge would like salty or sweet food better. While I was thinking, the judge said that we had thirty minutes left. I continued to make my Bacon King smash burger.</p>
<p>I could feel my heart beating fast. I shakily put the patty into the pan and fried them. I then put the juicy meat on the toasted bun. Alex was watching me the whole time. He was already done baking his cake. I felt a thousand pairs of eyes looking at me. There was no time to be nervous. I put the butter lettuce on the meat and assembled the burger.</p>
<p>Alex looked toward me. “It is no use trying to act tough. A loser will always remain a loser,” he said. But I ignored his words.</p>
<p>Then we presented our finished results to the judge. He first took a bite out of the chocolate cake. He said that the sweetness of the chocolate cake mixed with the sourness of the strawberry on top was perfect. Then he ate my burger. He did not say anything. Cold sweat fell on my face. The whole room was so quiet that you could hear a pin drop on the ground.</p>
<p>I gulped. Did the judge dislike my burger? That was when I saw him take another bite out of the burger. After that, he looked at Alex and me. “Well?” said Alex impatiently. “Who won?” The judge stood up. He said it was very hard to decide which recipe was tastier than the other. Though he equally liked the two foods, he said that my burger had the perfect attributes a burger should have. I grinned because I knew I won the contest.</p>
<p>If I had made a fancy unfamiliar recipe, I may have never won this contest. I learnt that we should always play to our strengths and do what we are good at.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Courage</title>
    <url>/blog/courage/</url>
    <content><![CDATA[<p>by Edgar Albert Guest</p>
<p>Courage isn’t a brilliant dash,</p>
<p>A daring deed in a moment’s flash;</p>
<p>It isn’t an instantaneous thing</p>
<p>Born of despair with a sudden spring</p>
<p>It isn’t a creature of flickered hope</p>
<p>Or the final tug at a slipping rope;</p>
<p>But it’s something deep in the soul of man</p>
<p>That is working always to serve some plan.</p>
<p>Courage isn’t the last resort</p>
<p>In the work of life or the game of sport;</p>
<p>It isn’t a thing that a man can call</p>
<p>At some future time when he’s apt to fall;</p>
<p>If he hasn’t it now, he will have it not</p>
<p>When the strain is great and the pace is hot.</p>
<p>For who would strive for a distant goal</p>
<p>Must always have courage within his soul.</p>
<p>Courage isn’t a dazzling light</p>
<p>That flashes and passes away from sight;</p>
<p>It’s a slow, unwavering, ingrained trait</p>
<p>With the patience to work and the strength to wait.</p>
<p>It’s part of a man when his skies are blue,</p>
<p>It’s part of him when he has work to do.</p>
<p>The brave man never is freed of it.</p>
<p>He has it when there is no need of it.</p>
<p>Courage was never designed for show;</p>
<p>It isn’t a thing that can come and go;</p>
<p>It’s written in victory and defeat</p>
<p>And every trial a man may meet.</p>
<p>It’s part of his hours, his days and his years,</p>
<p>Back of his smiles and behind his tears.</p>
<p>Courage is more than a daring deed:</p>
<p>It’s the breath of life and a strong man’s creed.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Use sysbench for CockroachDB performance benchmarking</title>
    <url>/blog/crdb-sysbench/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>Cockroach uses TPC-C as the official OLTP workload benchmark since it’s a more realistic measurement by modeling the real world applications.</p>
<p>However, <a href="https://github.com/akopytov/sysbench">sysbench</a> is a straight-forward throughput&#x2F;latency benchmarking tool. It is a scriptable multi-threaded benchmark tool based on LuaJIT. It is most frequently used for database benchmarks, but can also be used to create arbitrarily complex workloads that do not involve a database server.</p>
<ul>
<li>oltp_*.lua: a collection of OLTP-like database benchmarks</li>
<li>fileio: a filesystem-level benchmark</li>
<li>cpu: a simple CPU benchmark</li>
<li>memory: a memory access benchmark</li>
<li>threads: a thread-based scheduler benchmark</li>
<li>mutex: a POSIX mutex benchmark</li>
</ul>
<p>This Cockroach <a href="https://www.cockroachlabs.com/blog/unpacking-competitive-benchmarks/">blog</a> explains why we use sysbench for competitive benchmarks.</p>
<p>Sysbench contains a collection of simple SQL workloads. These workloads perform low-level SQL operations. For example, they run concurrent INSERT or UPDATE statements on rows as fast as possible. It gives us a picture on the system performance under different access patterns. Unlike TPC-C, Sysbench does not attempt to model a real application.</p>
<p>Sysbench includes the following workloads:</p>
<ul>
<li>oltp_point_select: single-row point selects</li>
<li>oltp_insert: single-row inserts</li>
<li>oltp_delete: single-row deletes</li>
<li>oltp_update_index: single-row update on column that requires update to secondary index</li>
<li>oltp_update_non_index single-row: update on column that does not require update to secondary index</li>
<li>oltp_read_only: transactions that run collection of small scans</li>
<li>oltp_read_write: transactions that run collection of small scans and writes</li>
<li>oltp_write_only: transactions that run collection of writes</li>
</ul>
<p>Sysbench supports the following two database drivers.</p>
<ul>
<li>mysql - MySQL driver</li>
<li>pgsql - PostgreSQL driver</li>
</ul>
<p>From this Cockroach <a href="https://www.cockroachlabs.com/blog/why-postgres/">blog</a>, CockroachDB is compatible with PostgreSQL.</p>
<p>In this article, we will explore how to run sysbench with CockroachDB using pqsql driver.</p>
<h2 id="Install-the-CockroachDB-cluster"><a href="#Install-the-CockroachDB-cluster" class="headerlink" title="Install the CockroachDB cluster"></a>Install the CockroachDB cluster</h2><p>Refer to this <a href="https://www.flamingbytes.com/blog/cockroachdb-performance-benchmarking/">post</a> on how to deploy CockroachDB cluster.</p>
<pre><code>[root@crdb_node1 ~]# cockroach version
Build Tag:        v22.1.6
Build Time:       2022/08/23 17:05:04
Distribution:     CCL
Platform:         linux amd64 (x86_64-pc-linux-gnu)
Go Version:       go1.17.11
C Compiler:       gcc 6.5.0
Build Commit ID:  760a8253ae6478d69da0330133e3efec8e950e4e
Build Type:       release
</code></pre>
<h2 id="Interact-with-the-CockroachDB"><a href="#Interact-with-the-CockroachDB" class="headerlink" title="Interact with the CockroachDB"></a>Interact with the CockroachDB</h2><h3 id="Use-the-CockroachDB-built-in-client"><a href="#Use-the-CockroachDB-built-in-client" class="headerlink" title="Use the CockroachDB built-in client"></a>Use the CockroachDB built-in client</h3><p>CockroachDB comes with a built-in client for executing SQL statements from an interactive shell or directly from the command line.</p>
<p>To use this client, run the cockroach sql command as following:</p>
<pre><code>[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &#39;create database testdb&#39;

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &#39;show databases&#39;
  database_name | owner | primary_region | regions | survival_goal
----------------+-------+----------------+---------+----------------
  defaultdb     | root  | NULL           | &#123;&#125;      | NULL
  postgres      | root  | NULL           | &#123;&#125;      | NULL
  system        | node  | NULL           | &#123;&#125;      | NULL
  testdb        | root  | NULL           | &#123;&#125;      | NULL
(4 rows)

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &#39;show tables from testdb&#39;
SHOW TABLES 0

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &#39;drop database testdb&#39;
</code></pre>
<h3 id="Use-the-Postgres-client"><a href="#Use-the-Postgres-client" class="headerlink" title="Use the Postgres client"></a>Use the Postgres client</h3><p>The cockroachDB can also be interacted by using Postgres client.</p>
<p>To install the Postgres client:</p>
<pre><code>[root@node0 ~]# yum -y install postgresql postgresql-libs
Installed:
  postgresql.x86_64 0:9.2.24-8.el7_9
</code></pre>
<p>To create the database and user:</p>
<pre><code>[root@node0 ~]# psql -h node1 -U root -p 26257
psql (9.2.24, server 13.0.0)

root=&gt; create database testdb;
root=&gt; create user tester;
root=&gt; grant all on database testdb to tester;

root=&gt; show databases;
 database_name | owner | primary_region | regions | survival_goal
---------------+-------+----------------+---------+---------------
 defaultdb     | root  |                | &#123;&#125;      |
 postgres      | root  |                | &#123;&#125;      |
 system        | node  |                | &#123;&#125;      |
 testdb        | root  |                | &#123;&#125;      |
(4 rows)

root=&gt; show users;
 username | options | member_of
----------+---------+-----------
 admin    |         | &#123;&#125;
 root     |         | &#123;admin&#125;
 tester   |         | &#123;&#125;
(3 rows)

root=&gt; \c testdb;
psql (9.2.24, server 13.0.0)
WARNING: psql version 9.2, server version 13.0.
         Some psql features might not work.
You are now connected to database &quot;testdb&quot; as user &quot;root&quot;.

testdb=&gt; \dt;
                          List of relations
    Schema    |       Name        | Type  |          Owner
--------------+-------------------+-------+--------------------------
 pg_extension | geography_columns | table | unknown (OID=3233629770)
 pg_extension | geometry_columns  | table | unknown (OID=3233629770)
 pg_extension | spatial_ref_sys   | table | unknown (OID=3233629770)
(3 rows)

testdb=&gt;  SELECT * FROM pg_catalog.pg_tables where schemaname != &#39;pg_catalog&#39; AND schemaname != &#39;information_schema&#39; and schemaname != &#39;crdb_internal&#39;;
  schemaname  |     tablename     | tableowner | tablespace | hasindexes | hasrules | hastriggers | rowsecurity
--------------+-------------------+------------+------------+------------+----------+-------------+-------------
 pg_extension | geography_columns | node       |            | f          | f        | f           | f
 pg_extension | geometry_columns  | node       |            | f          | f        | f           | f
 pg_extension | spatial_ref_sys   | node       |            | f          | f        | f           | f
(3 rows)
</code></pre>
<h2 id="Install-sysbench"><a href="#Install-sysbench" class="headerlink" title="Install sysbench"></a>Install sysbench</h2><p>To install sysbench:</p>
<pre><code>[root@node0 ~]# curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash^C
[root@node0 ~]# sudo yum -y install sysbench
[root@node0 ~]# sysbench --version
sysbench 1.0.20
</code></pre>
<p>To get familiar with the sysbench parameters:</p>
<pre><code>[root@node0 ~]# sysbench --help
Usage:
  sysbench [options]... [testname] [command]

Commands implemented by most tests: prepare run cleanup help

General options:
  --threads=N                     number of threads to use [1]
  --events=N                      limit for total number of events [0]
  --time=N                        limit for total execution time in seconds [10]
  --forced-shutdown=STRING        number of seconds to wait after the --time limit before forcing shutdown, or &#39;off&#39; to disable [off]
  --thread-stack-size=SIZE        size of stack per thread [64K]
  --rate=N                        average transactions rate. 0 for unlimited rate [0]
  --report-interval=N             periodically report intermediate statistics with a specified interval in seconds. 0 disables intermediate reports [0]
  --report-checkpoints=[LIST,...] dump full statistics and reset all counters at specified points in time. The argument is a list of comma-separated values representing the amount of time in seconds elapsed from start of test when report checkpoint(s) must be performed. Report checkpoints are off by default. []
  --debug[=on|off]                print more debugging info [off]
  --validate[=on|off]             perform validation checks where possible [off]
  --help[=on|off]                 print help and exit [off]
  --version[=on|off]              print version and exit [off]
  --config-file=FILENAME          File containing command line options
  --tx-rate=N                     deprecated alias for --rate [0]
  --max-requests=N                deprecated alias for --events [0]
  --max-time=N                    deprecated alias for --time [0]
  --num-threads=N                 deprecated alias for --threads [1]

Pseudo-Random Numbers Generator options:
  --rand-type=STRING random numbers distribution &#123;uniform,gaussian,special,pareto&#125; [special]
  --rand-spec-iter=N number of iterations used for numbers generation [12]
  --rand-spec-pct=N  percentage of values to be treated as &#39;special&#39; (for special distribution) [1]
  --rand-spec-res=N  percentage of &#39;special&#39; values to use (for special distribution) [75]
  --rand-seed=N      seed for random number generator. When 0, the current time is used as a RNG seed. [0]
  --rand-pareto-h=N  parameter h for pareto distribution [0.2]

Log options:
  --verbosity=N verbosity level &#123;5 - debug, 0 - only critical messages&#125; [3]

  --percentile=N       percentile to calculate in latency statistics (1-100). Use the special value of 0 to disable percentile calculations [95]
  --histogram[=on|off] print latency histogram in report [off]

General database options:

  --db-driver=STRING  specifies database driver to use (&#39;help&#39; to get list of available drivers) [mysql]
  --db-ps-mode=STRING prepared statements usage mode &#123;auto, disable&#125; [auto]
  --db-debug[=on|off] print database-specific debug information [off]


Compiled-in database drivers:
  mysql - MySQL driver
  pgsql - PostgreSQL driver

mysql options:
  --mysql-host=[LIST,...]          MySQL server host [localhost]
  --mysql-port=[LIST,...]          MySQL server port [3306]
  --mysql-socket=[LIST,...]        MySQL socket
  --mysql-user=STRING              MySQL user [sbtest]
  --mysql-password=STRING          MySQL password []
  --mysql-db=STRING                MySQL database name [sbtest]
  --mysql-ssl[=on|off]             use SSL connections, if available in the client library [off]
  --mysql-ssl-cipher=STRING        use specific cipher for SSL connections []
  --mysql-compression[=on|off]     use compression, if available in the client library [off]
  --mysql-debug[=on|off]           trace all client library calls [off]
  --mysql-ignore-errors=[LIST,...] list of errors to ignore, or &quot;all&quot; [1213,1020,1205]
  --mysql-dry-run[=on|off]         Dry run, pretend that all MySQL client API calls are successful without executing them [off]

pgsql options:
  --pgsql-host=STRING     PostgreSQL server host [localhost]
  --pgsql-port=N          PostgreSQL server port [5432]
  --pgsql-user=STRING     PostgreSQL user [sbtest]
  --pgsql-password=STRING PostgreSQL password []
  --pgsql-db=STRING       PostgreSQL database name [sbtest]

Compiled-in tests:
  fileio - File I/O test
  cpu - CPU performance test
  memory - Memory functions speed test
  threads - Threads subsystem performance test
  mutex - Mutex performance test

See &#39;sysbench &lt;testname&gt; help&#39; for a list of options for each test.
</code></pre>
<p>Sysbench includes the following lua scripts to simulate OLTP workloads.</p>
<pre><code>[root@node0 ~]# ls -l /usr/share/sysbench/
total 60
-rwxr-xr-x   1 root root  1452 Apr 24  2020 bulk_insert.lua
-rw-r--r--   1 root root 14369 Apr 24  2020 oltp_common.lua
-rwxr-xr-x   1 root root  1290 Apr 24  2020 oltp_delete.lua
-rwxr-xr-x   1 root root  2415 Apr 24  2020 oltp_insert.lua
-rwxr-xr-x   1 root root  1265 Apr 24  2020 oltp_point_select.lua
-rwxr-xr-x   1 root root  1649 Apr 24  2020 oltp_read_only.lua
-rwxr-xr-x   1 root root  1824 Apr 24  2020 oltp_read_write.lua
-rwxr-xr-x   1 root root  1118 Apr 24  2020 oltp_update_index.lua
-rwxr-xr-x   1 root root  1127 Apr 24  2020 oltp_update_non_index.lua
-rwxr-xr-x   1 root root  1440 Apr 24  2020 oltp_write_only.lua
-rwxr-xr-x   1 root root  1919 Apr 24  2020 select_random_points.lua
-rwxr-xr-x   1 root root  2118 Apr 24  2020 select_random_ranges.lua
drwxr-xr-x   4 root root    49 Dec  8 20:39 tests

[root@node0 ~]# ls -l /usr/share/sysbench/tests/include/oltp_legacy/
total 52
-rw-r--r-- 1 root root 1195 Apr 24  2020 bulk_insert.lua
-rw-r--r-- 1 root root 4696 Apr 24  2020 common.lua
-rw-r--r-- 1 root root  366 Apr 24  2020 delete.lua
-rw-r--r-- 1 root root 1171 Apr 24  2020 insert.lua
-rw-r--r-- 1 root root 3004 Apr 24  2020 oltp.lua
-rw-r--r-- 1 root root  368 Apr 24  2020 oltp_simple.lua
-rw-r--r-- 1 root root  527 Apr 24  2020 parallel_prepare.lua
-rw-r--r-- 1 root root  369 Apr 24  2020 select.lua
-rw-r--r-- 1 root root 1448 Apr 24  2020 select_random_points.lua
-rw-r--r-- 1 root root 1556 Apr 24  2020 select_random_ranges.lua
-rw-r--r-- 1 root root  369 Apr 24  2020 update_index.lua
-rw-r--r-- 1 root root  578 Apr 24  2020 update_non_index.lua
</code></pre>
<p>To get more options for each specific lua workload:</p>
<pre><code>[root@node0 ~]# sysbench /usr/share/sysbench/oltp_insert.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_insert.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &#39;redshift&#39;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&#39;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]

[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_write_only.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &#39;redshift&#39;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&#39;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]

[root@node0 ~]# sysbench /usr/share/sysbench/oltp_read_only.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_read_only.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &#39;redshift&#39;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&#39;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]

[root@node0 ~]# sysbench /usr/share/sysbench/oltp_point_select.lua help
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

oltp_point_select.lua options:
  --auto_inc[=on|off]           Use AUTO_INCREMENT column as Primary Key (for MySQL), or its alternatives in other DBMS. When disabled, use client-generated IDs [on]
  --create_secondary[=on|off]   Create a secondary index in addition to the PRIMARY KEY [on]
  --delete_inserts=N            Number of DELETE/INSERT combinations per transaction [1]
  --distinct_ranges=N           Number of SELECT DISTINCT queries per transaction [1]
  --index_updates=N             Number of UPDATE index queries per transaction [1]
  --mysql_storage_engine=STRING Storage engine, if MySQL is used [innodb]
  --non_index_updates=N         Number of UPDATE non-index queries per transaction [1]
  --order_ranges=N              Number of SELECT ORDER BY queries per transaction [1]
  --pgsql_variant=STRING        Use this PostgreSQL variant when running with the PostgreSQL driver. The only currently supported variant is &#39;redshift&#39;. When enabled, create_secondary is automatically disabled, and delete_inserts is set to 0
  --point_selects=N             Number of point SELECT queries per transaction [10]
  --range_selects[=on|off]      Enable/disable all range SELECT queries [on]
  --range_size=N                Range size for range SELECT queries [100]
  --secondary[=on|off]          Use a secondary index in place of the PRIMARY KEY [off]
  --simple_ranges=N             Number of simple range SELECT queries per transaction [1]
  --skip_trx[=on|off]           Don&#39;t start explicit transactions and execute all queries in the AUTOCOMMIT mode [off]
  --sum_ranges=N                Number of SELECT SUM() queries per transaction [1]
  --table_size=N                Number of rows per table [10000]
  --tables=N                    Number of tables [1]
</code></pre>
<h2 id="Prepare-for-the-benchmark"><a href="#Prepare-for-the-benchmark" class="headerlink" title="Prepare for the benchmark"></a>Prepare for the benchmark</h2><p>Before running the benchmark, the database should be created and the data tables should be populated.</p>
<p>To create the database:</p>
<pre><code>[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &#39;create database testdb;&#39;
</code></pre>
<p>To populate the database:</p>
<pre><code>[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=tester --pgsql-password= --table_size=100000 --tables=24 --threads=1 --db-driver=pgsql prepare
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Creating table &#39;sbtest1&#39;...
Inserting 100000 records into &#39;sbtest1&#39;
Creating a secondary index on &#39;sbtest1&#39;...
Creating table &#39;sbtest2&#39;...
Inserting 100000 records into &#39;sbtest2&#39;
Creating a secondary index on &#39;sbtest2&#39;...
Creating table &#39;sbtest3&#39;...
Inserting 100000 records into &#39;sbtest3&#39;
Creating a secondary index on &#39;sbtest3&#39;...
Creating table &#39;sbtest4&#39;...
Inserting 100000 records into &#39;sbtest4&#39;
Creating a secondary index on &#39;sbtest4&#39;...
Creating table &#39;sbtest5&#39;...
Inserting 100000 records into &#39;sbtest5&#39;
Creating a secondary index on &#39;sbtest5&#39;...
Creating table &#39;sbtest6&#39;...
Inserting 100000 records into &#39;sbtest6&#39;
Creating a secondary index on &#39;sbtest6&#39;...
Creating table &#39;sbtest7&#39;...
Inserting 100000 records into &#39;sbtest7&#39;
Creating a secondary index on &#39;sbtest7&#39;...
Creating table &#39;sbtest8&#39;...
Inserting 100000 records into &#39;sbtest8&#39;
Creating a secondary index on &#39;sbtest8&#39;...
Creating table &#39;sbtest9&#39;...
Inserting 100000 records into &#39;sbtest9&#39;
Creating a secondary index on &#39;sbtest9&#39;...
Creating table &#39;sbtest10&#39;...
Inserting 100000 records into &#39;sbtest10&#39;
Creating a secondary index on &#39;sbtest10&#39;...
Creating table &#39;sbtest11&#39;...
Inserting 100000 records into &#39;sbtest11&#39;
Creating a secondary index on &#39;sbtest11&#39;...
Creating table &#39;sbtest12&#39;...
Inserting 100000 records into &#39;sbtest12&#39;
Creating a secondary index on &#39;sbtest12&#39;...
Creating table &#39;sbtest13&#39;...
Inserting 100000 records into &#39;sbtest13&#39;
Creating a secondary index on &#39;sbtest13&#39;...
Creating table &#39;sbtest14&#39;...
Inserting 100000 records into &#39;sbtest14&#39;
Creating a secondary index on &#39;sbtest14&#39;...
Creating table &#39;sbtest15&#39;...
Inserting 100000 records into &#39;sbtest15&#39;
Creating a secondary index on &#39;sbtest15&#39;...
Creating table &#39;sbtest16&#39;...
Inserting 100000 records into &#39;sbtest16&#39;
Creating a secondary index on &#39;sbtest16&#39;...
Creating table &#39;sbtest17&#39;...
Inserting 100000 records into &#39;sbtest17&#39;
Creating a secondary index on &#39;sbtest17&#39;...
Creating table &#39;sbtest18&#39;...
Inserting 100000 records into &#39;sbtest18&#39;
Creating a secondary index on &#39;sbtest18&#39;...
Creating table &#39;sbtest19&#39;...
Inserting 100000 records into &#39;sbtest19&#39;
Creating a secondary index on &#39;sbtest19&#39;...
Creating table &#39;sbtest20&#39;...
Inserting 100000 records into &#39;sbtest20&#39;
Creating a secondary index on &#39;sbtest20&#39;...
Creating table &#39;sbtest21&#39;...
Inserting 100000 records into &#39;sbtest21&#39;
Creating a secondary index on &#39;sbtest21&#39;...
Creating table &#39;sbtest22&#39;...
Inserting 100000 records into &#39;sbtest22&#39;
Creating a secondary index on &#39;sbtest22&#39;...
Creating table &#39;sbtest23&#39;...
Inserting 100000 records into &#39;sbtest23&#39;
Creating a secondary index on &#39;sbtest23&#39;...
Creating table &#39;sbtest24&#39;...
Inserting 100000 records into &#39;sbtest24&#39;

testdb=&gt;  show tables from testdb;
 schema_name | table_name | type  | owner  | estimated_row_count | locality
-------------+------------+-------+--------+---------------------+----------
 public      | sbtest1    | table | tester |              100000 |
 public      | sbtest10   | table | tester |              100000 |
 public      | sbtest11   | table | tester |              100000 |
 public      | sbtest12   | table | tester |              100000 |
 public      | sbtest13   | table | tester |              100000 |
 public      | sbtest14   | table | tester |              100000 |
 public      | sbtest15   | table | tester |              100000 |
 public      | sbtest16   | table | tester |              100000 |
 public      | sbtest17   | table | tester |              100000 |
 public      | sbtest18   | table | tester |              100000 |
 public      | sbtest19   | table | tester |              100000 |
 public      | sbtest2    | table | tester |              100000 |
 public      | sbtest20   | table | tester |              100000 |
 public      | sbtest21   | table | tester |              100000 |
 public      | sbtest22   | table | tester |              100000 |
 public      | sbtest23   | table | tester |              100000 |
 public      | sbtest24   | table | tester |              100000 |
 public      | sbtest3    | table | tester |              100000 |
 public      | sbtest4    | table | tester |              100000 |
 public      | sbtest5    | table | tester |              100000 |
 public      | sbtest6    | table | tester |              100000 |
 public      | sbtest7    | table | tester |              100000 |
 public      | sbtest8    | table | tester |              100000 |
 public      | sbtest9    | table | tester |              100000 |
(24 rows)

testdb=&gt; select sum(range_size)/1000 from crdb_internal.ranges where database_name=&#39;testdb&#39;;
       ?column?
-----------------------
 602418.53400000000000
(1 row)
</code></pre>
<p>To populate the database with 2 or more threads:</p>
<pre><code>[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=root --pgsql-password= --table_size=1000000 --tables=24 --threads=2 --db-driver=pgsql prepare
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Initializing worker threads...

Creating table &#39;sbtest2&#39;...
Creating table &#39;sbtest1&#39;...
Inserting 1000000 records into &#39;sbtest1&#39;
Inserting 1000000 records into &#39;sbtest2&#39;
Creating a secondary index on &#39;sbtest2&#39;...
Creating table &#39;sbtest4&#39;...
Inserting 1000000 records into &#39;sbtest4&#39;
Creating a secondary index on &#39;sbtest1&#39;...
Creating table &#39;sbtest3&#39;...
Inserting 1000000 records into &#39;sbtest3&#39;
Creating a secondary index on &#39;sbtest4&#39;...
Creating a secondary index on &#39;sbtest3&#39;...
Creating table &#39;sbtest6&#39;...
Inserting 1000000 records into &#39;sbtest6&#39;
Creating table &#39;sbtest5&#39;...
Inserting 1000000 records into &#39;sbtest5&#39;
Creating a secondary index on &#39;sbtest5&#39;...
Creating a secondary index on &#39;sbtest6&#39;...
Creating table &#39;sbtest7&#39;...
Inserting 1000000 records into &#39;sbtest7&#39;
Creating table &#39;sbtest8&#39;...
Inserting 1000000 records into &#39;sbtest8&#39;
Creating a secondary index on &#39;sbtest7&#39;...
Creating a secondary index on &#39;sbtest8&#39;...
Creating table &#39;sbtest9&#39;...
Inserting 1000000 records into &#39;sbtest9&#39;
Creating table &#39;sbtest10&#39;...
Inserting 1000000 records into &#39;sbtest10&#39;
Creating a secondary index on &#39;sbtest10&#39;...
Creating table &#39;sbtest12&#39;...
Inserting 1000000 records into &#39;sbtest12&#39;
Creating a secondary index on &#39;sbtest9&#39;...
Creating table &#39;sbtest11&#39;...
Inserting 1000000 records into &#39;sbtest11&#39;
Creating a secondary index on &#39;sbtest12&#39;...
Creating table &#39;sbtest14&#39;...
Inserting 1000000 records into &#39;sbtest14&#39;
Creating a secondary index on &#39;sbtest11&#39;...
Creating table &#39;sbtest13&#39;...
Inserting 1000000 records into &#39;sbtest13&#39;
Creating a secondary index on &#39;sbtest14&#39;...
Creating table &#39;sbtest16&#39;...
Inserting 1000000 records into &#39;sbtest16&#39;
Creating a secondary index on &#39;sbtest13&#39;...
Creating table &#39;sbtest15&#39;...
Inserting 1000000 records into &#39;sbtest15&#39;
Creating a secondary index on &#39;sbtest16&#39;...
Creating table &#39;sbtest18&#39;...
Inserting 1000000 records into &#39;sbtest18&#39;
Creating a secondary index on &#39;sbtest15&#39;...
Creating table &#39;sbtest17&#39;...
Inserting 1000000 records into &#39;sbtest17&#39;
Creating a secondary index on &#39;sbtest18&#39;...
Creating table &#39;sbtest20&#39;...
Inserting 1000000 records into &#39;sbtest20&#39;
Creating a secondary index on &#39;sbtest17&#39;...
Creating table &#39;sbtest19&#39;...
Inserting 1000000 records into &#39;sbtest19&#39;
Creating a secondary index on &#39;sbtest20&#39;...
Creating table &#39;sbtest22&#39;...
Inserting 1000000 records into &#39;sbtest22&#39;
Creating a secondary index on &#39;sbtest19&#39;...
Creating table &#39;sbtest21&#39;...
Inserting 1000000 records into &#39;sbtest21&#39;
Creating a secondary index on &#39;sbtest22&#39;...
Creating table &#39;sbtest24&#39;...
Inserting 1000000 records into &#39;sbtest24&#39;
Creating a secondary index on &#39;sbtest21&#39;...
Creating table &#39;sbtest23&#39;...
Inserting 1000000 records into &#39;sbtest23&#39;
Creating a secondary index on &#39;sbtest24&#39;...
Creating a secondary index on &#39;sbtest23&#39;...
</code></pre>
<h2 id="Run-sysbench"><a href="#Run-sysbench" class="headerlink" title="Run sysbench"></a>Run sysbench</h2><p>Now we can run sysbench on the populated database. We can increase the threads to stress the database with more workloads in order to measure the system performance limit.</p>
<pre><code>[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=tester --pgsql-password= --table_size=100000 --tables=24 --threads=1 --time=60 --report-interval=10 --db-driver=pgsql run
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Report intermediate results every 10 second(s)
Initializing random number generator from current time

Initializing worker threads...

Threads started!

[ 10s ] thds: 1 tps: 80.86 qps: 485.66 (r/w/o: 0.00/84.66/401.00) lat (ms,95%): 20.37 err/s: 0.00 reconn/s: 0.00
[ 20s ] thds: 1 tps: 82.60 qps: 495.41 (r/w/o: 0.00/92.00/403.41) lat (ms,95%): 20.74 err/s: 0.00 reconn/s: 0.00
[ 30s ] thds: 1 tps: 81.00 qps: 485.79 (r/w/o: 0.00/94.10/391.69) lat (ms,95%): 20.74 err/s: 0.00 reconn/s: 0.00
[ 40s ] thds: 1 tps: 81.00 qps: 486.21 (r/w/o: 0.00/102.90/383.31) lat (ms,95%): 20.37 err/s: 0.00 reconn/s: 0.00
[ 50s ] thds: 1 tps: 79.20 qps: 474.90 (r/w/o: 0.00/103.10/371.80) lat (ms,95%): 21.50 err/s: 0.00 reconn/s: 0.00
[ 60s ] thds: 1 tps: 78.20 qps: 469.39 (r/w/o: 0.00/107.30/362.09) lat (ms,95%): 21.11 err/s: 0.00 reconn/s: 0.00
SQL statistics:
    queries performed:
        read:                            0
        write:                           5842
        other:                           23138
        total:                           28980
    transactions:                        4830   (80.48 per sec.)
    queries:                             28980  (482.90 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          60.0081s
    total number of events:              4830

Latency (ms):
         min:                                    6.87
         avg:                                   12.42
         max:                                   32.90
         95th percentile:                       21.11
         sum:                                59978.07

Threads fairness:
    events (avg/stddev):           4830.0000/0.00
    execution time (avg/stddev):   59.9781/0.00
</code></pre>
<h2 id="Cleanup-the-database"><a href="#Cleanup-the-database" class="headerlink" title="Cleanup the database"></a>Cleanup the database</h2><p>Once the sysbench is done, we can cleanup the test data as below.</p>
<pre><code>[root@node0 ~]# sysbench /usr/share/sysbench/oltp_write_only.lua --pgsql-host=node1 --pgsql-port=26257 --pgsql-db=testdb --pgsql-user=tester --pgsql-password= --table_size=100000 --tables=24 --threads=1 --db-driver=pgsql cleanup
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Dropping table &#39;sbtest1&#39;...
Dropping table &#39;sbtest2&#39;...
Dropping table &#39;sbtest3&#39;...
Dropping table &#39;sbtest4&#39;...
Dropping table &#39;sbtest5&#39;...
Dropping table &#39;sbtest6&#39;...
Dropping table &#39;sbtest7&#39;...
Dropping table &#39;sbtest8&#39;...
Dropping table &#39;sbtest9&#39;...
Dropping table &#39;sbtest10&#39;...
Dropping table &#39;sbtest11&#39;...
Dropping table &#39;sbtest12&#39;...
Dropping table &#39;sbtest13&#39;...
Dropping table &#39;sbtest14&#39;...
Dropping table &#39;sbtest15&#39;...
Dropping table &#39;sbtest16&#39;...
Dropping table &#39;sbtest17&#39;...
Dropping table &#39;sbtest18&#39;...
Dropping table &#39;sbtest19&#39;...
Dropping table &#39;sbtest20&#39;...
Dropping table &#39;sbtest21&#39;...
Dropping table &#39;sbtest22&#39;...
Dropping table &#39;sbtest23&#39;...
Dropping table &#39;sbtest24&#39;...

testdb=&gt; show tables from testdb;
 schema_name | table_name | type | owner | estimated_row_count | locality
-------------+------------+------+-------+---------------------+----------
(0 rows)

[root@node0 ~]# cockroach sql --insecure --host=node1:26257 -e &quot;drop database testdb&quot;
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/akopytov/sysbench">https://github.com/akopytov/sysbench</a></li>
<li><a href="https://www.cockroachlabs.com/blog/why-postgres/">https://www.cockroachlabs.com/blog/why-postgres/</a></li>
<li><a href="https://www.cockroachlabs.com/compare/cockroachdb-vs-postgresql/">https://www.cockroachlabs.com/compare/cockroachdb-vs-postgresql/</a></li>
<li><a href="https://www.cockroachlabs.com/blog/postgresql-vs-cockroachdb/">https://www.cockroachlabs.com/blog/postgresql-vs-cockroachdb/</a></li>
<li><a href="https://www.cockroachlabs.com/blog/unpacking-competitive-benchmarks/">https://www.cockroachlabs.com/blog/unpacking-competitive-benchmarks/</a></li>
<li><a href="https://blog.purestorage.com/purely-informational/how-to-benchmark-mysql-performance/">https://blog.purestorage.com/purely-informational/how-to-benchmark-mysql-performance/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>CockroachDB</tag>
        <tag>Sysbench</tag>
      </tags>
  </entry>
  <entry>
    <title>Create Image Gallery in Jekyll without Plugin</title>
    <url>/blog/create-image-gallery-in-jekyll-without-plugin/</url>
    <content><![CDATA[<p>An image gallery can be easily built by using <strong>LightBox</strong> and <strong>Image Gallery</strong> scripts in Jekyll.</p>
<ol>
<li>LightBox</li>
</ol>
<p><a href="https://jekyllcodex.org/without-plugin/lightbox/">Lightbox</a> is a solution that loads your image links, your Youtube links and your Vimeo links automatically in a minimalistic and responsive pseudo window&#x2F;overlay. No adjustment to your links is required, just follow the instructions to install the css and js script.</p>
<ol start="2">
<li>Image Gallery</li>
</ol>
<p>The script <a href="https://jekyllcodex.org/without-plugin/image-gallery/">Image Gallery</a> creates an image gallery. The script reads all images from a specific (user-defined) folder in Jekyll, automagically crops them to 300px squares, using an image resize proxy service and shows them in rows of five. Just follow the very easy instrutions to install it and we are good to go.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>Jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title>Data volume unit</title>
    <url>/blog/data-volume-unit/</url>
    <content><![CDATA[<h2 id="Data-volumes"><a href="#Data-volumes" class="headerlink" title="Data volumes"></a>Data volumes</h2><p>The volume of data in a single file or file system can be described by a unit called a byte. However, data volumes can become very large when dealing with Earth satellite data. Below is a table to explain data volume units (Credit:  Roy Williams, Center for Advanced Computing Research at the California Insittute of Technology).</p>
<ul>
<li>Kilo- means 1,000; a Kilobyte is one thousand bytes.</li>
<li>Mega- means 1,000,000; a Megabyte is a million bytes.</li>
<li>Giga- means 1,000,000,000; a Gigabyte is a billion bytes.</li>
<li>Tera- means 1,000,000,000,000; a Terabyte is a trillion bytes.</li>
<li>Peta- means 1,000,000,000,000,000; a Petabyte is 1,000 Terabytes.</li>
<li>Exa- means 1,000,000,000,000,000,000; an Exabyte is 1,000 Petabytes.</li>
<li>Zetta- means 1,000,000,000,000,000,000,000; a Zettabyte is 1,000 Exabytes.</li>
<li>Yotta- means 1,000,000,000,000,000,000,000,000; a Yottabyte is 1,000 Zettabytes.</li>
</ul>
<p><strong>Examples of Data Volumes:</strong></p>
<p><img src="/images/data-volumes-examples.png" alt="Image"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://mynasadata.larc.nasa.gov/basic-page/data-volume-units#:~:text=Data%20Volumes%3A,dealing%20with%20Earth%20satellite%20data.">https://mynasadata.larc.nasa.gov/basic-page/data-volume-units#:~:text&#x3D;Data Volumes%3A,dealing with Earth satellite data.</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
  </entry>
  <entry>
    <title>Database concepts you should know</title>
    <url>/blog/database-concepts-you-should-know/</url>
    <content><![CDATA[<h2 id="SQL-vs-NoSQL"><a href="#SQL-vs-NoSQL" class="headerlink" title="SQL vs. NoSQL"></a>SQL vs. NoSQL</h2><p><img src="/images/sql-vs-nosql.png" alt="Image"></p>
<p><a href="https://www.mongodb.com/nosql-explained/nosql-vs-sql">Source</a></p>
<p><strong>Benefits of NoSQL databases</strong></p>
<p>NoSQL databases offer many benefits over relational databases. NoSQL databases have flexible data models, scale horizontally, have incredibly fast queries, and are easy for developers to work with.</p>
<ul>
<li>Flexible data models</li>
</ul>
<p>NoSQL databases typically have very flexible schemas. A flexible schema allows you to easily make changes to your database as requirements change. You can iterate quickly and continuously integrate new application features to provide value to your users faster.</p>
<ul>
<li>Horizontal scaling</li>
</ul>
<p>Most SQL databases require you to scale-up vertically (migrate to a larger, more expensive server) when you exceed the capacity requirements of your current server. Conversely, most NoSQL databases allow you to scale-out horizontally, meaning you can add cheaper, commodity servers whenever you need to.</p>
<ul>
<li>Fast queries</li>
</ul>
<p>Queries in NoSQL databases can be faster than SQL databases. Why? Data in SQL databases is typically normalized, so queries for a single object or entity require you to join data from multiple tables. As your tables grow in size, the joins can become expensive. However, data in NoSQL databases is typically stored in a way that is optimized for queries. The rule of thumb when you use MongoDB is Data that is accessed together should be stored together. Queries typically do not require joins, so the queries are very fast.</p>
<ul>
<li>Easy for developers</li>
</ul>
<p>Some NoSQL databases like MongoDB map their data structures to those of popular programming languages. This mapping allows developers to store their data in the same way that they use it in their application code. While it may seem like a trivial advantage, this mapping can allow developers to write less code, leading to faster development time and fewer bugs.</p>
<p><a href="https://www.mongodb.com/nosql-explained/nosql-vs-sql">Source</a></p>
<h2 id="Apache-Cassandra"><a href="#Apache-Cassandra" class="headerlink" title="Apache Cassandra"></a>Apache Cassandra</h2><p>Cassandra is a NoSQL distributed database. By design, NoSQL databases are lightweight, open-source, non-relational, and largely distributed. Counted among their strengths are horizontal scalability, distributed architectures, and a flexible approach to schema definition.</p>
<p>NoSQL databases enable rapid, ad-hoc organization and analysis of extremely high-volume, disparate data types. That’s become more important in recent years, with the advent of Big Data and the need to rapidly scale databases in the cloud. Cassandra is among the NoSQL databases that have addressed the constraints of previous data management technologies, such as SQL databases.</p>
<p><img src="/images/apache-cassandra-diagrams.jpeg" alt="Image"></p>
<p><a href="https://cassandra.apache.org/_/cassandra-basics.html">Source</a></p>
<h2 id="ACID-and-CAP-theorem"><a href="#ACID-and-CAP-theorem" class="headerlink" title="ACID and CAP theorem"></a>ACID and CAP theorem</h2><p>A clear explanation can be referenced <a href="https://medium.com/@pranabj.aec/acid-cap-and-base-cc73dee43f8c">here</a></p>
<h2 id="Database-Index"><a href="#Database-Index" class="headerlink" title="Database Index"></a>Database Index</h2><p>A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure. Indexes are used to quickly locate data without having to search every row in a database table every time a database table is accessed. Indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access of ordered records.</p>
<p>An index is a copy of selected columns of data, from a table, that is designed to enable very efficient search. An index normally includes a “key” or direct link to the original row of data from which it was copied, to allow the complete row to be retrieved efficiently. Some databases extend the power of indexing by letting developers create indexes on column values that have been transformed by functions or expressions. For example, an index could be created on upper(last_name), which would only store the upper-case versions of the last_name field in the index. Another option sometimes supported is the use of partial indices, where index entries are created only for those records that satisfy some conditional expression. A further aspect of flexibility is to permit indexing on user-defined functions, as well as expressions formed from an assortment of built-in functions.</p>
<p><a href="https://en.wikipedia.org/wiki/Database_index#:~:text=A%20database%20index%20is%20a,maintain%20the%20index%20data%20structure.">Source</a></p>
<p><strong>How are indexes created?</strong></p>
<p>In a database, data is stored in rows which are organized into tables. Each row has a unique key which distinguishes it from all other rows and those keys are stored in an index for quick retrieval.</p>
<p>Since keys are stored in indexes, each time a new row with a unique key is added, the index is automatically updated. However, sometimes we need to be able to quickly lookup data that is not stored as a key. For example, we may need to quickly lookup customers by telephone number. It would not be a good idea to use a unique constraint because we can have multiple customers with the same phone number. In these cases, we can create our own indexes.</p>
<p><a href="https://www.codecademy.com/article/sql-indexes">Source</a></p>
<h2 id="Sharding"><a href="#Sharding" class="headerlink" title="Sharding"></a>Sharding</h2><p>Sharding is a method of splitting and storing a single logical dataset in multiple databases. By distributing the data among multiple machines, a cluster of database systems can store larger dataset and handle additional requests. Sharding is necessary if a dataset is too large to be stored in a single database. Moreover, many sharding strategies allow additional machines to be added. Sharding allows a database cluster to scale along with its data and traffic growth.</p>
<p>Sharding is also referred as horizontal partitioning. The distinction of horizontal vs vertical comes from the traditional tabular view of a database. A database can be split vertically — storing different tables &amp; columns in a separate database, or horizontally — storing rows of a same table in multiple database nodes.</p>
<p><a href="https://en.wikipedia.org/wiki/Shard_(database_architecture)">Source1</a><br><a href="https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6#:~:text=Sharding%20is%20a%20method%20of,stored%20in%20a%20single%20database.">Source2</a></p>
<h2 id="Amazon-DynamoDB"><a href="#Amazon-DynamoDB" class="headerlink" title="Amazon DynamoDB"></a>Amazon DynamoDB</h2><p>Amazon DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale. DynamoDB offers built-in security, continuous backups, automated multi-region replication, in-memory caching, and data export tools.</p>
<p><a href="https://aws.amazon.com/dynamodb/">Source</a></p>
<h2 id="SSTable"><a href="#SSTable" class="headerlink" title="SSTable"></a>SSTable</h2><p>Sorted Strings Table (SSTable) is a persistent file format used by Scylla, Apache Cassandra, and other NoSQL databases to take the in-memory data stored in memtables, order it for fast access, and store it on disk in a persistent, ordered, immutable set of files. Immutable means SSTables are never modified. They are later merged into new SSTables or deleted as data is updated.</p>
<p><img src="/images/sstable-diagram.png" alt="Image"></p>
<p><a href="http://distributeddatastore.blogspot.com/2013/08/cassandra-sstable-storage-format.html">Source1</a><br><a href="https://www.scylladb.com/glossary/sstable/#:~:text=Sorted%20Strings%20Table%20(SSTable)%20is,means%20SSTables%20are%20never%20modified.">Source2</a></p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
  </entry>
  <entry>
    <title>Deploy application with kubernetes statefulset</title>
    <url>/blog/deploy-application-with-kubernetes-statefulset/</url>
    <content><![CDATA[<p>StatefulSet is the workload API object used to manage stateful applications.</p>
<p>Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p>
<p>If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.</p>
<p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">Source</a></p>
<p>The example below demonstrates the components of a StatefulSet.</p>
<pre><code>$ cat perfbench-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: perfbench
spec:
  serviceName: perfbench
  replicas: 1
  selector:
    matchLabels:
      app: perfbench
  template:
    metadata:
      labels:
        app: perfbench
    spec:
      containers:
      - name: perfbench
        image: noname/perfbench:latest
        volumeMounts:
        - name: perfbench-data
          mountPath: /perfdata
        - name: perfbench-log
          mountPath: /perflog
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: perfbench-data
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 20Gi
  - metadata:
      name: perfbench-log
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
</code></pre>
<p>The statefulset can be created as below.</p>
<pre><code>$ kubectl apply -f perfbench-statefulset.yaml
</code></pre>
<p>In case of any failure to create the statefulset, you can check the events with the following commands.</p>
<pre><code>$ kubectl describe pod perfbench-0
Name:         perfbench-0
Namespace:    default
Priority:     0
Node:         &lt;hostname&gt;/&lt;ip-address&gt;
Start Time:   Sat, 26 Feb 2022 06:22:27 +0000
Labels:       app=perfbench
              controller-revision-hash=perfbench-7657fb8779
              statefulset.kubernetes.io/pod-name=perfbench-0
Annotations:  cni.projectcalico.org/containerID: c8569d9a3f01f546ca92fb2d0cba98d4d971933e53ab83373ed34c91040d92bc
              cni.projectcalico.org/podIP: 192.168.201.145/32
              cni.projectcalico.org/podIPs: 192.168.201.145/32
Status:       Running
IP:           192.168.201.145
IPs:
  IP:           192.168.201.145
Controlled By:  StatefulSet/perfbench
Containers:
  perfbench:
    Container ID:   docker://b6a2c838837395c1c85913d4735e3167d3cea6b5ed3ade276584461d643eaee5
    Image:          noname/perfbench:latest
    Image ID:       docker-pullable://&lt;noname/perfbench@sha256:5460e3c04ea972afcde3db092b514919867f87974d012f046c538ac816c7aaae
    Port:           &lt;none&gt;
    Host Port:      &lt;none&gt;
    State:          Running
      Started:      Sat, 26 Feb 2022 06:22:35 +0000
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /perfdata from perfbench-data (rw)
      /perflog from perfbench-log (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x8qhw (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  perfbench-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  perfbench-data-perfbench-0
    ReadOnly:   false
  perfbench-log:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  perfbench-log-perfbench-0
    ReadOnly:   false
  kube-api-access-x8qhw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              &lt;none&gt;
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  20s (x2 over 21s)  default-scheduler  0/3 nodes are available: 3 pod has unbound immediate PersistentVolumeClaims.
  Normal   Scheduled         18s                default-scheduler  Successfully assigned default/perfbench-0 to &lt;hostname&gt;
  Normal   Pulling           16s                kubelet            Pulling image &quot;&lt;noname/perfbench:latest&quot;
  Normal   Pulled            11s                kubelet            Successfully pulled image &quot;&lt;noname/perfbench:latest&quot; in 4.368378217s
  Normal   Created           10s                kubelet            Created container perfbench
  Normal   Started           10s                kubelet            Started container perfbench
</code></pre>
<p>You can check the pod status and login to the pod container as below.</p>
<pre><code>$ kubectl get pod
NAME         READY   STATUS    RESTARTS   AGE
perfbench-0   1/1     Running   0          3m44s

$ kubectl exec -it perfbench-0 -- bash
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">https://kubernetes.io/docs/concepts/storage/storage-classes/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/containers/images/">https://kubernetes.io/docs/concepts/containers/images/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Deploy ceph cluster on Ubuntu 18.04 and CentOS 7.8</title>
    <url>/blog/deploy-ceph-cluster-on-ubuntu-18-04-and-centos-7-8/</url>
    <content><![CDATA[<p>In this article, we learn to deploy ceph cluster on ubuntu 18.04. Three nodes are used for this study.</p>
<p>We target to deploy the most recent ceph release which is called <strong>Pacific</strong>. With this release, we can use <strong>cephadm</strong> to create a ceph cluster by <strong>bootstrapping</strong> on a single host and expanding the cluster to additional hosts.</p>
<h2 id="Intro-to-ceph"><a href="#Intro-to-ceph" class="headerlink" title="Intro to ceph"></a>Intro to ceph</h2><p>Whether you want to provide Ceph Object Storage and&#x2F;or Ceph Block Device services to Cloud Platforms, deploy a Ceph Filesystem or use Ceph for another purpose, all Ceph Storage Cluster deployments begin with setting up each Ceph Node, your network, and the Ceph Storage Cluster. A Ceph Storage Cluster requires at least one Ceph Monitor, Ceph Manager, and Ceph OSD (Object Storage Daemon). The Ceph Metadata Server is also required when running Ceph Filesystem clients.</p>
<ul>
<li><p><strong>Monitors</strong>: A Ceph Monitor (ceph-mon) maintains maps of the cluster state, including the monitor map, manager map, the OSD map, and the CRUSH map. These maps are critical cluster state required for Ceph daemons to coordinate with each other. Monitors are also responsible for managing authentication between daemons and clients. At least three monitors are normally required for redundancy and high availability.</p>
</li>
<li><p><strong>Managers</strong>: A Ceph Manager daemon (ceph-mgr) is responsible for keeping track of runtime metrics and the current state of the Ceph cluster, including storage utilization, current performance metrics, and system load. The Ceph Manager daemons also host python-based plugins to manage and expose Ceph cluster information, including a web-based dashboard and REST API. At least two managers are normally required for high availability.</p>
</li>
<li><p><strong>Ceph OSDs</strong>: A Ceph OSD (object storage daemon, ceph-osd) stores data, handles data replication, recovery, rebalancing, and provides some monitoring information to Ceph Monitors and Managers by checking other Ceph OSD Daemons for a heartbeat. At least 3 Ceph OSDs are normally required for redundancy and high availability.</p>
</li>
<li><p><strong>MDSs</strong>: A Ceph Metadata Server (MDS, ceph-mds) stores metadata on behalf of the Ceph Filesystem (i.e., Ceph Block Devices and Ceph Object Storage do not use MDS). Ceph Metadata Servers allow POSIX file system users to execute basic commands (like ls, find, etc.) without placing an enormous burden on the Ceph Storage Cluster.</p>
</li>
</ul>
<p>Ceph stores data as objects within logical storage pools. Using the CRUSH algorithm, Ceph calculates which placement group should contain the object, and further calculates which Ceph OSD Daemon should store the placement group. The CRUSH algorithm enables the Ceph Storage Cluster to scale, rebalance, and recover dynamically.</p>
<h2 id="Deploy-a-ceph-storage-cluster"><a href="#Deploy-a-ceph-storage-cluster" class="headerlink" title="Deploy a ceph storage cluster"></a>Deploy a ceph storage cluster</h2><h3 id="Prepare-Ubuntu-Linux-and-packages"><a href="#Prepare-Ubuntu-Linux-and-packages" class="headerlink" title="Prepare Ubuntu Linux and packages"></a>Prepare Ubuntu Linux and packages</h3><p>From ceph installation guide, the following system requirements must be met before deployment.</p>
<ul>
<li><p>Python 3</p>
</li>
<li><p>Systemd</p>
</li>
<li><p>Podman or Docker for running containers</p>
</li>
<li><p>Time synchronization (such as chrony or NTP)</p>
</li>
<li><p>LVM2 for provisioning storage devices</p>
<p>  root@host1:~# cat &#x2F;etc&#x2F;*release<br>  DISTRIB_ID&#x3D;Ubuntu<br>  DISTRIB_RELEASE&#x3D;16.04</p>
</li>
</ul>
<p><strong>Upgrade to Ubuntu 18.0.4</strong></p>
<pre><code>root@host1:~# apt install update-manager-core

root@host1:~# do-release-upgrade -c
Checking for a new Ubuntu release
New release &#39;18.04.5 LTS&#39; available.
Run &#39;do-release-upgrade&#39; to upgrade to it.

root@host1:~# do-release-upgrade

root@host1:~# cat /etc/*release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
</code></pre>
<p><strong>Install python3</strong></p>
<pre><code>root@host1:~# apt-get install python3
</code></pre>
<p><strong>Install docker</strong></p>
<p>Refer to <a href="https://docs.docker.com/engine/install/ubuntu/">here</a></p>
<p><strong>Install ntp</strong></p>
<pre><code>root@host1:~# apt-get install ntp
root@host1:~# service ntp start
root@host1:~# timedatectl set-timezone UTC
</code></pre>
<p><strong>Install lvm2</strong></p>
<pre><code>root@host1:~# apt-get install lvm2
</code></pre>
<p><strong>Check and disable firewall status</strong></p>
<pre><code>root@host1:~# ufw status
</code></pre>
<p><strong>Add cluster nodes to &#x2F;etc&#x2F;hosts</strong></p>
<p><strong>Configure passwordless ssh from primary host to the others</strong></p>
<h2 id="Install-cephadm"><a href="#Install-cephadm" class="headerlink" title="Install cephadm"></a>Install cephadm</h2><p>The cephadm command can</p>
<ul>
<li><p>bootstrap a new cluster</p>
</li>
<li><p>launch a containerized shell with a working Ceph CLI</p>
</li>
<li><p>aid in debugging containerized Ceph daemons</p>
<p>  root@host1:<del># curl –silent –remote-name –location <a href="https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm">https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm</a><br>  root@host1:</del># ls<br>  cephadm<br>  root@host1:~# chmod +x cephadm</p>
<p>  root@host1:<del># .&#x2F;cephadm add-repo –release pacific<br>  root@host1:</del># .&#x2F;cephadm install<br>  root@host1:~# which cephadm<br>  &#x2F;usr&#x2F;sbin&#x2F;cephadm</p>
</li>
</ul>
<h2 id="Bootstrap-a-new-cluster"><a href="#Bootstrap-a-new-cluster" class="headerlink" title="Bootstrap a new cluster"></a>Bootstrap a new cluster</h2><p>The first step in creating a new Ceph cluster is running the <strong>cephadm bootstrap</strong> command on the Ceph cluster’s first host. The act of running the <strong>cephadm bootstrap</strong> command on the Ceph cluster’s first host creates the Ceph cluster’s first “monitor daemon”, and that monitor daemon needs an IP address. You must pass the IP address of the Ceph cluster’s first host to the <strong>ceph bootstrap</strong> command, so you’ll need to know the IP address of that host.</p>
<pre><code>root@host1:~# cephadm bootstrap --mon-ip &lt;host1-ip&gt; --allow-fqdn-hostname
Ceph Dashboard is now available at:

         URL: https://host1:8443/
        User: admin
    Password: btauef87vj

Enabling client.admin keyring and conf on hosts with &quot;admin&quot; label
You can access the Ceph CLI with:

    sudo /usr/sbin/cephadm shell --fsid ad30a6fc-068f-11ec-8323-000c29bf98ea -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

    ceph telemetry on

For more information see:

    https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

root@host1:~# docker ps
CONTAINER ID   IMAGE                        COMMAND                  CREATED         STATUS         PORTS     NAMES
a946ae868dbc   prom/alertmanager:v0.20.0    &quot;/bin/alertmanager -…&quot;   6 minutes ago   Up 6 minutes             ceph-ad30a6fc-068f-11ec-8323-000c29bf98ea-alertmanager.host1
504d9271b24c   ceph/ceph-grafana:6.7.4      &quot;/bin/sh -c &#39;grafana…&quot;   6 minutes ago   Up 6 minutes             ceph-ad30a6fc-068f-11ec-8323-000c29bf98ea-grafana.host1
622a5e234406   prom/prometheus:v2.18.1      &quot;/bin/prometheus --c…&quot;   6 minutes ago   Up 6 minutes             ceph-ad30a6fc-068f-11ec-8323-000c29bf98ea-prometheus.host1
6c2b0440d4c1   prom/node-exporter:v0.18.1   &quot;/bin/node_exporter …&quot;   6 minutes ago   Up 6 minutes             ceph-ad30a6fc-068f-11ec-8323-000c29bf98ea-node-exporter.host1
8bc618e9ffa3   ceph/ceph                    &quot;/usr/bin/ceph-crash…&quot;   6 minutes ago   Up 6 minutes             ceph-ad30a6fc-068f-11ec-8323-000c29bf98ea-crash.host1
b57a021238ba   ceph/ceph:v16                &quot;/usr/bin/ceph-mgr -…&quot;   7 minutes ago   Up 7 minutes             ceph-ad30a6fc-068f-11ec-8323-000c29bf98ea-mgr.host1.ltfphc
e812853ef17d   ceph/ceph:v16                &quot;/usr/bin/ceph-mon -…&quot;   7 minutes ago   Up 7 minutes             ceph-ad30a6fc-068f-11ec-8323-000c29bf98ea-mon.host1
</code></pre>
<h2 id="Enable-ceph-CLI"><a href="#Enable-ceph-CLI" class="headerlink" title="Enable ceph CLI"></a>Enable ceph CLI</h2><p>To execute ceph commands, you can also run commands like this:</p>
<pre><code>root@host1:~# cephadm shell -- ceph -s
Inferring fsid ad30a6fc-068f-11ec-8323-000c29bf98ea
Inferring config /var/lib/ceph/ad30a6fc-068f-11ec-8323-000c29bf98ea/mon.host1/config
Using recent ceph image ceph/ceph@sha256:829ebf54704f2d827de00913b171e5da741aad9b53c1f35ad59251524790eceb
  cluster:
    id:     ad30a6fc-068f-11ec-8323-000c29bf98ea
    health: HEALTH_WARN
            OSD count 0 &lt; osd_pool_default_size 3

  services:
    mon: 1 daemons, quorum host1 (age 9m)
    mgr: host1.ltfphc(active, since 10m)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:
</code></pre>
<p>Cephadm does not require any Ceph packages to be installed on the host. However, it recommends enabling easy access to the ceph command.</p>
<p>You can install the ceph-common package, which contains all of the ceph commands, including ceph, rbd, mount.ceph (for mounting CephFS file systems), etc.:</p>
<pre><code>root@host1:~# cephadm add-repo --release pacific
Installing repo GPG key from https://download.ceph.com/keys/release.gpg...
Installing repo file at /etc/apt/sources.list.d/ceph.list...
Updating package list...
Completed adding repo.
root@host1:~# cephadm install ceph-common
Installing packages [&#39;ceph-common&#39;]...

root@host1:~# ceph -v
ceph version 16.2.5 (0883bdea7337b95e4b611c768c0279868462204a) pacific (stable)

root@host1:~# ceph status
  cluster:
    id:     ad30a6fc-068f-11ec-8323-000c29bf98ea
    health: HEALTH_WARN
            OSD count 0 &lt; osd_pool_default_size 3

  services:
    mon: 1 daemons, quorum host1 (age 11m)
    mgr: host1.ltfphc(active, since 12m)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:
</code></pre>
<h2 id="Adding-additional-hosts-to-the-cluster"><a href="#Adding-additional-hosts-to-the-cluster" class="headerlink" title="Adding additional hosts to the cluster"></a>Adding additional hosts to the cluster</h2><p>To add each new host to the cluster, perform two steps:</p>
<ol>
<li><p>Install the cluster’s public SSH key in the new host’s root user’s authorized_keys file:</p>
<p> root@host1:<del># ssh-copy-id -f -i &#x2F;etc&#x2F;ceph&#x2F;ceph.pub root@host2<br> root@host1:</del># ssh-copy-id -f -i &#x2F;etc&#x2F;ceph&#x2F;ceph.pub root@host3</p>
</li>
<li><p>Tell Ceph that the new node is part of the cluster:</p>
<p> root@host1:<del># ceph orch host add host2 <host2-ip> –labels _admin<br> root@host1:</del># ceph orch host add host3 <host3-ip> –labels _admin</p>
</li>
</ol>
<p>Wait for a while until the monitor detects the new hosts. Verify the new added hosts as below.</p>
<pre><code>root@host1:~# cat /etc/ceph/ceph.conf
# minimal ceph.conf for ad30a6fc-068f-11ec-8323-000c29bf98ea
[global]
    fsid = ad30a6fc-068f-11ec-8323-000c29bf98ea
    mon_host = [v2:&lt;host2-ip&gt;:3300/0,v1:&lt;host2-ip&gt;:6789/0] [v2:&lt;host3-ip&gt;:3300/0,v1:&lt;host3-ip&gt;:6789/0] [v2:&lt;host1-ip&gt;:3300/0,v1:&lt;host1-ip&gt;:6789/0]

root@host1:~# ceph status
  cluster:
    id:     ad30a6fc-068f-11ec-8323-000c29bf98ea
    health: HEALTH_WARN
            clock skew detected on mon.host2, mon.host3
            OSD count 0 &lt; osd_pool_default_size 3

  services:
    mon: 3 daemons, quorum host1,host2,host3 (age 115s)
    mgr: host1.ltfphc(active, since 30m), standbys: host2.dqlsnk
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:
</code></pre>
<h2 id="Adding-storage"><a href="#Adding-storage" class="headerlink" title="Adding storage"></a>Adding storage</h2><p>To add storage to the cluster, either tell Ceph to consume any available and unused device:</p>
<pre><code>ceph orch apply osd --all-available-devices
</code></pre>
<p>Or Deploy OSDs with specified storage devices.</p>
<h3 id="Listing-storage-devices"><a href="#Listing-storage-devices" class="headerlink" title="Listing storage devices"></a>Listing storage devices</h3><p>In order to deploy an OSD, there must be a storage device that is available on which the OSD will be deployed.</p>
<p>Run this command to display an inventory of storage devices on all cluster hosts:</p>
<pre><code>root@host1:~# ceph orch device ls
Hostname                            Path      Type  Serial  Size   Health   Ident  Fault  Available
host2  /dev/sdb  hdd           85.8G  Unknown  N/A    N/A    Yes
host3  /dev/sdb  hdd           85.8G  Unknown  N/A    N/A    Yes
host1  /dev/sdb  hdd           85.8G  Unknown  N/A    N/A    Yes
</code></pre>
<p>A storage device is considered available if all of the following conditions are met:</p>
<ul>
<li>The device must have no partitions.</li>
<li>The device must not have any LVM state.</li>
<li>The device must not be mounted.</li>
<li>The device must not contain a file system.</li>
<li>The device must not contain a Ceph BlueStore OSD.</li>
<li>The device must be larger than 5 GB.</li>
</ul>
<p>Ceph will not provision an OSD on a device that is not available.</p>
<h3 id="Creating-new-OSDs"><a href="#Creating-new-OSDs" class="headerlink" title="Creating new OSDs"></a>Creating new OSDs</h3><p>There are a few ways to create new OSDs:</p>
<ul>
<li><p>Tell Ceph to consume any available and unused storage device:</p>
<p>  ceph orch apply osd –all-available-devices</p>
</li>
</ul>
<p>After running the above command:</p>
<ul>
<li>If you add new disks to the cluster, they will automatically be used to create new OSDs.</li>
<li>If you remove an OSD and clean the LVM physical volume, a new OSD will be created automatically.</li>
</ul>
<p>If you want to avoid this behavior (disable automatic creation of OSD on available devices), use the <em>unmanaged</em> parameter:</p>
<pre><code>ceph orch apply osd --all-available-devices --unmanaged=true
</code></pre>
<ul>
<li><p>Create an OSD from a specific device on a specific host:</p>
<p>  ceph orch daemon add osd <em><host></em>:<em><device-path></em></p>
</li>
</ul>
<p>For example:</p>
<pre><code>ceph orch daemon add osd host1:/dev/sdb 
</code></pre>
<p>In our case, we use the following commands to create OSDs for the three nodes. We only need run the commands from host1.</p>
<pre><code>root@host1:~#   ceph orch daemon add osd host1:/dev/sdb
Created osd(s) 0 on host &#39;host1&#39;
root@host1:~# ceph orch daemon add osd host2:/dev/sdb
Created osd(s) 1 on host &#39;host2&#39;
root@host1:~# ceph orch daemon add osd host3:/dev/sdb
Created osd(s) 2 on host &#39;host3&#39;

root@host1:~# ceph status
  cluster:
    id:     ad30a6fc-068f-11ec-8323-000c29bf98ea
    health: HEALTH_WARN
            clock skew detected on mon.host2, mon.host3
            59 slow ops, oldest one blocked for 130 sec, mon.host2 has slow ops

  services:
    mon: 3 daemons, quorum host1,host2,host3 (age 2m)
    mgr: host1.ltfphc(active, since 102s), standbys: host2.dqlsnk
    osd: 3 osds: 3 up (since 7m), 3 in (since 7m)

  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   15 MiB used, 240 GiB / 240 GiB avail
    pgs:     1 active+clean
</code></pre>
<h3 id="Rry-Run"><a href="#Rry-Run" class="headerlink" title="Rry Run"></a>Rry Run</h3><p>The –dry-run flag causes the orchestrator to present a preview of what will happen without actually creating the OSDs.</p>
<p>For example:</p>
<pre><code>ceph orch apply osd --all-available-devices --dry-run
</code></pre>
<h2 id="Create-a-pool"><a href="#Create-a-pool" class="headerlink" title="Create a pool"></a>Create a pool</h2><p>Pools are logical partitions for storing objects. When you first deploy a cluster without creating a pool, Ceph uses the default pools for storing data.</p>
<p>By default, Ceph makes 3 replicas of RADOS objects. Ensure you have a realistic number of placement groups. Ceph recommends approximately 100 per OSD and always use the nearest power of 2.</p>
<pre><code>root@host1:~# ceph osd lspools
1 device_health_metrics
root@host1:~# ceph osd pool create datapool 128 128
pool &#39;datapool&#39; created
root@host1:~# ceph osd lspools
1 device_health_metrics
2 datapool


root@host1:~# ceph osd pool ls detail
pool 1 &#39;device_health_metrics&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 22 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 &#39;datapool&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 39 flags hashpspool stripe_width 0

root@host1:~# ceph osd pool get datapool all
size: 3
min_size: 2
pg_num: 128
pgp_num: 128
crush_rule: replicated_rule
hashpspool: true
nodelete: false
nopgchange: false
nosizechange: false
write_fadvise_dontneed: false
noscrub: false
nodeep-scrub: false
use_gmt_hitset: 1
fast_read: 0
pg_autoscale_mode: on
</code></pre>
<p>On the admin node, use the rbd tool to initialize the pool for use by RBD:</p>
<pre><code>[ceph: root@host1 /]# rbd pool init datapool
</code></pre>
<h2 id="Create-rbd-volume-and-map-to-a-block-device-on-the-host"><a href="#Create-rbd-volume-and-map-to-a-block-device-on-the-host" class="headerlink" title="Create rbd volume and map to a block device on the host"></a>Create rbd volume and map to a block device on the host</h2><p>The rbd command enables you to create, list, introspect and remove block device images. You can also use it to clone images, create snapshots, rollback an image to a snapshot, view a snapshot, etc.</p>
<pre><code>root@host1:~# rbd create --size 512000 datapool/rbdvol1
root@host1:~# rbd map datapool/rbdvol1
rbd: sysfs write failed
RBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable datapool/rbdvol1 object-map fast-diff deep-flatten&quot;.
In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.
rbd: map failed: (6) No such device or address

root@host1:~# dmesg | tail
[50268.015821] cgroup: cgroup: disabling cgroup2 socket matching due to net_prio or net_cls activation
[59168.019848] Key type ceph registered
[59168.020080] libceph: loaded (mon/osd proto 15/24)
[59168.023667] rbd: loaded (major 252)
[59168.028478] libceph: mon2 &lt;host1-ip&gt;:6789 session established
[59168.028571] libceph: mon2 &lt;host1-ip&gt;:6789 socket closed (con state OPEN)
[59168.028594] libceph: mon2 &lt;host1-ip&gt;:6789 session lost, hunting for new mon
[59175.101037] libceph: mon0 &lt;host1-ip&gt;:6789 session established
[59175.101413] libceph: client14535 fsid ad30a6fc-068f-11ec-8323-000c29bf98ea
[59175.105601] rbd: image rbdvol1: image uses unsupported features: 0x38

root@host1:~# rbd feature disable datapool/rbdvol1 object-map fast-diff deep-flatten

root@host1:~# rbd map datapool/rbdvol1
/dev/rbd0

root@host1:~# rbd showmapped
id  pool      namespace  image    snap  device
0   datapool             rbdvol1  -     /dev/rbd0

root@host1:~# lsblk
NAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                                                                                                     8:0    0  128G  0 disk
├─sda1                                                                                                  8:1    0  127G  0 part /
├─sda2                                                                                                  8:2    0    1K  0 part
└─sda5                                                                                                  8:5    0  975M  0 part
sdb                                                                                                     8:16   0   80G  0 disk
└─ceph--bc7eff08--2ac6--44a5--b941--5444c4a8600a-osd--block--b4dfb938--05af--413d--a327--18d26fc75b8d 253:0    0   80G  0 lvm
rbd0                                                                                                  252:0    0  100G  0 disk

root@host1:~# ls -la /dev/rbd/datapool/rbdvol1
lrwxrwxrwx 1 root root 10 Aug 26 19:35 /dev/rbd/datapool/rbdvol1 -&gt; ../../rbd0

root@host1:~# ls -la /dev/rbd0
brw-rw---- 1 root disk 252, 0 Aug 26 19:35 /dev/rbd0

root@host1:~# rbd status datapool/rbdvol1
Watchers:
    watcher=&lt;host1-ip&gt;:0/2778790200 client.14556 cookie=18446462598732840967

root@host1:~# rbd info datapool/rbdvol1
rbd image &#39;rbdvol1&#39;:
    size 100 GiB in 25600 objects
    order 22 (4 MiB objects)
    snapshot_count: 0
    id: 38bebe718b2f
    block_name_prefix: rbd_data.38bebe718b2f
    format: 2
    features: layering, exclusive-lock
    op_features:
    flags:
    create_timestamp: Thu Aug 26 19:31:29 2021
    access_timestamp: Thu Aug 26 19:31:29 2021
    modify_timestamp: Thu Aug 26 19:31:29 2021
</code></pre>
<h2 id="Create-filesystem-and-mount-rbd-volume"><a href="#Create-filesystem-and-mount-rbd-volume" class="headerlink" title="Create filesystem and mount rbd volume"></a>Create filesystem and mount rbd volume</h2><p>You can use Linux standard commands to create filesystem on the volume and mount it for different purpose.</p>
<h2 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h2><ol>
<li>Ceph does not support pacific or later on centos7.8</li>
</ol>
<p>If you are installing Ceph with version of pacific on CentOS 7.8, you may see the following issue.</p>
<pre><code>$ cat /etc/*release
CentOS Linux release 7.8.2003 (Core)
NAME=&quot;CentOS Linux&quot;

$ uname -r
5.7.12-1.el7.elrepo.x86_64

# ./cephadm add-repo --release pacific
ERROR: Ceph does not support pacific or later for this version of this linux distro and therefore cannot add a repo for it
</code></pre>
<p>You can install Ceph with version “octopus” instead.</p>
<pre><code>$ ./cephadm add-repo --release octopus
Writing repo to /etc/yum.repos.d/ceph.repo...
Enabling EPEL...
Completed adding repo
</code></pre>
<p>Note: <strong>cephadm</strong> is new in Ceph release v15.2.0 (Octopus) and does not support older versions of Ceph.</p>
<ol start="2">
<li><p>Invalid GPG Key</p>
<p> $ .&#x2F;cephadm install<br> Installing packages [‘cephadm’]…<br> Non-zero exit code 1 from yum install -y cephadm<br> yum: stdout Loaded plugins: fastestmirror, langpacks, priorities<br> yum: stdout Loading mirror speeds from cached hostfile<br> yum: stdout  * base: pxe.dev.purestorage.com<br> yum: stdout  * centosplus: pxe.dev.purestorage.com<br> yum: stdout  * epel: mirror.lax.genesisadaptive.com<br> yum: stdout  * extras: pxe.dev.purestorage.com<br> yum: stdout  * updates: pxe.dev.purestorage.com<br> yum: stdout 279 packages excluded due to repository priority protections<br> yum: stdout Resolving Dependencies<br> yum: stdout –&gt; Running transaction check<br> yum: stdout —&gt; Package cephadm.noarch 2:15.2.14-0.el7 will be installed<br> yum: stdout –&gt; Finished Dependency Resolution<br> yum: stdout<br> yum: stdout Dependencies Resolved<br> yum: stdout<br> yum: stdout &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<br> yum: stdout  Package        Arch          Version                  Repository          Size<br> yum: stdout &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<br> yum: stdout Installing:<br> yum: stdout  cephadm        noarch        2:15.2.14-0.el7          Ceph-noarch         55 k<br> yum: stdout<br> yum: stdout Transaction Summary<br> yum: stdout &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<br> yum: stdout Install  1 Package<br> yum: stdout<br> yum: stdout Total download size: 55 k<br> yum: stdout Installed size: 223 k<br> yum: stdout Downloading packages:<br> yum: stdout Public key for cephadm-15.2.14-0.el7.noarch.rpm is not installed<br> yum: stdout Retrieving key from <a href="https://download.ceph.com/keys/release.gpg">https://download.ceph.com/keys/release.gpg</a><br> yum: stderr warning: &#x2F;var&#x2F;cache&#x2F;yum&#x2F;x86_64&#x2F;7&#x2F;Ceph-noarch&#x2F;packages&#x2F;cephadm-15.2.14-0.el7.noarch.rpm: Header V4 RSA&#x2F;SHA256 Signature, key ID    460f3994: NOKEY<br> yum: stderr<br> yum: stderr<br> yum: stderr Invalid GPG Key from <a href="https://download.ceph.com/keys/release.gpg">https://download.ceph.com/keys/release.gpg</a>: No key found in given key data<br> Traceback (most recent call last):<br>   File “.&#x2F;cephadm”, line 8432, in <module><br> main()<br>   File “.&#x2F;cephadm”, line 8420, in main<br> r &#x3D; ctx.func(ctx)<br>   File “.&#x2F;cephadm”, line 6384, in command_install<br> pkg.install(ctx.packages)<br>   File “.&#x2F;cephadm”, line 6231, in install<br> call_throws(self.ctx, [self.tool, ‘install’, ‘-y’] + ls)<br>   File “.&#x2F;cephadm”, line 1461, in call_throws<br> raise RuntimeError(‘Failed command: %s’ % ‘ ‘.join(command))<br> RuntimeError: Failed command: yum install -y cephadm</p>
</li>
</ol>
<p>Based on <a href="https://docs.ceph.com/en/mimic/install/get-packages/">Ceph Documentation</a> , execute the following to install the release.asc key.</p>
<pre><code>$ rpm --import &#39;https://download.ceph.com/keys/release.asc&#39;
</code></pre>
<p>Install cephadm package again and it succeeds.</p>
<pre><code>$ ./cephadm install
Installing packages [&#39;cephadm&#39;]...

$ which cephadm
/usr/sbin/cephadm
</code></pre>
<ol start="3">
<li><p>Failed to add host during bootstrap</p>
<p> $ cephadm bootstrap –mon-ip 192.168.1.183<br> Adding host host1…<br> Non-zero exit code 22 from &#x2F;usr&#x2F;bin&#x2F;docker run –rm –ipc&#x3D;host –net&#x3D;host –entrypoint &#x2F;usr&#x2F;bin&#x2F;ceph -e CONTAINER_IMAGE&#x3D;docker.io&#x2F;ceph&#x2F;ceph:v15    -e NODE_NAME&#x3D;host1 -v &#x2F;var&#x2F;log&#x2F;ceph&#x2F;ccc938de-0c30-11ec-8c3f-ac1f6bc8d268:&#x2F;var&#x2F;log&#x2F;ceph:z -v &#x2F;tmp&#x2F;ceph-tmpdqxjp0ly:&#x2F;etc&#x2F;ceph&#x2F;ceph.client.admin.   keyring:z -v &#x2F;tmp&#x2F;ceph-tmpmt5hrjo9:&#x2F;etc&#x2F;ceph&#x2F;ceph.conf:z docker.io&#x2F;ceph&#x2F;ceph:v15 orch host add host1<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr Error EINVAL: Failed to connect to host1 (host1).<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr Please make sure that the host is reachable and accepts connections using the cephadm SSH key<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr To add the cephadm SSH key to the host:<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr &gt; ceph cephadm get-pub-key &gt; ~&#x2F;ceph.pub<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr &gt; ssh-copy-id -f -i ~&#x2F;ceph.pub root@host1<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr To check that the host is reachable:<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr &gt; ceph cephadm get-ssh-config &gt; ssh_config<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr &gt; ceph config-key get mgr&#x2F;cephadm&#x2F;ssh_identity_key &gt; ~&#x2F;cephadm_private_key<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr &gt; chmod 0600 ~&#x2F;cephadm_private_key<br> &#x2F;usr&#x2F;bin&#x2F;ceph: stderr &gt; ssh -F ssh_config -i ~&#x2F;cephadm_private_key root@host1<br> ERROR: Failed to add host <host1>: Failed command: &#x2F;usr&#x2F;bin&#x2F;docker run –rm –ipc&#x3D;host –net&#x3D;host –entrypoint &#x2F;usr&#x2F;bin&#x2F;ceph -e    CONTAINER_IMAGE&#x3D;docker.io&#x2F;ceph&#x2F;ceph:v15 -e NODE_NAME&#x3D;host1 -v &#x2F;var&#x2F;log&#x2F;ceph&#x2F;ccc938de-0c30-11ec-8c3f-ac1f6bc8d268:&#x2F;var&#x2F;log&#x2F;ceph:z -v &#x2F;tmp&#x2F;   ceph-tmpdqxjp0ly:&#x2F;etc&#x2F;ceph&#x2F;ceph.client.admin.keyring:z -v &#x2F;tmp&#x2F;ceph-tmpmt5hrjo9:&#x2F;etc&#x2F;ceph&#x2F;ceph.conf:z docker.io&#x2F;ceph&#x2F;ceph:v15 orch host add    host1</p>
</li>
</ol>
<p>Note: If there are multiple networks and interfaces, be sure to choose one that will be accessible by any host accessing the Ceph cluster.</p>
<p>Make sure passwordless ssh is configured on each host.</p>
<ol start="4">
<li><p>Remove ceph cluster</p>
<p> $ cephadm  rm-cluster –fsid ccc938de-0c30-11ec-8c3f-ac1f6bc8d268 –force</p>
</li>
<li><p>ceph-common installation failure</p>
<p> $ cephadm install ceph-common<br> Installing packages [‘ceph-common’]…<br> Non-zero exit code 1 from yum install -y ceph-common<br> yum: stdout Loaded plugins: fastestmirror, langpacks, priorities<br> yum: stdout Loading mirror speeds from cached hostfile<br> yum: stdout  * base: pxe.dev.purestorage.com<br> yum: stdout  * centosplus: pxe.dev.purestorage.com<br> yum: stdout  * epel: mirror.lax.genesisadaptive.com<br> yum: stdout  * extras: pxe.dev.purestorage.com<br> yum: stdout  * updates: pxe.dev.purestorage.com<br> yum: stdout 279 packages excluded due to repository priority protections<br> yum: stdout Resolving Dependencies<br> yum: stdout –&gt; Running transaction check<br> yum: stdout —&gt; Package ceph-common.x86_64 1:10.2.5-4.el7 will be installed<br> yum: stdout –&gt; Processing Dependency: python-rbd &#x3D; 1:10.2.5-4.el7 for package: 1:ceph-common-10.2.5-4.el7.x86_64<br> yum: stdout Package python-rbd is obsoleted by python3-rbd, but obsoleting package does not provide for requirements<br> yum: stdout –&gt; Processing Dependency: python-rados &#x3D; 1:10.2.5-4.el7 for package: 1:ceph-common-10.2.5-4.el7.x86_64<br> yum: stdout Package python-rados is obsoleted by python3-rados, but obsoleting package does not provide for requirements<br> yum: stdout –&gt; Processing Dependency: hdparm for package: 1:ceph-common-10.2.5-4.el7.x86_64<br> yum: stdout –&gt; Processing Dependency: gdisk for package: 1:ceph-common-10.2.5-4.el7.x86_64<br> yum: stdout –&gt; Processing Dependency: libboost_regex-mt.so.1.53.0()(64bit) for package: 1:ceph-common-10.2.5-4.el7.x86_64<br> yum: stdout –&gt; Processing Dependency: libboost_program_options-mt.so.1.53.0()(64bit) for package: 1:ceph-common-10.2.5-4.el7.x86_64<br> yum: stdout –&gt; Running transaction check<br> yum: stdout —&gt; Package boost-program-options.x86_64 0:1.53.0-28.el7 will be installed<br> yum: stdout —&gt; Package boost-regex.x86_64 0:1.53.0-28.el7 will be installed<br> yum: stdout –&gt; Processing Dependency: libicuuc.so.50()(64bit) for package: boost-regex-1.53.0-28.el7.x86_64<br> yum: stdout –&gt; Processing Dependency: libicui18n.so.50()(64bit) for package: boost-regex-1.53.0-28.el7.x86_64<br> yum: stdout –&gt; Processing Dependency: libicudata.so.50()(64bit) for package: boost-regex-1.53.0-28.el7.x86_64<br> yum: stdout —&gt; Package ceph-common.x86_64 1:10.2.5-4.el7 will be installed<br> yum: stdout –&gt; Processing Dependency: python-rbd &#x3D; 1:10.2.5-4.el7 for package: 1:ceph-common-10.2.5-4.el7.x86_64<br> yum: stdout Package python-rbd is obsoleted by python3-rbd, but obsoleting package does not provide for requirements<br> yum: stdout –&gt; Processing Dependency: python-rados &#x3D; 1:10.2.5-4.el7 for package: 1:ceph-common-10.2.5-4.el7.x86_64<br> yum: stdout Package python-rados is obsoleted by python3-rados, but obsoleting package does not provide for requirements<br> yum: stdout —&gt; Package gdisk.x86_64 0:0.8.10-3.el7 will be installed<br> yum: stdout —&gt; Package hdparm.x86_64 0:9.43-5.el7 will be installed<br> yum: stdout –&gt; Running transaction check<br> yum: stdout —&gt; Package ceph-common.x86_64 1:10.2.5-4.el7 will be installed<br> yum: stdout –&gt; Processing Dependency: python-rbd &#x3D; 1:10.2.5-4.el7 for package: 1:ceph-common-10.2.5-4.el7.x86_64<br> yum: stdout Package python-rbd is obsoleted by python3-rbd, but obsoleting package does not provide for requirements<br> yum: stdout –&gt; Processing Dependency: python-rados &#x3D; 1:10.2.5-4.el7 for package: 1:ceph-common-10.2.5-4.el7.x86_64<br> yum: stdout Package python-rados is obsoleted by python3-rados, but obsoleting package does not provide for requirements<br> yum: stdout —&gt; Package libicu.x86_64 0:50.2-4.el7_7 will be installed<br> yum: stdout –&gt; Finished Dependency Resolution<br> yum: stdout  You could try using –skip-broken to work around the problem<br> yum: stdout  You could try running: rpm -Va –nofiles –nodigest<br> yum: stderr Error: Package: 1:ceph-common-10.2.5-4.el7.x86_64 (base)<br> yum: stderr            Requires: python-rbd &#x3D; 1:10.2.5-4.el7<br> yum: stderr            Available: 1:python-rbd-10.2.5-4.el7.x86_64 (base)<br> yum: stderr                python-rbd &#x3D; 1:10.2.5-4.el7<br> yum: stderr            Available: 2:python3-rbd-15.2.14-0.el7.x86_64 (Ceph)<br> yum: stderr                python-rbd &#x3D; 2:15.2.14-0.el7<br> yum: stderr Error: Package: 1:ceph-common-10.2.5-4.el7.x86_64 (base)<br> yum: stderr            Requires: python-rados &#x3D; 1:10.2.5-4.el7<br> yum: stderr            Available: 1:python-rados-10.2.5-4.el7.x86_64 (base)<br> yum: stderr                python-rados &#x3D; 1:10.2.5-4.el7<br> yum: stderr            Available: 2:python3-rados-15.2.14-0.el7.x86_64 (Ceph)<br> yum: stderr                python-rados &#x3D; 2:15.2.14-0.el7<br> Traceback (most recent call last):<br>   File “&#x2F;usr&#x2F;sbin&#x2F;cephadm”, line 6242, in <module><br> r &#x3D; args.func()<br>   File “&#x2F;usr&#x2F;sbin&#x2F;cephadm”, line 5073, in command_install<br> pkg.install(args.packages)<br>   File “&#x2F;usr&#x2F;sbin&#x2F;cephadm”, line 4931, in install<br> call_throws([self.tool, ‘install’, ‘-y’] + ls)<br>   File “&#x2F;usr&#x2F;sbin&#x2F;cephadm”, line 1112, in call_throws<br> raise RuntimeError(‘Failed command: %s’ % ‘ ‘.join(command))<br> RuntimeError: Failed command: yum install -y ceph-common</p>
</li>
<li><p>cephadm log</p>
</li>
</ol>
<p>&#x2F;var&#x2F;log&#x2F;ceph&#x2F;cephadm.log</p>
<ol start="7">
<li><p>rbd image map failed</p>
<p> [ceph: root@host1 &#x2F;]# rbd map datapool&#x2F;rbdvol1<br> modinfo: ERROR: Module alias rbd not found.<br> modprobe: FATAL: Module rbd not found in directory &#x2F;lib&#x2F;modules&#x2F;5.7.12-1.el7.elrepo.x86_64<br> rbd: failed to load rbd kernel module (1)<br> rbd: sysfs write failed<br> In some cases useful info is found in syslog - try “dmesg | tail”.<br> rbd: map failed: (2) No such file or directory</p>
<p> [root@host1 ~]# modprobe rbd<br> [root@host1 ~]# lsmod | grep rbd<br> rbd                   106496  0<br> libceph               331776  1 rbd</p>
</li>
<li><p>rbd image map failed on other cluster nodes</p>
<p> [ceph: root@host2 &#x2F;]# rbd map datapool&#x2F;rbdvol5 –id admin<br> 2021-09-21T19:49:49.384+0000 7f91ea781500 -1 auth: unable to find a keyring on &#x2F;etc&#x2F;ceph&#x2F;ceph.client.admin.keyring,&#x2F;etc&#x2F;ceph&#x2F;ceph.keyring,&#x2F;etc&#x2F;ceph&#x2F;keyring,&#x2F;etc&#x2F;ceph&#x2F;keyring.bin,: (2) No such file or directory<br> rbd: sysfs write failed<br> 2021-09-21T19:49:49.387+0000 7f91ea781500 -1 auth: unable to find a keyring on &#x2F;etc&#x2F;ceph&#x2F;ceph.client.admin.keyring,&#x2F;etc&#x2F;ceph&#x2F;ceph.keyring,&#x2F;etc&#x2F;ceph&#x2F;keyring,&#x2F;etc&#x2F;ceph&#x2F;keyring.bin,: (2) No such file or directory<br> 2021-09-21T19:49:49.387+0000 7f91ea781500 -1 AuthRegistry(0x5633b09431e0) no keyring found at &#x2F;etc&#x2F;ceph&#x2F;ceph.client.admin.keyring,&#x2F;etc&#x2F;ceph&#x2F;ceph.keyring,&#x2F;etc&#x2F;ceph&#x2F;keyring,&#x2F;etc&#x2F;ceph&#x2F;keyring.bin,, disabling cephx<br> 2021-09-21T19:49:49.388+0000 7f91ea781500 -1 auth: unable to find a keyring on &#x2F;etc&#x2F;ceph&#x2F;ceph.client.admin.keyring,&#x2F;etc&#x2F;ceph&#x2F;ceph.keyring,&#x2F;etc&#x2F;ceph&#x2F;keyring,&#x2F;etc&#x2F;ceph&#x2F;keyring.bin,: (2) No such file or directory<br> 2021-09-21T19:49:49.388+0000 7f91ea781500 -1 AuthRegistry(0x7fffd357c350) no keyring found at &#x2F;etc&#x2F;ceph&#x2F;ceph.client.admin.keyring,&#x2F;etc&#x2F;ceph&#x2F;ceph.keyring,&#x2F;etc&#x2F;ceph&#x2F;keyring,&#x2F;etc&#x2F;ceph&#x2F;keyring.bin,, disabling cephx<br> 2021-09-21T19:49:49.389+0000 7f91d9b68700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [1]<br> 2021-09-21T19:49:49.389+0000 7f91da369700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [1]<br> 2021-09-21T19:49:49.389+0000 7f91dab6a700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [1]<br> 2021-09-21T19:49:49.389+0000 7f91ea781500 -1 monclient: authenticate NOTE: no keyring found; disabled cephx authentication<br> rbd: couldnot connect to the cluster!<br> In some cases useful info is found in syslog - try “dmesg | tail”.<br> rbd: map failed: (22) Invalid argument</p>
<p> [ceph: root@host2 &#x2F;]# ls &#x2F;etc&#x2F;ceph<br> ceph.conf  rbdmap</p>
</li>
</ol>
<p>Copy the &#x2F;etc&#x2F;ceph&#x2F;ceph.keyring from admin node host1 to host2</p>
<pre><code>[ceph: root@host2 /]# ls /etc/ceph
ceph.conf  ceph.keyring  rbdmap

[ceph: root@host2 /]# rbd map datapool/rbdvol5
[ceph: root@host2 /]# rbd device list
id  pool      namespace  image    snap  device
0   datapool             rbdvol5  -     /dev/rbd0
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.ceph.com/en/latest/releases/">https://docs.ceph.com/en/latest/releases/</a></li>
<li><a href="https://docs.ceph.com/en/mimic/start/intro/">https://docs.ceph.com/en/mimic/start/intro/</a></li>
<li><a href="https://docs.ceph.com/en/latest/cephadm/install/">https://docs.ceph.com/en/latest/cephadm/install/</a></li>
<li><a href="https://docs.ceph.com/en/latest/cephadm/osd/#cephadm-deploy-osds">https://docs.ceph.com/en/latest/cephadm/osd/#cephadm-deploy-osds</a></li>
<li><a href="https://docs.ceph.com/en/latest/rados/operations/pools/">https://docs.ceph.com/en/latest/rados/operations/pools/</a></li>
<li><a href="https://docs.ceph.com/en/latest/rados/configuration/pool-pg-config-ref/">https://docs.ceph.com/en/latest/rados/configuration/pool-pg-config-ref/</a></li>
<li><a href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/">https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/</a></li>
<li><a href="https://sabaini.at/pages/ceph-cheatsheet.html">https://sabaini.at/pages/ceph-cheatsheet.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Deploy CockroachDB in kubernetes cluster</title>
    <url>/blog/deploy-cockroachdb-in-kubernetes-cluster/</url>
    <content><![CDATA[<h2 id="Deploy-Kubernetes-cluster"><a href="#Deploy-Kubernetes-cluster" class="headerlink" title="Deploy Kubernetes cluster"></a>Deploy Kubernetes cluster</h2><pre><code>$ kubectl get nodes
NAME            STATUS   ROLES    AGE    VERSION
node0   Ready    &lt;none&gt;   115d   v1.19.2
node1   Ready    &lt;none&gt;   115d   v1.19.2
node2   Ready    &lt;none&gt;   115d   v1.19.2
node3   Ready    master   115d   v1.19.2
</code></pre>
<h2 id="Start-CockroachDB-cluster"><a href="#Start-CockroachDB-cluster" class="headerlink" title="Start CockroachDB cluster"></a>Start CockroachDB cluster</h2><p>Start the CockroachDB nodes with a configuration file that has been customized for performance:</p>
<pre><code>$ curl -O https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/performance/cockroachdb-statefulset-insecure.yaml

$ kubectl create -f cockroachdb-statefulset-insecure.yaml
service/cockroachdb-public created
service/cockroachdb created
poddisruptionbudget.policy/cockroachdb-budget created
statefulset.apps/cockroachdb created
</code></pre>
<p>Confirm that three pods are Running successfully. Note that they will not be considered Ready until after the cluster has been initialized:</p>
<pre><code>$ kubectl get pods
NAME            READY     STATUS    RESTARTS   AGE
cockroachdb-0   0/1       Running   0          2m
cockroachdb-1   0/1       Running   0          2m
cockroachdb-2   0/1       Running   0          2m
</code></pre>
<p>Confirm that the persistent volumes and corresponding claims were created successfully for all three pods:</p>
<pre><code>$ kubectl get pvc
NAME                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
datadir-cockroachdb-0   Bound    pvc-ac210538-2a20-4d99-9b5d-c5a03d733b4d   1Ti        RWO            px-repl1       35s
datadir-cockroachdb-1   Bound    pvc-54f24a59-a063-46bb-9645-d4292eb483a3   1Ti        RWO            px-repl1       25s
datadir-cockroachdb-2   Bound    pvc-0ebd7fa7-dc36-4c77-ab9d-003d10680f56   1Ti        RWO            px-repl1       15s

$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                           STORAGECLASS   REASON   AGE
pvc-0ebd7fa7-dc36-4c77-ab9d-003d10680f56   1Ti        RWO            Delete           Bound    default/datadir-cockroachdb-2   px-repl1                17s
pvc-54f24a59-a063-46bb-9645-d4292eb483a3   1Ti        RWO            Delete           Bound    default/datadir-cockroachdb-1   px-repl1                27s
pvc-ac210538-2a20-4d99-9b5d-c5a03d733b4d   1Ti        RWO            Delete           Bound    default/datadir-cockroachdb-0   px-repl1                27s
</code></pre>
<p>Before initialize the CockroachDB cluster, check the network(use ping command) to make sure each host can communicate with each other. Otherwise, you may run into connection failure between cluster nodes.</p>
<p>In my case, I want to run the cluster over the private network which doesn’t have DNS setup. So, I just add the private IP addresses and Kubernetes qualified hostnames and CockroachDB node names in &#x2F;etc&#x2F;hosts file on each host as below.</p>
<pre><code>10.0.0.1 ip-10-10-0-1 cockroachdb-0.cockroachdb
10.0.0.2 ip-10-10-0-2 cockroachdb-1.cockroachdb
10.0.0.3 ip-10-10-0-3 cockroachdb-2.cockroachdb
</code></pre>
<p>Use the provided cluster-init.yaml file to perform a one-time initialization that joins the CockroachDB nodes into a single cluster:</p>
<pre><code>$ kubectl create \
-f https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/cluster-init.yaml
</code></pre>
<p>Confirm that cluster initialization has completed successfully. The job should be considered successful and the Kubernetes pods should soon be considered Ready:</p>
<pre><code>$ kubectl get job cluster-init
NAME           COMPLETIONS   DURATION   AGE
cluster-init   1/1           7s         27s
kubectl get pods
NAME                 READY   STATUS      RESTARTS   AGE
cluster-init-cqf8l   0/1     Completed   0          56s
cockroachdb-0        1/1     Running     0          7m51s
cockroachdb-1        1/1     Running     0          7m51s
cockroachdb-2        1/1     Running     0          7m51s
</code></pre>
<h2 id="Use-the-built-in-SQL-client"><a href="#Use-the-built-in-SQL-client" class="headerlink" title="Use the built-in SQL client"></a>Use the built-in SQL client</h2><pre><code>$ kubectl run cockroachdb -it --image=cockroachdb/cockroach:v22.1.7 --rm --restart=Never -- sql --insecure --host=cockroachdb-public

root@cockroachdb-public:26257/defaultdb&gt; show databases;
  database_name | owner | primary_region | regions | survival_goal
----------------+-------+----------------+---------+----------------
  defaultdb     | root  | NULL           | &#123;&#125;      | NULL
  postgres      | root  | NULL           | &#123;&#125;      | NULL
  system        | node  | NULL           | &#123;&#125;      | NULL
(3 rows)


Time: 5ms total (execution 4ms / network 1ms)
</code></pre>
<h2 id="Run-TPC-C-workload-on-the-CockroachDB-cluster"><a href="#Run-TPC-C-workload-on-the-CockroachDB-cluster" class="headerlink" title="Run TPC-C workload on the CockroachDB cluster"></a>Run TPC-C workload on the CockroachDB cluster</h2><pre><code>$ kubectl run cockroachdb -it --image=cockroachdb/cockroach:v22.1.7 --rm --restart=Never -- bash
[root@cockroachdb cockroach]# cockroach node ls --insecure --host=cockroachdb-public
  id
------
   1
   2
   3
(3 rows)

[root@cockroachdb cockroach]# cockroach workload init tpcc &quot;postgresql://root@cockroachdb-public:26257?sslmode=disable&quot; --warehouses 10 --drop
I220922 19:48:15.543311 1 workload/workloadsql/dataload.go:146  [-] 1  imported warehouse (0s, 10 rows)
I220922 19:48:15.580634 1 workload/workloadsql/dataload.go:146  [-] 2  imported district (0s, 100 rows)
I220922 19:48:20.098986 1 workload/workloadsql/dataload.go:146  [-] 3  imported customer (5s, 300000 rows)
I220922 19:48:21.601978 1 workload/workloadsql/dataload.go:146  [-] 4  imported history (2s, 300000 rows)
I220922 19:48:24.317881 1 workload/workloadsql/dataload.go:146  [-] 5  imported order (3s, 300000 rows)
I220922 19:48:24.540100 1 workload/workloadsql/dataload.go:146  [-] 6  imported new_order (0s, 90000 rows)
I220922 19:48:24.998829 1 workload/workloadsql/dataload.go:146  [-] 7  imported item (0s, 100000 rows)
I220922 19:48:34.737491 1 workload/workloadsql/dataload.go:146  [-] 8  imported stock (10s, 1000000 rows)
I220922 19:48:49.030366 1 workload/workloadsql/dataload.go:146  [-] 9  imported order_line (14s, 3001222 rows)

[root@cockroachdb cockroach]# cockroach sql --insecure --database tpcc --host=cockroachdb-public:26257 -e &#39;show tables&#39;
  schema_name | table_name | type  | owner | estimated_row_count | locality
--------------+------------+-------+-------+---------------------+-----------
  public      | customer   | table | root  |              300000 | NULL
  public      | district   | table | root  |                 100 | NULL
  public      | history    | table | root  |              300000 | NULL
  public      | item       | table | root  |              100000 | NULL
  public      | new_order  | table | root  |               90000 | NULL
  public      | order      | table | root  |              300000 | NULL
  public      | order_line | table | root  |             3001222 | NULL
  public      | stock      | table | root  |             1000000 | NULL
  public      | warehouse  | table | root  |                  10 | NULL
(9 rows)

[root@cockroachdb cockroach]# cockroach workload run tpcc --warehouses=10 --ramp=10s --duration=20s postgres://root@cockroachdb-public:26257?sslmode=disable
I220922 19:52:41.633625 1 workload/cli/run.go:414  [-] 1  creating load generator...
Initializing 20 connections...
Initializing 0 idle connections...
Initializing 100 workers and preparing statements...
I220922 19:52:51.646000 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 10.012371773s)

_elapsed_______tpmC____efc__avg(ms)__p50(ms)__p90(ms)__p95(ms)__p99(ms)_pMax(ms)
   20.0s      153.0 119.0%     38.2     44.0     48.2     48.2     50.3     52.4
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.cockroachlabs.com/docs/v22.1/deploy-cockroachdb-with-kubernetes-insecure">https://www.cockroachlabs.com/docs/v22.1/deploy-cockroachdb-with-kubernetes-insecure</a></li>
<li><a href="https://www.cockroachlabs.com/docs/stable/kubernetes-performance.html">https://www.cockroachlabs.com/docs/stable/kubernetes-performance.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>CockroachDB</tag>
      </tags>
  </entry>
  <entry>
    <title>Deploy kubernetes statefulset with affinity</title>
    <url>/blog/deploy-kubernetes-statefulset-with-affinity/</url>
    <content><![CDATA[<p>Here is an example to demonstrate how to assign statefulset and pod to the target worker node.</p>
<p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">affinity</a> helps define which node will be assigned for the pod by using the node labels.</p>
<pre><code>$ kubectl get nodes -o wide --show-labels

$ kubectl label nodes &lt;node-name&gt; noname.io/fio=true

$ cat statefulset-node-select.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: fiobench
spec:
  serviceName: fiobench
  replicas: 1
  selector:
    matchLabels:
      app: fiobench
  template:
    metadata:
      labels:
        app: fiobench
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: noname.io/fio
                operator: In
                values:
                - &quot;true&quot;
      containers:
      - name: fiobench
        image: noname/perfbench:fio
        imagePullPolicy: Always
        volumeMounts:
        - name: fiobench-data-1
          mountPath: /fiodata1
        - name: fiobench-data-2
          mountPath: /fiodata2
        securityContext:
          privileged: true
      imagePullSecrets:
      - name: regcred
  volumeClaimTemplates:
  - metadata:
      name: fiobench-data-1
    spec:
      storageClassName: fio-repl-node-select
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1400Gi
  - metadata:
      name: fiobench-data-2
    spec:
      storageClassName: fio-repl-node-select
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1400Gi

$ kubectl apply -f statefulset-node-select.yaml
$ kubectl get statefulset
$ kubectl get pod -o wide
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Deploy systemtap on multiple systems</title>
    <url>/blog/deploy-systemtap-on-multiple-systems/</url>
    <content><![CDATA[<p>Normally, SystemTap scripts can only be run on systems where SystemTap is deployed together with the following required kernel packages.</p>
<ul>
<li>kernel-devel-$(uname -r)</li>
<li>kernel-debuginfo-$(uname -r)</li>
<li>kernel-debuginfo-common-$(uname -m)-$(uname -r)</li>
</ul>
<p>In the case that it’s neither feasible nor desired to install the denpendent packages on all the target systems. Using cross-instrumentation can work around. Cross-instrumentation is the process of generating SystemTap instrumentation modules from a SystemTap script on one system to be used on another system. This process offers the following benefits:</p>
<ul>
<li>The kernel information packages for various machines can be installed on a single host machine.</li>
<li>Each target machine only needs one package to be installed to use the generated SystemTap instrumentation module: systemtap-runtime.</li>
</ul>
<p>To build the instrumentation module, do the following on the host which has the dependent kernel packages installed:</p>
<pre><code>[root@host1 ~]# uname -r
5.18.10-1.el7.elrepo.x86_64

[root@host1 ~]# rpm -qa | egrep &quot;kernel-debug|kernel-devel|systemtap&quot;
systemtap-client-4.0-13.el7.x86_64
systemtap-runtime-4.0-13.el7.x86_64
kernel-devel-3.10.0-1160.el7.x86_64
systemtap-devel-4.0-13.el7.x86_64
kernel-debuginfo-common-x86_64-3.10.0-1160.el7.x86_64
kernel-debuginfo-3.10.0-1160.el7.x86_64
systemtap-4.0-13.el7.x86_64

[root@host1 ~]# stap -r 3.10.0-1160.el7.x86_64 -e &#39;probe vfs.read &#123;printf(&quot;read performed\n&quot;);&#125; probe timer.s(5)&#123;exit()&#125;&#39; -m ported_stap -p4
ported_stap.ko

[root@host1 ~]# file ported_stap.ko
ported_stap.ko: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), BuildID[sha1]=1ad58adccafc9bf629f1edcbc285e9090e84dc35, not stripped

[root@host1 ~]# ls -la ported_stap.ko
-rw-r--r-- 1 root root 103608 Jul 20 22:33 ported_stap.ko
</code></pre>
<p>Once the instrumentation module is compiled, copy it to the target system and then run it using:</p>
<pre><code>[root@host1 ~]# scp ported_stap.ko host2:/root

[root@host2 ~]# uname -r
3.10.0-1160.el7.x86_64

[root@host2 ~]# rpm -qa | egrep &quot;kernel-debug|kernel-devel|systemtap&quot;
systemtap-runtime-4.0-13.el7.x86_64

[root@host2 ~]# staprun ported_stap.ko
read performed
read performed
read performed
&lt;...&gt;
</code></pre>
<p>When users run a SystemTap script, a kernel module is built out of that script. SystemTap then loads the module into the kernel, allowing it to extract the specified data directly from the kernel. From the following command output, it’s clear that the SystemTap kernel module is dynamically loaded during the script run.</p>
<pre><code>[root@host2 ~]# lsmod | grep ported_stap
ported_stap           139049  2
[root@host2 ~]# lsmod | grep ported_stap
[root@host2 ~]#
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/systemtap_beginners_guide/index">SystemTap beginners guide</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>SystemTap</tag>
      </tags>
  </entry>
  <entry>
    <title>Difference between soft links and hard links</title>
    <url>/blog/difference-between-soft-links-and-hard-links/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>A hard link acts as a copy (mirrored) of the selected file. It accesses the data available in the original file. If the earlier selected file is deleted, the hard link to the file will still contain the data of that file.</p>
<p>A soft link (aka Symbolic link) acts as a pointer or a reference to the file name. It does not access the data available in the original file. If the earlier file is deleted, the soft link will be pointing to a file that does not exist anymore.</p>
<span id="more"></span>

<h2 id="Inode-number"><a href="#Inode-number" class="headerlink" title="Inode number"></a>Inode number</h2><p>Create a test file:</p>
<pre><code>$ touch testfile
$ echo helloworld &gt; testfile 
$ cat testfile 
helloworld
</code></pre>
<p>Create a hard link:</p>
<pre><code>$ ln testfile testfile1
</code></pre>
<p>Create a soft link:</p>
<pre><code>$ ln -s testfile testfile2
</code></pre>
<p>Hard link file(testfile1) takes the same inode number.</p>
<p>Soft link file(testfile2) takes a different inode number.</p>
<pre><code>$ ls -li
total 8
658540 -rw-rw-r-- 2 vboxuser vboxuser 11 May 21 20:53 testfile
658540 -rw-rw-r-- 2 vboxuser vboxuser 11 May 21 20:53 testfile1
658543 lrwxrwxrwx 1 vboxuser vboxuser  8 May 21 20:53 testfile2 -&gt; testfile
</code></pre>
<h2 id="Directory"><a href="#Directory" class="headerlink" title="Directory"></a>Directory</h2><p>Hard link is not allowed for directory while soft link can be created for directory.</p>
<pre><code>$ mkdir testdir
$ ln testdir testdir1
ln: testdir: hard link not allowed for directory

$ ln -s testdir testdir1

$ ls -li | grep testdir
658555 drwxrwxr-x 2 vboxuser vboxuser 4096 May 21 20:58 testdir
658963 lrwxrwxrwx 1 vboxuser vboxuser    7 May 21 20:58 testdir1 -&gt; testdir
</code></pre>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>Soft link only points to the file name, it does not retain data of the file.</p>
<pre><code>$ ls -la
-rw-rw-r--  2 vboxuser vboxuser   11 May 21 20:53 testfile
-rw-rw-r--  2 vboxuser vboxuser   11 May 21 20:53 testfile1
lrwxrwxrwx  1 vboxuser vboxuser    8 May 21 20:53 testfile2 -&gt; testfile
</code></pre>
<p>If the original file is removed, the hard link will still work as it accesses the data the original was having access to.</p>
<p>If the original file is removed, the soft link will not work as it doesn’t access the original file’s data.</p>
<pre><code>$ rm -f testfile
$ cat testfile1
helloworld
$ cat testfile2
cat: testfile2: No such file or directory
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Distributed system concepts you should know</title>
    <url>/blog/distributed-system-concepts-you-should-know/</url>
    <content><![CDATA[<p>This post contains basic explanations for concepts you should know related to distributed system.</p>
<h2 id="API-and-REST-API"><a href="#API-and-REST-API" class="headerlink" title="API and REST API"></a>API and REST API</h2><p>Application Programming Interface, abbreviated as API, enables connection between computers or computer programs. It is a Software Interface that offers services to other software to enhance the required functionalities.</p>
<p>REST API is an API that follows a set of rules for an application and services to communicate with each other. As it is constrained to REST architecture, REST API is referred to as RESTful API. REST APIs provide a way of accessing web services in a flexible way without massive processing capabilities.</p>
<p><img src="/images/rest-api-model.png" alt="Image"></p>
<p><a href="https://www.rlogical.com/wp-content/uploads/2021/08/rest-api-model.png">Source</a></p>
<p>Below are the underlying rules of REST API:</p>
<ul>
<li>Statelessness - Systems aligning with the REST paradigm are bound to become stateless. For Client-Server communication, stateless constraint enforces servers to remain unaware of the client state and vice-versa. A constraint is applied by using resources instead of commands, and they are nouns of the web that describe any object, document, or thing to store&#x2F;send to other resources.</li>
<li>Cacheable - Cache helps servers to mitigate some constraints of statelessness. It is a critical factor that has improved the performance of modern web applications. Caching not only enhances the performance on the client-side but also scales significant results on the server-side. A well-established cache mechanism would drastically reduce the average response time of your server.</li>
<li>Decoupled - REST is a distributed approach, where client and server applications are decoupled from each other. Irrespective of where the requests are initiated, the only information the client application knows is the Uniform Resource Identifier (URI) of the requested resource. A server application should pass requested data via HTTP but should not try modifying the client application.</li>
<li>Layered - A Layered system makes a REST architecture scalable. With RESTful architecture, Client and Server applications are decoupled, so the calls and responses of REST APIs go through different layers. As REST API is layered, it should be designed such that neither Client nor Server identifies its communication with end applications or an intermediary.</li>
</ul>
<p><a href="https://hevodata.com/learn/api-vs-rest-api/">Source</a></p>
<h2 id="Concurrency"><a href="#Concurrency" class="headerlink" title="Concurrency"></a>Concurrency</h2><p>Concurrency allows different parts of a program to run at the same time without affecting the outcome. For example, two people tried to withdraw $1000 from the same bank account in which only has $1500. Concurrency ensures the two people can’t overdraw the account.</p>
<p>There are three common tactics to ensure concurrency.</p>
<ul>
<li>Locking - A mechanism where a process has the right to update or write data. When a process acquires a lock, other processes can’t update or write.</li>
<li>Atomicity - An atomic action is an action whose intermediate state can’t be seen by other processes or threads.</li>
<li>Transaction - A sequence of atomic operations.</li>
</ul>
<h2 id="Message-Queues"><a href="#Message-Queues" class="headerlink" title="Message Queues"></a>Message Queues</h2><p>Queues are a component of service-based architectures. It accept client messages for delivery to a service, then hold the message until the service requests delivery.Once a queue has accepted a message, it provides a strong guarantee that the message will eventually be read and processed.Messages remain in the queue and available for delivery until the server confirms that it has finished with the message and deletes it.</p>
<h2 id="Microservice-Architecture"><a href="#Microservice-Architecture" class="headerlink" title="Microservice Architecture"></a>Microservice Architecture</h2><p>A microservice architecture – a variant of the service-oriented architecture (SOA) structural style – arranges an application as a collection of loosely-coupled services. In a microservices architecture, services are fine-grained and the protocols are lightweight. The goal is that teams can bring their services to life independent of others. Loose coupling reduces all types of dependencies and the complexities around it, as service developers do not need to care about the users of the service, they do not force their changes onto users of the service. Therefore it allows organizations developing software to grow fast, and big, as well as use off the shelf services easier. Communication requirements are less. But it comes at a cost to maintain the decoupling. Interfaces need to be designed carefully and treated as a public API. Techniques like having multiple interfaces on the same service, or multiple versions of the same service, to not break existing users code.</p>
<p><a href="https://en.wikipedia.org/wiki/Microservices">Source</a></p>
<h2 id="Proxy-vs-Reverse-Proxy"><a href="#Proxy-vs-Reverse-Proxy" class="headerlink" title="Proxy vs. Reverse Proxy"></a>Proxy vs. Reverse Proxy</h2><p>A proxy server, sometimes referred to as a forward proxy, is a server that routes traffic between client(s) and another system, usually external to the network. By doing so, it can regulate traffic according to preset policies, convert and mask client IP addresses, enforce security protocols, and block unknown traffic.</p>
<p>A reverse proxy is a type of proxy server.  Unlike a traditional proxy server, which is used to protect clients, a reverse proxy is used to protect servers. A reverse proxy is a server that accepts a request from a client, forwards the request to another one of many other servers, and returns the results from the server that actually processed the request to the client as if the proxy server had processed the request itself. The client only communicates directly with the reverse proxy server and it does not know that some other server actually processed its request.</p>
<p><a href="https://www.strongdm.com/blog/difference-between-proxy-and-reverse-proxy#:~:text=A%20traditional%20forward%20proxy%20server,traffic%20to%20an%20external%20network.&amp;text=A%20reverse%20proxy%2C%20on%20the,%2C%20users%2C%20and%20application%20servers.">Source</a></p>
<h2 id="Horzontal-vs-Vertical-Scaling"><a href="#Horzontal-vs-Vertical-Scaling" class="headerlink" title="Horzontal vs. Vertical Scaling"></a>Horzontal vs. Vertical Scaling</h2><p>Horizontal scaling (aka scaling out) refers to adding additional nodes or machines to your infrastructure to cope with new demands. If you are hosting an application on a server and find that it no longer has the capacity or capabilities to handle traffic, adding a server may be your solution.</p>
<p>While horizontal scaling refers to adding additional nodes, vertical scaling describes adding more power to your current machines. For instance, if your server requires more processing power, vertical scaling would mean upgrading the CPUs. You can also vertically scale the memory, storage, or network speed.</p>
<p><a href="https://www.cloudzero.com/blog/horizontal-vs-vertical-scaling#:~:text=While%20horizontal%20scaling%20refers%20to,%2C%20storage%2C%20or%20network%20speed.">Source</a></p>
<h2 id="Distributed-Cache"><a href="#Distributed-Cache" class="headerlink" title="Distributed Cache"></a>Distributed Cache</h2><p><strong>Caching</strong> means saving frequently accessed data in-memory, that is in RAM instead of the hard drive. Accessing data from RAM is always faster than accessing it from the hard drive.</p>
<p><img src="/images/caching-in-a-web-app.jpeg" alt="Image"></p>
<p>Caching serves the below-stated purposes in web applications.</p>
<ol>
<li>It reduces application latency by notches. Simply, due to the fact that it has all the frequently accessed data stored in RAM, it doesn’t has to talk to the hard drive when the user requests for the data. This makes the application response times faster.</li>
<li>It intercepts all the user data requests before they go to the database. This averts the database bottleneck issue. The database is hit with comparatively lesser number of requests eventually making the application as a whole performant.</li>
<li>Caching often comes in really handy in bringing the application running costs down.</li>
</ol>
<p>Caching can be leveraged at every layer of the web application architecture be it a database, CDN, DNS etc.</p>
<p>A <strong>distributed cache</strong> is a cache which has its data spread across several nodes in a cluster also across several clusters across several data centres located around the globe.</p>
<p><img src="/images/distributed-cache.jpeg" alt="Image"></p>
<p>Being deployed on multiple nodes helps with the horizontal scalability, instances can be added on the fly as per the demand.</p>
<p>Distributed caching is being primarily used in the industry today, for having the potential to scale on demand &amp; being highly available.</p>
<p>Scalability, High Availability, Fault-tolerance are crucial to the large scale services running online today. Businesses cannot afford to have their services go offline. Think about health services, stock markets, military. They have no scope for going down. They are distributed across multiple nodes with a pretty solid amount of redundancy.</p>
<p>Distributed cache, &amp; not just cache, distributed systems are the preferred choice for cloud computing. Solely due to the ability to scale &amp; being available.</p>
<p>Google Cloud uses Memcache for caching data on its public cloud platform. Redis is used by internet giants for caching, NoSQL datastore &amp; several other use cases.</p>
<p><a href="https://www.scaleyourapp.com/distributed-cache-101-the-only-guide-youll-ever-need/">Source1</a><br><a href="https://en.wikipedia.org/wiki/Distributed_cache">Source2</a><br><a href="https://hazelcast.com/glossary/distributed-cache/">Source3</a></p>
<h2 id="Content-Delivery-Network-CDN"><a href="#Content-Delivery-Network-CDN" class="headerlink" title="Content Delivery Network(CDN)"></a>Content Delivery Network(CDN)</h2><p>A content delivery network (CDN) refers to a geographically distributed group of servers which work together to provide fast delivery of Internet content.</p>
<p>A CDN allows for the quick transfer of assets needed for loading Internet content including HTML pages, javascript files, stylesheets, images, and videos. The popularity of CDN services continues to grow, and today the majority of web traffic is served through CDNs, including traffic from major sites like Facebook, Netflix, and Amazon.</p>
<p><a href="https://www.cloudflare.com/learning/cdn/what-is-a-cdn/">Source</a></p>
<h2 id="Hadoop-Distributed-File-System-HDFS"><a href="#Hadoop-Distributed-File-System-HDFS" class="headerlink" title="Hadoop Distributed File System(HDFS)"></a>Hadoop Distributed File System(HDFS)</h2><p>The Apache™ Hadoop® project develops open-source software for reliable, scalable, distributed computing.</p>
<p>The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.</p>
<p><a href="https://hadoop.apache.org/">Source</a></p>
<h2 id="Map-Reduce"><a href="#Map-Reduce" class="headerlink" title="Map Reduce"></a>Map Reduce</h2><p>MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster.</p>
<p>A MapReduce program is composed of a map procedure, which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The “MapReduce System” (also called “infrastructure” or “framework”) orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance.</p>
<p><a href="https://en.wikipedia.org/wiki/MapReduce">Source</a><br><a href="https://www.talend.com/resources/what-is-mapreduce/">Example</a></p>
<h2 id="Apache-Zookeeper"><a href="#Apache-Zookeeper" class="headerlink" title="Apache Zookeeper"></a>Apache Zookeeper</h2><p>ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them ,which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.</p>
<p>ZooKeeper aims at distilling the essence of these different services into a very simple interface to a centralized coordination service. The service itself is distributed and highly reliable. Consensus, group management, and presence protocols will be implemented by the service so that the applications do not need to implement them on their own. Application specific uses of these will consist of a mixture of specific components of Zoo Keeper and application specific conventions. ZooKeeper Recipes shows how this simple service can be used to build much more powerful abstractions.</p>
<p><a href="https://zookeeper.apache.org/">Source</a></p>
<h2 id="Apache-Kafka"><a href="#Apache-Kafka" class="headerlink" title="Apache Kafka"></a>Apache Kafka</h2><p>Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.</p>
<p><a href="https://kafka.apache.org/">Source</a></p>
<h2 id="Read-write-quorum"><a href="#Read-write-quorum" class="headerlink" title="Read-write quorum"></a>Read-write quorum</h2><p>Read-write quorums define two configurable parameters, R and W. R is the minimum number of nodes that must participate in a read operation, and W the minimum number of nodes that must participate in a write operation.</p>
<p>Read-Write Quorum Systems Made Practical - Michael Whittaker, Aleksey Charapko, Joseph M. Hellerstein, Heidi Howard, Ion Stoica</p>
<p>This <a href="https://code.likeagirl.io/distributed-computing-quorum-consistency-in-replication-96e64f7b5c6">paper</a> reviews some concepts of the quorum systems, and it presents a concrete tool named “Quoracle” that explores the trade-offs of the read-write quorum systems.</p>
<p>Quoracle provides an alternative to the majority quorum systems that are widely adopted in distributed systems. The majority quorum can be defined as</p>
<p>\frac{n}{2} where n&#x3D;number of nodes</p>
<p>In the case of a read-write quorum systems the majority is represented in a similar way:</p>
<p>r &#x3D; w &#x3D; \frac{n}{2} + 1</p>
<p>where r and w are the read and write quorums.</p>
<p><a href="https://mwhittaker.github.io/publications/quoracle.pdf">Source</a></p>
<h2 id="Gossip-protocol"><a href="#Gossip-protocol" class="headerlink" title="Gossip protocol"></a>Gossip protocol</h2><p>Gossip protocol is a communication protocol that allows state sharing in distributed systems. Most modern systems use this peer-to-peer protocol to disseminate information to all the members in a network or cluster.</p>
<p>This protocol is used in a decentralized system that does not have any central node to keep track of all nodes and know if a node is down or not.</p>
<p><strong>Gathering state information by multicasting</strong></p>
<p>So, how does a node know every other node’s current state in a decentralized distributed system?</p>
<p>The simplest way to do this is to have every node maintain heartbeats with every other node. Heartbeat is a periodic message sent to a central monitoring server or other servers in the system to show that it is alive and functioning. When a node goes down, it stops sending out heartbeats, and everyone else finds out immediately. But then O(N^2) messages get sent to every tick (N being the number of nodes), which is an expensive operation in any sizable cluster.</p>
<p><strong>How the protocol works</strong></p>
<p>The Gossip protocol is used to repair the problems caused by multicasting; it is a type of communication where a piece of information or gossip in this scenario, is sent from one or more nodes to a set of other nodes in a network. This is useful when a group of clients in the network require the same data at the same time. But there are many problems that occur during multicasting, if there are many nodes present at the recipient end, latency increases; the average time for a receiver to receive a multicast.</p>
<p>To get this multicast message or gossip across the desired targets in the group, the gossip protocol sends out the gossip periodically to random nodes in the network, once a random node receives the gossip, it is said to be infected due to the gossip. Now the random node that receives the gossip does the same thing as the sender, it sends multiple copies of the gossip to random targets. This process continues until the target nodes get the multicast. This process turns the infected nodes to uninfected nodes after sending the gossip out to random nodes.</p>
<p><a href="https://www.educative.io/edpresso/what-is-gossip-protocol">Source1</a><br><a href="https://www.geeksforgeeks.org/the-gossip-protocol-in-cloud-computing/">Source2</a></p>
<h2 id="Fan-Out"><a href="#Fan-Out" class="headerlink" title="Fan Out"></a>Fan Out</h2><p>Let’s try to understand how fan out apporach works based on the system design of twitter <a href="https://www.geeksforgeeks.org/design-twitter-a-system-design-interview-question/">here</a>.</p>
<p>Fanout simply means spreading the data from a single point. Let’s see how to use it. Whenever a tweet is made by a user (Followee) do a lot of preprocessing and distribute the data into different users (followers) home timelines. In this process, you won’t have to make any database queries. You just need to go to the cache by user_id and access the home timeline data in Redis. So this process will be much faster and easier because it is an in-memory we get the list of tweets. Here is the complete flow of this approach.</p>
<ul>
<li>User X is followed by three people and this user has a cache called user timeline. X Tweeted something.</li>
<li>Through Load Balancer tweet will flow into back-end servers.</li>
<li>Server node will save tweet in DB&#x2F;cache</li>
<li>Server node will fetch all the users that follow User X from the cache.</li>
<li>Server node will inject this tweet into in-memory timelines of his followers (fanout)</li>
<li>All followers of User X will see the tweet of User X in their timeline. It will be refreshed and updated every time a user will visit on his&#x2F;her timeline.</li>
</ul>
<p><img src="/images/Fanout-System-Design-Twitter.png" alt="Image"></p>
<p><a href="https://www.geeksforgeeks.org/design-twitter-a-system-design-interview-question/">Source</a></p>
<h2 id="GUID-and-UUID"><a href="#GUID-and-UUID" class="headerlink" title="GUID and UUID"></a>GUID and UUID</h2><p>GUID (aka UUID) is an acronym for ‘Globally Unique Identifier’ (or ‘Universally Unique Identifier’). It is a 128-bit integer number used to identify resources. The term GUID is generally used by developers working with Microsoft technologies, while UUID is used everywhere else.</p>
<p><a href="https://en.wikipedia.org/wiki/Universally_unique_identifier">Source1</a><br><a href="https://www.guidgenerator.com/">Source2</a></p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
  </entry>
  <entry>
    <title>Docker-based fio benchmarking - Part One</title>
    <url>/blog/docker-based-fio-benchmarking-part-one/</url>
    <content><![CDATA[<h2 id="Why-Docker-based-fio-benchmarking"><a href="#Why-Docker-based-fio-benchmarking" class="headerlink" title="Why Docker-based fio benchmarking"></a>Why Docker-based fio benchmarking</h2><p><a href="https://fio.readthedocs.io/en/latest/fio_doc.html">fio</a> is a flexible I&#x2F;O tester which generates I&#x2F;O and measure I&#x2F;O performance on the target storage system. In the case we want to run the fio workload on the cloud deployments, we can containerize fio. Also we can encapsulate necessary packages in the docker image so that it can be easily deployed to avoid package dependency.</p>
<p>There are ready-to-use fio docker image online if you search with google. In this article, we discuss how to create a docker image which consumes a python script to run fio workload.</p>
<h2 id="Build-docker-image-with-Dockerfile"><a href="#Build-docker-image-with-Dockerfile" class="headerlink" title="Build docker image with Dockerfile"></a>Build docker image with Dockerfile</h2><p>Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Using docker build users can create an automated build that executes several command-line instructions in succession.</p>
<p>The following is Dockerfile we are going to use to build the docker image.</p>
<pre><code>$ cat Dockerfile
FROM python:3-alpine

RUN apk add --no-cache \
    fio==3.28-r1 \
    sudo \
    lsblk \
    util-linux \
    procps

COPY perfbench/perfbench.py /
COPY perfbench/run.sh /

ENTRYPOINT [ &quot;/run.sh&quot; ]
</code></pre>
<p>We use Alpine Linux which is a security-oriented, lightweight Linux distribution based on musl libc and busybox. Since we need python support, we leverge the official <strong>python:3-alpine</strong> image which is based on <a href="https://alpinelinux.org/">Alpine Linux</a>.</p>
<p>We install the latest supported fio-3.28 to the docker image. And we install packages like sudo, lsblk, util-linux and procps which are needed by the python script. We copy the python script and wrapper shell script to the root directory. The run.sh script will be run once the container is started in order to run fio benchmark.</p>
<p>The following is the run.sh script.</p>
<pre><code>$ cat perfbench/run.sh
#!/bin/sh

[ -z &quot;$FIO_DATA_DIR&quot; ] &amp;&amp; echo &quot;FIO_DATA_DIR variable is required.&quot; &amp;&amp; exit 1;
[ -z &quot;$FIO_LOG_DIR&quot; ] &amp;&amp; echo &quot;FIO_LOG_DIR variable is required.&quot; &amp;&amp; exit 1;
[ ! -d &quot;$FIO_DATA_DIR&quot; ] &amp;&amp; echo &quot;The data directory $FIO_DATA_DIR does not exist.&quot; &amp;&amp; exit 1;
[ ! -d &quot;$FIO_LOG_DIR&quot; ] &amp;&amp; echo &quot;The result directory $FIO_LOG_DIR does not exit.&quot; &amp;&amp; exit 1;
echo &quot;Running fio benchmark on directory $FIO_DATA_DIR&quot;
python perfbench.py --dir $FIO_DATA_DIR --logdir $FIO_LOG_DIR
</code></pre>
<p>Now, we can build the docker image with the Dockerfile.</p>
<pre><code>$ docker build -t perfbench .
Sending build context to Docker daemon  19.46kB
Step 1/5 : FROM python:3-alpine
Step 2/5 : RUN apk add --no-cache     fio==3.28-r1     sudo     lsblk     util-linux     procps
Step 3/5 : COPY perfbench/perfbench.py /
Step 4/5 : COPY perfbench/run.sh /
Step 5/5 : ENTRYPOINT /run.sh
Successfully built 9c0957911607
Successfully tagged perfbench:latest

$ docker image list
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
perfbench                            latest              f63e13d57991        32 minutes ago      60MB
python                               3-alpine            c7100ae3ac4d        2 weeks ago         48.7MB
</code></pre>
<h2 id="Run-fio-benchmark-with-docker"><a href="#Run-fio-benchmark-with-docker" class="headerlink" title="Run fio benchmark with docker"></a>Run fio benchmark with docker</h2><p>Use the following command to run fio benchmark. Note that we have defined the fio benchmark logic in the customized python script consumed by the container.</p>
<pre><code>$ docker run --rm --privileged -v /data:/data -e FIO_DATA_DIR=/data -e FIO_LOG_DIR=/data/result perfbench
</code></pre>
<p>Note that the option “–privileged” is to allow the python script in the container to drop cache in this case. The same purpose can also be approached with the following method. Then we can do “echo 3 &gt; drop_caches” in the container to drop cache.</p>
<pre><code>$ docker run --rm -v /proc/sys/vm/drop_caches:/drop_caches -v /data:/data -e FIO_DATA_DIR=/data -e FIO_RESULT_DIR=/data/result perfbench
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a></li>
<li><a href="https://alpinelinux.org/">https://alpinelinux.org/</a></li>
<li><a href="https://hub.docker.com/_/alpine">https://hub.docker.com/_&#x2F;alpine</a></li>
<li><a href="https://pkgs.alpinelinux.org/packages">https://pkgs.alpinelinux.org/packages</a></li>
<li><a href="https://docs.docker.com/engine/reference/commandline/run/">https://docs.docker.com/engine/reference/commandline/run/</a></li>
<li><a href="https://hub.docker.com/_/python">https://hub.docker.com/_&#x2F;python</a></li>
<li><a href="https://stackoverflow.com/questions/51875802/docker-containers-drop-cache-without-root-other-options">https://stackoverflow.com/questions/51875802/docker-containers-drop-cache-without-root-other-options</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Fio</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker-based fio benchmarking - Part Two</title>
    <url>/blog/docker-based-fio-benchmarking-part-two/</url>
    <content><![CDATA[<p>In this article, we will build a docker image which is based on python versioned alpine Linux. Alpine Linux is much smaller than most distribution base images (~5MB), and thus leads to much slimmer images in general.</p>
<p>In the python versioned alpine Linux image, we can add additional packages to support our python script “perfbench.py”. The following is the Dockerfile we will use to build the docker image.</p>
<pre><code>cat Dockerfile
FROM python:3-alpine

RUN apk add --no-cache \
    bash \
    sudo \
    lsblk \
    util-linux \
    procps \
    fio==3.28-r1

COPY perfbench/perfbench.py /
</code></pre>
<p>Now that we have Dockerfile defined, we can build the docker image as below.</p>
<pre><code>$ docker build -t perfbench .

$ docker images
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
perfbench                            latest              fb9441429ea1        21 minutes ago      61.2MB
python                               3-alpine            08d07b62c1c9        2 days ago          48.6MB
</code></pre>
<h2 id="Detached-mode"><a href="#Detached-mode" class="headerlink" title="Detached mode"></a>Detached mode</h2><p>To start a container in detached mode, we can use the -d option.</p>
<pre><code>$ docker run -t -d --privileged --name myperfbench -v /data:/data perfbench

$ docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                         NAMES
3473aa8332cb        perfbench            &quot;python3&quot;                4 seconds ago       Up 3 seconds                                      myperfbench

$ docker exec -it myperfbench bash
bash-5.1# cat /etc/alpine-release
3.15.0
bash-5.1# python --version
Python 3.10.2
</code></pre>
<p>Notes:</p>
<p>1.<br>Adding the “-t” flag prevents the container from exiting when running in the background. It allocates a pseudo-tty. You would see the following issue if it’s not specified.</p>
<pre><code>$ docker run -d --name myperfbench perfbench

$ docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                     PORTS                         NAMES
ed997c56e9e9        perfbench            &quot;python3&quot;                6 seconds ago       Exited (0) 4 seconds ago                                 myperfbench
</code></pre>
<p>2.<br>Using “–privileged” flag to give extended privileges to the container. For example, we can drop cache inside the container with this privilege.</p>
<h2 id="Foreground-mode"><a href="#Foreground-mode" class="headerlink" title="Foreground mode"></a>Foreground mode</h2><p>For interactive processes (like a shell), you must use -i -t together in order to allocate a tty for the container process.</p>
<pre><code>$ docker run -it --rm --privileged --name myperfbench -v /data:/data perfbench bash

$ docker run --rm --privileged --name myperfbench -v /data:/data perfbench bash -c &quot;python perfbench.py --dir /data --logdir /data/result&quot;
</code></pre>
<p>Note:</p>
<ul>
<li>By default a container’s file system persists even after the container exits. This makes debugging a lot easier (since you can inspect the final state) and you retain all your data by default. But if you are running short-term foreground processes, these container file systems can really pile up. If instead you’d like Docker to automatically clean up the container and remove the file system when the container exits, you can add the –rm flag.</li>
</ul>
<h2 id="Push-docker-image-to-docker-repository"><a href="#Push-docker-image-to-docker-repository" class="headerlink" title="Push docker image to docker repository"></a>Push docker image to docker repository</h2><pre><code>$ docker tag perfbench:latest noname/perfbench:latest
$ docker push noname/perfbench:latest
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://hub.docker.com/_/python">https://hub.docker.com/_&#x2F;python</a></li>
<li><a href="https://docs.docker.com/engine/reference/run/">https://docs.docker.com/engine/reference/run/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Fio</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>dstat - A replacement for vmstat, iostat and ifstat</title>
    <url>/blog/dstat-a-replacement-for-vmstat-iostat-and-ifstat/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>dstat is a versatile tool for generating system resource statistics. It can be a replacement for vmstat, iostat and ifstat. It overcomes some of the limitations and adds some extra features.</p>
<p>Dstat allows you to view all of your system resources instantly, you can eg. compare disk usage in combination with interrupts from your IDE controller, or compare the network bandwidth numbers directly with the disk throughput (in the same interval).</p>
<h2 id="Usage-examples"><a href="#Usage-examples" class="headerlink" title="Usage examples"></a>Usage examples</h2><p>Display statistics of major OS components:</p>
<p><img src="/images/dstat-1.png" alt="Image"></p>
<p>Relate disk-throughput with network-usage (eth0), total CPU-usage and system counters:</p>
<p><img src="/images/dstat-1.png" alt="Image"></p>
<p>Check dstat’s behaviour and the system impact of dstat:</p>
<p><img src="/images/dstat-2.png" alt="Image"></p>
<p>Use the time plugin together with cpu, net, disk, system, load, proc and top_cpu plugins:</p>
<p><img src="/images/dstat-3.png" alt="Image"></p>
<p>Display stats of process using most of the CPU or memory:</p>
<p><img src="/images/dstat-top-cpu-mem.png" alt="Image"></p>
<p>Display stats of process using most of the CPU and memory:</p>
<p><img src="/images/dstat-top-cpu-and-mem.png" alt="Image"></p>
<p>Relate cpu stats with interrupts per device:</p>
<p><img src="/images/dstat-4.png" alt="Image"></p>
<p>Display information that was to be displayed by vmstat tool:</p>
<p><img src="/images/dstat-vmstat.png" alt="Image"></p>
<p>Display the list of all plugins:</p>
<pre><code>root ~$ dstat --list
internal:
    aio, cpu, cpu24, disk, disk24, disk24old, epoch, fs, int, int24, io, ipc, load, lock, mem, net, page, page24, proc, raw, socket, swap, swapold, sys, tcp, time, udp, unix, vm
/usr/share/dstat:
    battery, battery-remain, cpufreq, dbus, disk-tps, disk-util, dstat, dstat-cpu, dstat-ctxt, dstat-mem, fan, freespace, gpfs, gpfs-ops, helloworld, innodb-buffer, innodb-io, innodb-ops, lustre, memcache-hits, mysql-io, mysql-keys, mysql5-cmds, mysql5-conn,
    mysql5-io, mysql5-keys, net-packets, nfs3, nfs3-ops, nfsd3, nfsd3-ops, ntp, postfix, power, proc-count, qmail, rpc, rpcd, sendmail, snooze, squid, test, thermal, top-bio, top-bio-adv, top-childwait, top-cpu, top-cpu-adv, top-cputime, top-cputime-avg, top-int,
    top-io, top-io-adv, top-latency, top-latency-avg, top-mem, top-oom, utmp, vm-memctl, vmk-hba, vmk-int, vmk-nic, vz-cpu, vz-io, vz-ubc, wifi
</code></pre>
<p>Force float values to be printed:</p>
<pre><code>root ~$ dstat --float 2
You did not select any stats, using -cdngy by default.
----total-cpu-usage---- -dsk/total- -net/total- ---paging-- ---system--
usr sys idl wai hiq siq| read  writ| recv  send|  in   out | int   csw
4.5 1.8  94 0.1   0 0.1| 251B 76.1k|   0     0 |   0     0 |4619  8021
2.3 1.1  96 0.1   0 0.1|   0  54.0k|15.9k 7215B|   0     0 |3497  6381
5.3 2.4  92 0.1   0 0.1|   0   112k|18.1k 17.4k|   0     0 |5323  9184
9.4 2.3  88 0.1   0 0.3|   0   116k|24.5k  193k|   0     0 |5370  9091
2.3 1.3  96 0.1   0   0|   0  82.0k|18.0k 10.6k|   0     0 |3448  6127 ^C
</code></pre>
<p>Display the time, cpu, mem, network, disk, system load, process stats with 2 seconds delay between 5 updates, and save output to a csv file:</p>
<p><img src="/images/dstat-output-csv.png" alt="Image"></p>
<p>Learn more about how the dstat is implemented:</p>
<p>Check out the source code <a href="https://github.com/dstat-real/dstat">here</a>. Credit to <em>Dag Wieers</em>.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://linux.die.net/man/1/dstat">https://linux.die.net/man/1/dstat</a></li>
<li><a href="https://www.geeksforgeeks.org/dstat-command-in-linux-with-examples/">https://www.geeksforgeeks.org/dstat-command-in-linux-with-examples/</a></li>
<li><a href="https://github.com/dstat-real/dstat">https://github.com/dstat-real/dstat</a></li>
<li><a href="https://github.com/scottchiefbaker/dool">https://github.com/scottchiefbaker/dool</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Observability</tag>
      </tags>
  </entry>
  <entry>
    <title>Dynamically tracing with user-defined tracepoint in perf</title>
    <url>/blog/dynamically-tracing-with-user-defined-tracepoint-in-perf/</url>
    <content><![CDATA[<p>In this post, we are going to explore how to use <a href="/blog/perf-the-official-linux-profiler/" title="Perf - The official Linux profiler">Perf - The official Linux profiler</a> for dynamic tracing with user-defined tracepoint. When we say dynamic tracing, the kernel event(function) to be traced is not predefined in perf. Instead, we add the tracepoint event manually.</p>
<p>We will study it based on a real world nfs write performance issue.  With the 4k sequential write from fio, the throughput is much lower with newer kernel(5.x) compared to older kernel(3.x). By profiling the system, it’s clear to see the call stack and samples of nfsd are very different for the two kernel versions.</p>
<p><img src="/images/nfsd_perf_issue.png" alt="Image"></p>
<h2 id="Add-tracepoint-in-perf"><a href="#Add-tracepoint-in-perf" class="headerlink" title="Add tracepoint in perf"></a>Add tracepoint in perf</h2><p>At first look, I want to add a tracepoint for the kernel function “nfsd_vfs_write” since it appears in the main code path of nfsd for both kernel versions. But perf complains the error of “out of .text” as below.</p>
<pre><code>[root@host1 ~]# perf probe --add nfsd_vfs_write 
nfsd_vfs_write is out of .text, skip it.
  Error: Failed to add events. Reason: No such file or directory (Code: -2)
</code></pre>
<p>By checking the exported kernel symbols from &#x2F;proc&#x2F;kallsyms, the symbol type is lowercase “t” for the function “nfsd_vfs_write”.</p>
<pre><code>[root@host1 ~]# cat /proc/kallsyms |  egrep -i -w &quot;nfsd_vfs_write&quot;
ffffffffc094fdd0 t nfsd_vfs_write	[nfsd] 

[root@host1 ~]# perf probe -F | egrep -i &quot;nfsd_vfs_write$&quot;
nfsd_vfs_write
</code></pre>
<p>Based on the manual page of nm, lowercase means it is local symbol. It’s likely that the local symbol can’t be added as probe event.</p>
<blockquote>
<p>The symbol type.</p>
<blockquote>
<p>If lowercase, the symbol is usually local; if uppercase, the symbol is global (external).</p>
<p>“T” “t” The symbol is in the text (code) section.</p>
</blockquote>
</blockquote>
<p>If we still want to trace and understand the overhead for the function “nfsd_vfs_write”, ftrace is a way to go.</p>
<p>In this post, we want to discuss how to add a probe event in perf. So we try a different function “vfs_fsync_range” whose type is global symbol.</p>
<p>With the following commands, the probe for “vfs_fsync_range” is added to perf.</p>
<pre><code>[root@host1 ~]# uname -r
3.10.0-1160.el7.x86_64
[root@host1 ~]# cat /proc/kallsyms | awk &#39;&#123;if($2==&quot;T&quot;)print&#125;&#39; | grep -i vfs_fsync_range
ffffffff960837a0 T vfs_fsync_range

[root@host1 ~]# perf list | grep probe

[root@host1 ~]# perf probe vfs_fsync_range
Added new event:
  probe:vfs_fsync_range (on vfs_fsync_range)

You can now use it in all perf tools, such as:

    perf record -e probe:vfs_fsync_range -aR sleep 1

[root@host1 ~]# perf list | grep probe
  probe:vfs_fsync_range                              [Tracepoint event]
</code></pre>
<h2 id="Trace-the-user-defined-probe-event"><a href="#Trace-the-user-defined-probe-event" class="headerlink" title="Trace the user-defined probe event"></a>Trace the user-defined probe event</h2><p>At this point, we had the tracepoint for the function “vfs_fsync_range” added in perf. We are able to sample the stack trace and check the function runtime from <strong>perf script</strong> output. This can be useful because without the dynamic tracing like this, we can’t identify the overhead(runtime) for the target function. The similar tracing can be done with ftrace. Here we just study how to use perf to dynamically add a tracepoint for sampling in perf. However, based on the experiments, ftrace seems more powerful when to identify the kernel function overhead compared to perf.</p>
<p>The following commands show how to record the stack traces for the target tracepoint and how to extract the call time from it. By comparing the function call time for the differnt kernels, we could identify the possible issue in nfsd call stack.</p>
<pre><code>[root@host1 ~]#  perf record -e probe:vfs_fsync_range -aR sleep 10
[ perf record: Woken up 57 times to write data ]
[ perf record: Captured and wrote 16.284 MB perf.data (233480 samples) ]

[root@host1 ~]# ls -la perf.data
-rw------- 1 root root 17120542 Jul 13 23:50 perf.data

[root@host1 ~]#  perf record -e probe:vfs_fsync_range -g -aR sleep 10
[ perf record: Woken up 146 times to write data ]
[ perf record: Captured and wrote 38.179 MB perf.data (248546 samples) ]

[root@host1 ~]# ls -la perf.data
-rw------- 1 root root 40079506 Jul 13 23:52 perf.data

[root@host1 ~]# perf report --stdio
# To display the perf.data header info, please use --header/--header-only options.
#
#
# Total Lost Samples: 0
#
# Samples: 248K of event &#39;probe:vfs_fsync_range&#39;
# Event count (approx.): 248546
#
# Children      Self  Trace output
# ........  ........  ..................
#
   100.00%   100.00%  (ffffffff960837a0)
            |
            ---ret_from_fork_nospec_end
               kthread
               nfsd
               svc_process
               svc_process_common
               nfsd_dispatch
               nfsd4_proc_compound
               nfsd4_write
               vfs_fsync_range

[root@host1 ~]# perf script
nfsd  3592 [025]   677.464643: probe:vfs_fsync_range: (ffffffff960837a0)
        ffffffff960837a1 vfs_fsync_range+0x1 ([kernel.kallsyms])
        ffffffffc07a0b0f nfsd4_write+0x1cf ([kernel.kallsyms])
        ffffffffc07a267d nfsd4_proc_compound+0x3dd ([kernel.kallsyms])
        ffffffffc078d810 nfsd_dispatch+0xe0 ([kernel.kallsyms])
        ffffffffc0f61850 svc_process_common+0x400 ([kernel.kallsyms])
        ffffffffc0f61d13 svc_process+0xf3 ([kernel.kallsyms])
        ffffffffc078d16f nfsd+0xdf ([kernel.kallsyms])
        ffffffff95ec5c21 kthread+0xd1 ([kernel.kallsyms])
        ffffffff96593df7 ret_from_fork_nospec_end+0x0 ([kernel.kallsyms])
&lt;...&gt;
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Perf</tag>
      </tags>
  </entry>
  <entry>
    <title>Emulate a slow block device with dm-delay</title>
    <url>/blog/emulate-a-slow-block-device-with-dm-delay/</url>
    <content><![CDATA[<p>It might be necessary to emulate a slow block device when you don’t have a slow disk for debugging some high latency performance issue. The device mapper driver in Linux provides a solution called “delay” target.</p>
<span id="more"></span>

<p>In this post, we will learn how to use the “delay” target to delay reads and writes to a block device.</p>
<p>In the following experiment, we add extra latency to the NVME disk in order to emulate slower disk.</p>
<h2 id="NVME-latency"><a href="#NVME-latency" class="headerlink" title="NVME latency"></a>NVME latency</h2><p>Firstly, we run fio read on raw NVME disk to understand the baseline latency.</p>
<pre><code>[root@perf-vm2 ~]# echo 3 &gt; /proc/sys/vm/drop_caches
[root@perf-vm2 ~]# fio --blocksize=4k --ioengine=libaio --readwrite=read --filesize=5G --group_reporting --direct=1 --iodepth=128 --name=job1 --filename=/dev/sdc
job1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=128
fio-3.7
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=944MiB/s,w=0KiB/s][r=242k,w=0 IOPS][eta 00m:00s]
job1: (groupid=0, jobs=1): err= 0: pid=2966: Fri May 19 18:27:48 2023
   read: IOPS=245k, BW=956MiB/s (1002MB/s)(5120MiB/5356msec)
    slat (nsec): min=1465, max=465769, avg=2633.66, stdev=11980.49
    clat (usec): min=125, max=1941, avg=520.06, stdev=107.83
     lat (usec): min=145, max=1947, avg=522.76, stdev=107.17
    clat percentiles (usec):
     |  1.00th=[  306],  5.00th=[  429], 10.00th=[  478], 20.00th=[  486],
     | 30.00th=[  490], 40.00th=[  490], 50.00th=[  498], 60.00th=[  523],
     | 70.00th=[  529], 80.00th=[  529], 90.00th=[  570], 95.00th=[  676],
     | 99.00th=[  881], 99.50th=[ 1434], 99.90th=[ 1565], 99.95th=[ 1663],
     | 99.99th=[ 1811]
   bw (  KiB/s): min=848840, max=1047024, per=100.00%, avg=980152.50, stdev=56118.97, samples=10
   iops        : min=212210, max=261756, avg=245038.10, stdev=14029.75, samples=10
  lat (usec)   : 250=0.01%, 500=52.21%, 750=45.61%, 1000=1.31%
  lat (msec)   : 2=0.86%
  cpu          : usr=11.15%, sys=71.11%, ctx=9203, majf=0, minf=162
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.1%
     issued rwts: total=1310720,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=956MiB/s (1002MB/s), 956MiB/s-956MiB/s (1002MB/s-1002MB/s), io=5120MiB (5369MB), run=5356-5356msec

Disk stats (read/write):
  sdc: ios=1261305/0, merge=0/0, ticks=317744/0, in_queue=317668, util=89.88%
</code></pre>
<p>While fio is running, we issue the command “iostat -ktdx 1 sdc” to monitor the latency. It shows the read latency(r_await time) is ~0.25ms.</p>
<pre><code>[root@perf-vm2 ~]# iostat -ktdx 1 sdc
05/19/2023 06:27:46 PM
Device:         rrqm/s   wrqm/s       r/s     w/s    rkB/s      wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sdc               0.00     0.00 251770.00    0.00 1007080.00     0.00     8.00    64.04    0.25    0.25    0.00   0.00  99.00
</code></pre>
<h2 id="Create-dm-delay-target"><a href="#Create-dm-delay-target" class="headerlink" title="Create dm-delay target"></a>Create dm-delay target</h2><p>Now, we use dmsetup utility to create a delayed target layer on top of the raw NVME. We specify the read delay as 50ms.</p>
<pre><code>[root@perf-vm2 ~]# size=$(blockdev --getsize /dev/sdc)
[root@perf-vm2 ~]# echo $size
209715200

[root@perf-vm2 ~]# echo &quot;0 $size delay /dev/sdc 0 50&quot; | dmsetup create delayed
[root@perf-vm2 ~]# dmsetup table delayed
0 209715200 delay 8:32 0 50
[root@perf-vm2 ~]# ls -la /dev/mapper/ | grep delayed
lrwxrwxrwx  1 root root       7 May 19 18:37 delayed -&gt; ../dm-3
</code></pre>
<h2 id="Check-the-latency-of-dm-delay"><a href="#Check-the-latency-of-dm-delay" class="headerlink" title="Check the latency of dm-delay"></a>Check the latency of dm-delay</h2><p>We can check the latency introduced by dm-delay while fio is running.</p>
<pre><code>[root@perf-vm2 ~]# echo 3 &gt; /proc/sys/vm/drop_caches
[root@perf-vm2 ~]# fio --blocksize=4k --ioengine=libaio --readwrite=read --filesize=5G --group_reporting --direct=1 --iodepth=128 --name=job1 --filename=/dev/dm-3
job1: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=128
fio-3.7
Starting 1 process
^Cbs: 1 (f=1): [R(1)][4.3%][r=10.0MiB/s,w=0KiB/s][r=2562,w=0 IOPS][eta 08m:13s]
fio: terminating on signal 2
Jobs: 1 (f=1): [R(1)][4.5%][r=10.0MiB/s,w=0KiB/s][r=2562,w=0 IOPS][eta 08m:12s]
job1: (groupid=0, jobs=1): err= 0: pid=3138: Fri May 19 18:39:39 2023
   read: IOPS=2559, BW=9.00MiB/s (10.5MB/s)(229MiB/22905msec)
    slat (nsec): min=915, max=132881, avg=3115.23, stdev=3538.89
    clat (usec): min=49420, max=52624, avg=50006.18, stdev=201.19
     lat (usec): min=49424, max=52628, avg=50009.46, stdev=200.86
    clat percentiles (usec):
     |  1.00th=[49546],  5.00th=[49546], 10.00th=[49546], 20.00th=[50070],
     | 30.00th=[50070], 40.00th=[50070], 50.00th=[50070], 60.00th=[50070],
     | 70.00th=[50070], 80.00th=[50070], 90.00th=[50070], 95.00th=[50070],
     | 99.00th=[50594], 99.50th=[51119], 99.90th=[51119], 99.95th=[52691],
     | 99.99th=[52691]
   bw (  KiB/s): min= 9632, max=10520, per=99.81%, avg=10217.24, stdev=117.17, samples=45
   iops        : min= 2408, max= 2630, avg=2554.31, stdev=29.29, samples=45
  lat (msec)   : 50=52.82%, 100=47.18%
  cpu          : usr=0.11%, sys=1.20%, ctx=2973, majf=0, minf=161
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=99.9%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.1%
     issued rwts: total=58624,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
   READ: bw=9.00MiB/s (10.5MB/s), 9.00MiB/s-9.00MiB/s (10.5MB/s-10.5MB/s), io=229MiB (240MB), run=22905-22905msec

Disk stats (read/write):
    dm-3: ios=58624/0, merge=0/0, ticks=2925349/0, in_queue=2930643, util=99.66%, aggrios=58624/0, aggrmerge=0/0, aggrticks=65/0, aggrin_queue=65, aggrutil=0.06%
  sdc: ios=58624/0, merge=0/0, ticks=65/0, in_queue=65, util=0.06%
</code></pre>
<p>We can see that the r_await time is 50ms on dm-3 which is the dm-delay target on top of sdc.</p>
<pre><code>[root@perf-vm2 ~]# iostat -ktdx 1
05/19/2023 06:39:35 PM
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sdc               0.00     0.00 2560.00    0.00 10240.00     0.00     8.00     0.00    0.00    0.00    0.00   0.00   0.00
dm-3              0.00     0.00 2560.00    0.00 10240.00     0.00     8.00   128.00   50.00   50.00    0.00   0.39 100.00
</code></pre>
<p>We also can specify the write latency(e.g. 100ms) as below.</p>
<pre><code>[root@perf-vm2 ~]# echo &quot;0 $size delay /dev/sdc 0 50 /dev/sdc 0 100&quot; | dmsetup create delayed

[root@perf-vm2 ~]# dmsetup table delayed
0 209715200 delay 8:32 0 50 8:32 0 100

[root@perf-vm2 ~]# ls -la /dev/mapper/| grep delayed
lrwxrwxrwx  1 root root       7 May 19 18:41 delayed -&gt; ../dm-3
</code></pre>
<p>We run the fio write against the dm-delay target.</p>
<pre><code>[root@perf-vm2 ~]# echo 3 &gt; /proc/sys/vm/drop_caches
[root@perf-vm2 ~]# fio --blocksize=4k --ioengine=libaio --readwrite=write --filesize=5G --group_reporting --direct=1 --iodepth=128 --name=job1 --filename=/dev/dm-3
job1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=128
fio-3.7
Starting 1 process
^Cbs: 1 (f=1): [W(1)][0.9%][r=0KiB/s,w=5125KiB/s][r=0,w=1281 IOPS][eta 17m:25s]
fio: terminating on signal 2

job1: (groupid=0, jobs=1): err= 0: pid=3210: Fri May 19 18:42:36 2023
  write: IOPS=1259, BW=5039KiB/s (5160kB/s)(53.0MiB/10772msec)
    slat (nsec): min=1771, max=49839k, avg=13004.20, stdev=591607.93
    clat (msec): min=99, max=174, avg=100.68, stdev= 6.72
     lat (msec): min=99, max=174, avg=100.69, stdev= 6.75
    clat percentiles (msec):
     |  1.00th=[  100],  5.00th=[  101], 10.00th=[  101], 20.00th=[  101],
     | 30.00th=[  101], 40.00th=[  101], 50.00th=[  101], 60.00th=[  101],
     | 70.00th=[  101], 80.00th=[  101], 90.00th=[  101], 95.00th=[  101],
     | 99.00th=[  102], 99.50th=[  169], 99.90th=[  174], 99.95th=[  174],
     | 99.99th=[  176]
   bw (  KiB/s): min= 3352, max= 5416, per=99.91%, avg=5033.62, stdev=396.81, samples=21
   iops        : min=  838, max= 1354, avg=1258.38, stdev=99.20, samples=21
  lat (msec)   : 100=52.92%, 250=47.08%
  cpu          : usr=0.29%, sys=0.74%, ctx=1123, majf=0, minf=30
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.2%, &gt;=64=99.5%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.1%
     issued rwts: total=0,13569,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=128

Run status group 0 (all jobs):
  WRITE: bw=5039KiB/s (5160kB/s), 5039KiB/s-5039KiB/s (5160kB/s-5160kB/s), io=53.0MiB (55.6MB), run=10772-10772msec

Disk stats (read/write):
    dm-3: ios=41/13568, merge=0/0, ticks=2118/1344129, in_queue=1351987, util=100.00%, aggrios=42/13569, aggrmerge=0/0, aggrticks=18/25, aggrin_queue=43, aggrutil=0.20%
  sdc: ios=42/13569, merge=0/0, ticks=18/25, in_queue=43, util=0.20%
</code></pre>
<p>The write latency on dm-3 is 100ms as below iostat shows.</p>
<pre><code>[root@perf-vm2 ~]# iostat -ktdx 1
05/19/2023 06:42:33 PM
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sdc               0.00     0.00    0.00 1280.00     0.00  5120.00     8.00     0.00    0.00    0.00    0.00   0.00   0.00
dm-3              0.00     0.00    0.00 1280.00     0.00  5120.00     8.00   128.00  100.00    0.00  100.00   0.78 100.00
</code></pre>
<h2 id="Suspending-I-Os"><a href="#Suspending-I-Os" class="headerlink" title="Suspending I&#x2F;Os"></a>Suspending I&#x2F;Os</h2><p>The device mapper can also suspend and resume I&#x2F;Os.</p>
<pre><code>[root@perf-vm2 ~]# dmsetup suspend /dev/dm-3
[root@perf-vm2 ~]# dmsetup resume /dev/dm-3


[root@perf-vm2 ~]# iostat -ktdx 1
05/19/2023 06:43:55 PM
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sdc               0.00     0.00    0.00 1073.00     0.00  4292.00     8.00     0.07    0.07    0.00    0.07   0.00   0.20
dm-3              0.00     0.00    0.00  945.00     0.00  3780.00     8.00    78.85   94.44    0.00   94.44   0.65  61.70

05/19/2023 06:43:56 PM
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sdc               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
dm-3              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00


05/19/2023 06:44:06 PM
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sdc               0.00     0.00    0.00 1280.00     0.00  5120.00     8.00     0.08    0.06    0.00    0.06   0.02   2.80
dm-3              0.00     0.00    0.00 1280.00     0.00  5120.00     8.00   127.65  100.46    0.00  100.46   0.78 100.00
^C
</code></pre>
<h2 id="Create-a-ramdisk"><a href="#Create-a-ramdisk" class="headerlink" title="Create a ramdisk"></a>Create a ramdisk</h2><p>If you want to emulate a fast disk without using NVME, the ramdisk(aka RAM backed disk) can be used. The size is capped by the physical RAM size.</p>
<pre><code># load the brd kernel module. rd_nr is the maximum number of ramdisks. rd_size is the ramdisk size in KB.
$ sudo modprobe brd rd_nr=1 rd_size=1048576

$ ls -l /dev/ram0
brw-rw---- 1 root disk 1, 0 Aug 24 20:00 /dev/ram0

$ sudo blockdev --getsize /dev/ram0 # Display the size in 512-byte sectors
2097152
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.kernel.org/admin-guide/device-mapper/delay.html">https://docs.kernel.org/admin-guide/device-mapper/delay.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Dm-delay</tag>
      </tags>
  </entry>
  <entry>
    <title>Enable ssh passwordless login for large amount of servers</title>
    <url>/blog/enable-ssh-passwordless-login/</url>
    <content><![CDATA[<h2 id="Install-sshpass-package"><a href="#Install-sshpass-package" class="headerlink" title="Install sshpass package"></a>Install sshpass package</h2><p>To install sshpass package on the source server from which to enable ssh passwordless login to remote servers:</p>
<pre><code>-bash-4.2# rpm -ivh sshpass-1.06-1.el7.x86_64.rpm
</code></pre>
<h2 id="Enable-ssh-passwordless-login-to-remote-servers"><a href="#Enable-ssh-passwordless-login-to-remote-servers" class="headerlink" title="Enable ssh passwordless login to remote servers"></a>Enable ssh passwordless login to remote servers</h2><p>To enable ssh passwordless login to a large amount of remote servers, we can use the following command to automate without needs of providing password for each server.</p>
<pre><code>-bash-4.2# for n in `seq 0 239`
do
    sshpass -p &quot;password&quot; ssh-copy-id -i /root/.ssh/id_rsa.pub -o StrictHostKeyChecking=no Client-hostname$n
done
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Administration</tag>
      </tags>
  </entry>
  <entry>
    <title>Estimate the scale of the system</title>
    <url>/blog/estimate-the-scale-of-the-system/</url>
    <content><![CDATA[<p>It is always a good idea to estimate the scale of the system as it help reflect if the designed system could fulfill the functional requirements. The requirements might include:</p>
<ul>
<li>Number of users</li>
<li>Number of active users(NAU)</li>
<li>Requrests per second(RPS)</li>
<li>Logins per seconds</li>
<li>Transactions per second(TPS) for E-commerce</li>
<li>Likes&#x2F;dislikes per second, shares per second, comments per second for social media sites</li>
<li>Searches per second for sites with a search feature</li>
<li>Storage needed</li>
<li>Servers needed</li>
<li>Network bandwidth needed</li>
</ul>
<p>To estimate hardware resource needed, we need to understand that there are four major resources in a computer system.</p>
<ul>
<li>CPU</li>
<li>Memory</li>
<li>Storage</li>
<li>Network</li>
</ul>
<h2 id="Estimate-servers-needed"><a href="#Estimate-servers-needed" class="headerlink" title="Estimate servers needed"></a>Estimate servers needed</h2><p>The modern computer system is a multi-processor system. It varies from single CPU core to multiple CPU cores. The following is a 32 CPU threads system.</p>
<pre><code>$ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                32
On-line CPU(s) list:   0-31
Thread(s) per core:    2
Core(s) per socket:    8
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 63
Model name:            Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz
</code></pre>
<p>In order to estimate how many servers are needed, we can approach in the following manner.</p>
<ol>
<li>How much work can single CPU do?</li>
<li>How much work can single server do?</li>
<li>How many servers are needed?</li>
</ol>
<p>Let’s have an example to go through this approach.</p>
<p>Let’s say it takes 100ms for a sinlge-core CPU system to handle single client request. It means the system can handle 10 requests per second. So, we can extrapolate a 32-core system can handle 320 requests per second. Let’s say we have to handle 320,000 requests per second(RPS). It means 1000 servers are needed.</p>
<p>Notice that this is a rough calculation only considering CPU needs. In real case, there might be other performance bottleneck to handle 320 requests per second in a system. For example, the system might be already I&#x2F;O bound before running out of CPU bandwidth. But this method still gives us a estimation at the high level.</p>
<h2 id="Estimate-storage-needed"><a href="#Estimate-storage-needed" class="headerlink" title="Estimate storage needed"></a>Estimate storage needed</h2><p>To estimate storage needed, we can approach as below.</p>
<ol>
<li>Identify the different data types</li>
<li>Estimate the space needed for each data type</li>
<li>Get the total space needed</li>
</ol>
<p>Let’s take YouTube as an example to understand this approach.</p>
<ul>
<li>Data types: videos, thumbnail images and comments.</li>
<li>Let’s assume there are roughly 2B users and 5% users(100M users) upload videos consistently. On average, each user has a weekly upload(~50 videos per year). Roughly, 13M videos(100M*50&#x2F;365) are uploaded daily. Let’s assume the video is 10 minutes long on average and it takes 50MB storage space after compression. Let’s say each video has a thumbnail image of 20KB. Each video has about 5 comments and the size of each comment is 200 bytes. In total, the space need for each video is 50MB + 20KB + 1KB, roughly 50MB. By multiplying 13M videos, it needs 619TB storage in a day.</li>
</ul>
<h2 id="Estimate-network-bandwidth-needed"><a href="#Estimate-network-bandwidth-needed" class="headerlink" title="Estimate network bandwidth needed"></a>Estimate network bandwidth needed</h2><p>Determine the incoming and outgoing data for network bandwidth estimation</p>
<ul>
<li>We already know there would be ~619TB data uploaded to YouTube in a day. Dividing this by the number of seconds in a day(619TB&#x2F;86400 seconds), the incoming data to YouTube would be 7.3GB&#x2F;s.</li>
<li>Let’s say 10% of YouTube users are daily active users. With approximately 200M daily users, let’s assume a user watches 10 videos a day. Then YouTube would have 2B views in a day. This would result in ~93PB outgoing data in a day. Dividing this by the number of seconds in a day(93PB&#x2F;86400 seconds), the outgoing speed would be 1128GB&#x2F;s.</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>System Design</tag>
      </tags>
  </entry>
  <entry>
    <title>fio benchmark on multiple devices</title>
    <url>/blog/fio-benchmark-on-multiple-devices/</url>
    <content><![CDATA[<p>In this post, we study how to run fio benchmark on multiple devices. We also try to understand how the iodepth reflects on each device.</p>
<p>We start with single device and the following global parameters are used.</p>
<ul>
<li>blocksize&#x3D;16k</li>
<li>filesize&#x3D;50G (write&#x2F;read 50G data on each device)</li>
<li>iodepth&#x3D;64 (will explain more with the experiment)</li>
<li>end_fsync</li>
<li>group_reporting</li>
</ul>
<h2 id="Write-single-device"><a href="#Write-single-device" class="headerlink" title="Write single device"></a>Write single device</h2><p><strong>Using one job to write single device <em>&#x2F;dev&#x2F;nvme2n1</em></strong>:</p>
<pre><code>$ fio --ioengine=libaio --direct=1 --readwrite=write --blocksize=16k --filesize=50G --end_fsync=1 --iodepth=64 --group_reporting --name=job1 --filename=/dev/nvme2n1
job1: (g=0): rw=write, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=libaio, iodepth=64
fio-3.7
Starting 1 process
Jobs: 1 (f=1): [W(1)][100.0%][r=0KiB/s,w=1906MiB/s][r=0,w=122k IOPS][eta 00m:00s]
job1: (groupid=0, jobs=1): err= 0: pid=32600: Fri Apr 22 22:19:32 2022
  write: IOPS=116k, BW=1820MiB/s (1908MB/s)(50.0GiB/28134msec)
    slat (nsec): min=1362, max=64140, avg=2467.92, stdev=1052.40
    clat (usec): min=4, max=4503, avg=546.60, stdev=554.19
     lat (usec): min=12, max=4505, avg=549.15, stdev=554.20
    clat percentiles (usec):
     |  1.00th=[   13],  5.00th=[   18], 10.00th=[   23], 20.00th=[   34],
     | 30.00th=[   50], 40.00th=[   90], 50.00th=[  474], 60.00th=[  619],
     | 70.00th=[  775], 80.00th=[ 1029], 90.00th=[ 1418], 95.00th=[ 1631],
     | 99.00th=[ 1942], 99.50th=[ 2057], 99.90th=[ 2311], 99.95th=[ 2474],
     | 99.99th=[ 3228]
   bw (  MiB/s): min= 1664, max= 1966, per=100.00%, avg=1821.04, stdev=86.43, samples=56
   iops        : min=106554, max=125860, avg=116546.80, stdev=5531.48, samples=56
  lat (usec)   : 10=0.01%, 20=7.69%, 50=22.50%, 100=10.42%, 250=4.17%
  lat (usec)   : 500=5.92%, 750=17.86%, 1000=10.58%
  lat (msec)   : 2=20.14%, 4=0.71%, 10=0.01%
  cpu          : usr=12.36%, sys=36.52%, ctx=1591388, majf=0, minf=17
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%
     issued rwts: total=0,3276800,0,1 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=1820MiB/s (1908MB/s), 1820MiB/s-1820MiB/s (1908MB/s-1908MB/s), io=50.0GiB (53.7GB), run=28134-28134msec

Disk stats (read/write):
  nvme2n1: ios=88/3276800, merge=0/0, ticks=10/1784551, in_queue=1784561, util=99.65%
</code></pre>
<p>In the iostat, w&#x2F;s(writes per second) on nvme2n1 is 116k which is equal to the fio iops. The avgqu-sz(queue depth) is equal to fio iodepth <em>64</em>.</p>
<pre><code>$ iostat -ktdx 5 | egrep &quot;Device|nvme2n1|nvme3n1&quot;
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 116077.60     0.00 1857241.60    32.00    63.36    0.55    0.00    0.55   0.01 100.00
nvme3n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 116236.40     0.00 1859782.40    32.00    63.48    0.55    0.00    0.55   0.01 100.02
nvme3n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 116508.00     0.00 1864128.00    32.00    63.50    0.55    0.00    0.55   0.01  99.98
nvme3n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 115832.60     0.00 1853321.60    32.00    63.49    0.55    0.00    0.55   0.01 100.02
nvme3n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 117750.00     0.00 1884000.00    32.00    63.48    0.54    0.00    0.54   0.01 100.00
nvme3n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
</code></pre>
<h2 id="Write-two-devices"><a href="#Write-two-devices" class="headerlink" title="Write two devices"></a>Write two devices</h2><p><strong>Using two jobs to write two devices <em>&#x2F;dev&#x2F;nvme2n1</em> and <em>&#x2F;dev&#x2F;nvme3n1</em> separately</strong>:</p>
<pre><code>$ fio --ioengine=libaio --direct=1 --readwrite=write --blocksize=16k --filesize=50G --end_fsync=1 --iodepth=64 --group_reporting --name=job1 --filename=/dev/nvme2n1 --name=job2 --filename=/dev/nvme3n1
job1: (g=0): rw=write, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=libaio, iodepth=64
job2: (g=0): rw=write, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=libaio, iodepth=64
fio-3.7
Starting 2 processes
Jobs: 1 (f=1): [W(1),_(1)][100.0%][r=0KiB/s,w=2904MiB/s][r=0,w=186k IOPS][eta 00m:00s]
job1: (groupid=0, jobs=2): err= 0: pid=32892: Fri Apr 22 22:22:05 2022
  write: IOPS=233k, BW=3648MiB/s (3825MB/s)(100GiB/28072msec)
    slat (nsec): min=1356, max=57113, avg=2474.06, stdev=784.80
    clat (usec): min=6, max=4165, avg=539.57, stdev=563.55
     lat (usec): min=12, max=4167, avg=542.13, stdev=563.57
    clat percentiles (usec):
     |  1.00th=[   13],  5.00th=[   16], 10.00th=[   21], 20.00th=[   32],
     | 30.00th=[   44], 40.00th=[   67], 50.00th=[  490], 60.00th=[  619],
     | 70.00th=[  775], 80.00th=[ 1037], 90.00th=[ 1450], 95.00th=[ 1647],
     | 99.00th=[ 1926], 99.50th=[ 2024], 99.90th=[ 2278], 99.95th=[ 2376],
     | 99.99th=[ 2900]
   bw (  MiB/s): min= 1611, max= 1981, per=50.57%, avg=1844.83, stdev=83.99, samples=109
   iops        : min=103146, max=126830, avg=118069.08, stdev=5375.32, samples=109
  lat (usec)   : 10=0.01%, 20=9.06%, 50=24.62%, 100=11.34%, 250=2.22%
  lat (usec)   : 500=2.86%, 750=18.76%, 1000=10.17%
  lat (msec)   : 2=20.38%, 4=0.59%, 10=0.01%
  cpu          : usr=12.43%, sys=36.55%, ctx=3200368, majf=0, minf=32
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%
     issued rwts: total=0,6553600,0,2 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=3648MiB/s (3825MB/s), 3648MiB/s-3648MiB/s (3825MB/s-3825MB/s), io=100GiB (107GB), run=28072-28072msec

Disk stats (read/write):
  nvme2n1: ios=88/3276800, merge=0/0, ticks=10/1782540, in_queue=1782549, util=99.67%
  nvme3n1: ios=88/3276800, merge=0/0, ticks=13/1745688, in_queue=1745702, util=97.57%
</code></pre>
<p>Note that each job writes one device separately. The iops doubles compared to single device write.</p>
<p>In the iostat, the w&#x2F;s on each device is 116k and the total w&#x2F;s on two devices are ~233k. The avgqu-sz on each device is 64 which is expected and equal to fio iodepth <em>64</em>.</p>
<pre><code>$ iostat -ktdx 5 | egrep &quot;Device|nvme2n1|nvme3n1&quot;
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 116848.20     0.00 1869571.20    32.00    63.56    0.54    0.00    0.54   0.01 100.00
nvme3n1           0.00     0.00    0.00 119530.00     0.00 1912480.00    32.00    63.57    0.53    0.00    0.53   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 116253.00     0.00 1860048.00    32.00    63.57    0.55    0.00    0.55   0.01 100.00
nvme3n1           0.00     0.00    0.00 119619.80     0.00 1913916.80    32.00    63.58    0.53    0.00    0.53   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 116381.20     0.00 1862099.20    32.00    63.56    0.55    0.00    0.55   0.01 100.08
nvme3n1           0.00     0.00    0.00 118331.00     0.00 1893296.00    32.00    63.57    0.54    0.00    0.54   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 116712.20     0.00 1867395.20    32.00    63.57    0.54    0.00    0.54   0.01 100.00
nvme3n1           0.00     0.00    0.00 119082.40     0.00 1905318.40    32.00    63.56    0.53    0.00    0.53   0.01 100.00
</code></pre>
<h2 id="Read-single-device"><a href="#Read-single-device" class="headerlink" title="Read single device"></a>Read single device</h2><p><strong>Using one job to read single device <em>&#x2F;dev&#x2F;nvme2n1</em></strong>:</p>
<pre><code>$ fio --ioengine=libaio --direct=1 --readwrite=read --blocksize=16k --filesize=50G --end_fsync=1 --iodepth=64 --group_reporting --name=job1 --filename=/dev/nvme2n1
job1: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=libaio, iodepth=64
fio-3.7
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=2825MiB/s,w=0KiB/s][r=181k,w=0 IOPS][eta 00m:00s]
job1: (groupid=0, jobs=1): err= 0: pid=33037: Fri Apr 22 22:24:12 2022
   read: IOPS=181k, BW=2820MiB/s (2957MB/s)(50.0GiB/18153msec)
    slat (nsec): min=1274, max=52836, avg=1738.48, stdev=799.29
    clat (usec): min=75, max=2997, avg=352.48, stdev=89.63
     lat (usec): min=76, max=2999, avg=354.28, stdev=89.64
    clat percentiles (usec):
     |  1.00th=[  192],  5.00th=[  229], 10.00th=[  245], 20.00th=[  273],
     | 30.00th=[  302], 40.00th=[  322], 50.00th=[  351], 60.00th=[  371],
     | 70.00th=[  392], 80.00th=[  416], 90.00th=[  461], 95.00th=[  506],
     | 99.00th=[  627], 99.50th=[  676], 99.90th=[  807], 99.95th=[  889],
     | 99.99th=[  988]
   bw (  MiB/s): min= 2781, max= 2826, per=100.00%, avg=2820.55, stdev= 7.65, samples=36
   iops        : min=178016, max=180900, avg=180515.03, stdev=489.56, samples=36
  lat (usec)   : 100=0.01%, 250=11.85%, 500=82.74%, 750=5.23%, 1000=0.17%
  lat (msec)   : 2=0.01%, 4=0.01%
  cpu          : usr=14.06%, sys=44.38%, ctx=2003314, majf=0, minf=271
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%
     issued rwts: total=3276800,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=2820MiB/s (2957MB/s), 2820MiB/s-2820MiB/s (2957MB/s-2957MB/s), io=50.0GiB (53.7GB), run=18153-18153msec

Disk stats (read/write):
  nvme2n1: ios=3274022/0, merge=0/0, ticks=1149877/0, in_queue=1149877, util=99.53%
</code></pre>
<p>In the iostat, r&#x2F;s(reads per second) on nvme2n1 is ~181k which is equal to the fio iops. The avgqu-sz(queue depth) is equal to fio iodepth <em>64</em>.</p>
<pre><code>$ iostat -ktdx 5 | egrep &quot;Device|nvme2n1|nvme3n1&quot;
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 180561.80    0.00 2888988.80     0.00    32.00    63.44    0.35    0.35    0.00   0.01 100.02
nvme3n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 180625.60    0.00 2890009.60     0.00    32.00    63.42    0.35    0.35    0.00   0.01 100.00
nvme3n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 180660.80    0.00 2890572.80     0.00    32.00    63.44    0.35    0.35    0.00   0.01 100.04
nvme3n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
</code></pre>
<h2 id="Read-two-devices"><a href="#Read-two-devices" class="headerlink" title="Read two devices"></a>Read two devices</h2><p><strong>Using two jobs to read two devices <em>&#x2F;dev&#x2F;nvme2n1</em> and <em>&#x2F;dev&#x2F;nvme3n1</em> separately</strong>:</p>
<pre><code>$ fio --ioengine=libaio --direct=1 --readwrite=read --blocksize=16k --filesize=50G --end_fsync=1 --iodepth=64 --group_reporting --name=job1 --filename=/dev/nvme2n1  --name=job2 --filename=/dev/nvme3n1
job1: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=libaio, iodepth=64
job2: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=libaio, iodepth=64
fio-3.7
Starting 2 processes
Jobs: 2 (f=2): [R(2)][100.0%][r=5639MiB/s,w=0KiB/s][r=361k,w=0 IOPS][eta 00m:00s]
job1: (groupid=0, jobs=2): err= 0: pid=33148: Fri Apr 22 22:25:16 2022
   read: IOPS=360k, BW=5628MiB/s (5901MB/s)(100GiB/18195msec)
    slat (nsec): min=1272, max=54671, avg=1803.20, stdev=748.72
    clat (usec): min=70, max=1344, avg=353.14, stdev=87.70
     lat (usec): min=73, max=1352, avg=355.00, stdev=87.70
    clat percentiles (usec):
     |  1.00th=[  186],  5.00th=[  225], 10.00th=[  245], 20.00th=[  277],
     | 30.00th=[  306], 40.00th=[  330], 50.00th=[  351], 60.00th=[  371],
     | 70.00th=[  392], 80.00th=[  416], 90.00th=[  461], 95.00th=[  502],
     | 99.00th=[  611], 99.50th=[  660], 99.90th=[  775], 99.95th=[  873],
     | 99.99th=[  979]
   bw (  MiB/s): min= 2779, max= 2819, per=50.02%, avg=2814.82, stdev= 6.34, samples=72
   iops        : min=177878, max=180456, avg=180148.69, stdev=405.55, samples=72
  lat (usec)   : 100=0.01%, 250=11.87%, 500=83.07%, 750=4.92%, 1000=0.12%
  lat (msec)   : 2=0.01%
  cpu          : usr=14.32%, sys=45.95%, ctx=3778567, majf=0, minf=541
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%
     issued rwts: total=6553600,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=5628MiB/s (5901MB/s), 5628MiB/s-5628MiB/s (5901MB/s-5901MB/s), io=100GiB (107GB), run=18195-18195msec

Disk stats (read/write):
  nvme2n1: ios=3265872/0, merge=0/0, ticks=1149825/0, in_queue=1149825, util=99.51%
  nvme3n1: ios=3267478/0, merge=0/0, ticks=1149712/0, in_queue=1149712, util=99.52%
</code></pre>
<p>Note that each job reads one device separately. The iops doubles compared to single device write.</p>
<p>In the iostat, the r&#x2F;s on each device is 180k and the total w&#x2F;s on two devices are ~360k. The avgqu-sz on each device is 64 which is expected and equal to fio iodepth <em>64</em>.</p>
<pre><code>$ iostat -ktdx 5 | egrep &quot;Device|nvme2n1|nvme3n1&quot;
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 180121.20    0.00 2881939.20     0.00    32.00    63.44    0.35    0.35    0.00   0.01 100.04
nvme3n1           0.00     0.00 180181.60    0.00 2882905.60     0.00    32.00    63.43    0.35    0.35    0.00   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 180242.20    0.00 2883875.20     0.00    32.00    63.44    0.35    0.35    0.00   0.01 100.00
nvme3n1           0.00     0.00 180260.00    0.00 2884160.00     0.00    32.00    63.44    0.35    0.35    0.00   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 180221.40    0.00 2883542.40     0.00    32.00    63.44    0.35    0.35    0.00   0.01 100.00
nvme3n1           0.00     0.00 180278.40    0.00 2884454.40     0.00    32.00    63.43    0.35    0.35    0.00   0.01 100.00
</code></pre>
<h2 id="Incorrect-way-to-write-read-multiple-devices"><a href="#Incorrect-way-to-write-read-multiple-devices" class="headerlink" title="Incorrect way to write&#x2F;read multiple devices"></a>Incorrect way to write&#x2F;read multiple devices</h2><p><strong>Using one job to write two devices:</strong></p>
<pre><code>$ fio --ioengine=libaio --direct=1 --readwrite=write --blocksize=16k --filesize=50G --end_fsync=1 --iodepth=64 --group_reporting --name=job1 --filename=/dev/nvme2n1:/dev/nvme3n1
job1: (g=0): rw=write, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=libaio, iodepth=64
fio-3.7
Starting 1 process
Jobs: 1 (f=2): [W(1)][100.0%][r=0KiB/s,w=3537MiB/s][r=0,w=226k IOPS][eta 00m:00s]
job1: (groupid=0, jobs=1): err= 0: pid=33535: Fri Apr 22 22:59:18 2022
  write: IOPS=210k, BW=3284MiB/s (3444MB/s)(100GiB/31177msec)
    slat (nsec): min=1376, max=52293, avg=2321.83, stdev=1055.59
    clat (usec): min=2, max=3190, avg=301.76, stdev=424.79
     lat (usec): min=12, max=3192, avg=304.15, stdev=424.78
    clat percentiles (usec):
     |  1.00th=[   13],  5.00th=[   16], 10.00th=[   20], 20.00th=[   26],
     | 30.00th=[   32], 40.00th=[   40], 50.00th=[   57], 60.00th=[  112],
     | 70.00th=[  334], 80.00th=[  635], 90.00th=[  979], 95.00th=[ 1254],
     | 99.00th=[ 1663], 99.50th=[ 1811], 99.90th=[ 2073], 99.95th=[ 2147],
     | 99.99th=[ 2343]
   bw (  MiB/s): min= 2935, max= 3785, per=99.93%, avg=3282.15, stdev=221.08, samples=62
   iops        : min=187876, max=242266, avg=210057.40, stdev=14148.71, samples=62
  lat (usec)   : 4=0.01%, 10=0.01%, 20=11.44%, 50=35.69%, 100=11.67%
  lat (usec)   : 250=8.20%, 500=8.42%, 750=8.43%, 1000=6.59%
  lat (msec)   : 2=9.38%, 4=0.17%
  cpu          : usr=19.43%, sys=52.57%, ctx=1390981, majf=0, minf=22
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%
     issued rwts: total=0,6553600,0,2 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=3284MiB/s (3444MB/s), 3284MiB/s-3284MiB/s (3444MB/s-3444MB/s), io=100GiB (107GB), run=31177-31177msec

Disk stats (read/write):
  nvme2n1: ios=59/3276800, merge=0/0, ticks=6/1239109, in_queue=1239116, util=99.68%
  nvme3n1: ios=57/3276800, merge=0/0, ticks=7/649690, in_queue=649696, util=99.70%
</code></pre>
<p>In the iostat, the w&#x2F;s on each device is ~102k. The avgqu-sz on the two devices are different(40 and 22) and the total queue depth is about 64. This is something we don’t expect on the benchmark. We usually expect the queue depth is identical on all the devices under benchmark workload.</p>
<pre><code>$ iostat -ktdx 5 | egrep &quot;Device|nvme2n1|nvme3n1&quot;
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 102307.80     0.00 1636924.80    32.00    39.91    0.39    0.00    0.39   0.01 100.00
nvme3n1           0.00     0.00    0.00 102309.40     0.00 1636950.40    32.00    22.12    0.22    0.00    0.22   0.01 100.04
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 101318.40     0.00 1621094.40    32.00    40.48    0.40    0.00    0.40   0.01 100.00
nvme3n1           0.00     0.00    0.00 101295.80     0.00 1620732.80    32.00    21.48    0.21    0.00    0.21   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 100715.00     0.00 1611440.00    32.00    39.92    0.40    0.00    0.40   0.01 100.00
nvme3n1           0.00     0.00    0.00 100736.60     0.00 1611785.60    32.00    22.21    0.22    0.00    0.22   0.01 100.02
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 101647.20     0.00 1626355.20    32.00    40.23    0.40    0.00    0.40   0.01 100.00
nvme3n1           0.00     0.00    0.00 101632.20     0.00 1626115.20    32.00    21.81    0.21    0.00    0.21   0.01  99.98
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 109138.00     0.00 1746208.00    32.00    43.43    0.40    0.00    0.40   0.01 100.04
nvme3n1           0.00     0.00    0.00 109157.60     0.00 1746521.60    32.00    16.06    0.15    0.00    0.15   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 114420.80     0.00 1830732.80    32.00    36.36    0.32    0.00    0.32   0.01 100.00
nvme3n1           0.00     0.00    0.00 114415.40     0.00 1830646.40    32.00    21.52    0.19    0.00    0.19   0.01 100.00
</code></pre>
<p><strong>Using two jobs to write two devices:</strong></p>
<pre><code>$ fio --ioengine=libaio --direct=1 --readwrite=write --blocksize=16k --filesize=50G --end_fsync=1 --iodepth=64 --numjobs=2 --group_reporting --name=job1 --filename=/dev/nvme2n1:/dev/nvme3n1
job1: (g=0): rw=write, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=libaio, iodepth=64
...
fio-3.7
Starting 2 processes
Jobs: 2 (f=4): [W(2)][100.0%][r=0KiB/s,w=3139MiB/s][r=0,w=201k IOPS][eta 00m:00s]
job1: (groupid=0, jobs=2): err= 0: pid=33670: Fri Apr 22 23:09:45 2022
  write: IOPS=216k, BW=3378MiB/s (3542MB/s)(200GiB/60623msec)
    slat (nsec): min=1361, max=55969, avg=2845.77, stdev=1030.55
    clat (usec): min=5, max=6608, avg=588.58, stdev=938.21
     lat (usec): min=11, max=6610, avg=591.51, stdev=938.21
    clat percentiles (usec):
     |  1.00th=[   13],  5.00th=[   14], 10.00th=[   16], 20.00th=[   21],
     | 30.00th=[   25], 40.00th=[   29], 50.00th=[   34], 60.00th=[   40],
     | 70.00th=[  330], 80.00th=[ 1663], 90.00th=[ 2311], 95.00th=[ 2540],
     | 99.00th=[ 2966], 99.50th=[ 3097], 99.90th=[ 3458], 99.95th=[ 3621],
     | 99.99th=[ 4047]
   bw (  MiB/s): min= 1498, max= 1895, per=50.01%, avg=1689.53, stdev=95.93, samples=242
   iops        : min=95908, max=121316, avg=108129.62, stdev=6139.49, samples=242
  lat (usec)   : 10=0.01%, 20=19.61%, 50=45.92%, 100=2.35%, 250=1.43%
  lat (usec)   : 500=2.03%, 750=1.82%, 1000=1.70%
  lat (msec)   : 2=9.28%, 4=15.82%, 10=0.01%
  cpu          : usr=12.52%, sys=35.52%, ctx=4384091, majf=0, minf=42
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%
     issued rwts: total=0,13107200,0,4 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
  WRITE: bw=3378MiB/s (3542MB/s), 3378MiB/s-3378MiB/s (3542MB/s-3542MB/s), io=200GiB (215GB), run=60623-60623msec

Disk stats (read/write):
  nvme2n1: ios=118/6553600, merge=0/0, ticks=11/5128161, in_queue=5128173, util=99.87%
  nvme3n1: ios=90/6553600, merge=0/0, ticks=10/2544319, in_queue=2544330, util=99.86%
</code></pre>
<p>In this experiment, by setting numjobs&#x3D;2, there are two cloned jobs to run the same workload. Each job writes two devices.</p>
<p>In the iostat, w&#x2F;s on each device is ~105k and the total w&#x2F;s is 210k which is close to fio iops. However, the avgqu-sz on each device is very different(113 vs. 14). The total avgqu-sz is 126 which is close to the fio iodepth of two jobs(2x64&#x3D;128).</p>
<p>Even though the total w&#x2F;s is close to our previous experiment which has two separate jobs write each device, the avgqu-sz is not the same as fio iodepth <em>64</em> on each device.</p>
<p>So, we prefer to use two separate jobs to write different devices when to benchmark multiple devices.</p>
<pre><code>$ iostat -ktdx 5 | egrep &quot;Device|nvme2n1|nvme3n1&quot;
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 104810.00     0.00 1676963.20    32.00   112.91    1.08    0.00    1.08   0.01 100.04
nvme3n1           0.00     0.00    0.00 104813.00     0.00 1677008.00    32.00    13.72    0.13    0.00    0.13   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 103548.00     0.00 1656764.80    32.00   112.87    1.09    0.00    1.09   0.01 100.04
nvme3n1           0.00     0.00    0.00 103550.60     0.00 1656809.60    32.00    13.74    0.13    0.00    0.13   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 104651.80     0.00 1674428.80    32.00   112.75    1.08    0.00    1.08   0.01 100.00
nvme3n1           0.00     0.00    0.00 104644.20     0.00 1674307.20    32.00    13.89    0.13    0.00    0.13   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 105734.20     0.00 1691747.20    32.00   113.22    1.07    0.00    1.07   0.01 100.00
nvme3n1           0.00     0.00    0.00 105744.40     0.00 1691910.40    32.00    13.40    0.13    0.00    0.13   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 105221.00     0.00 1683536.00    32.00   117.70    1.12    0.00    1.12   0.01 100.00
nvme3n1           0.00     0.00    0.00 105215.60     0.00 1683449.60    32.00     8.93    0.08    0.00    0.08   0.01 100.02
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 114836.60     0.00 1837388.80    32.00    82.89    0.72    0.00    0.72   0.01 100.00
nvme3n1           0.00     0.00    0.00 114786.80     0.00 1836588.80    32.00    43.73    0.38    0.00    0.38   0.01  99.98
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 115051.20     0.00 1840816.00    32.00    78.09    0.68    0.00    0.68   0.01 100.04
nvme3n1           0.00     0.00    0.00 115083.60     0.00 1841337.60    32.00    48.55    0.42    0.00    0.42   0.01 100.04
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 114826.60     0.00 1837225.60    32.00    81.10    0.71    0.00    0.71   0.01 100.00
nvme3n1           0.00     0.00    0.00 114844.60     0.00 1837513.60    32.00    45.49    0.40    0.00    0.40   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 111133.00     0.00 1778128.00    32.00    48.09    0.43    0.00    0.43   0.01 100.02
nvme3n1           0.00     0.00    0.00 111080.80     0.00 1777292.80    32.00    78.49    0.71    0.00    0.71   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 106740.00     0.00 1707840.00    32.00    26.91    0.25    0.00    0.25   0.01 100.00
nvme3n1           0.00     0.00    0.00 106743.40     0.00 1707894.40    32.00    99.64    0.93    0.00    0.93   0.01 100.04
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00    0.00 106593.60     0.00 1705497.60    32.00    31.85    0.30    0.00    0.30   0.01 100.00
nvme3n1           0.00     0.00    0.00 106640.80     0.00 1706252.80    32.00    94.76    0.89    0.00    0.89   0.01 100.04
</code></pre>
<p><strong>Using one job to read two devices:</strong></p>
<pre><code>$ fio --ioengine=libaio --direct=1 --readwrite=read --blocksize=16k --filesize=50G --end_fsync=1 --iodepth=64 --group_reporting --name=job1 --filename=/dev/nvme2n1:/dev/nvme3n1
job1: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=libaio, iodepth=64
fio-3.7
Starting 1 process
Jobs: 1 (f=2): [R(1)][100.0%][r=4056MiB/s,w=0KiB/s][r=260k,w=0 IOPS][eta 00m:00s]
job1: (groupid=0, jobs=1): err= 0: pid=33378: Fri Apr 22 22:29:06 2022
   read: IOPS=258k, BW=4035MiB/s (4231MB/s)(100GiB/25375msec)
    slat (nsec): min=1268, max=80634, avg=1910.78, stdev=947.79
    clat (usec): min=51, max=2289, avg=245.54, stdev=92.47
     lat (usec): min=53, max=2291, avg=247.52, stdev=92.47
    clat percentiles (usec):
     |  1.00th=[   85],  5.00th=[  113], 10.00th=[  133], 20.00th=[  151],
     | 30.00th=[  172], 40.00th=[  204], 50.00th=[  239], 60.00th=[  289],
     | 70.00th=[  314], 80.00th=[  338], 90.00th=[  363], 95.00th=[  383],
     | 99.00th=[  441], 99.50th=[  457], 99.90th=[  498], 99.95th=[  529],
     | 99.99th=[  709]
   bw (  MiB/s): min= 3783, max= 4101, per=99.99%, avg=4034.89, stdev=43.41, samples=50
   iops        : min=242150, max=262484, avg=258233.02, stdev=2778.43, samples=50
  lat (usec)   : 100=2.81%, 250=48.78%, 500=48.31%, 750=0.09%, 1000=0.01%
  lat (msec)   : 2=0.01%, 4=0.01%
  cpu          : usr=17.61%, sys=59.69%, ctx=1442924, majf=0, minf=274
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%
     issued rwts: total=6553600,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=4035MiB/s (4231MB/s), 4035MiB/s-4035MiB/s (4231MB/s-4231MB/s), io=100GiB (107GB), run=25375-25375msec

Disk stats (read/write):
  nvme2n1: ios=3245464/0, merge=0/0, ticks=801718/0, in_queue=801719, util=99.67%
  nvme3n1: ios=3245472/0, merge=0/0, ticks=762968/0, in_queue=762969, util=99.67%


$ iostat -ktdx 5 | egrep &quot;Device|nvme2n1|nvme3n1&quot;
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 128705.40    0.00 2059286.40     0.00    32.00    31.86    0.25    0.25    0.00   0.01  99.98
nvme3n1           0.00     0.00 128703.00    0.00 2059248.00     0.00    32.00    30.47    0.24    0.24    0.00   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 129154.60    0.00 2066473.60     0.00    32.00    31.82    0.25    0.25    0.00   0.01 100.02
nvme3n1           0.00     0.00 129157.80    0.00 2066524.80     0.00    32.00    30.53    0.24    0.24    0.00   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 129703.80    0.00 2075260.80     0.00    32.00    31.93    0.25    0.25    0.00   0.01 100.00
nvme3n1           0.00     0.00 129702.40    0.00 2075238.40     0.00    32.00    30.42    0.23    0.23    0.00   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 129521.60    0.00 2072345.60     0.00    32.00    32.04    0.25    0.25    0.00   0.01 100.04
nvme3n1           0.00     0.00 129523.60    0.00 2072377.60     0.00    32.00    30.32    0.23    0.23    0.00   0.01 100.02
</code></pre>
<p><strong>Using two jobs to read two devices:</strong></p>
<pre><code>$ fio --ioengine=libaio --direct=1 --readwrite=read --blocksize=16k --filesize=50G --end_fsync=1 --iodepth=64 --numjobs=2 --group_reporting --name=job1 --filename=/dev/nvme2n1:/dev/nvme3n1
job1: (g=0): rw=read, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=libaio, iodepth=64
...
fio-3.7
Starting 2 processes
Jobs: 2 (f=4): [R(2)][100.0%][r=5606MiB/s,w=0KiB/s][r=359k,w=0 IOPS][eta 00m:00s]
job1: (groupid=0, jobs=2): err= 0: pid=33809: Fri Apr 22 23:18:37 2022
   read: IOPS=358k, BW=5597MiB/s (5869MB/s)(200GiB/36593msec)
    slat (nsec): min=1260, max=52904, avg=1967.07, stdev=877.86
    clat (usec): min=63, max=9900, avg=355.00, stdev=150.12
     lat (usec): min=65, max=9901, avg=357.03, stdev=150.12
    clat percentiles (usec):
     |  1.00th=[  165],  5.00th=[  198], 10.00th=[  219], 20.00th=[  245],
     | 30.00th=[  269], 40.00th=[  297], 50.00th=[  334], 60.00th=[  371],
     | 70.00th=[  412], 80.00th=[  453], 90.00th=[  510], 95.00th=[  562],
     | 99.00th=[  685], 99.50th=[  766], 99.90th=[ 2212], 99.95th=[ 2704],
     | 99.99th=[ 3195]
   bw (  MiB/s): min= 2725, max= 2811, per=50.00%, avg=2798.56, stdev= 8.75, samples=146
   iops        : min=174406, max=179932, avg=179107.78, stdev=559.92, samples=146
  lat (usec)   : 100=0.01%, 250=22.38%, 500=66.31%, 750=10.74%, 1000=0.32%
  lat (msec)   : 2=0.13%, 4=0.12%, 10=0.01%
  cpu          : usr=14.17%, sys=46.00%, ctx=5209795, majf=0, minf=550
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%
     issued rwts: total=13107200,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=5597MiB/s (5869MB/s), 5597MiB/s-5597MiB/s (5869MB/s-5869MB/s), io=200GiB (215GB), run=36593-36593msec

Disk stats (read/write):
  nvme2n1: ios=6516709/0, merge=0/0, ticks=2254035/0, in_queue=2254035, util=99.79%
  nvme3n1: ios=6516726/0, merge=0/0, ticks=2345468/0, in_queue=2345468, util=99.82%


$ iostat -ktdx 5 | egrep &quot;Device|nvme2n1|nvme3n1&quot;
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 179103.00    0.00 2865648.00     0.00    32.00    61.69    0.34    0.34    0.00   0.01 100.00
nvme3n1           0.00     0.00 179101.20    0.00 2865619.20     0.00    32.00    64.77    0.36    0.36    0.00   0.01 100.04
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 179355.00    0.00 2869680.00     0.00    32.00    61.83    0.34    0.34    0.00   0.01 100.08
nvme3n1           0.00     0.00 179356.40    0.00 2869702.40     0.00    32.00    64.64    0.36    0.36    0.00   0.01 100.08
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 179112.80    0.00 2865804.80     0.00    32.00    61.77    0.34    0.34    0.00   0.01 100.00
nvme3n1           0.00     0.00 179112.40    0.00 2865798.40     0.00    32.00    64.69    0.36    0.36    0.00   0.01 100.04
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 179088.00    0.00 2865408.00     0.00    32.00    61.85    0.35    0.35    0.00   0.01 100.02
nvme3n1           0.00     0.00 179087.20    0.00 2865395.20     0.00    32.00    64.61    0.36    0.36    0.00   0.01 100.04
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 179096.20    0.00 2865539.20     0.00    32.00    62.14    0.35    0.35    0.00   0.01  99.98
nvme3n1           0.00     0.00 179095.00    0.00 2865520.00     0.00    32.00    64.33    0.36    0.36    0.00   0.01 100.00
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00     0.00 179127.40    0.00 2866038.40     0.00    32.00    62.21    0.35    0.35    0.00   0.01 100.02
nvme3n1           0.00     0.00 179128.80    0.00 2866060.80     0.00    32.00    64.26    0.36    0.36    0.00   0.01 100.00
</code></pre>
<p>From the open files output from the command <em>ps</em> and <em>lsof</em>, we know that each job opens two devices for read.</p>
<pre><code>$ ps -ef |grep fio
root     33827 30166 63 23:22 pts/0    00:00:07 fio --ioengine=libaio --direct=1 --readwrite=read --blocksize=16k --filesize=50G --end_fsync=1 --iodepth=64 --numjobs=2 --group_reporting --name=job1 --filename=/dev/nvme2n1:/dev/nvme3n1
root     33925 33827 57 23:22 ?        00:00:06 fio --ioengine=libaio --direct=1 --readwrite=read --blocksize=16k --filesize=50G --end_fsync=1 --iodepth=64 --numjobs=2 --group_reporting --name=job1 --filename=/dev/nvme2n1:/dev/nvme3n1
root     33926 33827 57 23:22 ?        00:00:06 fio --ioengine=libaio --direct=1 --readwrite=read --blocksize=16k --filesize=50G --end_fsync=1 --iodepth=64 --numjobs=2 --group_reporting --name=job1 --filename=/dev/nvme2n1:/dev/nvme3n1
root     33945 30233  0 23:22 pts/1    00:00:00 grep --color=auto fio

$ lsof | grep nvme | grep fio
fio       33925                 root    3r      BLK              259,0       0t0      33809 /dev/nvme2n1
fio       33925                 root    4r      BLK             259,11       0t0      33820 /dev/nvme3n1
fio       33925                 root    5r      BLK             259,11       0t0      33820 /dev/nvme3n1
fio       33926                 root    3r      BLK              259,0       0t0      33809 /dev/nvme2n1
fio       33926                 root    4r      BLK              259,0       0t0      33809 /dev/nvme2n1
fio       33926                 root    5r      BLK             259,11       0t0      33820 /dev/nvme3n1
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Fio</tag>
      </tags>
  </entry>
  <entry>
    <title>fio benchmark on multiple files</title>
    <url>/blog/fio-benchmark-on-multiple-files/</url>
    <content><![CDATA[<h2 id="fio-directory-and-filename-options"><a href="#fio-directory-and-filename-options" class="headerlink" title="fio directory and filename options"></a>fio <em>directory</em> and <em>filename</em> options</h2><p>To run fio benchmark on multiple files or deives, we should understand the following fio options.</p>
<ul>
<li>directory&#x3D;str</li>
</ul>
<p>Prefix filenames with this directory. Used to place files in a different location than .&#x2F;. You can specify a number of directories by separating the names with a ‘:’ character. These directories will be assigned equally distributed to job clones created by numjobs as long as they are using generated filenames. If specific filename(s) are set fio will use the first listed directory, and thereby matching the filename semantic (which generates a file for each clone if not specified, but lets all clones use the same file if set).</p>
<ul>
<li>filename&#x3D;str</li>
</ul>
<p>Fio normally makes up a filename based on the job name, thread number, and file number (see filename_format). If you want to share files between threads in a job or several jobs with fixed file paths, specify a filename for each of them to override the default. If the ioengine is file based, you can specify a number of files by separating the names with a ‘:’ colon. So if you wanted a job to open &#x2F;dev&#x2F;sda and &#x2F;dev&#x2F;sdb as the two working files, you would use filename&#x3D;&#x2F;dev&#x2F;sda:&#x2F;dev&#x2F;sdb. This also means that whenever this option is specified, nrfiles is ignored. The size of regular files specified by this option will be size divided by number of files unless an explicit size is specified by filesize.</p>
<h2 id="Run-fio-on-single-directory"><a href="#Run-fio-on-single-directory" class="headerlink" title="Run fio on single directory"></a>Run fio on single directory</h2><p>The following example runs four fio jobs on single directory <em>dir1</em>. Four different files are laid out automatically before the benchmark.</p>
<pre><code>$ fio --name=4kwrite --ioengine=libaio --directory=dir1 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
4kwrite: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=128
...
fio-3.7
Starting 4 processes
4kwrite: Laying out IO file (1 file / 10240MiB)
4kwrite: Laying out IO file (1 file / 10240MiB)
4kwrite: Laying out IO file (1 file / 10240MiB)
4kwrite: Laying out IO file (1 file / 10240MiB)
bs: 4 (f=4): [W(4)][4.5%][r=0KiB/s,w=394MiB/s][r=0,w=101k IOPS][eta 01m:46s]
&lt;...&gt;

$ ps -ef |grep fio | grep -v grep
root     25940 27212 23 21:10 pts/1    00:00:00 fio --name=4kwrite --ioengine=libaio --directory=dir1 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     25976 25940 27 21:10 ?        00:00:01 fio --name=4kwrite --ioengine=libaio --directory=dir1 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     25977 25940 28 21:10 ?        00:00:01 fio --name=4kwrite --ioengine=libaio --directory=dir1 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     25978 25940 28 21:10 ?        00:00:01 fio --name=4kwrite --ioengine=libaio --directory=dir1 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     25979 25940 27 21:10 ?        00:00:01 fio --name=4kwrite --ioengine=libaio --directory=dir1 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting

$ lsof | egrep &quot;dir1&quot;
fio       25976          root    3u      REG              253,2 10737418240   23782491 /dir1/4kwrite.0.0
fio       25977          root    3u      REG              253,2 10737418240   23782492 /dir1/4kwrite.3.0
fio       25978          root    3u      REG              253,2 10737418240   23782495 /dir1/4kwrite.2.0
fio       25979          root    3u      REG              253,2 10737418240    5234528 /dir1/4kwrite.1.0

$ ls -la dir1 | grep write
-rw-r--r-- 1 root root 10737418240 Mar  1 21:11 4kwrite.0.0
-rw-r--r-- 1 root root 10737418240 Mar  1 21:11 4kwrite.1.0
-rw-r--r-- 1 root root 10737418240 Mar  1 21:11 4kwrite.2.0
-rw-r--r-- 1 root root 10737418240 Mar  1 21:11 4kwrite.3.0
</code></pre>
<h2 id="Run-fio-on-multiple-directories"><a href="#Run-fio-on-multiple-directories" class="headerlink" title="Run fio on multiple directories"></a>Run fio on multiple directories</h2><p>The following example runs four fio jobs on two directories <em>dir1</em> and <em>dir2</em>. Two files are laid out automatically under each directory.</p>
<pre><code>$ fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting

$ ps -ef |grep fio | grep write
root     27362 27212  3 21:13 pts/1    00:00:01 fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     27396 27362 29 21:13 ?        00:00:08 fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     27397 27362 30 21:13 ?        00:00:08 fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     27398 27362 31 21:13 ?        00:00:09 fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     27399 27362 30 21:13 ?        00:00:08 fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting

$ lsof | egrep &quot;dir1|dir2&quot;
fio       27396          root    3u      REG              253,2 10737418240   23782491 /dir1/4kwrite.0.0
fio       27397          root    3u      REG              253,2 10737418240  538334779 /dir2/4kwrite.3.0
fio       27398          root    3u      REG              253,2 10737418240   23782492 /dir1/4kwrite.2.0
fio       27399          root    3u      REG              253,2 10737418240  538334780 /dir2/4kwrite.1.0

$ ls -ltr dir*/
dir2/:
total 20971520
-rw-r--r-- 1 root root 10737418240 Mar  1 21:13 4kwrite.3.0
-rw-r--r-- 1 root root 10737418240 Mar  1 21:13 4kwrite.1.0

dir1/:
total 20971520
-rw-r--r-- 1 root root 10737418240 Mar  1 21:13 4kwrite.2.0
-rw-r--r-- 1 root root 10737418240 Mar  1 21:13 4kwrite.0.0
</code></pre>
<p>If the option <em>filename</em> is specified, only the first listed directory will be used to create files.</p>
<pre><code>$ fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting

$ ps -ef |grep fio | grep write
root     29764 27212  8 21:17 pts/1    00:00:00 fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     29798 29764 33 21:17 ?        00:00:04 fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     29799 29764 35 21:17 ?        00:00:04 fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     29800 29764 35 21:17 ?        00:00:04 fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     29801 29764 33 21:17 ?        00:00:03 fio --name=4kwrite --ioengine=libaio --directory=dir1:dir2 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting

$ lsof | egrep &quot;dir1|dir2&quot;
fio       29798          root    3u      REG              253,2 10737418240   23782491 /dir1/testfile
fio       29799          root    3u      REG              253,2 10737418240   23782491 /dir1/testfile
fio       29800          root    3u      REG              253,2 10737418240   23782491 /dir1/testfile
fio       29801          root    3u      REG              253,2 10737418240   23782491 /dir1/testfile

$ ls -la dir*/
dir1/:
total 10485760
drwxr-xr-x 2 root root          22 Mar  1 21:17 .
drwxr-xr-x 7 root root         225 Mar  1 20:28 ..
-rw-r--r-- 1 root root 10737418240 Mar  1 21:18 testfile

dir2/:
total 0
drwxr-xr-x 2 root root   6 Mar  1 21:16 .
drwxr-xr-x 7 root root 225 Mar  1 20:28 ..
</code></pre>
<h2 id="Run-multiple-fio-jobs-on-single-file"><a href="#Run-multiple-fio-jobs-on-single-file" class="headerlink" title="Run multiple fio jobs on single file"></a>Run multiple fio jobs on single file</h2><p>The following example runs four jobs on single file.</p>
<pre><code>$ fio --name=4kwrite --ioengine=libaio --directory=dir1 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting

$ ps -ef |grep fio | grep write
root     28819 27212  9 21:16 pts/1    00:00:00 fio --name=4kwrite --ioengine=libaio --directory=dir1 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     28884 28819 34 21:16 ?        00:00:03 fio --name=4kwrite --ioengine=libaio --directory=dir1 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     28885 28819 33 21:16 ?        00:00:02 fio --name=4kwrite --ioengine=libaio --directory=dir1 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     28886 28819 36 21:16 ?        00:00:03 fio --name=4kwrite --ioengine=libaio --directory=dir1 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting
root     28887 28819 35 21:16 ?        00:00:03 fio --name=4kwrite --ioengine=libaio --directory=dir1 --filename=testfile --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --numjobs=4 --iodepth=128 --direct=1 --group_reporting

$ lsof | egrep &quot;dir1&quot;
fio       28884          root    3u      REG              253,2 10737418240   23782491 /dir1/testfile
fio       28885          root    3u      REG              253,2 10737418240   23782491 /dir1/testfile
fio       28886          root    3u      REG              253,2 10737418240   23782491 /dir1/testfile
fio       28887          root    3u      REG              253,2 10737418240   23782491 /dir1/testfile
</code></pre>
<h2 id="Run-fio-on-multiple-files-from-different-directories"><a href="#Run-fio-on-multiple-files-from-different-directories" class="headerlink" title="Run fio on multiple files from different directories"></a>Run fio on multiple files from different directories</h2><h3 id="One-job-writes-two-files"><a href="#One-job-writes-two-files" class="headerlink" title="One job writes two files"></a>One job writes two files</h3><p>In this example, there is one fio job to write two files from two different directories. The total iodepth on the two files is 128. Note that, the iodepth for each file is about 64 which is only half of the specified iodepth in the fio command.</p>
<pre><code>$ fio --blocksize=4k --filename=/mnt/dir1/testfile:/mnt/dir2/testfile --ioengine=libaio --readwrite=write --size=50G --name=test --numjobs=1 --group_reporting --direct=1 --iodepth=128 --end_fsync=1
test: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=128
fio-3.7
Starting 1 process
&lt;...&gt;

$ lsof | egrep &quot;/mnt/dir&quot;
fio       74145                 root    3u      REG              252,1 53687091200         11 /mnt/dir1/testfile
fio       74145                 root    4u      REG              252,2 53687091200         11 /mnt/dir2/testfile

$ iostat -ktdx 2
Device:                     rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
pxd!pxd90182615933154185     0.00     0.00    0.00 98857.00     0.00 395428.00     8.00    62.86    0.64    0.00    0.64   0.01 100.00
pxd!pxd798820514973607815     0.00     0.00    0.00 98858.00     0.00 395432.00     8.00    62.84    0.64    0.00    0.64   0.01 100.00
</code></pre>
<h3 id="Three-jobs-write-two-files"><a href="#Three-jobs-write-two-files" class="headerlink" title="Three jobs write two files"></a>Three jobs write two files</h3><p>In this example, there are three fio jobs and each job is to write two files. The actual iodepth on each file is ~184(roughly &#x3D; 128&#x2F;2 * 3) which is the accumulated iodepth from three jobs.</p>
<pre><code>$ fio --blocksize=4k --filename=/mnt/dir1/testfile:/mnt/dir2/testfile --ioengine=libaio --readwrite=write --size=50G --name=test --numjobs=3 --group_reporting --direct=1 --iodepth=128 --end_fsync=1
test: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=128
...
fio-3.7
Starting 3 processes
&lt;...&gt;

$ lsof | egrep &quot;/mnt/dir&quot;
fio       85081                 root    3u      REG              252,1 53687091200         11 /mnt/dir1/testfile
fio       85081                 root    4u      REG              252,2 53687091200         11 /mnt/dir2/testfile
fio       85082                 root    3u      REG              252,1 53687091200         11 /mnt/dir1/testfile
fio       85082                 root    4u      REG              252,2 53687091200         11 /mnt/dir2/testfile
fio       85083                 root    3u      REG              252,1 53687091200         11 /mnt/dir1/testfile
fio       85083                 root    4u      REG              252,2 53687091200         11 /mnt/dir2/testfile

$ iostat -ktex 2 
pxd!pxd90182615933154185     0.00     0.50    0.00 99324.50     0.00 397300.00     8.00   184.13    1.85    0.00    1.85   0.01 100.00
pxd!pxd798820514973607815     0.00     0.50    0.00 99324.50     0.00 397300.00     8.00   184.02    1.85    0.00    1.85   0.01 100.00
</code></pre>
<h3 id="Using-dedicated-jobs-writes-each-file"><a href="#Using-dedicated-jobs-writes-each-file" class="headerlink" title="Using dedicated jobs writes each file"></a>Using dedicated jobs writes each file</h3><p>In this example, there are two fio jobs and each job is to write a different file. The actual iodepth on each file is ~128 which is the same as the specified  iodepth in the fio command. This is usually expected pattern in the benchmark.</p>
<pre><code>$ fio --blocksize=4k --ioengine=libaio --readwrite=write --size=50G --direct=1 --iodepth=128 --end_fsync=1 --group_reporting --numjobs=1 --name=job1 --filename=/mnt/dir1/testfile --name=job2 --filename=/mnt/dir2/testfile
job1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=128
job2: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=128
fio-3.7
Starting 2 processes
&lt;...&gt;

$ lsof | egrep &quot;/mnt/dir&quot;
fio       79794                 root    3u      REG              252,1 53687091200         11 /mnt/dir1/testfile
fio       79795                 root    3u      REG              252,2 53687091200         11 /mnt/dir2/testfile

$ iostat -ktdx 2
pxd!pxd90182615933154185     0.00     0.00    0.00 94151.00     0.00 376604.00     8.00   127.01    1.35    0.00    1.35   0.01 100.00
pxd!pxd798820514973607815     0.00     0.00    0.00 94152.50     0.00 376610.00     8.00   127.01    1.35    0.00    1.35   0.01 100.00
</code></pre>
<p>In this example, there are four fio jobs and each file is written by two jobs. The actual iodepth on each file is ~256 which is the twice of the specified  iodepth in the fio command.</p>
<pre><code>$ fio --blocksize=4k --ioengine=libaio --readwrite=write --size=50G --direct=1 --iodepth=128 --end_fsync=1 --group_reporting --numjobs=2 --name=job1 --filename=/mnt/dir1/testfile --name=job2 --filename=/mnt/dir2/testfile
job1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=128
...
job2: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=128
...
fio-3.7
Starting 4 processes
&lt;...&gt;

$ lsof | egrep &quot;/mnt/dir&quot;
fio       81972                 root    3u      REG              252,1 53687091200         11 /mnt/dir1/testfile
fio       81973                 root    3u      REG              252,1 53687091200         11 /mnt/dir1/testfile
fio       81974                 root    3u      REG              252,2 53687091200         11 /mnt/dir2/testfile
fio       81975                 root    3u      REG              252,2 53687091200         11 /mnt/dir2/testfile

$ iostat -ktdx 2
pxd!pxd90182615933154185     0.00     0.50    0.00 93394.50     0.00 373580.00     8.00   254.94    2.73    0.00    2.73   0.01 100.00
pxd!pxd798820514973607815     0.00     0.50    0.00 93408.00     0.00 373634.00     8.00   254.96    2.73    0.00    2.73   0.01 100.00
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://fio.readthedocs.io/en/latest/fio_doc.html">https://fio.readthedocs.io/en/latest/fio_doc.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Fio</tag>
      </tags>
  </entry>
  <entry>
    <title>fio direct I/O error with 1k blocksize</title>
    <url>/blog/fio-direct-i-o-error-with-1k-blocksize/</url>
    <content><![CDATA[<p>When to run fio write with small blocksize(e.g. 1k), the following error is seen.</p>
<pre><code>$ fio --blocksize=1k --ioengine=libaio --readwrite=randwrite --filesize=2G --group_reporting --direct=1 --iodepth=128 --randrepeat=1 --end_fsync=1 --
name=job1 --numjobs=1 --filename=/mnt/fiomnt/fio.dat
job1: (g=0): rw=randwrite, bs=(R) 1024B-1024B, (W) 1024B-1024B, (T) 1024B-1024B, ioengine=libaio, iodepth=128
fio-3.7
Starting 1 process
fio: io_u error on file /mnt/fiomnt/fio.dat: Invalid argument: write offset=129521664, buflen=1024
fio: io_u error on file /mnt/fiomnt/fio.dat: Invalid argument: write offset=1589760000, buflen=1024
fio: pid=93922, err=22/file:io_u.c:1747, func=io_u error, error=Invalid argument
job1: (groupid=0, jobs=1): err=22 (file:io_u.c:1747, func=io_u error, error=Invalid argument): pid=93922: Wed Jan 25 00:42:00 2023
cpu : usr=0.00%, sys=0.00%, ctx=1, majf=0, minf=14
IO depths : 1=0.8%, 2=1.6%, 4=3.1%, 8=6.2%, 16=12.5%, 32=25.0%, &amp;gt;=64=50.8%
submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%
complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%
issued rwts: total=0,128,0,0 short=0,0,0,0 dropped=0,0,0,0
latency : target=0, window=0, percentile=100.00%, depth=128
</code></pre>
<p><strong>Cause:</strong></p>
<p>For direct I&#x2F;O,  the I&#x2F;O size has to be multiple of filesystem&#x2F;block device blocksize. In this case, the filesystem blocksize is 4k which can not be well aligned with the requested I&#x2F;O size(1k). To fix this, the filesystem blocksize should be less than or equal to 1k.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Fio</tag>
      </tags>
  </entry>
  <entry>
    <title>Fio fsync, end_fsync, fdatasync and sync</title>
    <url>/blog/fio-fsync-end_fsync-fdatasync-and-sync/</url>
    <content><![CDATA[<h2 id="Intro-to-fsync-end-fsync-fdatasync-and-sync"><a href="#Intro-to-fsync-end-fsync-fdatasync-and-sync" class="headerlink" title="Intro to fsync, end_fsync, fdatasync and sync"></a>Intro to fsync, end_fsync, fdatasync and sync</h2><ul>
<li>fsync&#x3D;int</li>
</ul>
<p>If writing to a file, issue an fsync(2) (or its equivalent) of the dirty data for every number of blocks given. For example, if you give 32 as a parameter, fio will sync the file after every 32 writes issued. If fio is using non-buffered I&#x2F;O, we may not sync the file. The exception is the sg I&#x2F;O engine, which synchronizes the disk cache anyway. Defaults to 0, which means fio does not periodically issue and wait for a sync to complete. Also see end_fsync and fsync_on_close.</p>
<ul>
<li>end_fsync&#x3D;bool</li>
</ul>
<p>If true, fsync(2) file contents when a write stage has completed. Default: false.</p>
<ul>
<li>fsync_on_close&#x3D;bool</li>
</ul>
<p>If true, fio will fsync(2) a dirty file on close. This differs from end_fsync in that it will happen on every file close, not just at the end of the job. Default: false.</p>
<ul>
<li>fdatasync&#x3D;int</li>
</ul>
<p>Like fsync but uses fdatasync(2) to only sync data and not metadata blocks. In Windows, DragonFlyBSD or OSX there is no fdatasync(2) so this falls back to using fsync(2). Defaults to 0, which means fio does not periodically issue and wait for a data-only sync to complete.</p>
<ul>
<li>sync&#x3D;str</li>
</ul>
<p>Whether, and what type, of synchronous I&#x2F;O to use for writes. The allowed values are:</p>
<ul>
<li>none - Do not use synchronous IO, the default.</li>
<li>0 - Same as none.</li>
<li>sync - Use synchronous file IO. For the majority of I&#x2F;O engines, this means using O_SYNC.</li>
<li>1 - Same as sync.</li>
<li>dsync - Use synchronous data IO. For the majority of I&#x2F;O engines, this means using O_DSYNC.</li>
</ul>
<p><a href="https://fio.readthedocs.io/en/latest/fio_doc.html">Source</a></p>
<h2 id="Create-a-100MB-file-only"><a href="#Create-a-100MB-file-only" class="headerlink" title="Create a 100MB file only"></a>Create a 100MB file only</h2><p>Here we only create a 100MB file which means “lay out IO file” in the fio context. There is no actual I&#x2F;O happening after the file creation even though we specify the I&#x2F;O related options, such as blocksize&#x3D;8k.</p>
<pre><code>$ strace -f -o strace.out fio --name=test --ioengine=libaio --blocksize=8k --readwrite=write --directory=/mnt/bench1 --nrfiles=1 --filesize=100m --fsync=1 --numjobs=1 --direct=1 --group_reporting --create_only=1
test: (g=0): rw=write, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=libaio, iodepth=1
fio-3.7
Starting 1 process
test: Laying out IO file (1 file / 100MiB)

Run status group 0 (all jobs):

Disk stats (read/write):
  nvme0n1: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00%
</code></pre>
<p>From the strace output, the disk space is allocated for the file.</p>
<pre><code>$ cat strace.out
[..]
15801 write(1, &quot;Starting 1 process\n&quot;, 19) = 19
15801 stat(&quot;/mnt/bench1/test.0.0&quot;, 0x7ffc477fcc20) = -1 ENOENT (No such file or directory)
15801 write(1, &quot;test: Laying out IO file (1 file&quot;..., 43)) = 43
15801 unlink(&quot;/mnt/bench1/test.0.0&quot;)    = -1 ENOENT (No such file or directory)
15801 open(&quot;/mnt/bench1/test.0.0&quot;, O_WRONLY|O_CREAT, 0644) = 3
15801 fallocate(3, 0, 0, 104857600)     = 0
15801 fadvise64(3, 0, 104857600, POSIX_FADV_DONTNEED) = 0
15801 close(3)
[..]
</code></pre>
<h2 id="Create-and-write-100MB-file-with-fsync-1"><a href="#Create-and-write-100MB-file-with-fsync-1" class="headerlink" title="Create and write 100MB file with fsync&#x3D;1"></a>Create and write 100MB file with fsync&#x3D;1</h2><p>Here we write 100MB data to a file with 8k blocksize.</p>
<pre><code>$ strace -f -o strace.out fio --name=test --ioengine=libaio --blocksize=8k --readwrite=write --directory=/mnt/bench1 --nrfiles=1 --filesize=100m --fsync=1 --numjobs=1 --direct=1 --group_reporting
test: (g=0): rw=write, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=libaio, iodepth=1
fio-3.7
Starting 1 process
test: Laying out IO file (1 file / 100MiB)
Jobs: 1 (f=1)
test: (groupid=0, jobs=1): err= 0: pid=15988: Mon Mar 14 22:28:51 2022
  write: IOPS=6174, BW=48.2MiB/s (50.6MB/s)(100MiB/2073msec)
    slat (usec): min=29, max=163, avg=56.07, stdev=12.67
    clat (usec): min=19, max=118, avg=30.21, stdev= 7.39
     lat (usec): min=52, max=197, avg=86.72, stdev=18.43
    clat percentiles (usec):
     |  1.00th=[   21],  5.00th=[   24], 10.00th=[   25], 20.00th=[   26],
     | 30.00th=[   27], 40.00th=[   29], 50.00th=[   29], 60.00th=[   30],
     | 70.00th=[   31], 80.00th=[   32], 90.00th=[   38], 95.00th=[   47],
     | 99.00th=[   61], 99.50th=[   67], 99.90th=[   80], 99.95th=[   84],
     | 99.99th=[  102]
   bw (  KiB/s): min=45700, max=53392, per=99.44%, avg=49121.00, stdev=3324.04, samples=4
   iops        : min= 5712, max= 6674, avg=6140.00, stdev=415.68, samples=4
  lat (usec)   : 20=0.66%, 50=95.25%, 100=4.07%, 250=0.02%
  fsync/fdatasync/sync_file_range:
    sync (nsec): min=95, max=7603, avg=235.07, stdev=105.18
    sync percentiles (nsec):
     |  1.00th=[  107],  5.00th=[  165], 10.00th=[  193], 20.00th=[  211],
     | 30.00th=[  211], 40.00th=[  213], 50.00th=[  217], 60.00th=[  221],
     | 70.00th=[  225], 80.00th=[  274], 90.00th=[  338], 95.00th=[  342],
     | 99.00th=[  398], 99.50th=[  426], 99.90th=[  486], 99.95th=[  540],
     | 99.99th=[ 6880]
  cpu          : usr=7.63%, sys=29.92%, ctx=89626, majf=0, minf=13
  IO depths    : 1=200.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     issued rwts: total=0,12800,0,12799 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
  WRITE: bw=48.2MiB/s (50.6MB/s), 48.2MiB/s-48.2MiB/s (50.6MB/s-50.6MB/s), io=100MiB (105MB), run=2073-2073msec

Disk stats (read/write):
  nvme0n1: ios=0/34843, merge=0/11135, ticks=0/400, in_queue=399, util=95.01%
</code></pre>
<p>From the strace output, fsync is issued after each 8k block write.</p>
<pre><code>$ cat strace.out
[..]
15988 open(&quot;/mnt/bench1/test.0.0&quot;, O_RDWR|O_CREAT|O_DIRECT, 0600) = 3
15988 fadvise64(3, 0, 104857600, POSIX_FADV_DONTNEED) = 0
15988 fadvise64(3, 0, 104857600, POSIX_FADV_SEQUENTIAL) = 0
15988 io_submit(0x7f819d8b1000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;5\340(\3148\240\231\26\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=0&#125;]) = 1
15988 io_getevents(0x7f819d8b1000, 1, 1, [&#123;data=0, obj=0xc88de0, res=8192, res2=0&#125;], NULL) = 1
15988 fsync(3)                          = 0
15988 io_submit(0x7f819d8b1000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;5\340(\3148\240\231\26\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=8192&#125;]) = 1
15988 io_getevents(0x7f819d8b1000, 1, 1, [&#123;data=0, obj=0xc88de0, res=8192, res2=0&#125;], NULL) = 1
15988 fsync(3)                          = 0
[..]
15988 io_submit(0x7f819d8b1000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;\0\340?\6\0\0\0\0\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=104849408&#125;]) = 1
15988 io_getevents(0x7f819d8b1000, 1, 1, [&#123;data=0, obj=0xc88de0, res=8192, res2=0&#125;], NULL) = 1
15988 getrusage(RUSAGE_THREAD, &#123;ru_utime=&#123;tv_sec=0, tv_usec=158715&#125;, ru_stime=&#123;tv_sec=0, tv_usec=620431&#125;, ...&#125;) = 0
15988 close(3)
[..]
</code></pre>
<h2 id="Create-and-write-100MB-file-with-end-fsync-1"><a href="#Create-and-write-100MB-file-with-end-fsync-1" class="headerlink" title="Create and write 100MB file with end_fsync&#x3D;1"></a>Create and write 100MB file with end_fsync&#x3D;1</h2><p>Here we write 100MB data to a file and fsync is issued after the job completes.</p>
<pre><code>$ strace -f -o strace.out fio --name=test --ioengine=libaio --blocksize=8k --readwrite=write --directory=/mnt/bench1 --nrfiles=1 --filesize=100m --end_fsync=1 --numjobs=1 --direct=1 --group_reporting
test: (g=0): rw=write, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=libaio, iodepth=1
fio-3.7
Starting 1 process
test: Laying out IO file (1 file / 100MiB)
Jobs: 1 (f=1)
test: (groupid=0, jobs=1): err= 0: pid=16648: Mon Mar 14 22:50:22 2022
  write: IOPS=9631, BW=75.2MiB/s (78.9MB/s)(100MiB/1329msec)
    slat (usec): min=42, max=131, avg=61.18, stdev=11.19
    clat (usec): min=29, max=100, avg=39.34, stdev= 6.50
     lat (usec): min=77, max=193, avg=101.11, stdev=10.93
    clat percentiles (nsec):
     |  1.00th=[32128],  5.00th=[32640], 10.00th=[33024], 20.00th=[34048],
     | 30.00th=[35072], 40.00th=[35584], 50.00th=[36608], 60.00th=[38656],
     | 70.00th=[43776], 80.00th=[44800], 90.00th=[46848], 95.00th=[50944],
     | 99.00th=[60672], 99.50th=[62720], 99.90th=[70144], 99.95th=[72192],
     | 99.99th=[80384]
   bw (  KiB/s): min=77168, max=77408, per=100.00%, avg=77288.00, stdev=169.71, samples=2
   iops        : min= 9646, max= 9676, avg=9661.00, stdev=21.21, samples=2
  lat (usec)   : 50=94.53%, 100=5.46%, 250=0.01%
  cpu          : usr=7.91%, sys=42.70%, ctx=51220, majf=0, minf=11
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     issued rwts: total=0,12800,0,1 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
  WRITE: bw=75.2MiB/s (78.9MB/s), 75.2MiB/s-75.2MiB/s (78.9MB/s-78.9MB/s), io=100MiB (105MB), run=1329-1329msec

Disk stats (read/write):
  nvme0n1: ios=0/12803, merge=0/5, ticks=0/203, in_queue=203, util=91.40%
</code></pre>
<p>From the strace output, the fsync is issued at the end of fio job.</p>
<pre><code>$ cat strace.out
[..]
16648 open(&quot;/mnt/bench1/test.0.0&quot;, O_RDWR|O_CREAT|O_DIRECT, 0600) = 3
16648 fadvise64(3, 0, 104857600, POSIX_FADV_DONTNEED) = 0
16648 fadvise64(3, 0, 104857600, POSIX_FADV_SEQUENTIAL) = 0
16648 io_submit(0x7fac95355000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;5\340(\3148\240\231\26\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=0&#125;]) = 1
16648 io_getevents(0x7fac95355000, 1, 1, [&#123;data=0, obj=0xa37de0, res=8192, res2=0&#125;], NULL) = 1
16648 io_submit(0x7fac95355000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;5\340(\3148\240\231\26\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=8192&#125;]) = 1
16648 io_getevents(0x7fac95355000, 1, 1, [&#123;data=0, obj=0xa37de0, res=8192, res2=0&#125;], NULL) = 1
[..]
16648 io_submit(0x7fac95355000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;\0\340?\6\0\0\0\0\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=104849408&#125;]) = 1
16648 io_getevents(0x7fac95355000, 1, 1, [&#123;data=0, obj=0xa37de0, res=8192, res2=0&#125;], NULL) = 1
16648 fsync(3)                          = 0
16648 close(3)                          = 0
[..]
</code></pre>
<h2 id="Create-and-write-100MB-file-with-fdatasync-1"><a href="#Create-and-write-100MB-file-with-fdatasync-1" class="headerlink" title="Create and write 100MB file with fdatasync&#x3D;1"></a>Create and write 100MB file with fdatasync&#x3D;1</h2><p>Here we write 100MB data to a file with 8k blocksize. fdatasync is issued after each block write.</p>
<pre><code>$ strace -f -o strace.out fio --name=test --ioengine=libaio --blocksize=8k --readwrite=write --directory=/mnt/bench1 --nrfiles=1 --filesize=100m --fdatasync=1 --numjobs=1 --direct=1 --group_reporting
test: (g=0): rw=write, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=libaio, iodepth=1
fio-3.7
Starting 1 process
test: Laying out IO file (1 file / 100MiB)
Jobs: 1 (f=1)
test: (groupid=0, jobs=1): err= 0: pid=23011: Tue Mar 15 02:45:48 2022
  write: IOPS=4857, BW=37.0MiB/s (39.8MB/s)(100MiB/2635msec)
    slat (usec): min=30, max=183, avg=70.02, stdev=12.85
    clat (usec): min=20, max=280, avg=45.46, stdev= 9.98
     lat (usec): min=54, max=348, avg=116.11, stdev=20.49
    clat percentiles (usec):
     |  1.00th=[   25],  5.00th=[   26], 10.00th=[   31], 20.00th=[   41],
     | 30.00th=[   45], 40.00th=[   46], 50.00th=[   48], 60.00th=[   49],
     | 70.00th=[   49], 80.00th=[   50], 90.00th=[   52], 95.00th=[   58],
     | 99.00th=[   75], 99.50th=[   84], 99.90th=[  137], 99.95th=[  143],
     | 99.99th=[  172]
   bw (  KiB/s): min=37168, max=43632, per=100.00%, avg=38934.40, stdev=2648.49, samples=5
   iops        : min= 4646, max= 5454, avg=4866.80, stdev=331.06, samples=5
  lat (usec)   : 50=86.37%, 100=13.41%, 250=0.21%, 500=0.01%
  fsync/fdatasync/sync_file_range:
    sync (nsec): min=107, max=11447, avg=232.89, stdev=147.40
    sync percentiles (nsec):
     |  1.00th=[  127],  5.00th=[  161], 10.00th=[  185], 20.00th=[  211],
     | 30.00th=[  221], 40.00th=[  227], 50.00th=[  231], 60.00th=[  241],
     | 70.00th=[  247], 80.00th=[  258], 90.00th=[  266], 95.00th=[  282],
     | 99.00th=[  334], 99.50th=[  350], 99.90th=[  516], 99.95th=[  620],
     | 99.99th=[ 9408]
  cpu          : usr=6.19%, sys=31.13%, ctx=89613, majf=0, minf=13
  IO depths    : 1=200.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     issued rwts: total=0,12800,0,0 short=12799,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
  WRITE: bw=37.0MiB/s (39.8MB/s), 37.0MiB/s-37.0MiB/s (39.8MB/s-39.8MB/s), io=100MiB (105MB), run=2635-2635msec

Disk stats (read/write):
  nvme0n1: ios=0/35262, merge=0/11272, ticks=0/416, in_queue=416, util=96.06%
</code></pre>
<p>From the strace output, fdatasync is issued after each 8k block write.</p>
<pre><code>$ cat strace.out
[..]
23011 open(&quot;/mnt/bench1/test.0.0&quot;, O_RDWR|O_CREAT|O_DIRECT, 0600) = 3
23011 fadvise64(3, 0, 104857600, POSIX_FADV_DONTNEED) = 0
23011 fadvise64(3, 0, 104857600, POSIX_FADV_SEQUENTIAL) = 0
23011 io_submit(0x7f1a71a81000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;5\340(\3148\240\231\26\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=0&#125;]) = 1
23011 io_getevents(0x7f1a71a81000, 1, 1, [&#123;data=0, obj=0x1207de0, res=8192, res2=0&#125;], NULL) = 1
23011 fdatasync(3)
[..]
23011 io_submit(0x7f1a71a81000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;\0 ?\6\0\0\0\0\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=104841216&#125;]) = 1
23011 io_getevents(0x7f1a71a81000, 1, 1, [&#123;data=0, obj=0x1207de0, res=8192, res2=0&#125;], NULL) = 1
23011 fdatasync(3)                      = 0
23011 io_submit(0x7f1a71a81000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;\0 ?\6\0\0\0\0\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=104849408&#125;]) = 1
23011 io_getevents(0x7f1a71a81000, 1, 1, [&#123;data=0, obj=0x1207de0, res=8192, res2=0&#125;], NULL) = 1
23011 getrusage(RUSAGE_THREAD, &#123;ru_utime=&#123;tv_sec=0, tv_usec=164077&#125;, ru_stime=&#123;tv_sec=0, tv_usec=820386&#125;, ...&#125;) = 0
23011 close(3)
[..]
</code></pre>
<h2 id="Create-and-write-100MB-file-with-sync-1"><a href="#Create-and-write-100MB-file-with-sync-1" class="headerlink" title="Create and write 100MB file with sync&#x3D;1"></a>Create and write 100MB file with sync&#x3D;1</h2><p>Here we write 100MB data, by doing 8k sequential write with 1 job. The I&#x2F;O is synchronous.</p>
<pre><code>$ strace -f -o strace.out fio --name=test --ioengine=libaio --blocksize=8k --readwrite=write --directory=/mnt/bench1 --nrfiles=1 --filesize=100m --sync=1 --numjobs=1 --direct=1 --group_reporting
test: (g=0): rw=write, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=libaio, iodepth=1
fio-3.7
Starting 1 process
test: Laying out IO file (1 file / 100MiB)
Jobs: 1 (f=1)
test: (groupid=0, jobs=1): err= 0: pid=16435: Mon Mar 14 22:43:59 2022
  write: IOPS=9377, BW=73.3MiB/s (76.8MB/s)(100MiB/1365msec)
    slat (usec): min=28, max=146, avg=47.45, stdev= 7.00
    clat (usec): min=33, max=581, avg=57.00, stdev=11.75
     lat (usec): min=79, max=619, avg=104.88, stdev=14.94
    clat percentiles (usec):
     |  1.00th=[   43],  5.00th=[   45], 10.00th=[   48], 20.00th=[   51],
     | 30.00th=[   52], 40.00th=[   53], 50.00th=[   57], 60.00th=[   59],
     | 70.00th=[   60], 80.00th=[   62], 90.00th=[   67], 95.00th=[   76],
     | 99.00th=[   95], 99.50th=[   99], 99.90th=[  113], 99.95th=[  131],
     | 99.99th=[  578]
   bw (  KiB/s): min=70896, max=77461, per=98.88%, avg=74178.50, stdev=4642.16, samples=2
   iops        : min= 8862, max= 9682, avg=9272.00, stdev=579.83, samples=2
  lat (usec)   : 50=18.82%, 100=80.74%, 250=0.42%, 750=0.02%
  cpu          : usr=4.62%, sys=28.52%, ctx=63963, majf=0, minf=11
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
     issued rwts: total=0,12800,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
  WRITE: bw=73.3MiB/s (76.8MB/s), 73.3MiB/s-73.3MiB/s (76.8MB/s-76.8MB/s), io=100MiB (105MB), run=1365-1365msec

Disk stats (read/write):
  nvme0n1: ios=0/32201, merge=0/10294, ticks=0/356, in_queue=356, util=91.92%
</code></pre>
<p>From the strace output, the file is opened with “O_SYNC” flag since we specified “–sync&#x3D;1” in the fio command. This means all the incoming writes are synchronous.</p>
<pre><code>$ cat strace.out
[..]
16435 open(&quot;/mnt/bench1/test.0.0&quot;, O_RDWR|O_CREAT|O_SYNC|O_DIRECT, 0600) = 3
16435 fadvise64(3, 0, 104857600, POSIX_FADV_DONTNEED) = 0
16435 fadvise64(3, 0, 104857600, POSIX_FADV_SEQUENTIAL) = 0
16435 io_submit(0x7fe4c1fe3000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;5\340(\3148\240\231\26\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=0&#125;]) = 1
16435 io_getevents(0x7fe4c1fe3000, 1, 1, [&#123;data=0, obj=0x1980de0, res=8192, res2=0&#125;], NULL) = 1
16435 io_submit(0x7fe4c1fe3000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;\0 \0\0\0\0\0\0\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=8192&#125;]) = 1
16435 io_getevents(0x7fe4c1fe3000, 1, 1, [&#123;data=0, obj=0x1980de0, res=8192, res2=0&#125;], NULL) = 1
[..]
16435 io_submit(0x7fe4c1fe3000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;\0\200?\6\0\0\0\0\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=8192, aio_offset=104849408&#125;]) = 1
16435 io_getevents(0x7fe4c1fe3000, 1, 1, [&#123;data=0, obj=0x1980de0, res=8192, res2=0&#125;], NULL) = 1
16435 getrusage(RUSAGE_THREAD, &#123;ru_utime=&#123;tv_sec=0, tv_usec=64153&#125;, ru_stime=&#123;tv_sec=0, tv_usec=389856&#125;, ...&#125;) = 0
16435 close(3)
[..]
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Fio</tag>
      </tags>
  </entry>
  <entry>
    <title>Fio initial write, overwrite and append write</title>
    <url>/blog/fio-initial-write-overwrite-and-append-write/</url>
    <content><![CDATA[<p>In this post, we try to study how the overwrite and file_append options work in fio benchmark.</p>
<h2 id="overwrite-and-file-append-options"><a href="#overwrite-and-file-append-options" class="headerlink" title="overwrite and file_append options"></a>overwrite and file_append options</h2><ul>
<li>overwrite&#x3D;bool</li>
</ul>
<p>If true, writes to a file will always overwrite existing data. If the file doesn’t already exist, it will be created before the write phase begins. If the file exists and is large enough for the specified write phase, nothing will be done. Default: false.</p>
<ul>
<li>file_append&#x3D;bool</li>
</ul>
<p>Perform I&#x2F;O after the end of the file. Normally fio will operate within the size of a file. If this option is set, then fio will append to the file instead. This has identical behavior to setting offset to the size of a file. This option is ignored on non-regular files.</p>
<ul>
<li>offset&#x3D;int</li>
</ul>
<p>Start I&#x2F;O at the provided offset in the file, given as either a fixed size in bytes, zones or a percentage. If a percentage is given, the generated offset will be aligned to the minimum blocksize or to the value of offset_align if provided. Data before the given offset will not be touched. This effectively caps the file size at real_size - offset. Can be combined with size to constrain the start and end range of the I&#x2F;O workload. A percentage can be specified by a number between 1 and 100 followed by ‘%’, for example, offset&#x3D;20% to specify 20%. In ZBD mode, value can be set as number of zones using ‘z’.</p>
<h2 id="Fio-commands"><a href="#Fio-commands" class="headerlink" title="Fio commands"></a>Fio commands</h2><p>The following fio commands are used to study initial write, overwrite and append write. We also have one more randread run at the last. Before each command run, we drop kernel cache with command “echo 3&gt;&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches”.</p>
<pre><code>$ fio --name=test --ioengine=libaio --blocksize=4k --readwrite=randwrite --directory=/mnt/bench1 --nrfiles=1 --size=1g --end_fsync=1 --numjobs=1 --direct=1
$ fio --name=test --ioengine=libaio --blocksize=4k --readwrite=randwrite --directory=/mnt/bench1 --nrfiles=1 --size=1g --end_fsync=1 --numjobs=1 --direct=1
$ fio --name=test --ioengine=libaio --blocksize=4k --readwrite=randwrite --directory=/mnt/bench1 --nrfiles=1 --size=1g --end_fsync=1 --numjobs=1 --direct=1 --overwrite=1
$ fio --name=test --ioengine=libaio --blocksize=4k --readwrite=randwrite --directory=/mnt/bench1 --nrfiles=1 --size=1g --end_fsync=1 --numjobs=1 --direct=1 --file_append=1
$ fio --name=test --ioengine=libaio --blocksize=4k --readwrite=randread --directory=/mnt/bench1 --nrfiles=1 --size=1g --end_fsync=1 --numjobs=1 --direct=1
</code></pre>
<h2 id="Fio-results"><a href="#Fio-results" class="headerlink" title="Fio results"></a>Fio results</h2><p>With the option “–file_append&#x3D;1”, fio will append to the end of the file and expand it to the proper size. The write performance is similar to the initial write.</p>
<p>With the option “–overwrite&#x3D;1”, fio will overwrite the existing data. If the file doesn’t already exist, it will be created before the write phase begins. If the file exists and is large enough for the specified write phase, nothing will be done.</p>
<p>The following table shows the fio results.<br>Test NameFio OptionsResultFile SizeInitial Writewrite: IOPS&#x3D;7009, BW&#x3D;27.4MiB&#x2F;s0GB -&gt; 1GBOverwrite–overwrite&#x3D;0(default)write: IOPS&#x3D;10.4k, BW&#x3D;40.4MiB&#x2F;s1GBOverwrite–overwrite&#x3D;1write: IOPS&#x3D;10.0k, BW&#x3D;39.2MiB&#x2F;s1GBAppend Write–file_append&#x3D;1write: IOPS&#x3D;6667, BW&#x3D;26.0MiB&#x2F;s1GB -&gt; 2GBRandom Readread: IOPS&#x3D;7769, BW&#x3D;30.3MiB&#x2F;s2GB</p>
<h2 id="Fio-syscalls-for-create-write-and-read"><a href="#Fio-syscalls-for-create-write-and-read" class="headerlink" title="Fio syscalls for create, write and read"></a>Fio syscalls for create, write and read</h2><p>The file space will be allocated when to lay out the file.</p>
<pre><code>29247 write(1, &quot;Starting 1 process\n&quot;, 19) = 19
29247 stat(&quot;/mnt/bench1/test.0.0&quot;, 0x7ffd59df9db0) = -1 ENOENT (No such file or directory)
29247 write(1, &quot;test: Laying out IO file (1 file&quot;..., 44) = 44
29247 unlink(&quot;/mnt/bench1/test.0.0&quot;)    = -1 ENOENT (No such file or directory)
29247 open(&quot;/mnt/bench1/test.0.0&quot;, O_WRONLY|O_CREAT, 0644) = 3
29247 fallocate(3, 0, 0, 1073741824)    = 0
29247 fadvise64(3, 0, 1073741824, POSIX_FADV_DONTNEED) = 0
29247 close(3)
</code></pre>
<p>The following are the syscalls when fio writes data with ioengine <em>libaio</em>.</p>
<pre><code>29429 open(&quot;/mnt/bench1/test.0.0&quot;, O_RDWR|O_CREAT|O_DIRECT, 0600) = 3
29429 fadvise64(3, 1073741824, 1073741824, POSIX_FADV_DONTNEED) = 0
29429 fadvise64(3, 1073741824, 1073741824, POSIX_FADV_RANDOM) = 0
29429 io_submit(0x7fbe9301c000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;5\340(\3148\240\231\26\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=4096, aio_offset=1138499584&#125;]) = 1
29429 io_getevents(0x7fbe9301c000, 1, 1, [&#123;data=0, obj=0xa77de0, res=4096, res2=0&#125;], NULL) = 1
29429 io_submit(0x7fbe9301c000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;\0\340`o\0\0\0\0\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=4096, aio_offset=1868619776&#125;]) = 1
29429 io_getevents(0x7fbe9301c000, 1, 1, [&#123;data=0, obj=0xa77de0, res=4096, res2=0&#125;], NULL) = 1
[..]
29429 io_submit(0x7fbe9301c000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PWRITE, aio_fildes=3, aio_buf=&quot;\0\300\357y\0\0\0\0\6\234j\251\362\315\351\n\200S*\7\t\345\r\25pJ%\367\v9\235\30&quot;..., aio_nbytes=4096, aio_offset=2046550016&#125;]) = 1
29429 io_getevents(0x7fbe9301c000, 1, 1, [&#123;data=0, obj=0xa77de0, res=4096, res2=0&#125;], NULL) = 1
29429 fsync(3)                          = 0
29429 close(3)
</code></pre>
<p>The following are the syscalls when fio reads data with ioengine <em>libaio</em>.</p>
<pre><code>29484 open(&quot;/mnt/bench1/test.0.0&quot;, O_RDONLY|O_DIRECT) = 3
29484 fadvise64(3, 0, 1073741824, POSIX_FADV_DONTNEED) = 0
29484 fadvise64(3, 0, 1073741824, POSIX_FADV_RANDOM) = 0
29484 io_submit(0x7f069d23a000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PREAD, aio_fildes=3, aio_buf=0x9d1000, aio_nbytes=4096, aio_offset=64757760&#125;]) = 1
29484 io_getevents(0x7f069d23a000, 1, 1, [&#123;data=0, obj=0x9d3da0, res=4096, res2=0&#125;], NULL) = 1
29484 io_submit(0x7f069d23a000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PREAD, aio_fildes=3, aio_buf=0x9d1000, aio_nbytes=4096, aio_offset=794877952&#125;]) = 1
29484 io_getevents(0x7f069d23a000, 1, 1, [&#123;data=0, obj=0x9d3da0, res=4096, res2=0&#125;], NULL) = 1
[..]
29484 io_submit(0x7f069d23a000, 1, [&#123;aio_lio_opcode=IOCB_CMD_PREAD, aio_fildes=3, aio_buf=0x9d1000, aio_nbytes=4096, aio_offset=972808192&#125;]) = 1
29484 io_getevents(0x7f069d23a000, 1, 1, [&#123;data=0, obj=0x9d3da0, res=4096, res2=0&#125;], NULL) = 1
29484 getrusage(RUSAGE_THREAD, &#123;ru_utime=&#123;tv_sec=1, tv_usec=706077&#125;, ru_stime=&#123;tv_sec=7, tv_usec=828474&#125;, ...&#125;) = 0
29484 close(3)
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Fio</tag>
      </tags>
  </entry>
  <entry>
    <title>Flame graph - A visualization for profiling and debugging</title>
    <url>/blog/flame-graph-a-visualization-for-profiling-and-debugging/</url>
    <content><![CDATA[<p>Flame graphs can be generated from the output of many different software profilers, including profiles for different resources and event types. The <a href="https://queue.acm.org/detail.cfm?id=2927301">article</a> written by Brendan Gregg describes how the flame graph works.</p>
<p>We can start to work with the flame graph by following commands.</p>
<pre><code>$ yum install git
$ git clone --depth 1 https://github.com/brendangregg/FlameGraph
$ cp result-15082021-201302/perf.data FlameGraph/
$ cd FlameGraph/
 
$ perf record -p $pids -a -g -- sleep 30
$ perf script | ./stackcollapse-perf.pl | ./flamegraph.pl &gt; perf16.svg
</code></pre>
<p><img src="/images/Gregg4.svg" alt="Image"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.brendangregg.com/flamegraphs.html">https://www.brendangregg.com/flamegraphs.html</a></li>
<li><a href="https://queue.acm.org/detail.cfm?id=2927301">https://queue.acm.org/detail.cfm?id=2927301</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Perf</tag>
        <tag>Flame Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>First Amendment</title>
    <url>/blog/first-amendment/</url>
    <content><![CDATA[<p>The First Amendment is one of the most important amendments to the Constitution. It allows U.S. citizens to express their ideas through words and deeds. It guarantees freedom of speech, the press, assembly, petitioning, and religion.<span id="more"></span></p>
<p>After the First Amendment is passed, people have the right to express themselves without fear of government interference. However, it is unclear what is not protected in what the government calls “speech”. So far, some unprotected forms of speech have included obscenity, real threats, lawless behavior, and defamation. Obscenity is the utterance of words or expressions that are strongly offensive. People can threaten others and even hurt others. In addition, they may perform acts prohibited by law, or damage someone’s reputation. The Supreme Court reviews speech on a case-by-case basis and considers the context in which it was made. For example, any speech deemed political, no matter how outrageous and offensive it might be, may still be protected by the First Amendment. Still, the power of the First Amendment is not unlimited. It does not protect you from private organizations or the actions of individuals.</p>
<p>The First Amendment also protects the freedom of the press. It allows people to express themselves through publications. It enables reporters and journalists to report freely on news and stories of public interest. Freedom of the press is essential for a democracy where the government is vital to its people. Supreme Court Justice Hugo Black said: “The media is for the governed, not for the governor.” The free press allows people to investigate and report on government wrongdoing. It is also a tool for ordinary citizens to express themselves and to be exposed to all kinds of information and opinions. During the Revolutionary War, many freedom activists were allowed to write inspirational essays, which boosted the morale of soldiers and citizens. One example of this is Thomas Paine’s <em>Common Sense</em>. Press freedom is also restricted. Defamatory speech is not protected.</p>
<p>Thanks to the First Amendment, freedom of assembly and petition is also protected. People have won the right to assemble peacefully for social, economic, religious, or political purposes or to gather as a group. It protects the right to protest the government, and citizens are free to sign petitions and even file lawsuits against the government. Individuals have the right to express their views to elected officials. The right to petition is basically the ability to demand that the government change its policies. The Supreme Court doesn’t need to really pay attention to this right because it’s part of free speech. To gain freedom of assembly, the government must allow people to use some public property, such as parks and sidewalks. Sometimes there is tension between freedom of assembly and public safety. As a result, the government has authority over when, where, and how speeches are heard.</p>
<p>The last freedom protected by the First Amendment is religion. The establishment clause stipulates that the Government has no right to establish a “national” religion or to favor one religion. This also applies to local and state governments. Governments must not act for the purpose of advancing a religion. Even praying in schools is forbidden because children feel pressured to participate. The government cannot compel anyone to practice a certain religion. Basically, people are free to practice whatever religion they choose, or not to practice any religion at all.</p>
<p>Without the First Amendment, people would lose freedom of speech, the press, petitions, assembly, and religion. They will be barred from expressing their opinions to the government and will not be able to express their ideas. This will make the government overriding because no one will be there to stop their wrongdoing. The republic will become a monarchy. Some acts of political expression may be punishable by law. Written opinions may be subject to legal action and not just defamation for making controversial facts or unpopular opinions. Since there is no First Amendment, the government may ban protest marches or rallies. The government can ban all religions or declare one religion to be the only one allowed. The government can review every written article, every video, and every broadcast. People could be arrested for making any comments the government doesn’t want to hear. Any oral or written comments made against any official organization are grounds for immediate arrest, even if it is valid. People will not be able to petition the government to express their ideas. When freedom disappears, there is no entrepreneurial spirit, which leads to the collapse of the government.</p>
<p>With the First Amendment, the United States worked hard to be what it is today. The rights it protects are among the unalienable rights in the Constitution. People are free to express their opinions and their natural rights are protected. The government represents the individual, and everyone is independent. They are free to say what they want to the government. Thanks to the First Amendment, the United States has become a free nation with a fair government.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
  </entry>
  <entry>
    <title>Flame Graph and stack trace visualization</title>
    <url>/blog/flame-graph-and-stack-trace-visualization/</url>
    <content><![CDATA[<p>A stack trace is series of functions which show the code path. With the Linux perf(1) tool, the stack samples are summarized as call tree. With the BCC profile(8) tool, the unique stack trace shows as a count. Both tools are very useful when there are only a few call stacks on-CPU. However, when the profiling result includes a huge number of stacks, looking through pages of stack output would not be that easy. Flame graphs were invented to solve this problem.</p>
<p>The following is a synthetic example for easier explanation of Flame Graph.</p>
<p><img src="/images/FlameGraph.png" alt="Image"></p>
<p>From this Flame Graph, three call stacks are included:</p>
<ul>
<li>func-a -&gt; func-b</li>
<li>func-a -&gt; func-b -&gt; func-c</li>
<li>func-a -&gt; func-b -&gt; func-d -&gt; func-e</li>
</ul>
<p>From left to right, it’s an alphabetical sort of function frames. Each box represents a function in the stack(aka “stack frame”). The width of the box reflects the presence in the profile.</p>
<p>From bottom to top, it represents the code flow.</p>
<p>The Flame Graph shows the CPU samples, which means the top edge represents the functions running on-CPU. The other function below the top edge are never sampled on-CPU directly.</p>
<p>In the above example Flame Graph, the func-c was directly on-CPU for 70% of the time, func-b was on-CPU for 20% of time, and func-e was on-CPU for 10% of the time during sampling.</p>
<p>Even though the functions below the top edge are not directly on-CPU durng sampling, it still helps determine the function ancestry.</p>
<p>So, to read a Flame Graph, we can look for the widest towers and understand them first since they consume much CPU cycles during sampling.</p>
<p>The following is a real example of Flame Graph.</p>
<p><img src="/images/Gregg4.svg" alt="Image"></p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Perf</tag>
        <tag>Flame Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>fsstat - Display details of a file system</title>
    <url>/blog/fsstat-display-details-of-a-file-system/</url>
    <content><![CDATA[<p>fsstat displays the details associated with a file system.  The output of this command is file system specific. At a minimum, the range of meta-data  values (inode numbers) and content units (blocks or clusters) are given. Also given are details from the Super Block, such as mount times and and features. For file systems that use groups (FFS and EXT2FS), the layout of each group is listed. For a FAT file system, the FAT table is displayed in a condensed format. Note that the data is in sectors and not in clusters.</p>
<span id="more"></span>
<p>To install fsstat package:</p>
<pre><code>$ sudo apt-get install sleuthkit
</code></pre>
<p>To display detail of a file system:</p>
<pre><code>$ sudo mount | grep ext4
/dev/sda3 on / type ext4 (rw,relatime,errors=remount-ro)

$ sudo fsstat /dev/sda3 | more
FILE SYSTEM INFORMATION
--------------------------------------------
File System Type: Ext4
Volume Name: 
Volume ID: 6f28eae2697f78bbf047526f6c4ba9d8

Last Written at: 2023-05-19 10:51:40 (PDT)
Last Checked at: 2022-12-26 20:56:27 (PST)

Last Mounted at: 2023-05-19 10:51:40 (PDT)
Unmounted properly
Last mounted on: /

Source OS: Linux
Dynamic Structure
Compat Features: Journal, Ext Attributes, Resize Inode, Dir Index
InCompat Features: Filetype, Needs Recovery, Extents, 64bit, Flexible Block Groups, 
Read Only Compat Features: Sparse Super, Large File, Huge File, Extra Inode Size

Journal ID: 00
Journal Inode: 8

METADATA INFORMATION
--------------------------------------------
Inode Range: 1 - 6520833
Root Directory: 2
Free Inodes: 5769769
Inode Size: 256
Orphan Inodes: 418372, 418366, 418362, 418203, 418153, 1311034, 1311033, 418374, 4594981, 4594004, 418389, 418383, 418381, 418373, 418370, 418371, 418369, 418368, 6291729, 418367, 4593820, 418365, 418364
, 418363, 418152, 418201, 418198, 418196, 4594983, 418361, 418339, 418160, 418158, 1310992, 1714779, 1714780, 418194, 418195, 1311035, 1311010, 

CONTENT INFORMATION
--------------------------------------------
Block Groups Per Flex Group: 16
Block Range: 0 - 26082303
Block Size: 4096
Free Blocks: 16313184

BLOCK GROUP INFORMATION
--------------------------------------------
Number of Block Groups: 796
Inodes per group: 8192
Blocks per group: 32768

Group: 0:
  Block Group Flags: [INODE_ZEROED] 
  Inode Range: 1 - 8192
  Block Range: 0 - 32767
  Layout:
    Super Block: 0 - 0
    Group Descriptor Table: 1 - 13
    Group Descriptor Growth Blocks: 14 - 1037
    Data bitmap: 1038 - 1038
    Inode bitmap: 1054 - 1054
    Inode Table: 1070 - 1581
    Data Blocks: 9262 - 32767
  Free Inodes: 8174 (99%)
  Free Blocks: 6286 (19%)
  Total Directories: 2
  Stored Checksum: 0x34D8

Group: 1:
  Block Group Flags: [INODE_UNINIT, INODE_ZEROED] 
  Inode Range: 8193 - 16384
  Block Range: 32768 - 65535
  Layout:
    Super Block: 32768 - 32768
    Group Descriptor Table: 32769 - 32781
    Group Descriptor Growth Blocks: 32782 - 33805
    Data bitmap: 1039 - 1039
    Inode bitmap: 1055 - 1055
    Inode Table: 1582 - 2093
    Data Blocks: 33806 - 65535
  Free Inodes: 8192 (100%)
  Free Blocks: 1008 (3%)
  Total Directories: 0
  Stored Checksum: 0x9FAA
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Observability</tag>
      </tags>
  </entry>
  <entry>
    <title>FUSE - a userspace filesystem framework</title>
    <url>/blog/fuse-a-userspace-filesystem-framework/</url>
    <content><![CDATA[<p><a href="https://www.kernel.org/doc/html/latest/filesystems/fuse.html">https://www.kernel.org/doc/html/latest/filesystems/fuse.html</a></p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>General steps for system performance analysis</title>
    <url>/blog/general-steps-for-system-performance-analysis/</url>
    <content><![CDATA[<p>This post tries to provide general steps for system performance analysis. We are going to cover the following objectives.</p>
<ul>
<li>The performance goals</li>
<li>Workload characterization</li>
<li>Drill-down analysis</li>
<li>Traditional performance tools</li>
<li>BCC&#x2F;BPF performance tools</li>
</ul>
<h2 id="The-Goals-of-performance-analysis"><a href="#The-Goals-of-performance-analysis" class="headerlink" title="The Goals of performance analysis"></a>The Goals of performance analysis</h2><p>In general, the goals of performance analysis are to improve end-user performance and reduce the operating cost. To achieve this, it’s important to make the performance measurable. We often use the following metrics to measure performance.</p>
<ul>
<li>Rates - operation or request rates per second</li>
<li>Throughput - data transferred per second</li>
<li>Latency - the time to accomplish a operation or request in milliseconds</li>
<li>Resource utilization - the resource usage in percentage</li>
<li>Cost - Price&#x2F;performance ratio</li>
</ul>
<p>The rates, throughput and latency are usually the most important metrics to check if the certain performance goal is met. For example, the throughput measured in MiB&#x2F;s for a daily database backup is too slow to complete in a given backup window. We need to investigate the issue from the backup application to system layer in order to find a solution to improve the performance. For a second example, the latency at cloud native storage volume layer is as high as 20ms while the underneath SSD disk latency shows less than 1ms. This requires further analysis at volume layer in order to find out the cause for the 19ms latency.</p>
<p>Performance optimization is endless effort. It depends on the goal you are targeting. So, setting the goals is the first step before you involve further performance analysis activities.</p>
<h2 id="Workload-characterization"><a href="#Workload-characterization" class="headerlink" title="Workload characterization"></a>Workload characterization</h2><p>Performance analysis is a process to analyze systematically. Understanding the system&#x2F;application configuration and applied workload are often needed before you do further performance analysis. This is the workload characterization.</p>
<p>The workload characterization tries to answer the following questions:</p>
<ul>
<li>What’s the running application? What’s the major components&#x2F;features used in the application?</li>
<li>What is the schedule to run the workload? What is the job concurrency?</li>
<li>What are the read&#x2F;write patterns? Mixed read and write, or read&#x2F;write only workload?</li>
<li>What are the rates, throughput and latency at application level?</li>
<li>What is the performance concern?</li>
</ul>
<p>Sometimes, you may get a description of the workload from end-users. However, the workload and its configuraiton are usualaly not described clearly enough by the users. It’s worth to characterize the workload with custom profiler. An application level workload profiler can be developed for this purpose. But this often requires application expertise.  At system level, you may leverage the BCC&#x2F;BPF performance tools to profile the workload.</p>
<h2 id="Drill-down-analysis"><a href="#Drill-down-analysis" class="headerlink" title="Drill-down analysis"></a>Drill-down analysis</h2><p>The drill-down analysis is to find a clue and drill deeper until you find the root cause for the performance issue.</p>
<p>The general process for drill-down analysis would be like these steps.</p>
<ol>
<li>Examine the high level performance metrics and identify the degraded performance point</li>
<li>For the target workload point with degradation, lazer focus on the four major system resources(CPU, memory, disk I&#x2F;O and network) to see what is the potential bottleneck</li>
<li>If it’s hardware bottleneck, it might be resolved by scaling up and scaling out the system resources. Otherwise, it could be a software bottleneck either from kernel space or user space.</li>
<li>Find a clue based on the collected metrics to drill down to the next level. Software bottleneck analysis often requires profiling and tracing effort to pinpoint the culprit.</li>
</ol>
<p>To identify a hardware bottleneck, you would check if any of the four major resources are saturated. For example, the system must be CPU bound if the CPU utilization is above 90%. The system must be disk I&#x2F;O bound if the disk is 100% busy and wait queue is unexpected large.</p>
<p>It’s likely that you could not find the root cause with one round of analysis if you go with the wrong direction. You have to repeat the above steps to identify the right direction for RCA. Keep in mind, finding a needle in haystack is not an easy work. You must be patient.</p>
<h2 id="Traditional-performance-tools"><a href="#Traditional-performance-tools" class="headerlink" title="Traditional performance tools"></a>Traditional performance tools</h2><p>During the drill-down performance analysis, you can use the following Linux built-in tools. They are simple but very powerful to help determine the next direction on the way of performance analysis.</p>
<ul>
<li>uptime - system loads in past 1 minute, 5 minutes and 15 minutes</li>
<li>dmesg - system error messages</li>
<li>vmstat - overview of system resource usage(CPU, Memory and disk&#x2F;network I&#x2F;O)</li>
<li>mpstat - per-CPU usage in different states</li>
<li>pidstat - CPU usage per process</li>
<li>iostat - disk I&#x2F;O statistics(throughput, IOPS, latency, etc)</li>
<li>netstat&#x2F;sar - network throughput, TCP&#x2F;IP connection stats</li>
<li>top - CPU&#x2F;Memory usage per process and more</li>
</ul>
<p>Please refer to this <a href="https://www.flamingbytes.com/blog/performance-analysis-in-60-seconds">post</a> for more detail on how to use Linux traditional tools to analyze performance.</p>
<h2 id="BCC-BPF-performance-tools"><a href="#BCC-BPF-performance-tools" class="headerlink" title="BCC&#x2F;BPF performance tools"></a>BCC&#x2F;BPF performance tools</h2><p>While the traditional tools always gives us a first look at the system performance especially on the resource usage, we can use(or create) BCC&#x2F;BPF tools for further performance analysis. Please refer to this <a href="https://www.flamingbytes.com/blog/bcc-performance-tools-checklist">post</a> for more detail.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>RCA</tag>
      </tags>
  </entry>
  <entry>
    <title>Getting started with BCC (BPF Compiler Collection)</title>
    <url>/blog/getting-started-with-bcc/</url>
    <content><![CDATA[<h2 id="Intro-to-BCC"><a href="#Intro-to-BCC" class="headerlink" title="Intro to BCC"></a>Intro to BCC</h2><p><a href="https://github.com/iovisor/bcc">BCC</a> is a toolkit for creating efficient kernel tracing and manipulation programs, and includes several useful tools and examples. It makes use of extended BPF (Berkeley Packet Filters), formally known as eBPF, a new feature that was first added to Linux 3.15. Much of what BCC uses requires Linux 4.1 and above.</p>
<h2 id="Install-BCC-from-packages"><a href="#Install-BCC-from-packages" class="headerlink" title="Install BCC from packages"></a>Install BCC from packages</h2><p>In general, a Linux kernel version 4.1 or newer is required.</p>
<pre><code>$ cat /etc/centos-release
CentOS Linux release 7.9.2009 (Core)

$ uname -r
5.7.12-1.el7.elrepo.x86_64
</code></pre>
<p>In addition, the kernel should have been compiled with the following <a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md">flags set</a>.</p>
<pre><code>CONFIG_BPF=y
CONFIG_BPF_SYSCALL=y
# [optional, for tc filters]
CONFIG_NET_CLS_BPF=m
# [optional, for tc actions]
CONFIG_NET_ACT_BPF=m
CONFIG_BPF_JIT=y
# [for Linux kernel versions 4.1 through 4.6]
CONFIG_HAVE_BPF_JIT=y
# [for Linux kernel versions 4.7 and later]
CONFIG_HAVE_EBPF_JIT=y
# [optional, for kprobes]
CONFIG_BPF_EVENTS=y
# Need kernel headers through /sys/kernel/kheaders.tar.xz
CONFIG_IKHEADERS=y
</code></pre>
<p>There are a few optional kernel flags needed for running bcc networking examples on vanilla kernel:</p>
<pre><code>CONFIG_NET_SCH_SFQ=m
CONFIG_NET_ACT_POLICE=m
CONFIG_NET_ACT_GACT=m
CONFIG_DUMMY=m
CONFIG_VXLAN=m
</code></pre>
<p>These kernel configuration might be set by default after the OS installation but you should double check as below.</p>
<pre><code>$ cat /boot/config-$(uname -r)
</code></pre>
<p><strong>To install the BCC tools from the official yum repository:</strong></p>
<pre><code>$ yum install bcc-tools
Installed:
  bcc-tools.x86_64 0:0.10.0-1.el7

Dependency Installed:
  bcc.x86_64 0:0.10.0-1.el7   python-bcc.x86_64 0:0.10.0-1.el7
</code></pre>
<p>The following BCC tools are pre-defined and available to use after installation.</p>
<pre><code>$ cd /usr/share/bcc
$ ls
introspection  tools
$ cd /usr/share/bcc/tools
$ ls
argdist bpflist cobjnew dcstat ext4dist funclatency javagc llcstat nfsslower opensnoop phpstat pythonstat rubystat sofdsnoop syncsnoop tcpaccept tcpsubnet vfscount bashreadline btrfsdist cpudist deadlock ext4slower funcslower javaobjnew mdflush nodegc perlcalls  pidpersec reset-trace runqlat softirqs syscount tcpconnect tcptop vfsstat biolatency btrfsslower cpuunclaimed deadlock.c filelife gethostlatency  javastat memleak nodestat perlflow profile rubycalls runqlen solisten tclcalls tcpconnlat tcptracer wakeuptime biosnoop cachestat dbslower doc fileslower hardirqs javathreads mountsnoop offcputime perlstat pythoncalls rubyflow runqslower sslsniff tclflow tcpdrop tplist xfsdist biotop cachetop dbstat drsnoop filetop javacalls killsnoop mysqld_qslower offwaketime phpcalls pythonflow rubygc shmsnoop stackcount tclobjnew tcplife trace xfsslower bitesize capable dcsnoop execsnoop funccount javaflow lib nfsdist oomkill phpflow pythongc rubyobjnew slabratetop statsnoop tclstat tcpretrans ttysnoop
</code></pre>
<p><strong>To add bcc directory to the $PATH:</strong></p>
<pre><code>$ vim .bash_profile
bcctools=/usr/share/bcc/tools
PATH=$PATH:$HOME/bin:$bcctools
export PATH
    
$ source ~/.bash_profile
$ echo $PATH
/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin:/usr/share/bcc/tools
</code></pre>
<h2 id="Install-BCC-from-source"><a href="#Install-BCC-from-source" class="headerlink" title="Install BCC from source"></a>Install BCC from source</h2><p>If you want to install a different version of BCC, you can refer to <a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md">here</a>. I tried this but it seems very tricky to install it successfully. I’ll not discuss it in this post.</p>
<h2 id="Use-the-BCC-tools"><a href="#Use-the-BCC-tools" class="headerlink" title="Use the BCC tools"></a>Use the BCC tools</h2><p>It’s not suprise if you see the following error for the first time run of BCC tools.</p>
<pre><code>$ biolatency
In file included from /virtual/main.c:2:
In file included from /lib/modules/5.7.12-1.el7.elrepo.x86_64/build/include/uapi/linux/ptrace.h:142:
In file included from /lib/modules/5.7.12-1.el7.elrepo.x86_64/build/arch/x86/include/asm/ptrace.h:5:
/lib/modules/5.7.12-1.el7.elrepo.x86_64/build/arch/x86/include/asm/segment.h:266:2: error: expected &#39;(&#39; after &#39;asm&#39;
        alternative_io (&quot;lsl %[seg],%[p]&quot;,
        ^
/lib/modules/5.7.12-1.el7.elrepo.x86_64/build/arch/x86/include/asm/alternative.h:240:2: note: expanded from macro &#39;alternative_io&#39;
        asm_inline volatile (ALTERNATIVE(oldinstr, newinstr, feature)   \
        ^
/lib/modules/5.7.12-1.el7.elrepo.x86_64/build/include/linux/compiler_types.h:201:24: note: expanded from macro &#39;asm_inline&#39;
#define asm_inline asm __inline
                       ^
In file included from /virtual/main.c:3:
</code></pre>
<p>This is because many BCC tools are broken with kernel 5.4+ and libbcc 0.10.</p>
<p><strong>To fix this problem:</strong></p>
<p>Modify as below for the BPF program definition.</p>
<p>Original code:</p>
<pre><code>$ vim biolatency
&lt;snippet&gt;
# define BPF program
bpf_text = &quot;&quot;&quot;
#include &lt;uapi/linux/ptrace.h&gt;
#include &lt;linux/blkdev.h&gt;
&lt;snippet&gt;
</code></pre>
<p>Modified code:</p>
<pre><code>$ vim biolatency
&lt;snippet&gt;
# define BPF program
bpf_text = &quot;&quot;&quot;
#ifdef asm_inline
#undef asm_inline
#define asm_inline asm
#endif
#include &lt;uapi/linux/ptrace.h&gt;
#include &lt;linux/blkdev.h&gt;
&lt;snippet&gt;
</code></pre>
<p><strong>Run the BCC tool again now:</strong></p>
<pre><code>$ biolatency
Tracing block device I/O... Hit Ctrl-C to end.
^C
    usecs               : count     distribution
    0 -&gt; 1          : 0        |                                        |
    2 -&gt; 3          : 0        |                                        |
    4 -&gt; 7          : 0        |                                        |
    8 -&gt; 15         : 0        |                                        |
    16 -&gt; 31        : 0        |                                        |
    32 -&gt; 63        : 6        |********************                    |
    64 -&gt; 127       : 12       |****************************************|
    128 -&gt; 255      : 3        |**********                              |
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/iovisor/bcc">https://github.com/iovisor/bcc</a></li>
<li><a href="https://blogs.oracle.com/linux/post/intro-to-bcc-1">https://blogs.oracle.com/linux/post/intro-to-bcc-1</a></li>
<li><a href="https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/12052">https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/12052</a></li>
<li><a href="https://blog.csdn.net/thesre/article/details/122508493">https://blog.csdn.net/thesre/article/details/122508493</a></li>
<li><a href="https://www.containiq.com/post/bcc-tools">https://www.containiq.com/post/bcc-tools</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>BCC</tag>
      </tags>
  </entry>
  <entry>
    <title>Getting started with bpftrace</title>
    <url>/blog/getting-started-with-bpftrace/</url>
    <content><![CDATA[<h2 id="Intro-to-bpftrace"><a href="#Intro-to-bpftrace" class="headerlink" title="Intro to bpftrace"></a>Intro to bpftrace</h2><p><a href="https://github.com/iovisor/bpftrace">bpftrace</a> is a high-level tracing language for Linux enhanced Berkeley Packet Filter (eBPF) available in recent Linux kernels (4.x). bpftrace uses LLVM as a backend to compile scripts to BPF-bytecode and makes use of BCC for interacting with the Linux BPF system, as well as existing Linux tracing capabilities: kernel dynamic tracing (kprobes), user-level dynamic tracing (uprobes), and tracepoints. The bpftrace language is inspired by awk and C, and predecessor tracers such as DTrace and SystemTap. bpftrace was created by Alastair Robertson.</p>
<h2 id="bpftrace-package-install"><a href="#bpftrace-package-install" class="headerlink" title="bpftrace package install"></a>bpftrace package install</h2><pre><code>$ curl https://repos.baslab.org/rhel/7/bpftools/bpftools.repo --output /etc/yum.repos.d/bpftools.repo
 
$ yum install bpftrace bpftrace-tools bpftrace-doc bcc-static bcc-tools
Installed:
  bcc-static.x86_64 0:0.21.0-1.el7  bpftrace.x86_64 0:0.13.0-2.el7  bpftrace-doc.noarch 0:0.13.0-2.el7   bpftrace-tools.noarch 0:0.13.0-2.el7
 
Updated:
  bcc-tools.x86_64 0:0.21.0-1.el7
 
Dependency Updated:
  bcc.x86_64 0:0.21.0-1.el7  python-bcc.noarch 0:0.21.0-1.el7
</code></pre>
<p>Bpftrace ships with many ready-to-run tools after installation.</p>
<pre><code>$ cd /usr/share/bpftrace/tools
$ ls
bashreadline.bt  biostacks.bt  cpuwalk.bt  execsnoop.bt       killsnoop.bt  naptime.bt    pidpersec.bt  setuids.bt    syncsnoop.bt  tcpconnect.bt  tcpretrans.bt   vfscount.bt   xfsdist.bt biolatency.bt    bitesize.bt   dcsnoop.bt  ext4dist.bt        loads.bt      oomkill.bt    runqlat.bt    statsnoop.bt  syscount.bt   tcpdrop.bt     tcpsynbl.bt     vfsstat.bt
biosnoop.bt      capable.bt    doc         gethostlatency.bt  mdflush.bt    opensnoop.bt  runqlen.bt    swapin.bt     tcpaccept.bt  tcplife.bt     threadsnoop.bt  writeback.bt
</code></pre>
<h2 id="A-first-look-at-bpftrace-tracing-system-call-open"><a href="#A-first-look-at-bpftrace-tracing-system-call-open" class="headerlink" title="A first look at bpftrace: tracing system call open()"></a>A first look at bpftrace: tracing system call open()</h2><p>Run the bpftrace program at the command line(a one-liner):</p>
<pre><code>$ bpftrace -e &#39;tracepoint:syscalls:sys_enter_open &#123; printf(&quot;%s %s\n&quot;, comm, str(args-&gt;filename)); &#125;&#39;
Attaching 1 probe...
</code></pre>
<p>From a different terminal, start a <em>iostat</em> process to be traced:</p>
<pre><code>$ iostat -ktdx 2
</code></pre>
<p>Monitor the tracing output:</p>
<pre><code>$ bpftrace -e &#39;tracepoint:syscalls:sys_enter_open &#123; printf(&quot;%s %s\n&quot;, comm, str(args-&gt;filename)); &#125;&#39;
&lt;omitted..&gt;
iostat /proc/uptime
iostat /proc/stat
iostat /proc/diskstats
iostat /proc/uptime
iostat /proc/stat
iostat /proc/diskstats
iostat /proc/uptime
iostat /proc/stat
iostat /proc/diskstats
^C
</code></pre>
<p>The output shows the process name and the filename passed to the open syscall system-wide. In the above example, the <em>iostat</em> process opens the following files every 2 seconds.</p>
<pre><code>/proc/uptime
/proc/stat
/proc/diskstats
</code></pre>
<p>List all the open tracepoints:</p>
<pre><code>$ bpftrace -l &#39;tracepoint:syscalls:sys_enter_open*&#39;
tracepoint:syscalls:sys_enter_open
tracepoint:syscalls:sys_enter_open_by_handle_at
tracepoint:syscalls:sys_enter_open_tree
tracepoint:syscalls:sys_enter_openat
tracepoint:syscalls:sys_enter_openat2
</code></pre>
<p>Count the open(and variant) syscalls:</p>
<pre><code>$ bpftrace -e &#39;tracepoint:syscalls:sys_enter_open* &#123; @[probe]=count();&#125;&#39;
Attaching 5 probes...
^C
@[tracepoint:syscalls:sys_enter_open]: 66
@[tracepoint:syscalls:sys_enter_openat]: 3963
</code></pre>
<p>Bpftrace ships with opensnoop.bt which traces both the start and end of open&#x2F;openat syscall.</p>
<pre><code>$ cat opensnoop.bt
#!/usr/bin/bpftrace
/*
 * opensnoop        Trace open() syscalls.
 *                For Linux, uses bpftrace and eBPF.
 *
 * Also a basic example of bpftrace.
 *
 * USAGE: opensnoop.bt
 *
 * This is a bpftrace version of the bcc tool of the same name.
 *
 * Copyright 2018 Netflix, Inc.
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;)
 *
 * 08-Sep-2018        Brendan Gregg        Created this.
 */

BEGIN
&#123;
    printf(&quot;Tracing open syscalls... Hit Ctrl-C to end.\n&quot;);
    printf(&quot;%-6s %-16s %4s %3s %s\n&quot;, &quot;PID&quot;, &quot;COMM&quot;, &quot;FD&quot;, &quot;ERR&quot;, &quot;PATH&quot;);
&#125;

tracepoint:syscalls:sys_enter_open,
tracepoint:syscalls:sys_enter_openat
&#123;
    @filename[tid] = args-&gt;filename;
&#125;

tracepoint:syscalls:sys_exit_open,
tracepoint:syscalls:sys_exit_openat
/@filename[tid]/
&#123;
    $ret = args-&gt;ret;
    $fd = $ret &gt; 0 ? $ret : -1;
    $errno = $ret &gt; 0 ? 0 : - $ret;
    printf(&quot;%-6d %-16s %4d %3d %s\n&quot;, pid, comm, $fd, $errno, str(@filename[tid]));
    delete(@filename[tid]);
&#125;

END
&#123;
    clear(@filename);
&#125;
</code></pre>
<p>It outputs process id, command, fd and the opened file path.</p>
<pre><code>$ ./opensnoop.bt | egrep &quot;PID|iostat&quot;
PID    COMM               FD ERR PATH
28992  iostat              3   0 /etc/ld.so.cache
28992  iostat              3   0 /lib64/libc.so.6
28992  iostat              3   0 /usr/lib/locale/locale-archive
28992  iostat              3   0 /sys/devices/system/cpu
28992  iostat              3   0 /proc/diskstats
28992  iostat              3   0 /etc/localtime
28992  iostat              3   0 /proc/uptime
28992  iostat              3   0 /proc/stat
28992  iostat              3   0 /proc/diskstats
28992  iostat              4   0 /etc/sysconfig/sysstat.ioconf
28992  iostat              3   0 /proc/uptime
28992  iostat              3   0 /proc/stat
28992  iostat              3   0 /proc/diskstats
^C
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/iovisor/bpftrace/blob/master/INSTALL.md#CentOS-package">https://github.com/iovisor/bpftrace/blob/master/INSTALL.md#CentOS-package</a></li>
<li><a href="https://github.com/fbs/el7-bpf-specs/blob/master/README.md#repository">https://github.com/fbs/el7-bpf-specs/blob/master/README.md#repository</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Bpftrace</tag>
      </tags>
  </entry>
  <entry>
    <title>Getting started with Elasticsearch and Kibana</title>
    <url>/blog/getting-started-with-elasticsearch/</url>
    <content><![CDATA[<h2 id="Install-elasticsearch"><a href="#Install-elasticsearch" class="headerlink" title="Install elasticsearch"></a>Install elasticsearch</h2><h3 id="Add-user-for-elasticsearch"><a href="#Add-user-for-elasticsearch" class="headerlink" title="Add user for elasticsearch"></a>Add user for elasticsearch</h3><pre><code>[root@vm1 home]# groupadd es
[root@vm1 home]# useradd es -g es
[root@vm1 home]# passwd es
[root@vm1 home]# cd es
</code></pre>
<h3 id="Download-elasticsearch"><a href="#Download-elasticsearch" class="headerlink" title="Download elasticsearch"></a>Download elasticsearch</h3><pre><code>[root@vm1 es]# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.4.3-linux-x86_64.tar.gz
[root@vm1 es]# tar zxf elasticsearch-8.4.3-linux-x86_64.tar.gz
[root@vm1 es]# chown -R es:es /home/es
[root@vm1 es]# su es
[es@vm1 ~]$ cd elasticsearch-8.4.3/
</code></pre>
<h3 id="Start-elasticsearch"><a href="#Start-elasticsearch" class="headerlink" title="Start elasticsearch"></a>Start elasticsearch</h3><pre><code>[es@vm1 elasticsearch-8.4.3]$ bin/elasticsearch
[...]
[2022-10-17T17:07:49,984][INFO ][o.e.h.AbstractHttpServerTransport] [vm1] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[::]:9200&#125;
[...]

✅ Elasticsearch security features have been automatically configured!
✅ Authentication is enabled and cluster connections are encrypted.

ℹ️  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
  [...]

ℹ️  HTTP CA certificate SHA-256 fingerprint:
  [...]

ℹ️  Configure Kibana to use this cluster:
• Run Kibana and click the configuration link in the terminal when Kibana starts.
• Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
  [...]

ℹ️  Configure other nodes to join this cluster:
• On this node:
  ⁃ Create an enrollment token with `bin/elasticsearch-create-enrollment-token -s node`.
  ⁃ Uncomment the transport.host setting at the end of config/elasticsearch.yml.
  ⁃ Restart Elasticsearch.
• On other nodes:
  ⁃ Start Elasticsearch with `bin/elasticsearch --enrollment-token &lt;token&gt;`, using the enrollment token that you generated.
</code></pre>
<h3 id="Verify-elasticsearch"><a href="#Verify-elasticsearch" class="headerlink" title="Verify elasticsearch"></a>Verify elasticsearch</h3><pre><code>[root@vm2 es]# curl 10.10.10.1:9200
curl: (52) Empty reply from server
</code></pre>
<p>Modify elastic config file:</p>
<pre><code>[es@vm1 elasticsearch-8.4.3]$ vim config/elasticsearch.yml
# By default Elasticsearch is only accessible on localhost. Set a different
# address here to expose this node on the network:
#
#network.host: 192.168.0.1
network.host: 10.10.10.1

# Enable security features
xpack.security.enabled: false
</code></pre>
<p>Restart elasticsearch process and verify again:</p>
<pre><code>[root@vm2 es]# curl 10.10.10.1:9200
&#123;
  &quot;name&quot; : &quot;vm1&quot;,
  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,
  &quot;cluster_uuid&quot; : &quot;ZZ_MBiS5Qi-3RFSdyk_-Kg&quot;,
  &quot;version&quot; : &#123;
    &quot;number&quot; : &quot;8.4.3&quot;,
    &quot;build_flavor&quot; : &quot;default&quot;,
    &quot;build_type&quot; : &quot;tar&quot;,
    &quot;build_hash&quot; : &quot;42f05b9372a9a4a470db3b52817899b99a76ee73&quot;,
    &quot;build_date&quot; : &quot;2022-10-04T07:17:24.662462378Z&quot;,
    &quot;build_snapshot&quot; : false,
    &quot;lucene_version&quot; : &quot;9.3.0&quot;,
    &quot;minimum_wire_compatibility_version&quot; : &quot;7.17.0&quot;,
    &quot;minimum_index_compatibility_version&quot; : &quot;7.0.0&quot;
  &#125;,
  &quot;tagline&quot; : &quot;You Know, for Search&quot;
&#125;
</code></pre>
<h2 id="Install-Kibana"><a href="#Install-Kibana" class="headerlink" title="Install Kibana"></a>Install Kibana</h2><h3 id="Download-Kibana"><a href="#Download-Kibana" class="headerlink" title="Download Kibana"></a>Download Kibana</h3><pre><code>[root@vm2 es]# wget https://artifacts.elastic.co/downloads/kibana/kibana-8.4.3-linux-x86_64.tar.gz
[root@vm2 es]# tar zxf kibana-8.4.3-linux-x86_64.tar.gz
[root@vm2 es]# cd kibana-8.4.3/
[root@vm2 kibana-8.4.3]# chown -R es:es /home/es
</code></pre>
<h3 id="Start-Kibana"><a href="#Start-Kibana" class="headerlink" title="Start Kibana"></a>Start Kibana</h3><pre><code>[root@vm2 kibana-8.4.3]# bin/kibana
Kibana should not be run as root.  Use --allow-root to continue.

[root@vm2 kibana-8.4.3]# su es
[es@vm2 kibana-8.4.3]$ bin/kibana
[2022-10-17T17:41:59.539-07:00][INFO ][node] Kibana process configured with roles: [background_tasks, ui]
[2022-10-17T17:42:06.604-07:00][INFO ][http.server.Preboot] http server running at http://localhost:5601
[2022-10-17T17:42:06.644-07:00][INFO ][plugins-system.preboot] Setting up [1] plugins: [interactiveSetup]
[2022-10-17T17:42:06.646-07:00][INFO ][preboot] &quot;interactiveSetup&quot; plugin is holding setup: Validating Elasticsearch connection configuration…
[2022-10-17T17:42:06.681-07:00][INFO ][root] Holding setup until preboot stage is completed.

i Kibana has not been configured.

Go to http://localhost:5601/?code=263178 to get started.
</code></pre>
<p>Allow connections from remote users:</p>
<pre><code>[root@vm2 kibana-8.4.3]# vim config/kibana.yml
# To allow connections from remote users, set this parameter to a non-loopback address.
#server.host: &quot;localhost&quot;
server.host: &quot;10.10.10.2&quot;
</code></pre>
<p>Restart the kibana to reflect the change:</p>
<pre><code>[root@vm2 kibana-8.4.3]# su es
[es@vm2 kibana-8.4.3]$ bin/kibana
[...]
Go to http://10.10.10.2:5601/?code=293334 to get started.
</code></pre>
<h2 id="Connect-Kibana-to-Elasticsearch"><a href="#Connect-Kibana-to-Elasticsearch" class="headerlink" title="Connect Kibana to Elasticsearch"></a>Connect Kibana to Elasticsearch</h2><p>From the Browser, enter “<a href="http://10.10.10.2:5601/?code=293334">http://10.10.10.2:5601/?code=293334</a>“.</p>
<p>If you run into the following issue when to connect to elasticsearch server from kibana web UI, you can change the URL from “<a href="https://10.10.10.1:9200/">https://10.10.10.1:9200</a>“ to “<a href="http://10.10.10.1:9200/">http://10.10.10.1:9200</a>“ for testing purpose.</p>
<pre><code>[2022-11-09T10:55:02.691-08:00][ERROR][plugins.interactiveSetup.elasticsearch] Unable to connect to host &quot;https://10.10.10.1:9200&quot;: write EPROTO 139880583923648:error:1408F10B:SSL routines:ssl3_get_record:wrong version number:../deps/openssl/openssl/ssl/record/ssl3_record.c:332:
</code></pre>
<h3 id="Use-Dev-Tools-in-Kibana"><a href="#Use-Dev-Tools-in-Kibana" class="headerlink" title="Use Dev Tools in Kibana"></a>Use Dev Tools in Kibana</h3><p><img src="/images/kifana_dev_tools.png" alt="Image"></p>
<pre><code>GET _cluster/health
&#123;
  &quot;cluster_name&quot;: &quot;elasticsearch&quot;,
  &quot;status&quot;: &quot;green&quot;,
  &quot;timed_out&quot;: false,
  &quot;number_of_nodes&quot;: 1,
  &quot;number_of_data_nodes&quot;: 1,
  &quot;active_primary_shards&quot;: 10,
  &quot;active_shards&quot;: 10,
  &quot;relocating_shards&quot;: 0,
  &quot;initializing_shards&quot;: 0,
  &quot;unassigned_shards&quot;: 0,
  &quot;delayed_unassigned_shards&quot;: 0,
  &quot;number_of_pending_tasks&quot;: 0,
  &quot;number_of_in_flight_fetch&quot;: 0,
  &quot;task_max_waiting_in_queue_millis&quot;: 0,
  &quot;active_shards_percent_as_number&quot;: 100
&#125;
GET _nodes/stats
&#123;
  &quot;_nodes&quot;: &#123;
    &quot;total&quot;: 1,
    &quot;successful&quot;: 1,
    &quot;failed&quot;: 0
  &#125;,
  &quot;cluster_name&quot;: &quot;elasticsearch&quot;,
  &quot;nodes&quot;: &#123;
    [...]
  &#125;
&#125;
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.elastic.co/downloads/">https://www.elastic.co/downloads/</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Elastic Search</tag>
      </tags>
  </entry>
  <entry>
    <title>Getting started with Elastic Rally benchmark</title>
    <url>/blog/getting-started-with-esrally/</url>
    <content><![CDATA[<p>Rally is the macrobenchmarking framework for Elasticsearch. This post follows the instructions <a href="https://esrally.readthedocs.io/en/stable/install.html">here</a> to install Rally and run the very first benchmark(aka race).</p>
<h3 id="Install-python"><a href="#Install-python" class="headerlink" title="Install python"></a>Install python</h3><p>Python 3.8+ including pip3 is required for Rally.</p>
<pre><code>$ yum update
$ yum install openssl-devel bzip2-devel libffi-devel
$ yum groupinstall &quot;Development Tools&quot;

$ curl -O https://www.python.org/ftp/python/3.8.1/Python-3.8.1.tgz
$ tar zxf Python-3.8.1.tgz
$ mv Python-3.8.1 /usr/src
$ cd /usr/src/Python-3.8.1/
$ vim Modules/Setup 
SSL=/usr/local/ssl
_ssl _ssl.c \
        -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
        -L$(SSL)/lib -lssl -lcrypto

$ ./configure --enable-optimizations
$ make altinstall
$ python3.8 -m ssl

$ pip3 -V
pip 22.3 from /usr/local/lib/python3.8/site-packages/pip (python 3.8)
</code></pre>
<p><strong>Note</strong>: If you do not uncomment the 4 lines in Modules&#x2F;Setup, you would fail to install Rally with the ssl module unavailable issue as mentioned in the following troubleshooting section.</p>
<h3 id="Install-git"><a href="#Install-git" class="headerlink" title="Install git"></a>Install git</h3><p>Git is not required if all of the following conditions are met:</p>
<ul>
<li><p>You are using Rally only as a load generator (–pipeline&#x3D;benchmark-only) or you are referring to Elasticsearch configurations with –team-path.</p>
</li>
<li><p>You create your own tracks and refer to them with –track-path.</p>
</li>
<li><p>In all other cases, Rally requires git 1.9 or better. Verify with git –version</p>
<p>  $ yum -y remove git<br>  $ yum -y remove git-*<br>  $ yum install git<br>  $ git version<br>  git version 2.38.1</p>
</li>
</ul>
<h2 id="Install-JDK"><a href="#Install-JDK" class="headerlink" title="Install JDK"></a>Install JDK</h2><p>A JDK is required on all machines where you want to launch Elasticsearch. If you use Rally just as a load generator to benchmark remote clusters, no JDK is required. Refer to <a href="https://www.elastic.co/support/matrix#matrix_jvm">here</a> to determine the appropriate JDK version to run Elasticsearch.</p>
<pre><code>$ yum install java
$ java -version
openjdk version &quot;1.8.0_345&quot;
OpenJDK Runtime Environment (build 1.8.0_345-b01)
OpenJDK 64-Bit Server VM (build 25.345-b01, mixed mode)
</code></pre>
<p>To <a href="https://www.oracle.com/java/technologies/javase/jdk17-archive-downloads.html">download</a> and install a specific java version:</p>
<pre><code>$ wget https://download.oracle.com/java/17/archive/jdk-17.0.5_linux-x64_bin.rpm
$ rpm -ivh jdk-17.0.5_linux-x64_bin.rpm
$ java -version
java version &quot;17.0.5&quot; 2022-10-18 LTS
Java(TM) SE Runtime Environment (build 17.0.5+9-LTS-191)
Java HotSpot(TM) 64-Bit Server VM (build 17.0.5+9-LTS-191, mixed mode, sharing)

$ rpm -qi jdk-17-17.0.5-ga.x86_64
Name        : jdk-17
Epoch       : 2000
Version     : 17.0.5
Release     : ga
Architecture: x86_64
Install Date: Wed 02 Nov 2022 03:58:42 PM PDT
Group       : Development/Tools
Size        : 316751437
License     : https://java.com/freeuselicense
Signature   : RSA/SHA256, Tue 13 Sep 2022 09:36:17 AM PDT, Key ID 72f97b74ec551f03
Source RPM  : jdk-17-17.0.5-ga.src.rpm
Build Date  : Tue 13 Sep 2022 09:35:27 AM PDT
Build Host  : java.com
Relocations : /usr/java
Vendor      : Oracle Corporation
URL         : http://www.oracle.com/technetwork/java/javase/overview/index.html
Summary     : Java Platform Standard Edition Development Kit
Description :
The Java Platform Standard Edition Development Kit (JDK) includes both
the runtime environment (Java virtual machine, the Java platform classes
and supporting files) and development tools (compilers, debuggers,
tool libraries and other tools).
</code></pre>
<p>With java 1.8.0, you can run Elasticsearch 7.17.x or lower version.</p>
<p>To find the JDK, Rally expects the environment variable JAVA_HOME to be set on all targeted machines. To have more specific control, for example when you want to benchmark across a wide range of Elasticsearch releases, you can also set JAVAx_HOME where x is the major version of a JDK (e.g. JAVA8_HOME would point to a JDK 8 installation). Rally will then choose the highest supported JDK per version of Elasticsearch that is available.</p>
<pre><code>$ export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
$ echo $JAVA_HOME
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
</code></pre>
<h2 id="Install-Rally"><a href="#Install-Rally" class="headerlink" title="Install Rally"></a>Install Rally</h2><pre><code>$ pip3.8 install --upgrade pip

$ pip3.8 install esrally

$ esrally -h
usage: esrally [-h] [--version] &#123;race,list,info,create-track,generate,compare,download,install,start,stop&#125; ...

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

 You Know, for Benchmarking Elasticsearch.

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&#39;s version number and exit

subcommands:
  &#123;race,list,info,create-track,generate,compare,download,install,start,stop&#125;
    race                Run a benchmark
    list                List configuration options
    info                Show info about a track
    create-track        Create a Rally track from existing data
    generate            Generate artifacts
    compare             Compare two races
    download            Downloads an artifact
    install             Installs an Elasticsearch node locally
    start               Starts an Elasticsearch node locally
    stop                Stops an Elasticsearch node locally

Find out more about Rally at https://esrally.readthedocs.io/en/2.6.0/


$ esrally list tracks

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available tracks:

Name              Description                                                              Documents    Compressed Size    Uncompressed Size    Default Challenge        All Challenges
----------------  -----------------------------------------------------------------------  -----------  -----------------  -------------------  -----------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
dense_vector      Benchmark for dense vector indexing and search                           10,000,000   7.2 GB             19.5 GB              index-and-search         index-and-search
elastic/endpoint  Endpoint track                                                           0            0 bytes            0 bytes              default                  default
elastic/logs      Track for simulating logging workloads                                   14,009,078   N/A                N/A                  logging-indexing         cross-clusters-search,logging-disk-usage,logging-indexing-querying,logging-indexing,logging-querying,logging-snapshot-mount,logging-snapshot-restore,logging-snapshot,many-shards-quantitative,many-shards-snapshots
elastic/security  Track for simulating Elastic Security workloads                          77,513,777   N/A                N/A                  security-querying        index-alert-source-events,security-indexing-querying,security-indexing,security-querying
eql               EQL benchmarks based on endgame index of SIEM demo cluster               60,782,211   4.5 GB             109.2 GB             default                  default,index-sorting
geonames          POIs from Geonames                                                       11,396,503   252.9 MB           3.3 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts,significant-text
geopoint          Point coordinates from PlanetOSM                                         60,844,404   482.1 MB           2.3 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts
geopointshape     Point coordinates from PlanetOSM indexed as geoshapes                    60,844,404   470.8 MB           2.6 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts
geoshape          Shapes from PlanetOSM                                                    84,220,567   17.0 GB            58.7 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-big
http_logs         HTTP server log data                                                     247,249,096  1.2 GB             31.1 GB              append-no-conflicts      append-no-conflicts,runtime-fields,append-no-conflicts-index-only,append-sorted-no-conflicts,append-index-only-with-ingest-pipeline,update,append-no-conflicts-index-reindex-only
metricbeat        Metricbeat data                                                          1,079,600    87.7 MB            1.2 GB               append-no-conflicts      append-no-conflicts
nested            StackOverflow Q&amp;A stored as nested docs                                  11,203,029   663.3 MB           3.4 GB               nested-search-challenge  nested-search-challenge,index-only
noaa              Global daily weather measurements from NOAA                              33,659,481   949.4 MB           9.0 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,aggs,filter-aggs
nyc_taxis         Taxi rides in New York in 2015                                           165,346,692  4.5 GB             74.3 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts-index-only,update,append-ml,aggs
percolator        Percolator benchmark based on AOL queries                                2,000,000    121.1 kB           104.9 MB             append-no-conflicts      append-no-conflicts
pmc               Full text benchmark with academic papers from PMC                        574,199      5.5 GB             21.7 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts,append-fast-with-conflicts,indexing-querying
so                Indexing benchmark using up to questions and answers from StackOverflow  36,062,278   8.9 GB             33.1 GB              append-no-conflicts      append-no-conflicts,transform,frequent-items
so_vector         Benchmark for vector search with StackOverflow data                      2,000,000    12.3 GB            32.2 GB              index-and-search         index-and-search
sql               SQL query performance based on NOAA Weather data                         33,659,481   949.4 MB           9.0 GB               sql                      sql
tsdb              metricbeat information for elastic-app k8s cluster                       116,633,698  N/A                123.0 GB             append-no-conflicts      append-no-conflicts,downsample

-------------------------------
[INFO] SUCCESS (took 3 seconds)
-------------------------------


$ esrally list cars

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available cars:

Name                     Type    Description
-----------------------  ------  --------------------------------------
16gheap                  car     Sets the Java heap to 16GB
1gheap                   car     Sets the Java heap to 1GB
24gheap                  car     Sets the Java heap to 24GB
2gheap                   car     Sets the Java heap to 2GB
4gheap                   car     Sets the Java heap to 4GB
8gheap                   car     Sets the Java heap to 8GB
defaults                 car     Sets the Java heap to 1GB
basic-license            mixin   Basic License
debug-non-safepoints     mixin   More accurate CPU profiles
ea                       mixin   Enables Java assertions
fp                       mixin   Preserves frame pointers
g1gc                     mixin   Enables the G1 garbage collector
parallelgc               mixin   Enables the Parallel garbage collector
trial-license            mixin   Trial License
unpooled                 mixin   Enables Netty&#39;s unpooled allocator
x-pack-ml                mixin   X-Pack Machine Learning
x-pack-monitoring-http   mixin   X-Pack Monitoring (HTTP exporter)
x-pack-monitoring-local  mixin   X-Pack Monitoring (local exporter)
x-pack-security          mixin   X-Pack Security
zgc                      mixin   Enables the ZGC garbage collector

-------------------------------
[INFO] SUCCESS (took 6 seconds)
-------------------------------
</code></pre>
<h2 id="Run-the-first-race-with-Rally"><a href="#Run-the-first-race-with-Rally" class="headerlink" title="Run the first race with Rally"></a>Run the first race with Rally</h2><p>A “race” in Rally is the execution of a benchmarking experiment. You can choose different benchmarking scenarios (called tracks) for your benchmarks.</p>
<p>Rally should be run as a non-root user. We create a user “es” to run the following race.</p>
<pre><code>$ groupadd es
$ useradd es -g es
$ passwd es
$ cd /home/es


$ su - es
$ export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
$ esrally race --distribution-version=7.17.0 --track=geonames
    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Race id is [d3102b91-ac10-4383-b6a4-7b98d2831af7]
[INFO] Preparing for race ...
[INFO] Downloading Elasticsearch 7.17.0 (297.0 MB total size)                       [100%]
[INFO] Downloading track data (252.9 MB total size)                               [100.0%]
[INFO] Decompressing track data from [/home/es/.rally/benchmarks/data/geonames/documents-2.json.bz2] to [/home/es/.rally/benchmarks/data/geonames/documents-2.json] (resulting size: [3.30] GB) ...  [OK]
[INFO] Preparing file offset table for [/home/es/.rally/benchmarks/data/geonames/documents-2.json] ... [OK]
[INFO] Racing on track [geonames], challenge [append-no-conflicts] and car [&#39;defaults&#39;] with version [7.17.0].

Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [100% done]
Running refresh-after-index                                                    [100% done]
Running force-merge                                                            [100% done]
Running refresh-after-force-merge                                              [100% done]
Running wait-until-merges-finish                                               [100% done]
Running index-stats                                                            [100% done]
Running node-stats                                                             [100% done]
Running default                                                                [100% done]
Running term                                                                   [100% done]
Running phrase                                                                 [100% done]
Running country_agg_uncached                                                   [100% done]
Running country_agg_cached                                                     [100% done]
Running scroll                                                                 [100% done]
Running expression                                                             [100% done]
Running painless_static                                                        [100% done]
Running painless_dynamic                                                       [100% done]
Running decay_geo_gauss_function_score                                         [100% done]
Running decay_geo_gauss_script_score                                           [100% done]
Running field_value_function_score                                             [100% done]
Running field_value_script_score                                               [100% done]
Running large_terms                                                            [100% done]
Running large_filtered_terms                                                   [100% done]
Running large_prohibited_terms                                                 [100% done]
Running desc_sort_population                                                   [100% done]
Running asc_sort_population                                                    [100% done]
Running asc_sort_with_after_population                                         [100% done]
Running desc_sort_geonameid                                                    [100% done]
Running desc_sort_with_after_geonameid                                         [100% done]
Running asc_sort_geonameid                                                     [100% done]
Running asc_sort_with_after_geonameid                                          [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                         Metric |                           Task |           Value |    Unit |
|---------------------------------------------------------------:|-------------------------------:|----------------:|--------:|
|                     Cumulative indexing time of primary shards |                                |    12.7087      |     min |
|             Min cumulative indexing time across primary shards |                                |     0.0196167   |     min |
|          Median cumulative indexing time across primary shards |                                |     2.5376      |     min |
|             Max cumulative indexing time across primary shards |                                |     2.58153     |     min |
|            Cumulative indexing throttle time of primary shards |                                |     0.0108667   |     min |
|    Min cumulative indexing throttle time across primary shards |                                |     0           |     min |
| Median cumulative indexing throttle time across primary shards |                                |     0           |     min |
|    Max cumulative indexing throttle time across primary shards |                                |     0.00731667  |     min |
|                        Cumulative merge time of primary shards |                                |     7.13778     |     min |
|                       Cumulative merge count of primary shards |                                |    52           |         |
|                Min cumulative merge time across primary shards |                                |     0           |     min |
|             Median cumulative merge time across primary shards |                                |     1.20084     |     min |
|                Max cumulative merge time across primary shards |                                |     2.20237     |     min |
|               Cumulative merge throttle time of primary shards |                                |     1.59453     |     min |
|       Min cumulative merge throttle time across primary shards |                                |     0           |     min |
|    Median cumulative merge throttle time across primary shards |                                |     0.2181      |     min |
|       Max cumulative merge throttle time across primary shards |                                |     0.569333    |     min |
|                      Cumulative refresh time of primary shards |                                |     2.89785     |     min |
|                     Cumulative refresh count of primary shards |                                |   256           |         |
|              Min cumulative refresh time across primary shards |                                |     0.00186667  |     min |
|           Median cumulative refresh time across primary shards |                                |     0.577325    |     min |
|              Max cumulative refresh time across primary shards |                                |     0.599333    |     min |
|                        Cumulative flush time of primary shards |                                |     0.2239      |     min |
|                       Cumulative flush count of primary shards |                                |    14           |         |
|                Min cumulative flush time across primary shards |                                |     0.0023      |     min |
|             Median cumulative flush time across primary shards |                                |     0.0471833   |     min |
|                Max cumulative flush time across primary shards |                                |     0.05055     |     min |
|                                        Total Young Gen GC time |                                |    18.57        |       s |
|                                       Total Young Gen GC count |                                |  2243           |         |
|                                          Total Old Gen GC time |                                |     3.541       |       s |
|                                         Total Old Gen GC count |                                |    66           |         |
|                                                     Store size |                                |     2.82211     |      GB |
|                                                  Translog size |                                |     3.07336e-07 |      GB |
|                                         Heap used for segments |                                |     0.733753    |      MB |
|                                       Heap used for doc values |                                |     0.0489769   |      MB |
|                                            Heap used for terms |                                |     0.557007    |      MB |
|                                            Heap used for norms |                                |     0.0753784   |      MB |
|                                           Heap used for points |                                |     0           |      MB |
|                                    Heap used for stored fields |                                |     0.0523911   |      MB |
|                                                  Segment count |                                |   103           |         |
|                                    Total Ingest Pipeline count |                                |     0           |         |
|                                     Total Ingest Pipeline time |                                |     0           |       s |
|                                   Total Ingest Pipeline failed |                                |     0           |         |
|                                                 Min Throughput |                   index-append | 87315.7         |  docs/s |
|                                                Mean Throughput |                   index-append | 87373.7         |  docs/s |
|                                              Median Throughput |                   index-append | 87368.1         |  docs/s |
|                                                 Max Throughput |                   index-append | 87440.9         |  docs/s |
|                                        50th percentile latency |                   index-append |   316.067       |      ms |
|                                        90th percentile latency |                   index-append |   458.448       |      ms |
|                                        99th percentile latency |                   index-append |  1152.53        |      ms |
|                                       100th percentile latency |                   index-append |  1316.07        |      ms |
|                                   50th percentile service time |                   index-append |   316.067       |      ms |
|                                   90th percentile service time |                   index-append |   458.448       |      ms |
|                                   99th percentile service time |                   index-append |  1152.53        |      ms |
|                                  100th percentile service time |                   index-append |  1316.07        |      ms |
|                                                     error rate |                   index-append |     0           |       % |
|                                                 Min Throughput |                    index-stats |    89.91        |   ops/s |
|                                                Mean Throughput |                    index-stats |    89.95        |   ops/s |
|                                              Median Throughput |                    index-stats |    89.95        |   ops/s |
|                                                 Max Throughput |                    index-stats |    89.97        |   ops/s |
|                                        50th percentile latency |                    index-stats |     4.36948     |      ms |
|                                        90th percentile latency |                    index-stats |     5.06188     |      ms |
|                                        99th percentile latency |                    index-stats |     5.51726     |      ms |
|                                      99.9th percentile latency |                    index-stats |     7.79772     |      ms |
|                                       100th percentile latency |                    index-stats |     9.64821     |      ms |
|                                   50th percentile service time |                    index-stats |     3.16338     |      ms |
|                                   90th percentile service time |                    index-stats |     3.67796     |      ms |
|                                   99th percentile service time |                    index-stats |     3.86689     |      ms |
|                                 99.9th percentile service time |                    index-stats |     4.13559     |      ms |
|                                  100th percentile service time |                    index-stats |     6.9374      |      ms |
[..]                                                 

----------------------------------
[INFO] SUCCESS (took 4199 seconds)
----------------------------------
</code></pre>
<p>You can save this report also to a file by using –report-file&#x3D;&#x2F;path&#x2F;to&#x2F;your&#x2F;report.md and save it as CSV with –report-format&#x3D;csv.</p>
<p>What did Rally just do?</p>
<ul>
<li>It downloaded and started Elasticsearch 7.17.0</li>
<li>It downloaded the relevant data for the geonames track</li>
<li>It ran the actual benchmark</li>
<li>And finally it reported the results</li>
</ul>
<h2 id="Rally-Configuration"><a href="#Rally-Configuration" class="headerlink" title="Rally Configuration"></a>Rally Configuration</h2><p>Rally stores its configuration in the file .rally&#x2F;rally.ini which is automatically created the first time Rally is executed.</p>
<pre><code>$ pwd
/home/es/.rally/benchmarks
$ ls
data  distributions  races  teams  tracks
$ ls distributions/
elasticsearch-7.17.0-linux-x86_64.tar.gz

$ ls tracks/default/
download.sh  elastic  eql  geonames  geopoint  geopointshape  geoshape  http_logs  metricbeat  nested  noaa  nyc_taxis  percolator  pmc  README.md  so  sql

$ ls races/d3102b91-ac10-4383-b6a4-7b98d2831af7/
race.json  rally-node-0

$ ls teams/default/
cars  LICENSE  NOTICE  plugins  README.md

$ ls -l data/geonames/
total 3723472
-rw-rw-r-- 1 es es 3547613828 Oct 27 17:01 documents-2.json
-rw-rw-r-- 1 es es  265208777 Oct 27 17:00 documents-2.json.bz2
-rw-rw-r-- 1 es es       4250 Oct 27 17:02 documents-2.json.offset


$ cat /home/es/.rally/rally.ini
[meta]
config.version = 17

[system]
env.name = local

[node]
root.dir = /home/es/.rally/benchmarks
src.root.dir = /home/es/.rally/benchmarks/src

[source]
remote.repo.url = https://github.com/elastic/elasticsearch.git
elasticsearch.src.subdir = elasticsearch

[benchmarks]
local.dataset.cache = /home/es/.rally/benchmarks/data

[reporting]
datastore.type = in-memory
datastore.host =
datastore.port =
datastore.secure = False
datastore.user =
datastore.password =


[tracks]
default.url = https://github.com/elastic/rally-tracks

[teams]
default.url = https://github.com/elastic/rally-teams

[defaults]
preserve_benchmark_candidate = false

[distributions]
release.cache = true
</code></pre>
<p>The benchmark data directory can be changed by modifying root.dir in rally.ini.</p>
<ul>
<li>root.dir (default: “~&#x2F;.rally&#x2F;benchmarks”): Rally uses this directory to store all benchmark-related data. It assumes that it has complete control over this directory and any of its subdirectories.</li>
<li>src.root.dir (default: “~&#x2F;.rally&#x2F;benchmarks&#x2F;src”): The directory where the source code of Elasticsearch or any plugins is checked out. Only relevant for benchmarks from sources.</li>
</ul>
<h2 id="Uninstall-python3"><a href="#Uninstall-python3" class="headerlink" title="Uninstall python3"></a>Uninstall python3</h2><p>The following are the optional commands in the case you need to uninstall python3 in CentOS.</p>
<pre><code>$ whereis python3
python3: /usr/bin/python3 /usr/bin/python3.6 /usr/bin/python3.6m /usr/lib/python3.6 /usr/lib64/python3.6 /usr/local/bin/python3.11 /usr/local/bin/python3.11-config /usr/local/lib/python3.11 /usr/include/python3.6m /usr/share/man/man1/python3.1.gz

$ whereis pip3
pip3: /usr/local/bin/pip3 /usr/local/bin/pip3.8 /usr/local/bin/pip3.10

$ rpm -qa | grep python3 --&gt; Only needed if you installed python3 by yum package installer.

$ whereis python3 |xargs rm -frv
$ whereis pip3 |xargs rm -frv
</code></pre>
<h2 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h2><ul>
<li><p>“WARNING: pip is configured with locations that require TLS&#x2F;SSL, however the ssl module in Python is not available.”</p>
<p>  $ pip3.8 install esrally<br>  WARNING: pip is configured with locations that require TLS&#x2F;SSL, however the ssl module in Python is not available.<br>  WARNING: Retrying (Retry(total&#x3D;4, connect&#x3D;None, read&#x3D;None, redirect&#x3D;None, status&#x3D;None)) after connection broken by ‘SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)’: &#x2F;simple&#x2F;esrally&#x2F;<br>  WARNING: Retrying (Retry(total&#x3D;3, connect&#x3D;None, read&#x3D;None, redirect&#x3D;None, status&#x3D;None)) after connection broken by ‘SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)’: &#x2F;simple&#x2F;esrally&#x2F;<br>  WARNING: Retrying (Retry(total&#x3D;2, connect&#x3D;None, read&#x3D;None, redirect&#x3D;None, status&#x3D;None)) after connection broken by ‘SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)’: &#x2F;simple&#x2F;esrally&#x2F;<br>  WARNING: Retrying (Retry(total&#x3D;1, connect&#x3D;None, read&#x3D;None, redirect&#x3D;None, status&#x3D;None)) after connection broken by ‘SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)’: &#x2F;simple&#x2F;esrally&#x2F;<br>  WARNING: Retrying (Retry(total&#x3D;0, connect&#x3D;None, read&#x3D;None, redirect&#x3D;None, status&#x3D;None)) after connection broken by ‘SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)’: &#x2F;simple&#x2F;esrally&#x2F;<br>  Could not fetch URL <a href="https://pypi.org/simple/esrally/">https://pypi.org/simple/esrally/</a>: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host&#x3D;’pypi.org’, port&#x3D;443): Max retries exceeded with url: &#x2F;simple&#x2F;esrally&#x2F; (Caused by SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)) - skipping<br>  ERROR: Could not find a version that satisfies the requirement esrally (from versions: none)<br>  ERROR: No matching distribution found for esrally<br>  WARNING: pip is configured with locations that require TLS&#x2F;SSL, however the ssl module in Python is not available.<br>  Could not fetch URL <a href="https://pypi.org/simple/pip/">https://pypi.org/simple/pip/</a>: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host&#x3D;’pypi.org’, port&#x3D;443): Max retries exceeded with url: &#x2F;simple&#x2F;pip&#x2F; (Caused by SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)) - skipping<br>  WARNING: There was an error checking the latest version of pip.</p>
</li>
</ul>
<p>To fix this, install the python after un-commenting the four lines in Modules&#x2F;Setup.</p>
<pre><code>$ curl -O https://www.python.org/ftp/python/3.8.1/Python-3.8.1.tgz
$ tar zxf Python-3.8.1.tgz
$ mv Python-3.8.1 /usr/src
$ cd /usr/src/Python-3.8.1/
$ vim Modules/Setup 
SSL=/usr/local/ssl
_ssl _ssl.c \
    -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
    -L$(SSL)/lib -lssl -lcrypto

$ ./configure --enable-optimizations
$ make altinstall
$ python3.8 -m ssl
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.elastic.co/blog/announcing-rally-benchmarking-for-elasticsearch?spm=a2c65.11461447.0.0.e26a498cbHyowi">https://www.elastic.co/blog/announcing-rally-benchmarking-for-elasticsearch?spm=a2c65.11461447.0.0.e26a498cbHyowi</a></li>
<li><a href="https://github.com/elastic/rally">https://github.com/elastic/rally</a></li>
<li><a href="https://github.com/elastic/rally-tracks">https://github.com/elastic/rally-tracks</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/quickstart.html">https://esrally.readthedocs.io/en/stable/quickstart.html</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/install.html">https://esrally.readthedocs.io/en/stable/install.html</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/configuration.html">https://esrally.readthedocs.io/en/stable/configuration.html</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/race.html">https://esrally.readthedocs.io/en/stable/race.html</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/faq.html">https://esrally.readthedocs.io/en/stable/faq.html</a></li>
<li><a href="https://www.python.org/ftp/python/">https://www.python.org/ftp/python/</a></li>
<li><a href="https://www.elastic.co/support/matrix#matrix_jvm">https://www.elastic.co/support/matrix#matrix_jvm</a></li>
<li><a href="https://stackoverflow.com/questions/56552390/how-to-fix-ssl-module-in-python-is-not-available-in-centos">https://stackoverflow.com/questions/56552390/how-to-fix-ssl-module-in-python-is-not-available-in-centos</a></li>
<li><a href="https://blog.searchhub.io/how-to-setup-elasticsearch-benchmarking">https://blog.searchhub.io/how-to-setup-elasticsearch-benchmarking</a></li>
<li><a href="https://techviewleo.com/install-java-openjdk-on-rocky-linux-centos/">https://techviewleo.com/install-java-openjdk-on-rocky-linux-centos/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Esrally</tag>
      </tags>
  </entry>
  <entry>
    <title>Getting started with PostgreSQL</title>
    <url>/blog/getting-started-with-postgresql/</url>
    <content><![CDATA[<h2 id="Install-the-PostgreSQL-from-YUM-repository"><a href="#Install-the-PostgreSQL-from-YUM-repository" class="headerlink" title="Install the PostgreSQL from YUM repository"></a>Install the PostgreSQL from YUM repository</h2><p><strong>Install the repository RPM:</strong></p>
<pre><code>$ yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm
</code></pre>
<p><strong>Install PostgreSQL:</strong></p>
<pre><code>$ yum install -y postgresql15-server
</code></pre>
<h2 id="Create-a-Database-Cluster"><a href="#Create-a-Database-Cluster" class="headerlink" title="Create a Database Cluster"></a>Create a Database Cluster</h2><p>Before you can do anything, you must initialize a database storage area on disk. We call this a database cluster.</p>
<pre><code>$ su - postgres

-bash-4.2$ /usr/pgsql-15/bin/pg_ctl --help
pg_ctl is a utility to initialize, start, stop, or control a PostgreSQL server.

Usage:
  pg_ctl init[db]   [-D DATADIR] [-s] [-o OPTIONS]
  pg_ctl start      [-D DATADIR] [-l FILENAME] [-W] [-t SECS] [-s]
                    [-o OPTIONS] [-p PATH] [-c]
  pg_ctl stop       [-D DATADIR] [-m SHUTDOWN-MODE] [-W] [-t SECS] [-s]
  pg_ctl restart    [-D DATADIR] [-m SHUTDOWN-MODE] [-W] [-t SECS] [-s]
                    [-o OPTIONS] [-c]
  pg_ctl reload     [-D DATADIR] [-s]
  pg_ctl status     [-D DATADIR]
  pg_ctl promote    [-D DATADIR] [-W] [-t SECS] [-s]
  pg_ctl logrotate  [-D DATADIR] [-s]
  pg_ctl kill       SIGNALNAME PID

Common options:
  -D, --pgdata=DATADIR   location of the database storage area
  -s, --silent           only print errors, no informational messages
  -t, --timeout=SECS     seconds to wait when using -w option
  -V, --version          output version information, then exit
  -w, --wait             wait until operation completes (default)
  -W, --no-wait          do not wait until operation completes
  -?, --help             show this help, then exit
If the -D option is omitted, the environment variable PGDATA is used.

Options for start or restart:
  -c, --core-files       allow postgres to produce core files
  -l, --log=FILENAME     write (or append) server log to FILENAME
  -o, --options=OPTIONS  command line options to pass to postgres
                         (PostgreSQL server executable) or initdb
  -p PATH-TO-POSTGRES    normally not necessary

Options for stop or restart:
  -m, --mode=MODE        MODE can be &quot;smart&quot;, &quot;fast&quot;, or &quot;immediate&quot;

Shutdown modes are:
  smart       quit after all clients have disconnected
  fast        quit directly, with proper shutdown (default)
  immediate   quit without complete shutdown; will lead to recovery on restart

Allowed signal names for kill:
  ABRT HUP INT KILL QUIT TERM USR1 USR2

Report bugs to &lt;pgsql-bugs@lists.postgresql.org&gt;.
PostgreSQL home page: &lt;https://www.postgresql.org/&gt;
</code></pre>
<p><strong>Create the data directory:</strong></p>
<pre><code>$ mkdir -p /mnt/pgsql15/data
$ chown -R postgres /mnt/pgsql15
</code></pre>
<p><strong>Create the database cluster:</strong></p>
<pre><code>$ su - postgres -c &quot;/usr/pgsql-15/bin/initdb -D /mnt/pgsql15/data&quot;

$ ls /mnt/pgsql15/data
base log pg_dynshmem pg_ident.conf pg_multixact pg_replslot pg_snapshots  pg_stat_tmp pg_tblspc PG_VERSION pg_xact postgresql.conf
global pg_commit_ts pg_hba.conf pg_logical pg_notify pg_serial pg_stat pg_subtrans pg_twophase pg_wal postgresql.auto.conf
</code></pre>
<p><strong>Start and stop the database server:</strong></p>
<pre><code>-bash-4.2$ /usr/pgsql-15/bin/pg_ctl -D /mnt/pgsql15/data -l logfile start
waiting for server to start.... done
server started

-bash-4.2$ /usr/pgsql-15/bin/pg_ctl -D /mnt/pgsql15/data stop
waiting for server to shut down.... done
server stopped

-bash-4.2$ /usr/pgsql-15/bin/pg_ctl -D /mnt/pgsql15/data -l logfile start
waiting for server to start.... done
server started

-bash-4.2$ ls -ltr
total 4
drwx------ 4 postgres postgres  51 Feb 13 20:48 15
-rw------- 1 postgres postgres 374 Feb 13 22:26 logfile
</code></pre>
<p><strong>Restart the database server:</strong></p>
<pre><code>-bash-4.2$ /usr/pgsql-15/bin/pg_ctl -D /mnt/pgsql15/data -l logfile restart
waiting for server to shut down.... done
server stopped
waiting for server to start.... done
server started
</code></pre>
<p><strong>Check the database service:</strong></p>
<pre><code>-bash-4.2$ /usr/pgsql-15/bin/pg_ctl -D /mnt/pgsql15/data status
pg_ctl: server is running (PID: 77038)
/usr/pgsql-15/bin/postgres &quot;-D&quot; &quot;/mnt/pgsql15/data&quot;

-bash-4.2$ ps aux | grep postgres | grep -v grep
root     73095  0.0  0.0 191900  4348 pts/0    S    21:49   0:00 su - postgres
postgres 73096  0.0  0.0 115560  3456 pts/0    S    21:49   0:00 -bash
postgres 77434  0.1  0.0 401372 23652 ?        Ss   22:26   0:00 /usr/pgsql-15/bin/postgres -D /mnt/pgsql15/data
postgres 77435  0.0  0.0 253204  5692 ?        Ss   22:26   0:00 postgres: logger
postgres 77436  0.0  0.0 401524  5728 ?        Ss   22:26   0:00 postgres: checkpointer
postgres 77437  0.0  0.0 401508  5784 ?        Ss   22:26   0:00 postgres: background writer
postgres 77439  0.0  0.0 401508 10372 ?        Ss   22:26   0:00 postgres: walwriter
postgres 77440  0.0  0.0 402992  8864 ?        Ss   22:26   0:00 postgres: autovacuum launcher
postgres 77441  0.0  0.0 402976  6904 ?        Ss   22:26   0:00 postgres: logical replication launcher
postgres 77480  0.0  0.0 155468  3784 pts/0    R+   22:27   0:00 ps aux
</code></pre>
<h2 id="Using-psql"><a href="#Using-psql" class="headerlink" title="Using psql"></a>Using psql</h2><p><a href="https://www.postgresql.org/docs/current/app-psql.html">psql</a> is a terminal-based front-end to PostgreSQL. It enables you to type in queries interactively, issue them to PostgreSQL, and see the query results. Alternatively, input can be from a file or from command line arguments. In addition, psql provides a number of meta-commands and various shell-like features to facilitate writing scripts and automating a wide variety of tasks.</p>
<p><strong>Create a database:</strong></p>
<pre><code>$ su - postgres
Last login: Mon Feb 13 20:12:36 UTC 2023 on pts/0

-bash-4.2$ psql
psql (15.2)
Type &quot;help&quot; for help.

postgres=# \l
                                                 List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    | ICU Locale | Locale Provider |   Access privileges
-----------+----------+----------+-------------+-------------+------------+-----------------+-----------------------
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
(3 rows)


postgres=# create database testdb;
CREATE DATABASE

postgres=# \l
                                                 List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    | ICU Locale | Locale Provider |   Access privileges
-----------+----------+----------+-------------+-------------+------------+-----------------+-----------------------
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
 testdb    | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
(4 rows)
</code></pre>
<p><strong>Drop database:</strong></p>
<pre><code>postgres=# drop database if exists testdb;
DROP DATABASE
postgres=# \l
                                                 List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    | ICU Locale | Locale Provider |   Access privileges
-----------+----------+----------+-------------+-------------+------------+-----------------+-----------------------
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
(3 rows)
</code></pre>
<p><strong>Manipulate the database remotely:</strong></p>
<pre><code>psql --host=10.13.121.243 --port=5432 --username=postgres -w -c &quot;create database testdb&quot;
psql --host=10.13.121.243 --port=5432 --username=postgres -w -c &quot;\l&quot;
psql --host=10.13.121.243 --port=5432 --username=postgres -w -d testdb -c &quot;\dt+&quot;
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.postgresql.org/download/linux/redhat/">https://www.postgresql.org/download/linux/redhat/</a></li>
<li><a href="https://www.postgresql.org/docs/15/creating-cluster.html">https://www.postgresql.org/docs/15/creating-cluster.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>PostgreSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Getting started with R</title>
    <url>/blog/getting-started-with-r/</url>
    <content><![CDATA[<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>The simplest way is to use Homebrew:</p>
<pre><code>$ brew install r
</code></pre>
<p>Another way is to download installation package from <a href="https://cloud.r-project.org/">https://cloud.r-project.org/</a></p>
<h2 id="“Hello-world”-of-R"><a href="#“Hello-world”-of-R" class="headerlink" title="“Hello world” of R"></a>“Hello world” of R</h2><h3 id="Run-it-from-R-console"><a href="#Run-it-from-R-console" class="headerlink" title="Run it from R console"></a>Run it from R console</h3><p>Command <code>R</code> will start a R console, and you can run R code inside R console.</p>
<pre><code>(base) ➜  benchling git:(b_test_pr) ✗ R

R version 4.2.2 (2022-10-31) -- &quot;Innocent and Trusting&quot;
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: aarch64-apple-darwin20 (64-bit)

...
&gt; print(&quot;hello,world&quot;)
[1] &quot;hello,world&quot;
</code></pre>
<h3 id="Run-it-from-terminal"><a href="#Run-it-from-terminal" class="headerlink" title="Run it from terminal"></a>Run it from terminal</h3><p><code>Rscript</code> is a binary front-end to R, for use in scripting applications, see <a href="https://linux.die.net/man/1/rscript">https://linux.die.net/man/1/rscript</a> for more detail.</p>
<pre><code>(base) ➜  R git:(b_test_pr) ✗ cat hello.R
print(&quot;hello,world&quot;)
(base) ➜  R git:(b_test_pr) ✗ Rscript hello.R
[1] &quot;hello,world&quot;
</code></pre>
<h2 id="Install-commonly-used-packages"><a href="#Install-commonly-used-packages" class="headerlink" title="Install commonly used packages"></a>Install commonly used packages</h2><p>R installation package comes along with a lot of useful packages, besides that, there are a lot of useful packages available from <a href="https://cran.r-project.org/web/packages/">CRAN</a>.</p>
<p>Here are top 10 most important packages in R for data science.</p>
<ul>
<li>ggplot2</li>
<li>data.table</li>
<li>dplyr</li>
<li>tidyr</li>
<li>Shiny</li>
<li>plotly</li>
<li>knitr</li>
<li>mlr3</li>
</ul>
<p>To install those packages from CRAN, we can just simiply follow below steps.</p>
<ul>
<li>Start R console</li>
<li>Call “install.packages(XXX)”</li>
</ul>
<p>Here is an example:</p>
<pre><code>&gt; install.packages(&quot;mlr3&quot;)
--- Please select a CRAN mirror for use in this session ---
Secure CRAN mirrors

 1: 0-Cloud [https]
 2: Australia (Canberra) [https]
 3: Australia (Melbourne 1) [https]
 ....
 Selection: 1
also installing the dependencies ‘globals’, ‘listenv’, ‘PRROC’, ‘future’, ‘future.apply’, ‘lgr’, ‘mlbench’, ‘mlr3measures’, ‘mlr3misc’, ‘parallelly’, ‘palmerpenguins’, ‘paradox’

trying URL &#39;https://cloud.r-project.org/bin/macosx/big-sur-arm64/contrib/4.2/globals_0.16.2.tgz&#39;
...
&gt;&gt; library(mlr3)
&gt; ?mlr3
</code></pre>
<p>As above, after installation completes, we can try to run <code>library(&lt;package name&gt;)</code> to verify, and run <code>?&lt;package name&gt;</code> to see its document.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.w3schools.com/r/r_get_started.asp">https://www.w3schools.com/r/r_get_started.asp</a></li>
<li><a href="https://www.datacamp.com/tutorial/top-ten-most-important-packages-in-r-for-data-science">https://www.datacamp.com/tutorial/top-ten-most-important-packages-in-r-for-data-science</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title>Getting started with systemtap for Linux system profiling</title>
    <url>/blog/getting-started-with-systemtap-for-linux-system-profiling/</url>
    <content><![CDATA[<h2 id="Intro-to-SystemTap"><a href="#Intro-to-SystemTap" class="headerlink" title="Intro to SystemTap"></a>Intro to SystemTap</h2><p>SystemTap is a tracing and probing tool that allows users to study and monitor the activities of the operating system (particularly, the kernel) in fine detail. It provides information similar to the output of tools like netstat, ps, top, and iostat; however, SystemTap is designed to provide more filtering and analysis options for collected information.</p>
<p>SystemTap can be used by system administrators as a performance monitoring tool for Red Hat Enterprise Linux 5 or later. It is most useful when other similar tools cannot precisely pinpoint a bottleneck in the system, thus requiring a deep analysis of system activity. In the same manner, application developers can also use SystemTap to monitor, in fine detail, how their application behaves within the Linux system.</p>
<p>SystemTap was originally developed to provide functionality for Red Hat Enterprise Linux similar to previous Linux probing tools such as dprobes and the Linux Trace Toolkit. SystemTap aims to supplement the existing suite of Linux monitoring tools by providing users with the infrastructure to track kernel activity. In addition, SystemTap combines this capability with two attributes:</p>
<ul>
<li>Flexibility: SystemTap’s framework allows users to develop simple scripts for investigating and monitoring a wide variety of kernel functions, system calls, and other events that occur in kernel space. With this, SystemTap is not so much a tool as it is a system that allows you to develop your own kernel-specific forensic and monitoring tools.</li>
<li>Ease-of-Use: as mentioned earlier, SystemTap allows users to probe kernel-space events without having to resort to the lengthy instrument, recompile, install, and reboot the kernel process.</li>
</ul>
<h2 id="Understanding-how-SystemTap-works"><a href="#Understanding-how-SystemTap-works" class="headerlink" title="Understanding how SystemTap works"></a>Understanding how SystemTap works</h2><p>SystemTap allows users to write and reuse simple scripts to deeply examine the activities of a running Linux system. These scripts can be designed to extract data, filter it, and summarize it quickly (and safely), enabling the diagnosis of complex performance (or even functional) problems.</p>
<p>The essential idea behind a SystemTap script is to name events, and to give them handlers. When SystemTap runs the script, SystemTap monitors for the event; once the event occurs, the Linux kernel then runs the handler as a quick sub-routine and then resumes its normal operation.</p>
<p>There are several kinds of events; entering or exiting a function, timer expiration, session termination, etc. A handler is a series of script language statements that specify the work to be done whenever the event occurs. This work normally includes extracting data from the event context, storing them into internal variables, and printing results.</p>
<h2 id="Setting-up-SystemTap-and-its-required-kernel-packages"><a href="#Setting-up-SystemTap-and-its-required-kernel-packages" class="headerlink" title="Setting up SystemTap and its required kernel packages"></a>Setting up SystemTap and its required kernel packages</h2><p>To deploy SystemTap, SystemTap packages along with the corresponding set of -devel, -debuginfo and -debuginfo-common-arch packages for the kernel need to be installed. To use SystemTap on more than one kernel where a system has multiple kernels installed, install the -devel and -debuginfo packages for each of those kernel versions.</p>
<p>SystemTap needs information about the kernel in order to place instrumentation in it (probe it). This information, which allows SystemTap to generate the code for the instrumentation, is contained in the matching kernel-devel, kernel-debuginfo, and kernel-debuginfo-common-arch packages.</p>
<p>To install SystemTap packages:</p>
<pre><code>[root@host1 ~]# cat /etc/centos-release
CentOS Linux release 7.9.2009 (Core)
[root@host1 ~]# uname -r
3.10.0-1160.el7.x86_64

[root@host1 ~]# yum install -y systemtap systemtap-runtime

[root@host1 ~]# rpm -qa | grep systemtap
systemtap-runtime-4.0-13.el7.x86_64
systemtap-devel-4.0-13.el7.x86_64
systemtap-4.0-13.el7.x86_64
systemtap-client-4.0-13.el7.x86_64
</code></pre>
<p>To install devel and debuginfo packages in CentOS(set to enabled&#x3D;1):</p>
<pre><code>[root@host1 ~]# vim /etc/yum.repos.d/CentOS-Debuginfo.repo
[base-debuginfo]
name=CentOS-7 - Debuginfo
baseurl=http://debuginfo.centos.org/7/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-Debug-7
enabled=1

[root@host1 ~]# yum install -y kernel-devel-$(uname -r) \
&gt; kernel-debuginfo-$(uname -r) \
&gt; kernel-debuginfo-common-$(uname -m)-$(uname -r)

[root@host1 ~]# rpm -qa | grep debuginfo
kernel-debuginfo-common-x86_64-3.10.0-1160.el7.x86_64
kernel-debuginfo-3.10.0-1160.el7.x86_64
</code></pre>
<p>The devel package was not installed successfully since it’s not available in the existing CentOS repository. It might be fixed by adding the expected repository for yum installation. The devel package is required by SystemTap otherwise the following error is seen.</p>
<pre><code>[root@host1 ~]# stap -v -e &#39;probe vfs.read &#123;printf(&quot;read performed\n&quot;); exit()&#125;&#39;
Checking &quot;/lib/modules/3.10.0-1160.el7.x86_64/build/.config&quot; failed with error: No such file or directory
Incorrect version or missing kernel-devel package, use: yum install kernel-devel-3.10.0-1160.el7.x86_64
</code></pre>
<p>Here we just install it directly from the downloaded format as below.</p>
<pre><code>[root@host1 ~]# wget https://rpmfind.net/linux/centos/7.9.2009/os/x86_64/Packages/kernel-devel-3.10.0-1160.el7.x86_64.rpm

[root@host1 ~]# rpm -ivh kernel-devel-3.10.0-1160.el7.x86_64.rpm

[root@host1 ~]# rpm -qa | grep kernel | grep  3.10.0
kernel-devel-3.10.0-1160.el7.x86_64
kernel-tools-libs-3.10.0-1160.el7.x86_64
kernel-tools-3.10.0-1160.el7.x86_64
kernel-debuginfo-common-x86_64-3.10.0-1160.el7.x86_64
kernel-debuginfo-3.10.0-1160.el7.x86_64
kernel-headers-3.10.0-1160.59.1.el7.x86_64
kernel-3.10.0-1160.el7.x86_64
</code></pre>
<p>To verify the SystemTap setup again:</p>
<pre><code>[root@host1 ~]# stap -v -e &#39;probe vfs.read &#123;printf(&quot;read performed\n&quot;); exit()&#125;&#39;
Pass 1: parsed user script and 474 library scripts using 271960virt/69264res/3504shr/65852data kb, in 640usr/30sys/672real ms.
Pass 2: analyzed script: 1 probe, 1 function, 7 embeds, 0 globals using 439304virt/232180res/4884shr/233196data kb, in 2180usr/950sys/2977real ms.
Pass 3: translated to C into &quot;/tmp/stap6kYO8U/stap_cc0f60b74db3020f09599659b9758c89_2771_src.c&quot; using 439304virt/232436res/5140shr/233196data kb, in 10usr/50sys/67real ms.
Pass 4: compiled C into &quot;stap_cc0f60b74db3020f09599659b9758c89_2771.ko&quot; in 8040usr/1720sys/9477real ms.
Pass 5: starting run.
read performed
Pass 5: run completed in 30usr/90sys/442real ms.
</code></pre>
<h2 id="SystemTap-scripts"><a href="#SystemTap-scripts" class="headerlink" title="SystemTap scripts"></a>SystemTap scripts</h2><p>For the most part, SystemTap scripts are the foundation of each SystemTap session. SystemTap scripts instruct SystemTap on what type of information to collect, and what to do once that information is collected. SystemTap scripts are made up of two components: events and handlers. Once a SystemTap session is underway, SystemTap monitors the operating system for the specified events and executes the handlers as they occur.</p>
<p>SystemTap scripts allow insertion of the instrumentation code without recompilation of the code and allows more flexibility with regard to handlers. Events serve as the triggers for handlers to run; handlers can be specified to record specified data and print it in a certain manner.</p>
<p>SystemTap scripts use the .stp file extension and contains probes written in the following format:</p>
<pre><code>probe event &#123;statements&#125;
</code></pre>
<p>Systemtap allows you to write functions to factor out code to be used by a number of probes. Thus, rather than repeatedly writing the same series of statements in multiple probes, you can just place the instructions in a function, as in:</p>
<pre><code>function function_name(arguments)&#123;statements&#125;
probe event &#123;function_name(arguments)&#125;
</code></pre>
<p>The statements in function_name are executed when the probe for event executes. The arguments are optional values passed into the function.</p>
<h2 id="Running-SystemTap-Scripts"><a href="#Running-SystemTap-Scripts" class="headerlink" title="Running SystemTap Scripts"></a>Running SystemTap Scripts</h2><p>SystemTap scripts are run through the command stap. stap can run SystemTap scripts from the standard input or from a file.</p>
<p>We have seen how to run SystemTap from the standard input when we tried to verify the installation in previous section.</p>
<p>We can also run it from a file as below.</p>
<pre><code>[root@host1 ~]# cat runfromfile.stp
probe vfs.read &#123;
    printf(&quot;read performed\n&quot;);
    exit()
&#125;

[root@host1 ~]# stap runfromfile.stp
read performed
</code></pre>
<p>At this point, we know what is SystemTap and how to deploy it. We will explore more meaningful usage of it in future posts.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://sourceware.org/systemtap/SystemTap_Beginners_Guide/">https://sourceware.org/systemtap/SystemTap_Beginners_Guide&#x2F;</a></li>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/systemtap_beginners_guide/index#introduction">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux&#x2F;7&#x2F;html-single&#x2F;systemtap_beginners_guide&#x2F;index#introduction</a></li>
<li><a href="https://pkgs.org/download/kernel">https://pkgs.org/download/kernel</a></li>
<li><a href="https://pkgs.org/download/kernel-devel">https://pkgs.org/download/kernel-devel</a></li>
<li><a href="https://linux.cc.iitk.ac.in/mirror/centos/elrepo/kernel/el7/x86_64/RPMS/">https://linux.cc.iitk.ac.in/mirror/centos/elrepo/kernel/el7/x86_64&#x2F;RPMS&#x2F;</a></li>
<li><a href="https://rpmfind.net/linux/rpm2html/search.php?query=kernel-devel">https://rpmfind.net/linux/rpm2html/search.php?query=kernel-devel</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>SystemTap</tag>
      </tags>
  </entry>
  <entry>
    <title>Hosting the Ghost blog on GitHub</title>
    <url>/blog/ghost-github/</url>
    <content><![CDATA[<h1 id="Install-Ghost-locally"><a href="#Install-Ghost-locally" class="headerlink" title="Install Ghost locally"></a>Install Ghost locally</h1><p><a href="https://pages.github.com/">GitHub Pages</a> is a great solution for hosting static website. If you use the Ghost to manage your website, you can <a href="https://ghost.org/docs/install/local/">install Ghost locally</a> and convert it to a static website in order to host it on Github.</p>
<h2 id="Install-Ubuntu-virtual-machine-on-Windows-optional"><a href="#Install-Ubuntu-virtual-machine-on-Windows-optional" class="headerlink" title="Install Ubuntu virtual machine on Windows(optional)"></a>Install Ubuntu virtual machine on Windows(optional)</h2><h2 id="Install-Ghost-CLI"><a href="#Install-Ghost-CLI" class="headerlink" title="Install Ghost-CLI"></a>Install Ghost-CLI</h2><p>Ghost-CLI is a commandline tool to help you get Ghost installed and configured for use, quickly and easily.</p>
<pre><code>$ npm install ghost-cli@latest -g
</code></pre>
<p>Ghost runs as background process and remains running until you stop or restart it. The following are some useful commands:</p>
<pre><code>ghost help
ghost ls
ghost log
ghost stop
ghost start
</code></pre>
<h2 id="Install-Ghost"><a href="#Install-Ghost" class="headerlink" title="Install Ghost"></a>Install Ghost</h2><pre><code>$ mkdir my-ghost-website
$ cd my-ghost-website
$ ghost install local
</code></pre>
<p>Once the Ghost is installed you can access the website on <a href="http://localhost:2368/ghost">http://localhost:2368/ghost</a> for Ghost Admin.</p>
<h1 id="Generate-the-static-website"><a href="#Generate-the-static-website" class="headerlink" title="Generate the static website"></a>Generate the static website</h1><p>In order to generate the static website, we’ll use the Ghost static site generator. For the Linux and macOS, this tool can be used directly.</p>
<pre><code>$ sudo npm install -g ghost-static-site-generator
</code></pre>
<p>Now, we can push the generated static pages to the Github repository named <strong>username.github.io</strong>.</p>
<p>To generate the static pages, we can run the following command. The static pages are generated in a folder called <em>static</em>.</p>
<pre><code>$ gssg --url https://username.github.io
</code></pre>
<h1 id="Push-the-static-pages-to-Github"><a href="#Push-the-static-pages-to-Github" class="headerlink" title="Push the static pages to Github"></a>Push the static pages to Github</h1><p>Before pushing the static pages to Github, a repository called <em>username.github.io</em> should be created on Github website.</p>
<p>The static pages can be pushed to Github repository as below.</p>
<pre><code>$ cd static
$ git init .
$ git remote add origin https://github.com/username/username.github.io.git
$ git branch -M main
$ git add .
$ git commit -m &#39;Init my website&#39;
$ git push -u origin main
</code></pre>
<p>After the push, Github will build and deploy the pages automatically. And the updated website will be available to access in a few minutes.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://dev.to/bassel/hosting-your-ghost-blog-on-github-pages-for-free-53hl">https://dev.to/bassel/hosting-your-ghost-blog-on-github-pages-for-free-53hl</a></li>
<li><a href="https://zzamboni.org/post/hosting-a-ghost-blog-in-github/#:~:text=Install%20and%20run%20Ghost%20locally,static%20website%20to%20GitHub%20Pages">https://zzamboni.org/post/hosting-a-ghost-blog-in-github/#:~:text&#x3D;Install and run Ghost locally,static website to GitHub Pages</a></li>
<li><a href="https://ghost.org/docs/reinstall/">https://ghost.org/docs/reinstall/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>Ghost</tag>
      </tags>
  </entry>
  <entry>
    <title>GlusterFS - A distributed file syste</title>
    <url>/blog/glusterfs-a-distributed-file-syste/</url>
    <content><![CDATA[<h2 id="What-is-Gluster"><a href="#What-is-Gluster" class="headerlink" title="What is Gluster?"></a>What is Gluster?</h2><p><a href="https://docs.gluster.org/en/latest/Administrator-Guide/GlusterFS-Introduction/">Gluster</a> is a scalable, distributed file system that aggregates disk storage resources from multiple servers into a single global namespace.</p>
<p>Advantages</p>
<ul>
<li>Scales to several petabytes</li>
<li>Handles thousands of clients</li>
<li>POSIX compatible</li>
<li>Uses commodity hardware</li>
<li>Can use any ondisk filesystem that supports extended attributes</li>
<li>Accessible using industry standard protocols like NFS and SMB</li>
<li>Provides replication, quotas, geo-replication, snapshots and bitrot detection</li>
<li>Allows optimization for different workloads</li>
<li>Open Source</li>
</ul>
<h2 id="Installation-and-configuration"><a href="#Installation-and-configuration" class="headerlink" title="Installation and configuration"></a>Installation and configuration</h2><ol>
<li><p>To install gluster and start gluster service:</p>
<p> [root@centos83-1 ~]# cat &#x2F;etc&#x2F;centos-release<br> CentOS Linux release 8.3.2011</p>
<p> [root@centos83-1 ~]# systemctl stop firewalld<br> [root@centos83-1 ~]# systemctl disable firewalld</p>
<p> [root@centos83-1 ~]# yum install -y centos-release-gluster<br> [root@centos83-1 ~]# yum install -y glusterfs-server<br> [root@centos83-1 ~]# rpm -qa |grep gluster<br> glusterfs-cli-8.3-1.el8.x86_64<br> libvirt-daemon-driver-storage-gluster-6.0.0-28.module_el8.3.0+555+a55c8938.x86_64<br> glusterfs-client-xlators-8.3-1.el8.x86_64<br> qemu-kvm-block-gluster-4.2.0-34.module_el8.3.0+555+a55c8938.x86_64<br> libglusterd0-8.3-1.el8.x86_64<br> glusterfs-8.3-1.el8.x86_64<br> pcp-pmda-gluster-5.1.1-3.el8.x86_64<br> glusterfs-fuse-8.3-1.el8.x86_64<br> centos-release-gluster8-1.0-1.el8.noarch<br> libglusterfs0-8.3-1.el8.x86_64<br> glusterfs-server-8.3-1.el8.x86_64</p>
<p> [root@centos83-1 ~]# systemctl enable glusterd<br> [root@centos83-1 ~]# systemctl restart glusterd</p>
<p> [root@centos83-1 ~]# systemctl status glusterd<br>  glusterd.service - GlusterFS, a clustered file-system server<br>  Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;glusterd.service; enabled; vendor preset: enabled)<br>  Active: active (running) since Tue 2021-01-05 17:28:42 PST; 1 months 22 days ago<br>   Docs: man:glusterd(8)<br>  Main PID: 1420 (glusterd)<br>  Tasks: 26 (limit: 409792)<br>  Memory: 152.3M<br>  CGroup: &#x2F;system.slice&#x2F;glusterd.service</p>
</li>
<li><p>To form a trusted storage pool with the second server:</p>
<p> [root@centos83-1 ~]# gluster peer probe centos83-2<br> [root@centos83-1 ~]# gluster peer status<br> Number of Peers: 1</p>
<p> Hostname: centos83-2<br> Uuid: b07d3d6e-4d6e-42a9-ad21-018223843fd5<br> State: Peer in Cluster (Connected)</p>
</li>
<li><p>To create brick on the first server:</p>
<p> [root@centos83-1 ~]# lsblk | egrep “NAME|sdb”<br> NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT<br> sdb                  8:16   0    1T  0 disk</p>
<p> [root@centos83-1 ~]# pvcreate &#x2F;dev&#x2F;sdb<br> [root@centos83-1 ~]# vgcreate vg_bricks &#x2F;dev&#x2F;sdb<br> [root@centos83-1 ~]# lvcreate -L 800g -n gfslv1 vg_bricks<br> [root@centos83-1 ~]# mkfs.xfs &#x2F;dev&#x2F;vg_bricks&#x2F;gfslv1<br> [root@centos83-1 ~]# mkdir -p &#x2F;bricks&#x2F;vm1_brick1<br> [root@centos83-1 ~]# vim &#x2F;etc&#x2F;fstab<br> &#x2F;dev&#x2F;vg_bricks&#x2F;gfslv1 &#x2F;bricks&#x2F;vm1_brick1        xfs     defaults        0 0<br> [root@centos83-1 ~]# mount -a<br> [root@centos83-1 ~]# df -h |grep gfs<br> &#x2F;dev&#x2F;mapper&#x2F;vg_bricks-gfslv1  800G  5.7G  794G   1% &#x2F;bricks&#x2F;vm1_brick1</p>
</li>
<li><p>To create brick on the second server:</p>
<p> [root@centos83-2 ~]# lsblk | egrep “NAME|sdb”<br> NAME               MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT<br> sdb                  8:16   0    1T  0 disk</p>
<p> [root@centos83-2 ~]# pvcreate &#x2F;dev&#x2F;sdb<br> [root@centos83-2 ~]# vgcreate vg_bricks &#x2F;dev&#x2F;sdb<br> [root@centos83-2 ~]# lvcreate -L 800g -n gfslv1 vg_bricks<br> [root@centos83-2 ~]# mkfs.xfs &#x2F;dev&#x2F;vg_bricks&#x2F;gfslv1<br> [root@centos83-2 ~]# mkdir -p &#x2F;bricks&#x2F;vm2_brick1<br> [root@centos83-2 ~]# vim &#x2F;etc&#x2F;fstab<br> &#x2F;dev&#x2F;vg_bricks&#x2F;gfslv1 &#x2F;bricks&#x2F;vm2_brick1        xfs     defaults        0 0<br> [root@centos83-2 ~]# mount -a<br> [root@centos83-2 ~]# df -h |grep gfs<br> &#x2F;dev&#x2F;mapper&#x2F;vg_bricks-gfslv1  800G  5.7G  794G   1% &#x2F;bricks&#x2F;vm2_brick1</p>
</li>
<li><p>To create distributed volume with the two bricks which are created on the two nodes:</p>
<p> [root@centos83-1 ~]# gluster volume create gv0 centos83-1:&#x2F;bricks&#x2F;vm1_brick1&#x2F;gv0 centos83-2:&#x2F;bricks&#x2F;vm2_brick1&#x2F;gv0<br> [root@centos83-1 ~]# gluster volume start gv0</p>
</li>
<li><p>To verify the volume status:</p>
<p> [root@centos83-1 ~]#  gluster volume info gv0</p>
<p> Volume Name: gv0<br> Type: Distribute<br> Volume ID: ee08d16a-f940-4ec2-aba8-5f1fcfe41bd4<br> Status: Started<br> Snapshot Count: 0<br> Number of Bricks: 2<br> Transport-type: tcp<br> Bricks:<br> Brick1: centos83-1:&#x2F;bricks&#x2F;vm1_brick1&#x2F;gv0<br> Brick2: centos83-2:&#x2F;bricks&#x2F;vm2_brick1&#x2F;gv0<br> Options Reconfigured:<br> storage.fips-mode-rchecksum: on<br> transport.address-family: inet<br> nfs.disable: on</p>
</li>
<li><p>To mount the distributed volume on one of the servers(treat it as client for simple demonstration):</p>
<p> [root@centos83-1 ~]# mkdir &#x2F;testmnt<br> [root@centos83-1 ~]# mount -t glusterfs centos83-2:&#x2F;gv0 &#x2F;testmnt<br> [root@centos83-1 ~]# df -h | grep testmnt<br> centos83-2:&#x2F;gv0               1.6T   28G  1.6T   2% &#x2F;testmnt</p>
</li>
</ol>
<p>As shown above, the usable storage size is the sum of the brick size from two nodes.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://docs.gluster.org/en/latest/Administrator-Guide/GlusterFS-Introduction/">https://docs.gluster.org/en/latest/Administrator-Guide/GlusterFS-Introduction/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>GlusterFS</tag>
      </tags>
  </entry>
  <entry>
    <title>go-ycsb load db terminated with message &#39;Got signal [hangup] to exit&#39;</title>
    <url>/blog/go-ycsb-hangup/</url>
    <content><![CDATA[<p>During database load phase with go-ycsb, the load process is terminated and the message ‘Got signal [hangup] to exit’ is reported.</p>
<pre><code>$ nohup ./bin/go-ycsb load cockroach -P workloads/workloadd --threads 96 -p pg.host=&lt;cockroach-host-ip&gt; -p pg.port=26257 -p pg.user=root -p pg.db=test -p pg.sslmode=disable -p dropdata=true  &amp;

INSERT - Takes(s): 6150.0, Count: 66394363, OPS: 10795.9, Avg(us): 8167, Min(us): 1515, Max(us): 28655, 99th(us): 26815, 99.9th(us): 28447, 99.99th(us): 28623
INSERT - Takes(s): 6160.0, Count: 66504914, OPS: 10796.3, Avg(us): 8166, Min(us): 1515, Max(us): 28655, 99th(us): 26815, 99.9th(us): 28447, 99.99th(us): 28623

Got signal [hangup] to exit.
Run finished, takes 1h42m46.015270222s
INSERT - Takes(s): 6166.0, Count: 66570612, OPS: 10796.4, Avg(us): 8166, Min(us): 1515, Max(us): 28655, 99th(us): 26815, 99.9th(us): 28447, 99.99th(us): 28639
</code></pre>
<p>To resolve this, after running nohup command, type “exit” to exit the current shell instead of closing the shell session directly.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/pingcap/tidb/issues/8675">https://github.com/pingcap/tidb/issues/8675</a></li>
<li><a href="https://github.com/pingcap/go-ycsb/blob/master/cmd/go-ycsb/main.go">https://github.com/pingcap/go-ycsb/blob/master/cmd/go-ycsb/main.go</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>YCSB</tag>
      </tags>
  </entry>
  <entry>
    <title>Google analytics and adsense in Hexo</title>
    <url>/blog/google-analytics-and-adsense-in-hexo-next/</url>
    <content><![CDATA[<h2 id="Google-analytics"><a href="#Google-analytics" class="headerlink" title="Google analytics"></a>Google analytics</h2><p><a href="https://marketingplatform.google.com/about/analytics/">Google Analytics 4</a> has replaced Universal Analytics(tracking id based). Starting July 1, 2023, Universal Analytics properties will stop processing data. If you want to keep using Google Analytics to measure your website traffic, you need a Google Analytics 4 property, aks Google tag.</p>
<p>To activate Google Analytics, Google provides you a tag named as “Google Tag”. You can find the “Google Tag” in Google analytics console as below.<span id="more"></span></p>
<ol>
<li>Go to Google analytics console</li>
<li>Click “Admin” on the left bottom corner</li>
<li>Click “Data Streams” under your website Property </li>
<li>Click the arrow to the right of your website name</li>
<li>In the Google Tag section, click “Configure tag settings”</li>
<li>Click “Installation Instructions” to the right of “Your Google tag”, you will see the Google Tag code piece for your account<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;!-- <span class="title class_">Google</span> tag (gtag.<span class="property">js</span>) --&gt;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">async</span> <span class="attr">src</span>=<span class="string">&quot;https://www.googletagmanager.com/gtag/js?id=G-xxxx&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">  <span class="variable language_">window</span>.<span class="property">dataLayer</span> = <span class="variable language_">window</span>.<span class="property">dataLayer</span> || [];</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">  <span class="keyword">function</span> <span class="title function_">gtag</span>(<span class="params"></span>)&#123;dataLayer.<span class="title function_">push</span>(<span class="variable language_">arguments</span>);&#125;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">  <span class="title function_">gtag</span>(<span class="string">&#x27;js&#x27;</span>, <span class="keyword">new</span> <span class="title class_">Date</span>());</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">  <span class="title function_">gtag</span>(<span class="string">&#x27;config&#x27;</span>, <span class="string">&#x27;G-xxxx&#x27;</span>);</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml"> </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span>  </span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Add-Google-Analytics-to-Hexo"><a href="#Add-Google-Analytics-to-Hexo" class="headerlink" title="Add Google Analytics to Hexo"></a>Add Google Analytics to Hexo</h2><p>There are multiple ways to configure Google Analytics with Hexo blog. You may embed the script to the theme code. In this example, we don’t want to change the code under theme, so we configure it outside of the theme source folder. </p>
<ol>
<li>Create a file “_config.next.yml” under website root, at same level as Hexo config file “_config.yml”. Note, since I’m using Hexo Next theme, I name it as “_config.next.yml”.</li>
<li>Add the following code to “_config.next.yml”<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="attr">custom_file_path</span>:</span><br><span class="line">  <span class="attr">head</span>: source/_third-party/google/google-adsense-analytics.<span class="property">swig</span></span><br></pre></td></tr></table></figure></li>
<li>Create folder and file as above path indicates</li>
<li>Add the Google Tag script you got from Google Analytics console to the file “source&#x2F;_third-party&#x2F;google&#x2F;google-adsense-analytics.swig”<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;!-- <span class="title class_">Google</span> tag (gtag.<span class="property">js</span>) <span class="keyword">for</span> analytics--&gt;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">async</span> <span class="attr">src</span>=<span class="string">&quot;https://www.googletagmanager.com/gtag/js?id=G-xxxx&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="language-javascript"></span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">  <span class="variable language_">window</span>.<span class="property">dataLayer</span> = <span class="variable language_">window</span>.<span class="property">dataLayer</span> || [];</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">  <span class="keyword">function</span> <span class="title function_">gtag</span>(<span class="params"></span>)&#123;dataLayer.<span class="title function_">push</span>(<span class="variable language_">arguments</span>);&#125;</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">  <span class="title function_">gtag</span>(<span class="string">&#x27;js&#x27;</span>, <span class="keyword">new</span> <span class="title class_">Date</span>());</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml">  <span class="title function_">gtag</span>(<span class="string">&#x27;config&#x27;</span>, <span class="string">&#x27;G-xxxx&#x27;</span>);</span></span></span><br><span class="line"><span class="language-javascript"><span class="language-xml"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br></pre></td></tr></table></figure></li>
<li>To this point, Google Analytics is added to Hexo. The Google tag script will be included in between “head” element of any page. You can verify it by viewing page source.</li>
</ol>
<h2 id="Google-Adsense"><a href="#Google-Adsense" class="headerlink" title="Google Adsense"></a>Google Adsense</h2><p>You can use <a href="https://adsense.google.com/start/">Google Adsense</a> if you want to value your blog content.</p>
<h2 id="Add-Google-Adsense-to-Hexo"><a href="#Add-Google-Adsense-to-Hexo" class="headerlink" title="Add Google Adsense to Hexo"></a>Add Google Adsense to Hexo</h2><p>Similar to Google Analytics, you just need to get the Google Adsense code piece and embed to your website.</p>
<ol>
<li>Go to Google Adsense</li>
<li>Click “Ads” on the left sidebar</li>
<li>Click “Get code”, copy and paste it to the file “source&#x2F;_third-party&#x2F;google&#x2F;google-adsense-analytics.swig”. It will embed the code in between “head” element of your website pages.<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;script async src=&quot;https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-xxxx&quot;</span><br><span class="line">  crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure></li>
<li>To this point, Google will automatically show ads in all the best places of the webpages for you</li>
</ol>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph implementation in Python</title>
    <url>/blog/graph-implementation-in-python/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>A graph is a data structure that consists of vertices that are connected via edges.</p>
<h2 id="Using-an-adjacency-matrix"><a href="#Using-an-adjacency-matrix" class="headerlink" title="Using an adjacency matrix"></a>Using an adjacency matrix</h2><pre><code># implement a graph using adjacency matrix
class Graph:
    def __init__(self, size):
        self.adj_matrix = []
        for i in range(size):
            self.adj_matrix.append([0 for i in range(size)])
        self.size = size

    def add_edge(self, v1, v2):
        if v1 != v2:
            self.adj_matrix[v1][v2] = 1
            self.adj_matrix[v2][v1] = 1

    def remove_edge(self, v1, v2):
        if self.adj_matrix[v1][v2] != 0:
            self.adj_matrix[v1][v2] = 0

    def print_matrix(self):
        for row in self.adj_matrix:
            for col in row:
                print(f&quot;&#123;col&#125; &quot;, end=&quot;&quot;)
            print()


g = Graph(5)
g.add_edge(0, 1)
g.add_edge(0, 2)
g.add_edge(1, 2)
g.add_edge(2, 0)
g.add_edge(2, 3)
g.print_matrix()

# Output:
# 0 1 1 0 0
# 1 0 1 0 0
# 1 1 0 1 0
# 0 0 1 0 0
# 0 0 0 0 0
</code></pre>
<p>Using adjacency matrix has a drawback to implement graph. Memory is allocated for all the edges whether it is present or not. This can be avoided by using the adjacency list.</p>
<h2 id="Using-an-adjacency-list"><a href="#Using-an-adjacency-list" class="headerlink" title="Using an adjacency list"></a>Using an adjacency list</h2><pre><code># Using linked list based deque to improve vertice removal efficiency

from collections import deque

class Graph:
    def __init__(self):
        self.adj_list = &#123;&#125;

    def add_edge(self, v1, v2):
        if v1 not in self.adj_list:
            self.adj_list[v1] = deque()

        if v2 not in self.adj_list[v1]:
            self.adj_list[v1].append(v2)

        if v2 not in self.adj_list:
            self.adj_list[v2] = deque()

        if v1 not in self.adj_list[v2]:
            self.adj_list[v2].append(v1)

    def remove_edge(self, v1, v2):
        if v1 in self.adj_list and v2 in self.adj_list:
            self.adj_list[v1].remove(v2)
            self.adj_list[v2].remove(v1)

    def print_graph(self):
        for v1 in self.adj_list:
            for v2 in self.adj_list[v1]:
                print(f&quot;(&#123;v1&#125;,&#123;v2&#125;) &quot;, end=&quot;&quot;)
            print()


g = Graph()
g.add_edge(0, 1)
g.add_edge(0, 2)
g.add_edge(1, 2)
g.add_edge(2, 0)
g.add_edge(2, 3)
g.print_graph()
print()
g.remove_edge(0, 1)
g.remove_edge(1, 2)
g.print_graph()

# Output:
# (0,1) (0,2)
# (1,0) (1,2)
# (2,0) (2,1) (2,3)
# (3,2)
#
# (0,2)
#
# (2,0) (2,3)
# (3,2)
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Greedy algorithm</title>
    <url>/blog/greedy-algorithm/</url>
    <content><![CDATA[<h2 id="What-is-Greedy-algorithm"><a href="#What-is-Greedy-algorithm" class="headerlink" title="What is Greedy algorithm"></a>What is Greedy algorithm</h2><blockquote>
<p>Greedy is an algorithmic paradigm that builds up a solution piece by piece, always choosing the next piece that offers the most obvious and immediate benefit. So the problems where choosing locally optimal also leads to global solution are the best fit for Greedy.</p>
</blockquote>
<span id="more"></span>

<h2 id="Leetcode-122-Best-Time-to-Buy-and-Sell-Stock-II"><a href="#Leetcode-122-Best-Time-to-Buy-and-Sell-Stock-II" class="headerlink" title="[Leetcode 122] Best Time to Buy and Sell Stock II"></a>[Leetcode 122] Best Time to Buy and Sell Stock II</h2><p>You are given an integer array prices where prices[i] is the price of a given stock on the ith day.</p>
<p>On each day, you may decide to buy and&#x2F;or sell the stock. You can only hold at most one share of the stock at any time. However, you can buy it then immediately sell it on the same day.</p>
<p>Find and return the maximum profit you can achieve.</p>
<!-- more -->

<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: prices = [7,1,5,3,6,4]</span><br><span class="line">Output: 7</span><br><span class="line">Explanation: Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4.</span><br><span class="line">Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3.</span><br><span class="line">Total profit is 4 + 3 = 7.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: prices = [1,2,3,4,5]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: Buy on day 1 (price = 1) and sell on day 5 (price = 5), profit = 5-1 = 4.</span><br><span class="line">Total profit is 4.</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: prices = [7,6,4,3,1]</span><br><span class="line">Output: 0</span><br><span class="line">Explanation: There is no way to make a positive profit, so we never buy the stock to achieve the maximum profit of 0.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; prices.length &lt;&#x3D; 3 * 104</li>
<li>0 &lt;&#x3D; prices[i] &lt;&#x3D; 104</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(prices)</span><br><span class="line">        <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        profit = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> prices[i] &gt; prices[i - <span class="number">1</span>]:</span><br><span class="line">                profit += prices[i] - prices[i - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> profit</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-134-Gas-Station"><a href="#Leetcode-134-Gas-Station" class="headerlink" title="[Leetcode 134] Gas Station"></a>[Leetcode 134] Gas Station</h2><p>There are n gas stations along a circular route, where the amount of gas at the ith station is gas[i].</p>
<p>You have a car with an unlimited gas tank and it costs cost[i] of gas to travel from the ith station to its next (i + 1)th station. You begin the journey with an empty tank at one of the gas stations.</p>
<p>Given two integer arrays gas and cost, return the starting gas station’s index if you can travel around the circuit once in the clockwise direction, otherwise return -1. If there exists a solution, it is guaranteed to be unique</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: gas = [1,2,3,4,5], cost = [3,4,5,1,2]</span><br><span class="line">Output: 3</span><br><span class="line">Explanation:</span><br><span class="line">Start at station 3 (index 3) and fill up with 4 unit of gas. Your tank = 0 + 4 = 4</span><br><span class="line">Travel to station 4. Your tank = 4 - 1 + 5 = 8</span><br><span class="line">Travel to station 0. Your tank = 8 - 2 + 1 = 7</span><br><span class="line">Travel to station 1. Your tank = 7 - 3 + 2 = 6</span><br><span class="line">Travel to station 2. Your tank = 6 - 4 + 3 = 5</span><br><span class="line">Travel to station 3. The cost is 5. Your gas is just enough to travel back to station 3.</span><br><span class="line">Therefore, return 3 as the starting index.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: gas = [2,3,4], cost = [3,4,3]</span><br><span class="line">Output: -1</span><br><span class="line">Explanation:</span><br><span class="line">You can&#x27;t start at station 0 or 1, as there is not enough gas to travel to the next station.</span><br><span class="line">Let&#x27;s start at station 2 and fill up with 4 unit of gas. Your tank = 0 + 4 = 4</span><br><span class="line">Travel to station 0. Your tank = 4 - 3 + 2 = 3</span><br><span class="line">Travel to station 1. Your tank = 3 - 3 + 3 = 3</span><br><span class="line">You cannot travel back to station 2, as it requires 4 unit of gas but you only have 3.</span><br><span class="line">Therefore, you can&#x27;t travel around the circuit once no matter where you start.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>n &#x3D;&#x3D; gas.length &#x3D;&#x3D; cost.length</li>
<li>1 &lt;&#x3D; n &lt;&#x3D; 105</li>
<li>0 &lt;&#x3D; gas[i], cost[i] &lt;&#x3D; 104</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">canCompleteCircuit</span>(<span class="params">self, gas: <span class="type">List</span>[<span class="built_in">int</span>], cost: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># total gas must be greater or equal to total cost in order to reach all gas stations</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span>(gas) &lt; <span class="built_in">sum</span>(cost):</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        startIdx = <span class="number">0</span></span><br><span class="line">        tank = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(gas)):</span><br><span class="line">            tank += gas[i] - cost[i]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># if tank is empty, we can&#x27;t get to next gas station</span></span><br><span class="line">            <span class="keyword">if</span> tank &lt; <span class="number">0</span>:</span><br><span class="line">                startIdx = i + <span class="number">1</span></span><br><span class="line">                tank = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> startIdx</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-135-Candy"><a href="#Leetcode-135-Candy" class="headerlink" title="[Leetcode 135] Candy"></a>[Leetcode 135] Candy</h2><p>There are n children standing in a line. Each child is assigned a rating value given in the integer array ratings.</p>
<p>You are giving candies to these children subjected to the following requirements:</p>
<p>Each child must have at least one candy.<br>Children with a higher rating get more candies than their neighbors.<br>Return the minimum number of candies you need to have to distribute the candies to the children.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: ratings = [1,0,2]</span><br><span class="line">Output: 5</span><br><span class="line">Explanation: You can allocate to the first, second and third child with 2, 1, 2 candies respectively.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: ratings = [1,2,2]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: You can allocate to the first, second and third child with 1, 2, 1 candies respectively.</span><br><span class="line">The third child gets 1 candy because it satisfies the above two conditions.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>n &#x3D;&#x3D; ratings.length</li>
<li>1 &lt;&#x3D; n &lt;&#x3D; 2 * 104</li>
<li>0 &lt;&#x3D; ratings[i] &lt;&#x3D; 2 * 104</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">candy</span>(<span class="params">self, ratings: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(ratings)</span><br><span class="line">        candies = [<span class="number">1</span>] * n</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            <span class="comment"># more candies should be given if the current rating is higher than its left neighbour</span></span><br><span class="line">            <span class="keyword">if</span> ratings[i] &gt; ratings[i - <span class="number">1</span>]:</span><br><span class="line">                candies[i] = candies[i - <span class="number">1</span>] + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># more candies should be given if the current rating is higher than its right neighbour</span></span><br><span class="line">            <span class="keyword">if</span> ratings[i] &gt; ratings[i + <span class="number">1</span>]:</span><br><span class="line">                candies[i] = <span class="built_in">max</span>(candies[i], candies[i + <span class="number">1</span>] + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(candies)</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-435-Non-overlapping-Intervals"><a href="#Leetcode-435-Non-overlapping-Intervals" class="headerlink" title="[Leetcode 435] Non-overlapping Intervals"></a>[Leetcode 435] Non-overlapping Intervals</h2><p>Given an array of intervals intervals where intervals[i] &#x3D; [starti, endi], return the minimum number of intervals you need to remove to make the rest of the intervals non-overlapping.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: intervals = [[1,2],[2,3],[3,4],[1,3]]</span><br><span class="line">Output: 1</span><br><span class="line">Explanation: [1,3] can be removed and the rest of the intervals are non-overlapping.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: intervals = [[1,2],[1,2],[1,2]]</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: You need to remove two [1,2] to make the rest of the intervals non-overlapping.</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: intervals = [[1,2],[2,3]]</span><br><span class="line">Output: 0</span><br><span class="line">Explanation: You don&#x27;t need to remove any of the intervals since they&#x27;re already non-overlapping.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; intervals.length &lt;&#x3D; 105</li>
<li>intervals[i].length &#x3D;&#x3D; 2</li>
<li>-5 * 104 &lt;&#x3D; starti &lt; endi &lt;&#x3D; 5 * 104</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">eraseOverlapIntervals</span>(<span class="params">self, intervals: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># sort by end time of a meeting</span></span><br><span class="line">        intervals.sort(key = <span class="keyword">lambda</span> x : x[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># count the first meeting in</span></span><br><span class="line">        prev = <span class="number">0</span></span><br><span class="line">        count = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># count meetings only if its start time is greater than end time of previous meeting</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(intervals)):</span><br><span class="line">            <span class="keyword">if</span> intervals[i][<span class="number">0</span>] &gt;= intervals[prev][<span class="number">1</span>]:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                prev = i</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(intervals) - count</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-452-Minimum-Number-of-Arrows-to-Burst-Balloons"><a href="#Leetcode-452-Minimum-Number-of-Arrows-to-Burst-Balloons" class="headerlink" title="[Leetcode 452] Minimum Number of Arrows to Burst Balloons"></a>[Leetcode 452] Minimum Number of Arrows to Burst Balloons</h2><p>There are some spherical balloons taped onto a flat wall that represents the XY-plane. The balloons are represented as a 2D integer array points where points[i] &#x3D; [xstart, xend] denotes a balloon whose horizontal diameter stretches between xstart and xend. You do not know the exact y-coordinates of the balloons.</p>
<p>Arrows can be shot up directly vertically (in the positive y-direction) from different points along the x-axis. A balloon with xstart and xend is burst by an arrow shot at x if xstart &lt;&#x3D; x &lt;&#x3D; xend. There is no limit to the number of arrows that can be shot. A shot arrow keeps traveling up infinitely, bursting any balloons in its path.</p>
<p>Given the array points, return the minimum number of arrows that must be shot to burst all balloons.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: points = [[10,16],[2,8],[1,6],[7,12]]</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: The balloons can be burst by 2 arrows:</span><br><span class="line">- Shoot an arrow at x = 6, bursting the balloons [2,8] and [1,6].</span><br><span class="line">- Shoot an arrow at x = 11, bursting the balloons [10,16] and [7,12].</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: points = [[1,2],[3,4],[5,6],[7,8]]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: One arrow needs to be shot for each balloon for a total of 4 arrows.</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: points = [[1,2],[2,3],[3,4],[4,5]]</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: The balloons can be burst by 2 arrows:</span><br><span class="line">- Shoot an arrow at x = 2, bursting the balloons [1,2] and [2,3].</span><br><span class="line">- Shoot an arrow at x = 4, bursting the balloons [3,4] and [4,5].</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; points.length &lt;&#x3D; 105</li>
<li>points[i].length &#x3D;&#x3D; 2</li>
<li>-231 &lt;&#x3D; xstart &lt; xend &lt;&#x3D; 231 - 1</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># e.g. points = [[10,16],[2,8],[1,6],[7,12]]               </span></span><br><span class="line">    <span class="comment">#    1         6  </span></span><br><span class="line">    <span class="comment">#      2            8 </span></span><br><span class="line">    <span class="comment">#                7            12</span></span><br><span class="line">    <span class="comment">#                         10            16</span></span><br><span class="line">    <span class="comment">#  1st arrow can burst [1,6] [2,8]</span></span><br><span class="line">    <span class="comment">#  2nd arrow is needed to burst [7,12] since 7 is greater than 6</span></span><br><span class="line">    <span class="comment">#  2nd arrow can also burst [10,16] since 10 is smaller than 12</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMinArrowShots</span>(<span class="params">self, points: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># sort by end value</span></span><br><span class="line">        points.sort(key = <span class="keyword">lambda</span> x : x[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        currEnd = points[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">        res = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(points)):</span><br><span class="line">            start = points[i][<span class="number">0</span>]</span><br><span class="line">            end = points[i][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> start &gt; currEnd:</span><br><span class="line">                res += <span class="number">1</span></span><br><span class="line">                currEnd = end</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-659-Split-Array-into-Consecutive-Subsequences"><a href="#Leetcode-659-Split-Array-into-Consecutive-Subsequences" class="headerlink" title="[Leetcode 659] Split Array into Consecutive Subsequences"></a>[Leetcode 659] Split Array into Consecutive Subsequences</h2><p>You are given an integer array nums that is sorted in non-decreasing order.</p>
<p>Determine if it is possible to split nums into one or more subsequences such that both of the following conditions are true:</p>
<ul>
<li>Each subsequence is a consecutive increasing sequence (i.e. each integer is exactly one more than the previous integer).</li>
<li>All subsequences have a length of 3 or more.</li>
</ul>
<p>Return true if you can split nums according to the above conditions, or false otherwise.</p>
<p>A subsequence of an array is a new array that is formed from the original array by deleting some (can be none) of the elements without disturbing the relative positions of the remaining elements. (i.e., [1,3,5] is a subsequence of [1,2,3,4,5] while [1,3,2] is not).</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,2,3,3,4,5]</span><br><span class="line">Output: true</span><br><span class="line">Explanation: nums can be split into the following subsequences:</span><br><span class="line">[1,2,3,3,4,5] --&gt; 1, 2, 3</span><br><span class="line">[1,2,3,3,4,5] --&gt; 3, 4, 5</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,2,3,3,4,4,5,5]</span><br><span class="line">Output: true</span><br><span class="line">Explanation: nums can be split into the following subsequences:</span><br><span class="line">[1,2,3,3,4,4,5,5] --&gt; 1, 2, 3, 4, 5</span><br><span class="line">[1,2,3,3,4,4,5,5] --&gt; 3, 4, 5</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,2,3,4,4,5]</span><br><span class="line">Output: false</span><br><span class="line">Explanation: It is impossible to split nums into consecutive increasing subsequences of length 3 or more.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; nums.length &lt;&#x3D; 104</li>
<li>-1000 &lt;&#x3D; nums[i] &lt;&#x3D; 1000</li>
<li>nums is sorted in non-decreasing order.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isPossible</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># count the numbers</span></span><br><span class="line">        freq = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            freq[num] = freq.get(num, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># tracking what is needed to form subsequence</span></span><br><span class="line">        need = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="comment"># current number is not available for any subsequence</span></span><br><span class="line">            <span class="keyword">if</span> freq[num] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># current number is needed for an existing subsequence</span></span><br><span class="line">            <span class="keyword">if</span> num <span class="keyword">in</span> need <span class="keyword">and</span> need[num] &gt; <span class="number">0</span>:</span><br><span class="line">                need[num] -= <span class="number">1</span>  <span class="comment"># reduce the current number from dict</span></span><br><span class="line">                freq[num] -= <span class="number">1</span>  <span class="comment"># reduce the current number from freq</span></span><br><span class="line">                </span><br><span class="line">                need[num+<span class="number">1</span>] = need.get(num+<span class="number">1</span>, <span class="number">0</span>) + <span class="number">1</span> <span class="comment"># add the next needed number to dict</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># can&#x27;t be part of existing subsequence, so have to create a new subsequence if possible</span></span><br><span class="line">            <span class="keyword">elif</span> num+<span class="number">1</span> <span class="keyword">in</span> freq <span class="keyword">and</span> freq[num+<span class="number">1</span>] &gt; <span class="number">0</span> <span class="keyword">and</span> num+<span class="number">2</span> <span class="keyword">in</span> freq <span class="keyword">and</span> freq[num+<span class="number">2</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                freq[num] -= <span class="number">1</span></span><br><span class="line">                freq[num+<span class="number">1</span>] -= <span class="number">1</span></span><br><span class="line">                freq[num+<span class="number">2</span>] -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                need[num+<span class="number">3</span>] = need.get(num+<span class="number">3</span>, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># can&#x27;t use for any subsequence, return false</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Hash Table implementation in Python</title>
    <url>/blog/hash-table-implementation-in-python/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>Hash Table is a data structure which maps keys to values for highly efficient data search. It stores data in an array by using a hash function to generate a slot to insert the data value.</p>
<p>The following are the general steps to insert a key&#x2F;value pair to the Hash Table.</p>
<ol>
<li>Compute the key’s hash code with a hash function</li>
<li>Find the available slot by mapping the hash code to an index of array</li>
<li>Insert the value to the available slot</li>
</ol>
<h2 id="Initial-implementation"><a href="#Initial-implementation" class="headerlink" title="Initial implementation"></a>Initial implementation</h2><p>Create an empty hash table of size 10:</p>
<pre><code>hash_table = [None] * 10
</code></pre>
<p>To create a hash function, we simply return the modulus of the array length. The modulo operator(%) yields the remainder from the division of the key by the length of hash table. This ensures the hash key always falls into the range of [0,len(hash_table)-1]. So a slot in the array can be always retrieved.</p>
<pre><code>def hash_func(key):
    return key % len(hash_table)
</code></pre>
<p>To insert key&#x2F;value pair to the hash table, we firstly get the hash key and then stores the value to the retrieved slot.</p>
<pre><code>def insert(key, value):
    hash_key = hash_func(key)
    hash_table[hash_key] = value
</code></pre>
<p>To test the code, we insert two key&#x2F;value pairs (5,6) and (5,8). Both values are stored in the slot 5 of the array. However, the value 8 replaces the existing value 6 at the same slot(with the same key 5).</p>
<pre><code>hash_table = [None] * 10
print(hash_table)
insert(5,6)
print(hash_table)

insert(5,8)
print(hash_table)

# Output:
# [None, None, None, None, None, None, None, None, None, None]
# [None, None, None, None, None, 6, None, None, None, None]
# [None, None, None, None, None, 8, None, None, None, None]
</code></pre>
<h2 id="Solve-the-collision"><a href="#Solve-the-collision" class="headerlink" title="Solve the collision"></a>Solve the collision</h2><p>A hash key collision would occur when the multiple keys hit the same slot(index) in the hash table(array).</p>
<p>There are generally two techniques to resolve a collision:</p>
<ul>
<li>Linear probing(open addressing)</li>
<li>Separate chaining(open hashing)</li>
</ul>
<h3 id="Linear-probing"><a href="#Linear-probing" class="headerlink" title="Linear probing"></a>Linear probing</h3><p>In linear probing(aka open addressing), all the entries are stored in the array itself instead of linked list. If the slot to insert entry is already occupied, the next slot will be searched sequentially.</p>
<pre><code>hash_key = hash_func(key)
while hash_table[hash_key]:
    hash_key = (hash_key + 1) % len(hash_table)
</code></pre>
<h3 id="Separate-chaining"><a href="#Separate-chaining" class="headerlink" title="Separate chaining"></a>Separate chaining</h3><p>Separate chaining is a very commonly used technique for collision resolution. It is usually implemented with linked list. All the entries will be inserted into a specific linked list.</p>
<p>In python, we create a hash table as nested list below. Each hash table bucket contains a list to store data entries.</p>
<pre><code>hash_table = [[] for _ in range(10)] # init with empty list
</code></pre>
<p>The same hash function can be used.</p>
<pre><code>def hash_func(key):
    return key % len(hash_table)
</code></pre>
<p>To insert an entry to the hash table, we simply append it to the list positioned by the hash key.</p>
<pre><code>def insert(key, value):
    hash_key = hash_func(key)
    hash_table[hash_key].append(value)
</code></pre>
<p>To test the code, we still insert two key&#x2F;value pairs (5,6) and (5,8). Now, the two values 6 and 8 are stored in the slot 5 of the same list.</p>
<pre><code>hash_table = [[] for _ in range(10)] # init with empty list
print(hash_table)
insert(5,6)
print(hash_table)
insert(5,8)
print(hash_table)

# Output:
# [[], [], [], [], [], [], [], [], [], []]
# [[], [], [], [], [], [6], [], [], [], []]
# [[], [], [], [], [], [6, 8], [], [], [], []]
</code></pre>
<h2 id="Final-implementation"><a href="#Final-implementation" class="headerlink" title="Final implementation"></a>Final implementation</h2><p>Now that we know about the hash function and how to resolve hash collision, we can implement the hash table with insert, delete and search functions. We use Python built-in function hash() to generate hash code from an generic object. This allows the hash table to support generic types like integer, string and so on.</p>
<pre><code>def hash_func(key):
    hash_key = hash(key)
    print(&quot;Hash key is &#123;&#125;&quot;.format(hash_key))
    return hash_key % len(hash_table)


def insert(key, value):
    hash_key = hash_func(key)
    bucket = hash_table[hash_key]

    # check if the key already exists
    existed = False
    for idx, pair in enumerate(bucket):
        k, v = pair
        if key == k:
            existed = True
            break

    if existed:
        # update the existing value
        bucket[idx] = (key, value)
    else:
        # add the new key/value pair
        bucket.append((key, value))


def delete(key):
    hash_key = hash_func(key)
    bucket = hash_table[hash_key]

    # check if the key already exists
    existed = False
    for idx, pair in enumerate(bucket):
        k, v = pair
        if key == k:
            existed = True
            break

    if existed:
        # delete the key/value pair
        del bucket[idx]
        print(&quot;key &#123;&#125; deleted&quot;.format(key))
    else:
        # key does not exist
        print(&quot;key &#123;&#125; not found&quot;.format(key))


def search(key):
    hash_key = hash_func(key)
    bucket = hash_table[hash_key]

    # check if the key already exists
    for idx, pair in enumerate(bucket):
        k, v = pair
        if key == k:
            return v


hash_table = [[] for _ in range(10)]  # init with empty list
print(hash_table)
insert(5, 6)
print(hash_table)
insert(5, 8)
print(hash_table)
insert(3, 4)
print(hash_table)
insert(9, 999)
print(hash_table)
insert(11, 111)
print(hash_table)
insert(20, 20)
print(hash_table)
insert(1000, 1000)
print(hash_table)
print(search(11))
delete(11)
insert(&quot;11&quot;, 222)
print(hash_table)
insert(&quot;aa&quot;, &quot;val&quot;)
print(hash_table)

# Output:
# [[], [], [], [], [], [], [], [], [], []]
# Hash key is 5
# [[], [], [], [], [], [(5, 6)], [], [], [], []]
# Hash key is 5
# [[], [], [], [], [], [(5, 8)], [], [], [], []]
# Hash key is 3
# [[], [], [], [(3, 4)], [], [(5, 8)], [], [], [], []]
# Hash key is 9
# [[], [], [], [(3, 4)], [], [(5, 8)], [], [], [], [(9, 999)]]
# Hash key is 11
# [[], [(11, 111)], [], [(3, 4)], [], [(5, 8)], [], [], [], [(9, 999)]]
# Hash key is 20
#[[(20, 20)], [(11, 111)], [], [(3, 4)], [], [(5, 8)], [], [], [], [(9, 999)]]
# Hash key is 1000
# [[(20, 20), (1000, 1000)], [(11, 111)], [], [(3, 4)], [], [(5, 8)], [], [], [], [(9, 999)]]
# Hash key is 11
# 111
# Hash key is 11
# key 11 deleted
# Hash key is 3453933796215988004
# [[(20, 20), (1000, 1000)], [], [], [(3, 4)], [(&#39;11&#39;, 222)], [(5, 8)], [], [], [], [(9, 999)]]
# Hash key is -185426594550641729
# [[(20, 20), (1000, 1000)], [(&#39;aa&#39;, &#39;val&#39;)], [], [(3, 4)], [(&#39;11&#39;, 222)], [(5, 8)], [], [], [], [(9, 999)]]
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
        <tag>Hashmap</tag>
      </tags>
  </entry>
  <entry>
    <title>Hashmap and set in Python</title>
    <url>/blog/hashmap-and-set/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p><strong>Hashmap</strong> is indexed data structures. A hash map makes use of a hash function to compute an index with a key into an array of buckets or slots. Its value is mapped to the bucket with the corresponding index. The key is unique and immutable. Hash function is the core of <a href="https://www.flamingbytes.com/blog/hash-table-implementation-in-python/">implementing a hash map</a>. It takes in the key and translates it to the index of a bucket in the bucket list. Ideal hashing should produce a different index for each key. However, collisions can occur. When hashing gives an existing index, we can simply use a bucket for multiple values by appending a list or by rehashing.<span id="more"></span></p>
<p>A <strong>Set</strong> in Python is an unordered collection data type that is iterable, mutable and has no duplicate elements. Set are represented by {} (values enclosed in curly braces). The major advantage of using a set, as opposed to a list, is that it has a highly optimized method for checking whether a specific element is contained in the set. This is based on a data structure known as a hash table. Since sets are unordered, we cannot access items using indexes as we do in lists.</p>
<h2 id="Leetcode-49-Group-Anagrams"><a href="#Leetcode-49-Group-Anagrams" class="headerlink" title="[Leetcode 49] Group Anagrams"></a>[Leetcode 49] Group Anagrams</h2><p>Given an array of strings strs, group the anagrams together. You can return the answer in any order. An Anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: strs = [&quot;eat&quot;,&quot;tea&quot;,&quot;tan&quot;,&quot;ate&quot;,&quot;nat&quot;,&quot;bat&quot;]</span><br><span class="line">Output: [[&quot;bat&quot;],[&quot;nat&quot;,&quot;tan&quot;],[&quot;ate&quot;,&quot;eat&quot;,&quot;tea&quot;]]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: strs = [&quot;&quot;]</span><br><span class="line">Output: [[&quot;&quot;]]</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: strs = [&quot;a&quot;]</span><br><span class="line">Output: [[&quot;a&quot;]]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; strs.length &lt;&#x3D; 10^4</li>
<li>0 &lt;&#x3D; strs[i].length &lt;&#x3D; 100</li>
<li>strs[i] consists of lowercase English letters.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">groupAnagrams</span>(<span class="params">self, strs: <span class="type">List</span>[<span class="built_in">str</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]]:</span><br><span class="line">        m1 = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> strs:</span><br><span class="line">            s1 = <span class="string">&#x27;&#x27;</span>.join(<span class="built_in">sorted</span>(s))</span><br><span class="line">            m1[s1].append(s)</span><br><span class="line"></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> m1.values():</span><br><span class="line">            res.append(group)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-128-Longest-Consecutive-Sequence"><a href="#Leetcode-128-Longest-Consecutive-Sequence" class="headerlink" title="[Leetcode 128] Longest Consecutive Sequence"></a>[Leetcode 128] Longest Consecutive Sequence</h2><p>Given an unsorted array of integers nums, return the length of the longest consecutive elements sequence.</p>
<p>You must write an algorithm that runs in O(n) time.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [100,4,200,1,3,2]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: The longest consecutive elements sequence is [1, 2, 3, 4]. Therefore its length is 4.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [0,3,7,2,5,8,4,6,0,1]</span><br><span class="line">Output: 9</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>0 &lt;&#x3D; nums.length &lt;&#x3D; 10^5</li>
<li>-10^9 &lt;&#x3D; nums[i] &lt;&#x3D; 10^9</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestConsecutive</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        s1 = <span class="built_in">set</span>(nums)</span><br><span class="line">        longest = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="comment"># found a new sequence and count its length</span></span><br><span class="line">            <span class="keyword">if</span> num - <span class="number">1</span> <span class="keyword">not</span> <span class="keyword">in</span> s1:</span><br><span class="line">                count = <span class="number">0</span></span><br><span class="line">                <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                    <span class="keyword">if</span> num <span class="keyword">in</span> s1:</span><br><span class="line">                        count += <span class="number">1</span></span><br><span class="line">                        num += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># save to longest</span></span><br><span class="line">                longest = <span class="built_in">max</span>(longest, count)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> longest</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-138-Copy-List-with-Random-Pointer"><a href="#Leetcode-138-Copy-List-with-Random-Pointer" class="headerlink" title="[Leetcode 138] Copy List with Random Pointer"></a>[Leetcode 138] Copy List with Random Pointer</h2><p>A linked list of length n is given such that each node contains an additional random pointer, which could point to any node in the list, or null.</p>
<p>Construct a deep copy of the list. The deep copy should consist of exactly n brand new nodes, where each new node has its value set to the value of its corresponding original node. Both the next and random pointer of the new nodes should point to new nodes in the copied list such that the pointers in the original list and copied list represent the same list state. None of the pointers in the new list should point to nodes in the original list.</p>
<p>For example, if there are two nodes X and Y in the original list, where X.random –&gt; Y, then for the corresponding two nodes x and y in the copied list, x.random –&gt; y.</p>
<p>Return the head of the copied linked list.</p>
<p>The linked list is represented in the input&#x2F;output as a list of n nodes. Each node is represented as a pair of [val, random_index] where:</p>
<ul>
<li>val: an integer representing Node.val</li>
<li>random_index: the index of the node (range from 0 to n-1) that the random pointer points to, or null if it does not point to any node.</li>
</ul>
<p>Your code will only be given the head of the original linked list. </p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [[7,null],[13,0],[11,4],[10,2],[1,0]]</span><br><span class="line">Output: [[7,null],[13,0],[11,4],[10,2],[1,0]]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [[1,1],[2,1]]</span><br><span class="line">Output: [[1,1],[2,1]]</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [[3,null],[3,0],[3,null]]</span><br><span class="line">Output: [[3,null],[3,0],[3,null]]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>0 &lt;&#x3D; n &lt;&#x3D; 1000</li>
<li>-10^4 &lt;&#x3D; Node.val &lt;&#x3D; 10^4</li>
<li>Node.random is null or is pointing to some node in the linked list.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># Definition for a Node.</span></span><br><span class="line"><span class="string">class Node:</span></span><br><span class="line"><span class="string">    def __init__(self, x: int, next: &#x27;Node&#x27; = None, random: &#x27;Node&#x27; = None):</span></span><br><span class="line"><span class="string">        self.val = int(x)</span></span><br><span class="line"><span class="string">        self.next = next</span></span><br><span class="line"><span class="string">        self.random = random</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">copyRandomList</span>(<span class="params">self, head: <span class="string">&#x27;Optional[Node]&#x27;</span></span>) -&gt; <span class="string">&#x27;Optional[Node]&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        oldToNew = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># copy a new node for each original node</span></span><br><span class="line">        curr = head</span><br><span class="line">        <span class="keyword">while</span> curr:</span><br><span class="line">            oldToNew[curr] = Node(curr.val)</span><br><span class="line">            curr = curr.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        curr = head</span><br><span class="line">        <span class="keyword">while</span> curr:</span><br><span class="line">            <span class="comment"># get() returns None if the key doesn&#x27;t exist</span></span><br><span class="line">            oldToNew[curr].<span class="built_in">next</span> = oldToNew.get(curr.<span class="built_in">next</span>) </span><br><span class="line">            oldToNew[curr].random = oldToNew.get(curr.random)</span><br><span class="line">            curr = curr.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> oldToNew[head]</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-290-Word-Pattern"><a href="#Leetcode-290-Word-Pattern" class="headerlink" title="[Leetcode 290] Word Pattern"></a>[Leetcode 290] Word Pattern</h2><p>Given a pattern and a string s, find if s follows the same pattern. Here follow means a full match, such that there is a bijection between a letter in pattern and a non-empty word in s.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: pattern = &quot;abba&quot;, s = &quot;dog cat cat dog&quot;</span><br><span class="line">Output: true</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: pattern = &quot;abba&quot;, s = &quot;dog cat cat fish&quot;</span><br><span class="line">Output: false</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: pattern = &quot;aaaa&quot;, s = &quot;dog cat cat dog&quot;</span><br><span class="line">Output: false</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; pattern.length &lt;&#x3D; 300</li>
<li>pattern contains only lower-case English letters.</li>
<li>1 &lt;&#x3D; s.length &lt;&#x3D; 3000</li>
<li>s contains only lowercase English letters and spaces ‘ ‘.</li>
<li>s does not contain any leading or trailing spaces.</li>
<li>All the words in s are separated by a single space.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wordPattern</span>(<span class="params">self, pattern: <span class="built_in">str</span>, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        m1 = &#123;&#125;</span><br><span class="line">        words = s.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(pattern) != <span class="built_in">len</span>(words):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># a pattern character can only be mapped to a word. </span></span><br><span class="line">        <span class="comment"># e.g. if &#x27;dog&#x27; is already mapped to &#x27;a&#x27;, then a different word like &#x27;fish&#x27; can&#x27;t be mapped to &#x27;a&#x27;</span></span><br><span class="line">        mapped = [<span class="literal">False</span>] * <span class="number">26</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(words):</span><br><span class="line">            <span class="comment"># get the corresponding letter in the pattern</span></span><br><span class="line">            letter = pattern[j]</span><br><span class="line">            idx = <span class="built_in">ord</span>(letter) - <span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># put new word in map if possible</span></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> m1:</span><br><span class="line">                <span class="keyword">if</span> mapped[idx]:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    m1[word] = letter</span><br><span class="line">                    mapped[idx] = <span class="literal">True</span> </span><br><span class="line"></span><br><span class="line">            <span class="comment"># check if the already mapped pattern letter equals the current letter</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> m1[word] != letter:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-380-Insert-Delete-GetRandom-O-1"><a href="#Leetcode-380-Insert-Delete-GetRandom-O-1" class="headerlink" title="[Leetcode 380] Insert Delete GetRandom O(1)"></a>[Leetcode 380] Insert Delete GetRandom O(1)</h2><p>Implement the RandomizedSet class:</p>
<ul>
<li>RandomizedSet() Initializes the RandomizedSet object.</li>
<li>bool insert(int val) Inserts an item val into the set if not present. Returns true if the item was not present, false otherwise.</li>
<li>bool remove(int val) Removes an item val from the set if present. Returns true if the item was present, false otherwise.</li>
<li>int getRandom() Returns a random element from the current set of elements (it’s guaranteed that at least one element exists when this method is called). Each element must have the same probability of being returned.</li>
</ul>
<p>You must implement the functions of the class such that each function works in average O(1) time complexity.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input:</span><br><span class="line">[&quot;RandomizedSet&quot;, &quot;insert&quot;, &quot;remove&quot;, &quot;insert&quot;, &quot;getRandom&quot;, &quot;remove&quot;, &quot;insert&quot;, &quot;getRandom&quot;]</span><br><span class="line">[[], [1], [2], [2], [], [1], [2], []]</span><br><span class="line">Output:</span><br><span class="line">[null, true, false, true, 2, true, false, 2]</span><br><span class="line"></span><br><span class="line">Explanation</span><br><span class="line">RandomizedSet randomizedSet = new RandomizedSet();</span><br><span class="line">randomizedSet.insert(1); // Inserts 1 to the set. Returns true as 1 was inserted successfully.</span><br><span class="line">randomizedSet.remove(2); // Returns false as 2 does not exist in the set.</span><br><span class="line">randomizedSet.insert(2); // Inserts 2 to the set, returns true. Set now contains [1,2].</span><br><span class="line">randomizedSet.getRandom(); // getRandom() should return either 1 or 2 randomly.</span><br><span class="line">randomizedSet.remove(1); // Removes 1 from the set, returns true. Set now contains [2].</span><br><span class="line">randomizedSet.insert(2); // 2 was already in the set, so return false.</span><br><span class="line">randomizedSet.getRandom(); // Since 2 is the only number in the set, getRandom() will always return 2.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>-231 &lt;&#x3D; val &lt;&#x3D; 231 - 1</li>
<li>At most 2 * 105 calls will be made to insert, remove, and getRandom.</li>
<li>There will be at least one element in the data structure when getRandom is called.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomizedSet</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.nums = [] <span class="comment"># for random access</span></span><br><span class="line">        self.<span class="built_in">map</span> = &#123;&#125;  <span class="comment"># value to index map helps locate the target value in array to be removed</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">insert</span>(<span class="params">self, val: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">if</span> val <span class="keyword">in</span> self.<span class="built_in">map</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        self.nums.append(val)  <span class="comment"># always append to the end of list</span></span><br><span class="line">        self.<span class="built_in">map</span>[val] = <span class="built_in">len</span>(self.nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">remove</span>(<span class="params">self, val: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">if</span> val <span class="keyword">not</span> <span class="keyword">in</span> self.<span class="built_in">map</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the index of target value</span></span><br><span class="line">        index = self.<span class="built_in">map</span>[val]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># swap the target value with the last value in array</span></span><br><span class="line">        <span class="keyword">if</span> index != <span class="built_in">len</span>(self.nums) - <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># replace with the last element</span></span><br><span class="line">            lastIdx = <span class="built_in">len</span>(self.nums) - <span class="number">1</span></span><br><span class="line">            lastVal = self.nums[lastIdx]</span><br><span class="line">            self.nums[index], self.nums[lastIdx] = lastVal, val</span><br><span class="line">            self.<span class="built_in">map</span>[lastVal] = index</span><br><span class="line"></span><br><span class="line">        <span class="comment"># remove the target value from array</span></span><br><span class="line">        self.nums.pop()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># remove the target value from map</span></span><br><span class="line">        <span class="keyword">del</span> self.<span class="built_in">map</span>[val]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># random access to the array</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getRandom</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> self.nums[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(self.nums) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Your RandomizedSet object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"># obj = RandomizedSet()</span></span><br><span class="line"><span class="comment"># param_1 = obj.insert(val)</span></span><br><span class="line"><span class="comment"># param_2 = obj.remove(val)</span></span><br><span class="line"><span class="comment"># param_3 = obj.getRandom()</span></span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-1657-Determine-if-Two-Strings-Are-Close"><a href="#Leetcode-1657-Determine-if-Two-Strings-Are-Close" class="headerlink" title="[Leetcode 1657] Determine if Two Strings Are Close"></a>[Leetcode 1657] Determine if Two Strings Are Close</h2><p>Two strings are considered close if you can attain one from the other using the following operations:</p>
<ul>
<li>Operation 1: Swap any two existing characters.<br>For example, abcde -&gt; aecdb</li>
<li>Operation 2: Transform every occurrence of one existing character into another existing character, and do the same with the other character.<br>For example, aacabb -&gt; bbcbaa (all a’s turn into b’s, and all b’s turn into a’s)</li>
</ul>
<p>You can use the operations on either string as many times as necessary.</p>
<p>Given two strings, word1 and word2, return true if word1 and word2 are close, and false otherwise.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: word1 = &quot;abc&quot;, word2 = &quot;bca&quot;</span><br><span class="line">Output: true</span><br><span class="line">Explanation: You can attain word2 from word1 in 2 operations.</span><br><span class="line">Apply Operation 1: &quot;abc&quot; -&gt; &quot;acb&quot;</span><br><span class="line">Apply Operation 1: &quot;acb&quot; -&gt; &quot;bca&quot;</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: word1 = &quot;a&quot;, word2 = &quot;aa&quot;</span><br><span class="line">Output: false</span><br><span class="line">Explanation: It is impossible to attain word2 from word1, or vice versa, in any number of operations.</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: word1 = &quot;cabbba&quot;, word2 = &quot;abbccc&quot;</span><br><span class="line">Output: true</span><br><span class="line">Explanation: You can attain word2 from word1 in 3 operations.</span><br><span class="line">Apply Operation 1: &quot;cabbba&quot; -&gt; &quot;caabbb&quot;</span><br><span class="line">Apply Operation 2: &quot;caabbb&quot; -&gt; &quot;baaccc&quot;</span><br><span class="line">Apply Operation 2: &quot;baaccc&quot; -&gt; &quot;abbccc&quot;</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; word1.length, word2.length &lt;&#x3D; 105</li>
<li>word1 and word2 contain only lowercase English letters.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">closeStrings</span>(<span class="params">self, word1: <span class="built_in">str</span>, word2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># e.g. word1=&quot;abc&quot;, word2=&quot;bca&quot; -&gt; Counter(&#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 1, &#x27;c&#x27;: 1&#125;) Counter(&#123;&#x27;b&#x27;: 1, &#x27;c&#x27;: 1, &#x27;a&#x27;: 1&#125;)</span></span><br><span class="line">        c1 , c2 = Counter(word1), Counter(word2)</span><br><span class="line">        <span class="keyword">return</span> c1.keys() == c2.keys() <span class="keyword">and</span> <span class="built_in">sorted</span>(c1.values()) == <span class="built_in">sorted</span>(c2.values())</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
        <tag>Hashmap</tag>
        <tag>Hashset</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello Hexo World</title>
    <url>/blog/hello-hexo-world/</url>
    <content><![CDATA[<p>The blog is migrated to <a href="https://hexo.io/">Hexo</a> today! Check <a href="https://hexo.io/docs/">documentation</a> for more info. <span id="more"></span></p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="What-is-Hexo"><a href="#What-is-Hexo" class="headerlink" title="What is Hexo?"></a>What is Hexo?</h3><p>Hexo is a fast, simple and powerful blog framework. You write posts in Markdown (or other markup languages) and Hexo generates static files with a beautiful theme in seconds.</p>
<h3 id="Installation-Requirements"><a href="#Installation-Requirements" class="headerlink" title="Installation Requirements"></a>Installation Requirements</h3><p>Installing Hexo is quite easy and only requires the following beforehand:</p>
<ul>
<li>Node.js (Should be at least Node.js 10.13, recommends 12.0 or higher)</li>
<li>Git</li>
</ul>
<h3 id="Install-Hexo"><a href="#Install-Hexo" class="headerlink" title="Install Hexo"></a>Install Hexo</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo npm install -g hexo-cli</span><br><span class="line"></span><br><span class="line">$ hexo -v</span><br><span class="line">hexo-cli: 4.3.1</span><br><span class="line">os: darwin 20.4.0 11.3</span><br><span class="line">node: 18.16.1</span><br><span class="line">acorn: 8.8.2</span><br><span class="line">ada: 1.0.4</span><br><span class="line">ares: 1.19.1</span><br><span class="line">brotli: 1.0.9</span><br><span class="line">cldr: 42.0</span><br><span class="line">icu: 72.1</span><br><span class="line">llhttp: 6.0.11</span><br><span class="line">modules: 108</span><br><span class="line">napi: 8</span><br><span class="line">nghttp2: 1.52.0</span><br><span class="line">nghttp3: 0.7.0</span><br><span class="line">ngtcp2: 0.8.1</span><br><span class="line">openssl: 3.0.9+quic</span><br><span class="line">simdutf: 3.2.2</span><br><span class="line">tz: 2022g</span><br><span class="line">undici: 5.21.0</span><br><span class="line">unicode: 15.0</span><br><span class="line">uv: 1.44.2</span><br><span class="line">uvwasi: 0.0.15</span><br><span class="line">v8: 10.2.154.26-node.26</span><br><span class="line">zlib: 1.2.13</span><br></pre></td></tr></table></figure>

<h3 id="Initialize-Hexo"><a href="#Initialize-Hexo" class="headerlink" title="Initialize Hexo"></a>Initialize Hexo</h3><p>Once Hexo is installed, run the following commands to initialize Hexo in the target <folder>.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo init &lt;site-folder&gt;</span><br><span class="line">$ <span class="built_in">cd</span> &lt;site-folder&gt;</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure>

<p>Once initialized, here’s what your project folder will look like:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">└── themes</span><br></pre></td></tr></table></figure>

<p>You can configure most settings in _config.yml.</p>
<h3 id="Install-Next-theme"><a href="#Install-Next-theme" class="headerlink" title="Install Next theme"></a>Install Next theme</h3><p>If you’re using Hexo 5.0 or later, the simplest way to install is through npm:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> hexo-site</span><br><span class="line">$ npm install hexo-theme-next</span><br></pre></td></tr></table></figure>

<p>Or you can clone the entire repository:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> hexo-site</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/next-theme/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure>

<h3 id="Install-hexo-generator-searchdb"><a href="#Install-hexo-generator-searchdb" class="headerlink" title="Install hexo-generator-searchdb"></a>Install hexo-generator-searchdb</h3><p>This is Search data generator plugin for Hexo.</p>
<p>This plugin is used for generating a search index file, which contains all the necessary data of your articles that you can use to write a local search engine for your blog. Supports both XML and JSON format output.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">% sudo npm install hexo-generator-searchdb</span><br></pre></td></tr></table></figure>

<h3 id="Related-Popular-Posts"><a href="#Related-Popular-Posts" class="headerlink" title="Related Popular Posts"></a>Related Popular Posts</h3><p>NexT supports the related posts functionality according to <a href="https://github.com/sergeyzwezdin/hexo-related-posts">hexo-related-posts</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">%  npm install hexo-related-posts --save-dev</span><br><span class="line"></span><br><span class="line">% hexo clean</span><br><span class="line">INFO  Validating config</span><br><span class="line">INFO  Deleted database.</span><br><span class="line"></span><br><span class="line">% hexo server</span><br><span class="line">INFO  Validating config</span><br><span class="line">INFO  ==================================</span><br><span class="line">  ███╗   ██╗███████╗██╗  ██╗████████╗</span><br><span class="line">  ████╗  ██║██╔════╝╚██╗██╔╝╚══██╔══╝</span><br><span class="line">  ██╔██╗ ██║█████╗   ╚███╔╝    ██║</span><br><span class="line">  ██║╚██╗██║██╔══╝   ██╔██╗    ██║</span><br><span class="line">  ██║ ╚████║███████╗██╔╝ ██╗   ██║</span><br><span class="line">  ╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝   ╚═╝</span><br><span class="line">========================================</span><br><span class="line">NexT version 8.18.1</span><br><span class="line">Documentation: https://theme-next.js.org</span><br><span class="line">========================================</span><br><span class="line">INFO  Start processing</span><br><span class="line">INFO  Calculating of related posts is enabled. Start processing...</span><br><span class="line">INFO  TF/IDF is calculated</span><br><span class="line">INFO  Related post processing <span class="keyword">done</span></span><br><span class="line">INFO  Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure>

<h3 id="Start-Hexo-server-locally"><a href="#Start-Hexo-server-locally" class="headerlink" title="Start Hexo server locally"></a>Start Hexo server locally</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">% cd &lt;site-folder&gt;</span><br><span class="line">% hexo server</span><br><span class="line">INFO  Validating config</span><br><span class="line">INFO  ==================================</span><br><span class="line">  ███╗   ██╗███████╗██╗  ██╗████████╗</span><br><span class="line">  ████╗  ██║██╔════╝╚██╗██╔╝╚══██╔══╝</span><br><span class="line">  ██╔██╗ ██║█████╗   ╚███╔╝    ██║</span><br><span class="line">  ██║╚██╗██║██╔══╝   ██╔██╗    ██║</span><br><span class="line">  ██║ ╚████║███████╗██╔╝ ██╗   ██║</span><br><span class="line">  ╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝   ╚═╝</span><br><span class="line">========================================</span><br><span class="line">NexT version 8.18.1</span><br><span class="line">Documentation: https://theme-next.js.org</span><br><span class="line">========================================</span><br><span class="line">INFO  Start processing</span><br><span class="line">INFO  Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
<h2 id="github-pages"><a href="#github-pages" class="headerlink" title="github-pages"></a>github-pages</h2><h3 id="Config-git-login-credential"><a href="#Config-git-login-credential" class="headerlink" title="Config git login credential"></a>Config git login credential</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;username&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;xxx&quot;</span></span><br><span class="line">git config credential.username <span class="string">&quot;username&quot;</span></span><br><span class="line">git config --global core.autocrlf <span class="literal">false</span></span><br><span class="line">git config --global --list</span><br></pre></td></tr></table></figure>

<h3 id="Use-one-command-deployment-if-you-prefer-not-to-upload-your-source-folder-to-GitHub"><a href="#Use-one-command-deployment-if-you-prefer-not-to-upload-your-source-folder-to-GitHub" class="headerlink" title="Use one-command deployment if you prefer not to upload your source folder to GitHub"></a>Use one-command deployment if you prefer not to upload your source folder to GitHub</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Install hexo-deployer-git</span></span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the following configurations to hexo _config.yml, (remove existing lines if any).</span></span><br><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: https://github.com/&lt;username&gt;/&lt;username.github.io&gt;</span><br><span class="line">  <span class="comment"># example, https://github.com/hexojs/hexojs.github.io</span></span><br><span class="line">  branch: gh-pages</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run hexo clean &amp;&amp; hexo deploy</span></span><br><span class="line">hexo clean </span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure>

<h3 id="Follow-the-instructions-here-to-change-the-branch-to-“gh-pages”-in-order-to-build-the-github-pages-website"><a href="#Follow-the-instructions-here-to-change-the-branch-to-“gh-pages”-in-order-to-build-the-github-pages-website" class="headerlink" title="Follow the instructions here to change the branch to “gh-pages” in order to build the github pages website."></a>Follow the instructions <a href="https://docs.github.com/en/pages/quickstart">here</a> to change the branch to “gh-pages” in order to build the github pages website.</h3><h3 id="Check-the-webpage-at-username-github-io"><a href="#Check-the-webpage-at-username-github-io" class="headerlink" title="Check the webpage at username.github.io."></a>Check the webpage at username.github.io.</h3><h3 id="Watch-for-File-Changes"><a href="#Watch-for-File-Changes" class="headerlink" title="Watch for File Changes"></a>Watch for File Changes</h3><p>Hexo can watch for file changes and regenerate files immediately. Hexo will compare the SHA1 checksum of your files and only write if file changes are detected.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate --watch</span><br></pre></td></tr></table></figure>

<h3 id="Deploy-After-Generating"><a href="#Deploy-After-Generating" class="headerlink" title="Deploy After Generating"></a>Deploy After Generating</h3><p>CNAME file should be added to source folder. Otherwise, the custom domain for github pages will be reset after deploy.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">ls</span> <span class="built_in">source</span></span><br><span class="line">Mode                 LastWriteTime         Length Name</span><br><span class="line">----                 -------------         ------ ----</span><br><span class="line">d-----         9/21/2023   6:34 PM                _posts</span><br><span class="line">d-----         9/19/2023   9:07 PM                about</span><br><span class="line">d-----         9/19/2023   9:07 PM                categories</span><br><span class="line">d-----         9/18/2023  12:42 PM                images</span><br><span class="line">d-----         9/19/2023   9:07 PM                tags</span><br><span class="line">-a----         9/21/2023   8:28 PM             38 CNAME</span><br></pre></td></tr></table></figure>

<p>The content of CNAME file is as below. It includes the domain name of the website.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">www.flamingbytes.com</span><br><span class="line">flamingbytes.com</span><br></pre></td></tr></table></figure>

<p>To deploy after generating, you can run one of the following commands. There is no difference between the two.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate --deploy</span><br><span class="line">$ hexo deploy --generate</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://hexo.io/docs/">https://hexo.io/docs/</a></li>
<li><a href="https://github.com/next-theme/hexo-theme-next">https://github.com/next-theme/hexo-theme-next</a></li>
<li><a href="https://theme-next.js.org/docs/">https://theme-next.js.org/docs/</a></li>
<li><a href="https://docs.github.com/en/pages/quickstart">https://docs.github.com/en/pages/quickstart</a></li>
<li><a href="https://hexo.io/docs/generating.html">https://hexo.io/docs/generating.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>How to configure SAR data collection on RHEL8</title>
    <url>/blog/how-to-configure-sar-data-collection-on-rhel8/</url>
    <content><![CDATA[<p>On RHEL8, it uses systemd instead of cron jobs to manage SAR data collection service.</p>
<p>Run the following command to check if the SAR data collection is started.</p>
<pre><code>[root@h04-11 ~]# cat /etc/redhat-release
Red Hat Enterprise Linux release 8.5 (Ootpa)

[root@h04-11 ~]# systemctl status sysstat-collect.timer
● sysstat-collect.timer - Run system activity accounting tool every 10 minutes
   Loaded: loaded (/usr/lib/systemd/system/sysstat-collect.timer; enabled; vendor preset: disabled)
   Active: inactive (dead)
  Trigger: n/a
</code></pre>
<p>If it’s not started, run the following command to start it.</p>
<pre><code>[root@h04-11 ~]# systemctl start sysstat-collect.timer
[root@h04-11 ~]# systemctl status sysstat-collect.timer
● sysstat-collect.timer - Run system activity accounting tool every 10 minutes
   Loaded: loaded (/usr/lib/systemd/system/sysstat-collect.timer; enabled; vendor preset: disabled)
   Active: active (waiting) since Tue 2022-01-04 19:49:54 UTC; 1s ago
  Trigger: Tue 2022-01-04 19:50:00 UTC; 4s left

Jan 04 19:49:54 h04-11 systemd[1]: Started Run system activity accounting tool every 10 minutes.
</code></pre>
<p>Check the sar file existence after a few minutes as below.</p>
<pre><code>[root@h04-11 ~]# ls -ltr /var/log/sa
total 0
[root@h04-11 ~]# date
Tue Jan  4 19:50:25 UTC 2022
[root@h04-11 ~]# ls -ltr /var/log/sa
total 12
-rw-r--r--. 1 root root 11632 Jan  4 19:50 sa04
</code></pre>
<p>Check the default interval of SAR data collection as below.</p>
<pre><code>[root@h04-11 ~]# systemctl cat sysstat-collect.timer
# /usr/lib/systemd/system/sysstat-collect.timer
# /usr/lib/systemd/system/sysstat-collect.timer
# (C) 2014 Tomasz Torcz &lt;tomek@pipebreaker.pl&gt;
#
# sysstat-11.7.3 systemd unit file:
#        Activates activity collector every 10 minutes

[Unit]
Description=Run system activity accounting tool every 10 minutes

[Timer]
OnCalendar=*:00/10

[Install]
WantedBy=sysstat.service
</code></pre>
<p>To change the interval of SAR data collection, edit the systemd unit file as below.</p>
<pre><code>[root@h04-11 ~]# export SYSTEMD_EDITOR=/usr/bin/vi
[root@h04-11 ~]# systemctl edit sysstat-collect.timer
</code></pre>
<p>Add the following to set the desired interval. In this example, we changed it from 10 minutes to 1 minute. The blank “OnCalendar&#x3D;” directive is there to remove the original setting.</p>
<pre><code>[Unit]
Description=Run system activity accounting tool every 1 minute

[Timer]
OnCalendar=
OnCalendar=*:00/1
</code></pre>
<p>Reload systemd service to apply the change.</p>
<pre><code>[root@h04-11 ~]# systemctl daemon-reload

[root@h04-11 ~]# systemctl cat sysstat-collect.timer
# /usr/lib/systemd/system/sysstat-collect.timer
# /usr/lib/systemd/system/sysstat-collect.timer
# (C) 2014 Tomasz Torcz &lt;tomek@pipebreaker.pl&gt;
#
# sysstat-11.7.3 systemd unit file:
#        Activates activity collector every 10 minutes

[Unit]
Description=Run system activity accounting tool every 10 minutes

[Timer]
OnCalendar=*:00/10

[Install]
WantedBy=sysstat.service

# /etc/systemd/system/sysstat-collect.timer.d/override.conf
[Unit]
Description=Run system activity accounting tool every 1 minute

[Timer]
OnCalendar=
OnCalendar=*:00/1

[root@h04-11 ~]# cat /etc/systemd/system/sysstat-collect.timer.d/override.conf
[Unit]
Description=Run system activity accounting tool every 1 minute

[Timer]
OnCalendar=
OnCalendar=*:00/1

[root@h04-11 ~]# systemctl status sysstat-collect.timer
● sysstat-collect.timer - Run system activity accounting tool every 1 minute
   Loaded: loaded (/usr/lib/systemd/system/sysstat-collect.timer; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/sysstat-collect.timer.d
           └─override.conf
   Active: active (running) since Tue 2022-01-04 19:49:54 UTC; 27min ago
  Trigger: n/a

Jan 04 19:49:54 h04-11 systemd[1]: Started Run system activity accounting tool every 10 minutes.
</code></pre>
<p>To verify if the interval of SAR data collection is modified successfully, do the following.</p>
<pre><code>[root@h04-11 ~]# sar -u -f /var/log/sa/sa04
Linux 4.18.0-348.2.1.el8_5.x86_64 (h04-11) 	01/04/2022 	_x86_64_	(32 CPU)

07:50:18 PM     CPU     %user     %nice   %system   %iowait    %steal     %idle
08:00:18 PM     all      0.11      0.00      0.13      0.00      0.00     99.76
08:10:08 PM     all      0.11      0.00      0.13      0.00      0.00     99.75
08:14:08 PM     all      0.12      0.00      0.13      0.00      0.00     99.74
08:15:35 PM     all      0.12      0.00      0.13      0.01      0.00     99.73
08:16:18 PM     all      0.12      0.00      0.14      0.02      0.00     99.72
08:17:07 PM     all      0.10      0.00      0.13      0.00      0.00     99.77
08:18:18 PM     all      0.12      0.00      0.13      0.00      0.00     99.75
Average:        all      0.11      0.00      0.13      0.00      0.00     99.75
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://access.redhat.com/solutions/276533">https://access.redhat.com/solutions/276533</a></li>
<li><a href="https://access.redhat.com/solutions/5115491">https://access.redhat.com/solutions/5115491</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Sar</tag>
      </tags>
  </entry>
  <entry>
    <title>How to delete partition in Linux</title>
    <url>/blog/how-to-delete-partition-in-linux/</url>
    <content><![CDATA[<h2 id="Identify-the-disk-which-contains-the-partitions"><a href="#Identify-the-disk-which-contains-the-partitions" class="headerlink" title="Identify the disk which contains the partitions"></a>Identify the disk which contains the partitions</h2><pre><code>$ lsblk
NAME                         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sr0                           11:0    1   9.5G  0 rom
nvme0n1                      259:6    0   1.5T  0 disk
├─nvme0n1p1                  259:7    0   200M  0 part /boot/efi
├─nvme0n1p2                  259:8    0     1G  0 part /boot
└─nvme0n1p3                  259:9    0   1.5T  0 part
  ├─centos_init500--c11-root 253:0    0    50G  0 lvm  /
  ├─centos_init500--c11-swap 253:1    0     4G  0 lvm  [SWAP]
  └─centos_init500--c11-home 253:2    0   1.4T  0 lvm  /home
nvme1n1                      259:10   0   1.5T  0 disk
├─nvme1n1p1                  259:13   0   100M  0 part
├─nvme1n1p5                  259:14   0     4G  0 part
├─nvme1n1p6                  259:15   0     4G  0 part
├─nvme1n1p7                  259:17   0 119.9G  0 part
└─nvme1n1p8                  259:18   0   1.3T  0 part


$ fdisk -l
[..]
Disk /dev/nvme1n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x7f33a7d2

        Device Boot      Start         End      Blocks   Id  System
/dev/nvme1n1p1               1  3125627534  1562813767   ee  GPT
[..]
</code></pre>
<h2 id="Delete-the-partitions"><a href="#Delete-the-partitions" class="headerlink" title="Delete the partitions"></a>Delete the partitions</h2><p>Select the disk that contains the partitions you intend to delete:</p>
<pre><code>$ fdisk /dev/nvme1n1
Welcome to fdisk (util-linux 2.23.2).

Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.
</code></pre>
<p>Delete the partition:</p>
<pre><code>Command (m for help): d
Selected partition 1
Partition 1 is deleted
</code></pre>
<p><strong>NOTE:</strong> The partition is automatically selected if there are no other partitions on disk. If the disk contains multiple partitions, select a partition that you want to delete.</p>
<p>Verify the paritions:</p>
<pre><code>Command (m for help): p

Disk /dev/nvme1n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x7f33a7d2

        Device Boot      Start         End      Blocks   Id  System
</code></pre>
<p>Save the change and quit fdisk:</p>
<pre><code>Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
</code></pre>
<p>Verify the partitions with lsblk:</p>
<pre><code>$ lsblk
NAME                         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0                           11:0    1  9.5G  0 rom
nvme0n1                      259:6    0  1.5T  0 disk
├─nvme0n1p1                  259:7    0  200M  0 part /boot/efi
├─nvme0n1p2                  259:8    0    1G  0 part /boot
└─nvme0n1p3                  259:9    0  1.5T  0 part
  ├─centos_init500--c11-root 253:0    0   50G  0 lvm  /
  ├─centos_init500--c11-swap 253:1    0    4G  0 lvm  [SWAP]
  └─centos_init500--c11-home 253:2    0  1.4T  0 lvm  /home
nvme1n1                      259:10   0  1.5T  0 disk
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Administration</tag>
      </tags>
  </entry>
  <entry>
    <title>How to deploy MySQL and phpMyAdmin with Docker</title>
    <url>/blog/how-to-deploy-mysql-and-phpmyadmin-with-docker/</url>
    <content><![CDATA[<h2 id="Create-the-MySQL-docker-container"><a href="#Create-the-MySQL-docker-container" class="headerlink" title="Create the MySQL docker container"></a>Create the MySQL docker container</h2><pre><code>$ docker pull mysql/mysql-server:5.7
$ docker run -d --name=mysqldb -e MYSQL_ROOT_HOST=% -e MYSQL_ROOT_PASSWORD=password -e MYSQL_DATABASE=perfdb -v /var/lib/osd/mounts/mysql_data:/var/lib/mysql -p 3306:3306 mysql/mysql-server:5.7
$ docker logs mysqldb
$ docker ps -a
CONTAINER ID   IMAGE                    COMMAND                  CREATED          STATUS                    PORTS                                                  NAMES
ff380903d3a0   mysql/mysql-server:5.7   &quot;/entrypoint.sh mysq…&quot;   34 seconds ago   Up 34 seconds (healthy)   0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp, 33060/tcp   mysqldb
</code></pre>
<p>Note:</p>
<ul>
<li>The option -e passes a value to the container environment variable.</li>
<li>The variable MYSQL_ROOT_HOST&#x3D;% creates a root user with permission to login from any IP address.</li>
<li>The variable MYSQL_ROOT_PASSWORD creates the root password of MySQL. If not provided, a random password will be generated.</li>
</ul>
<h2 id="Create-MySQL-database-and-restore-from-a-db-backup-if-exists"><a href="#Create-MySQL-database-and-restore-from-a-db-backup-if-exists" class="headerlink" title="Create MySQL database and restore from a db backup(if exists)"></a>Create MySQL database and restore from a db backup(if exists)</h2><pre><code>$ docker exec -it mysqldb bash

bash-4.2# mysql -V
mysql  Ver 14.14 Distrib 5.7.37, for Linux (x86_64) using  EditLine wrapper

bash-4.2# mysql -u root -p

mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| perfdb             |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.00 sec)

mysql&gt; SELECT user,authentication_string,plugin,host FROM mysql.user;
+---------------+-------------------------------------------+-----------------------+-----------+
| user          | authentication_string                     | plugin                | host      |
+---------------+-------------------------------------------+-----------------------+-----------+
| root          | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | mysql_native_password | localhost |
| mysql.session | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost |
| mysql.sys     | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost |
| healthchecker | *36C82179AFA394C4B9655005DD2E482D30A4BDF7 | mysql_native_password | localhost |
| root          | *2470C0C06DEE42FD1618BB99005ADCA2EC9D1E19 | mysql_native_password | %         |
+---------------+-------------------------------------------+-----------------------+-----------+
5 rows in set (0.01 sec)

bash-4.2# mysql -u root -p perfdb &lt; /var/lib/mysql/perfdb_dump_20220412_121339.sql
bash-4.4# ls -ltr /var/lib/mysql

mysql&gt; use mydb;
mysql&gt; show tables
mysql&gt; SELECT table_schema &quot;DB Name&quot;, ROUND(SUM(data_length + index_length) / 1024 / 1024, 1) &quot;DB Size in MB&quot;  FROM information_schema.tables  GROUP BY table_schema;
</code></pre>
<h2 id="Create-the-phpMyAdmin-docker-container"><a href="#Create-the-phpMyAdmin-docker-container" class="headerlink" title="Create the phpMyAdmin docker container"></a>Create the phpMyAdmin docker container</h2><pre><code>$ docker pull phpmyadmin/phpmyadmin:latest
$ docker run --name phpadmin -d --link mysqldb:db -p 8080:80 phpmyadmin/phpmyadmin
</code></pre>
<p>Note:</p>
<ul>
<li>The option –link provides access to another container running in the host. In our case, the container <em>mysqldb</em> created in previous step is linked and the resource accessed is the MySQL <em>db</em>.</li>
<li>The option -p provides mapping between the host port(8080) and the container port(80, which is used by the apache server for the phpMyAdmin web application in the container).</li>
</ul>
<h2 id="Check-the-docker-images-and-containers"><a href="#Check-the-docker-images-and-containers" class="headerlink" title="Check the docker images and containers"></a>Check the docker images and containers</h2><pre><code>$ docker images
REPOSITORY                    TAG       IMAGE ID       CREATED        SIZE
phpmyadmin/phpmyadmin         latest    5682e7556577   2 months ago   524MB
mysql/mysql-server            5.7       b3eaae317eb2   2 months ago   390MB

$ docker ps -a
CONTAINER ID   IMAGE                    COMMAND                  CREATED          STATUS                   PORTS                                                  NAMES
a19c6406ffff   phpmyadmin/phpmyadmin    &quot;/docker-entrypoint.…&quot;   38 seconds ago   Up 38 seconds            0.0.0.0:8080-&gt;80/tcp, :::8080-&gt;80/tcp                  phpadmin
ff380903d3a0   mysql/mysql-server:5.7   &quot;/entrypoint.sh mysq…&quot;   5 minutes ago    Up 5 minutes (healthy)   0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp, 33060/tcp   mysqldb
</code></pre>
<h2 id="Access-the-phpMyAdmin-from-browser"><a href="#Access-the-phpMyAdmin-from-browser" class="headerlink" title="Access the phpMyAdmin from browser"></a>Access the phpMyAdmin from browser</h2><p><img src="/images/phpmyadmin.png" alt="Image"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://hub.docker.com/r/mysql/mysql-server/">https://hub.docker.com/r/mysql/mysql-server/</a></li>
<li><a href="https://hub.docker.com/r/phpmyadmin/phpmyadmin/">https://hub.docker.com/r/phpmyadmin/phpmyadmin/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>How to enable TCP connections to PostgreSQL</title>
    <url>/blog/how-to-enable-tcp-connections-to-postgresql/</url>
    <content><![CDATA[<h2 id="Problem-statement"><a href="#Problem-statement" class="headerlink" title="Problem statement"></a>Problem statement</h2><p>By default, the TCP connection to PostgreSQL is disallowed. You would see the following like error when you try to connect.</p>
<pre><code># su - postgres
-bash-4.2$ psql --host=10.10.10.243 --port=5432 --username=postgres -w -c &quot;\l&quot;
psql: error: connection to server at &quot;10.10.10.243&quot;, port 5432 failed: Connection refused
Is the server running on that host and accepting TCP/IP connections?
</code></pre>
<span id="more"></span>    

<h2 id="How-to-enable"><a href="#How-to-enable" class="headerlink" title="How to enable"></a>How to enable</h2><p>Modify the following two configuration files to allow the TCP connections from any hosts and restart the Postgres service. You can specify the particular host to connect if needed by changing “0.0.0.0” to the host IP address.</p>
<pre><code># vim /mnt/pgdata/postgresql.conf
listen_addresses = &#39;*&#39;          # what IP address(es) to listen on;

# vim /mnt/pgdata/pg_hba.conf
# IPv4 local connections:
host    all             all             0.0.0.0/0           trust


# su - postgres -c &quot;/usr/pgsql-15/bin/pg_ctl -D /mnt/pgdata -l logfile restart&quot;
</code></pre>
<h2 id="Verify-the-TCP-connections"><a href="#Verify-the-TCP-connections" class="headerlink" title="Verify the TCP connections"></a>Verify the TCP connections</h2><pre><code>bash-4.2$ psql --host=10.10.10.243 --port=5432 --username=postgres -w -c &quot;create database testdb&quot;
CREATE DATABASE
-bash-4.2$ psql --host=10.10.10.243 --port=5432 --username=postgres -w -c &quot;\l&quot;
                                                 List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    | ICU Locale | Locale Provider |   Access privileges
-----------+----------+----------+-------------+-------------+------------+-----------------+-----------------------
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
 testdb    | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |            | libc            |
(4 rows)
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>PostgreSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Hide and display html elements</title>
    <url>/blog/how-to-hide-and-display-html-elements/</url>
    <content><![CDATA[<pre><code>&lt;body ng-app=&quot;myapp&quot;&gt;
    &lt;div class=&quot;container-fluid&quot; ng-controller=&quot;myctrl&quot;&gt;
        &lt;ul class=&quot;nav nav-tabs&quot; id=&quot;myTab&quot; role=&quot;tablist&quot; style=&quot;display:flex&quot;&gt;
            &lt;li class=&quot;nav-item&quot; role=&quot;presentation&quot; id=&quot;l1&quot;&gt;&lt;/li&gt;
            &lt;li class=&quot;nav-item&quot; role=&quot;presentation&quot; id=&quot;l2&quot;&gt;&lt;/li&gt;
        &lt;/ul&gt;

        &lt;div class=&quot;tab-content&quot; id=&quot;myTabContent&quot; style=&quot;display:flex&quot;&gt;
            &lt;div class=&quot;tab-pane fade show active&quot; id=&quot;pane1&quot; role=&quot;tabpanel&quot; aria-labelledby=&quot;tab1&quot;&gt;&lt;/div&gt;
            &lt;div class=&quot;tab-pane fade&quot; id=&quot;pane2&quot; role=&quot;tabpanel&quot; aria-labelledby=&quot;tab2&quot;&gt;&lt;/div&gt;
        &lt;/div&gt;

        &lt;div class=&quot;table-responsive&quot; id=&quot;table1&quot;&gt;&lt;/div&gt;
    &lt;/div&gt;
&lt;/body&gt;


angular.module(&quot;myapp&quot;, []).controller(&quot;myctrl&quot;, [&#39;$scope&#39;, &#39;$http&#39;, function (scope, http) &#123;
    scope.HideDisplayElements = function () &#123;
        // hide the tabs
        document.getElementById(&quot;myTabContent&quot;).style.display = &quot;none&quot;;   
        document.getElementById(&quot;myTab&quot;).style.display = &quot;none&quot;;

        // hide table
        document.getElementById(&quot;table1&quot;).style.display = &quot;flex&quot;;
    &#125;
&#125;
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.w3schools.com/jsref/prop_style_display.asp">https://www.w3schools.com/jsref/prop_style_display.asp</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Html</tag>
      </tags>
  </entry>
  <entry>
    <title>How to install and configure PostgreSQL on Amazon Linux</title>
    <url>/blog/how-to-install-and-configure-postgresql-on-amazon-linux/</url>
    <content><![CDATA[<h2 id="Download-and-install-updates"><a href="#Download-and-install-updates" class="headerlink" title="Download and install updates"></a>Download and install updates</h2><pre><code>[ec2-user@ip-192-168-28-223 ~]$ cat /etc/os-release
NAME=&quot;Amazon Linux&quot;
VERSION=&quot;2&quot;
ID=&quot;amzn&quot;
ID_LIKE=&quot;centos rhel fedora&quot;
VERSION_ID=&quot;2&quot;
PRETTY_NAME=&quot;Amazon Linux 2&quot;
ANSI_COLOR=&quot;0;33&quot;
CPE_NAME=&quot;cpe:2.3:o:amazon:amazon_linux:2&quot;
HOME_URL=&quot;https://amazonlinux.com/&quot;
[ec2-user@ip-192-168-28-223 ~]$ cat /etc/system-release
Amazon Linux release 2 (Karoo)

[ec2-user@ip-192-168-28-223 ~]$ sudo yum update -y
</code></pre>
<span id="more"></span>    

<h2 id="Add-PostgreSQL-Amazon-extras-repository"><a href="#Add-PostgreSQL-Amazon-extras-repository" class="headerlink" title="Add PostgreSQL Amazon extras repository"></a>Add PostgreSQL Amazon extras repository</h2><p>PostgreSQL is part of the amazon extras library.</p>
<p>To check the available postgresql version in the Amazon extras repository:</p>
<pre><code>[ec2-user@ip-192-168-28-223 ~]$ amazon-linux-extras | grep postgresql
  6  postgresql10             available    [ =10  =stable ]
 41  postgresql11             available    [ =11  =stable ]
 58  postgresql12             available    [ =stable ]
 59  postgresql13             available    [ =stable ]
 63  postgresql14=latest      enabled      [ =stable ]
</code></pre>
<p>To enable the Amazon extras repository:</p>
<pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo amazon-linux-extras enable postgresql14
</code></pre>
<h2 id="Install-PostgreSQL-server"><a href="#Install-PostgreSQL-server" class="headerlink" title="Install PostgreSQL server"></a>Install PostgreSQL server</h2><pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo yum install postgresql-server

Installed:
  postgresql-server.x86_64 0:14.3-2.amzn2.0.1

[ec2-user@ip-192-168-28-223 ~]$ sudo rpm -ql  postgresql-server | grep -v share | grep -v lib
/etc/pam.d/postgresql
/etc/postgresql-setup
/etc/postgresql-setup/upgrade
/etc/postgresql-setup/upgrade/postgresql.conf
/usr/bin/initdb
/usr/bin/pg_basebackup
/usr/bin/pg_checksums
/usr/bin/pg_controldata
/usr/bin/pg_ctl
/usr/bin/pg_receivewal
/usr/bin/pg_recvlogical
/usr/bin/pg_resetwal
/usr/bin/pg_rewind
/usr/bin/pg_verifybackup
/usr/bin/postgres
/usr/bin/postgresql-setup
/usr/bin/postgresql-upgrade
/usr/bin/postmaster
/usr/sbin/postgresql-new-systemd-unit
/var/run/postgresql
</code></pre>
<h2 id="Initialize-the-DB"><a href="#Initialize-the-DB" class="headerlink" title="Initialize the DB"></a>Initialize the DB</h2><pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo postgresql-setup initdb
</code></pre>
<h2 id="Add-the-PostgreSQL-service-to-the-system-service"><a href="#Add-the-PostgreSQL-service-to-the-system-service" class="headerlink" title="Add the PostgreSQL service to the system service"></a>Add the PostgreSQL service to the system service</h2><pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo systemctl start postgresql

[ec2-user@ip-192-168-28-223 ~]$ sudo systemctl enable postgresql

[ec2-user@ip-192-168-28-223 ~]$ sudo systemctl status postgresql
● postgresql.service - PostgreSQL database server
   Loaded: loaded (/usr/lib/systemd/system/postgresql.service; enabled; vendor preset: disabled)
   Active: active (running) since Sat 2023-05-27 19:18:00 UTC; 45s ago
 Main PID: 96673 (postmaster)
   CGroup: /system.slice/postgresql.service
           ├─96673 /usr/bin/postmaster -D /var/lib/pgsql/data
           ├─96674 postgres: logger
           ├─96676 postgres: checkpointer
           ├─96677 postgres: background writer
           ├─96678 postgres: walwriter
           ├─96679 postgres: autovacuum launcher
           ├─96680 postgres: stats collector
           └─96681 postgres: logical replication launcher
[...]
</code></pre>
<h2 id="Set-password-for-Postgres-user"><a href="#Set-password-for-Postgres-user" class="headerlink" title="Set password for Postgres user"></a>Set password for Postgres user</h2><pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo -u postgres psql

postgres=# alter user postgres password &#39;password&#39;;
ALTER ROLE
</code></pre>
<h2 id="Enable-remote-connections-to-PostgreSQL"><a href="#Enable-remote-connections-to-PostgreSQL" class="headerlink" title="Enable remote connections to PostgreSQL"></a>Enable remote connections to PostgreSQL</h2><p>Locate the line that starts with “listen_addresses“ and change it to “listen_addresses &#x3D; ‘*’“. This will allow connections from any IP address. You also can increase the max_connections as needed.</p>
<pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo vi /var/lib/pgsql/data/postgresql.conf
listen_addresses = ‘*’
max_connections = 10000
</code></pre>
<p>Next, open &#x2F;var&#x2F;lib&#x2F;pgsql&#x2F;data&#x2F;pg_hba.conf file to authenticate the remote access with password by modifying the following line.</p>
<pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo vi /var/lib/pgsql/data/pg_hba.conf
host    all          all            0.0.0.0/0  md5
</code></pre>
<p>Restart the PostgreSQL service to take effect:</p>
<pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo systemctl start postgresql
</code></pre>
<h2 id="Access-the-PostgreSQL-database-remotely"><a href="#Access-the-PostgreSQL-database-remotely" class="headerlink" title="Access the PostgreSQL database remotely"></a>Access the PostgreSQL database remotely</h2><p>Install the psql client:</p>
<pre><code>[ec2-user@ip-192-168-93-151 ~]$ sudo amazon-linux-extras install postgresql14

[ec2-user@ip-192-168-93-151 ~]$ which psql
/usr/bin/psql
</code></pre>
<p>Connect to the remote PostgreSQL server:</p>
<pre><code>[ec2-user@ip-192-168-93-151 ~]$  psql -h 192.168.28.223 -U postgres -d postgres
Password for user postgres:

postgres=# 
</code></pre>
<p>Create a database:</p>
<pre><code>postgres=# CREATE DATABASE testdb;
CREATE DATABASE
postgres=# \l
                                  List of databases
   Name    |  Owner   | Encoding |   Collate   |    Ctype    |   Access privileges
-----------+----------+----------+-------------+-------------+-----------------------
 postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
 template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
           |          |          |             |             | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +
           |          |          |             |             | postgres=CTc/postgres
 testdb    | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |
(4 rows)
</code></pre>
<p>To drop a database:</p>
<pre><code>postgres=# drop database testdb;
DROP DATABASE
</code></pre>
<h2 id="Using-pgbench-to-run-a-benchmark-test-on-PostgreSQL"><a href="#Using-pgbench-to-run-a-benchmark-test-on-PostgreSQL" class="headerlink" title="Using pgbench to run a benchmark test on PostgreSQL"></a>Using pgbench to run a benchmark test on PostgreSQL</h2><p><a href="https://www.postgresql.org/docs/current/pgbench.html">pgbench</a> is a simple program for running benchmark tests on PostgreSQL. It runs the same sequence of SQL commands over and over, possibly in multiple concurrent database sessions, and then calculates the average transaction rate (transactions per second). By default, pgbench tests a scenario that is loosely based on TPC-B, involving five SELECT, UPDATE, and INSERT commands per transaction. However, it is easy to test other cases by writing your own transaction script files.</p>
<p>Available built-in scripts are: tpcb-like, simple-update and select-only. You also can create your own script for benchmarking.</p>
<p>To install the pgbench:</p>
<pre><code>[ec2-user@ip-192-168-93-151 ~]$ sudo yum install postgresql-contrib

[ec2-user@ip-192-168-93-151 ~]$ which pgbench
/usr/bin/pgbench
</code></pre>
<p>To load data to the target database:</p>
<pre><code>[ec2-user@ip-192-168-93-151 ~]$ pgbench -h 192.168.28.223 -p 5432 -U postgres -i -s 1000 testdb
Password:
dropping old tables...
NOTICE:  table &quot;pgbench_accounts&quot; does not exist, skipping
NOTICE:  table &quot;pgbench_branches&quot; does not exist, skipping
NOTICE:  table &quot;pgbench_history&quot; does not exist, skipping
NOTICE:  table &quot;pgbench_tellers&quot; does not exist, skipping
creating tables...
generating data (client-side)...
100000000 of 100000000 tuples (100%) done (elapsed 213.06 s, remaining 0.00 s)
vacuuming...
creating primary keys...
done in 398.67 s (drop tables 0.00 s, create tables 0.01 s, client-side generate 218.56 s, vacuum 15.52 s, primary keys 164.58 s).
</code></pre>
<p>To run the benchmark on the database:</p>
<pre><code>[ec2-user@ip-192-168-93-151 ~]$ pgbench -h 192.168.28.223 -p 5432 -U postgres -c 200 -j 32 -t 100000 testdb -b simple-update
</code></pre>
<p>Refer to this <a href="https://www.flamingbytes.com/blog/run-pgbench-on-postgresql/">post</a> for more detail on how to run pgbench on PostgreSQL.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>PostgreSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>How to install BCC on Ubuntu 22.04</title>
    <url>/blog/how-to-install-bcc-on-ubuntu-22-04/</url>
    <content><![CDATA[<p><strong>To build the toolchain from source, one needs:</strong></p>
<p>LLVM 3.7.1 or newer, compiled with BPF support (default&#x3D;on)</p>
<p>Clang, built from the same tree as LLVM</p>
<p>cmake (&gt;&#x3D;3.1), gcc (&gt;&#x3D;4.7), flex, bison</p>
<p>LuaJIT, if you want Lua support</p>
<pre><code>root@ubuntu:~# cat /etc/*release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=22.04
DISTRIB_CODENAME=jammy
DISTRIB_DESCRIPTION=&quot;Ubuntu 22.04.1 LTS&quot;
PRETTY_NAME=&quot;Ubuntu 22.04.1 LTS&quot;
NAME=&quot;Ubuntu&quot;
VERSION_ID=&quot;22.04&quot;
VERSION=&quot;22.04.1 LTS (Jammy Jellyfish)&quot;
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL=&quot;https://www.ubuntu.com/&quot;
SUPPORT_URL=&quot;https://help.ubuntu.com/&quot;
BUG_REPORT_URL=&quot;https://bugs.launchpad.net/ubuntu/&quot;
PRIVACY_POLICY_URL=&quot;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot;
UBUNTU_CODENAME=jammy
root@ubuntu:~# uname -r
5.15.0-60-generic
</code></pre>
<h2 id="Install-build-dependencies"><a href="#Install-build-dependencies" class="headerlink" title="Install build dependencies"></a>Install build dependencies</h2><pre><code># For Jammy (22.04)
root@ubuntu:~# apt install -y bison build-essential cmake flex git libedit-dev \
libllvm14 llvm-14-dev libclang-14-dev python3 zlib1g-dev libelf-dev libfl-dev python3-distutils
</code></pre>
<h2 id="Install-and-compile-BCC"><a href="#Install-and-compile-BCC" class="headerlink" title="Install and compile BCC"></a>Install and compile BCC</h2><pre><code>root@ubuntu:~# git clone https://github.com/iovisor/bcc.git
root@ubuntu:~# mkdir bcc/build; cd bcc/build
root@ubuntu:~# cmake ..
root@ubuntu:~# make
root@ubuntu:~# make install
root@ubuntu:~# cmake -DPYTHON_CMD=python3 .. # build python3 binding
root@ubuntu:~# pushd src/python/
root@ubuntu:~# make
root@ubuntu:~# make install
root@ubuntu:~# popd
</code></pre>
<h2 id="A-first-look-at-BCC"><a href="#A-first-look-at-BCC" class="headerlink" title="A first look at BCC"></a>A first look at BCC</h2><pre><code>root@ubuntu:~# biolatency 
Tracing block device I/O... Hit Ctrl-C to end.
^C
     usecs               : count     distribution
     0 -&gt; 1          : 0        |                                        |
     2 -&gt; 3          : 0        |                                        |
     4 -&gt; 7          : 0        |                                        |
     8 -&gt; 15         : 0        |                                        |
     16 -&gt; 31        : 0        |                                        |
     32 -&gt; 63        : 0        |                                        |
     64 -&gt; 127       : 0        |                                        |
     128 -&gt; 255      : 1        |****************************************|
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/iovisor/bcc/blob/master/INSTALL.md#ubuntu---source">https://github.com/iovisor/bcc/blob/master/INSTALL.md#ubuntu---source</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>BCC</tag>
      </tags>
  </entry>
  <entry>
    <title>How to install GUI desktop on CentOS 7</title>
    <url>/blog/how-to-install-gui-desktop-on-centos-7/</url>
    <content><![CDATA[<p>Install GNOME desktop via the groups option of yum:</p>
<pre><code>$ yum update
$ yum -y groups install &quot;GNOME Desktop&quot;
</code></pre>
<p>Inform the startx command which desktop env to run:</p>
<pre><code>$ echo &quot;exec gnome-session&quot; &gt;&gt; ~/.xinitrc
</code></pre>
<p>Manually start the GUI desktop:</p>
<pre><code>$ startx
</code></pre>
<p>Automaticlly start the GUI desktop after reboot:</p>
<pre><code>$ systemctl set-default graphical.target
$ reboot
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Gnome</tag>
      </tags>
  </entry>
  <entry>
    <title>How to migrate the PostgreSQL data directory</title>
    <url>/blog/how-to-migrate-the-postgresql-data-directory/</url>
    <content><![CDATA[<h2 id="Check-the-current-installed-data-directory"><a href="#Check-the-current-installed-data-directory" class="headerlink" title="Check the current installed data directory"></a>Check the current installed data directory</h2><pre><code>[ec2-user@ip-192-168-93-151 ~]$  psql -h 192.168.28.223 -U postgres -d postgres
Password for user postgres:

postgres=# SHOW config_file;
             config_file
-------------------------------------
 /var/lib/pgsql/data/postgresql.conf
(1 row)

postgres=# SHOW data_directory;
   data_directory
---------------------
 /var/lib/pgsql/data
(1 row)
</code></pre>
<span id="more"></span>    

<h2 id="Create-a-new-data-directory"><a href="#Create-a-new-data-directory" class="headerlink" title="Create a new data directory"></a>Create a new data directory</h2><pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo mkfs.ext4 /dev/nvme6n1
[ec2-user@ip-192-168-28-223 ~]$ sudo mkdir /mnt/pgdata
[ec2-user@ip-192-168-28-223 ~]$ sudo mount /dev/nvme6n1 /mnt/pgdata
[ec2-user@ip-192-168-28-223 ~]$ sudo chown -R postgres:postgres /mnt/pgdata
[ec2-user@ip-192-168-28-223 ~]$ sudo chmod 700 /mnt/pgdata/
</code></pre>
<h2 id="Stop-the-PostgreSQL"><a href="#Stop-the-PostgreSQL" class="headerlink" title="Stop the PostgreSQL"></a>Stop the PostgreSQL</h2><pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo systemctl stop postgresql
</code></pre>
<h2 id="Migrate-the-data"><a href="#Migrate-the-data" class="headerlink" title="Migrate the data"></a>Migrate the data</h2><pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo yum install rsync
[ec2-user@ip-192-168-28-223 ~]$ sudo rsync -av /var/lib/pgsql/data /mnt/pgdata
</code></pre>
<h2 id="Modify-the-PostgreSQL-configuration-file"><a href="#Modify-the-PostgreSQL-configuration-file" class="headerlink" title="Modify the PostgreSQL configuration file"></a>Modify the PostgreSQL configuration file</h2><p>Modify the PostgreSQL configuration file:</p>
<pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo vim /var/lib/pgsql/data/postgresql.conf
data_directory = &#39;/mnt/pgdata/data&#39;             # use data in another directory
</code></pre>
<p>Modify the system service configuration file:</p>
<pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo vim /lib/systemd/system/postgresql.service
Environment=PGDATA=/mnt/pgdata/data
</code></pre>
<p>Restart the PostgreSQL service:</p>
<pre><code>[ec2-user@ip-192-168-28-223 ~]$ sudo systemctl daemon-reload
[ec2-user@ip-192-168-28-223 ~]$ sudo systemctl start postgresql
[ec2-user@ip-192-168-28-223 ~]$ sudo systemctl status postgresql
● postgresql.service - PostgreSQL database server
   Loaded: loaded (/usr/lib/systemd/system/postgresql.service; enabled; vendor preset: disabled)
   Active: active (running) since Sat 2023-05-27 20:09:13 UTC; 4s ago
  Process: 23901 ExecStartPre=/usr/libexec/postgresql-check-db-dir %N (code=exited, status=0/SUCCESS)
 Main PID: 23904 (postmaster)
   CGroup: /system.slice/postgresql.service
           ├─23904 /usr/bin/postmaster -D /mnt/pgdata/data
           ├─23907 postgres: logger
           ├─23909 postgres: checkpointer
           ├─23910 postgres: background writer
           ├─23911 postgres: walwriter
           ├─23912 postgres: autovacuum launcher
           ├─23913 postgres: stats collector
           └─23914 postgres: logical replication launcher
</code></pre>
<h2 id="Verify-the-new-data-directory"><a href="#Verify-the-new-data-directory" class="headerlink" title="Verify the new data directory"></a>Verify the new data directory</h2><pre><code>[ec2-user@ip-192-168-93-151 ~]$  psql -h 192.168.28.223 -U postgres -d postgres
Password for user postgres:
psql (14.3)
Type &quot;help&quot; for help.

postgres=# SHOW config_file;
           config_file
----------------------------------
 /mnt/pgdata/data/postgresql.conf
(1 row)

postgres=# SHOW data_directory;
  data_directory
------------------
 /mnt/pgdata/data
(1 row)
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>PostgreSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>How to redirect the default &quot;404 page not found&quot; error page in Ghost blog</title>
    <url>/blog/how-to-redirect-the-default-404-page-not-found-error-page-in-ghost-blog/</url>
    <content><![CDATA[<p>When users access a non-existent page, they may see a page which shows the message “404 page not found” in Ghost blog.</p>
<p>We can redirect the page to another page by creating a <strong>error-404.hbs</strong> file under the theme folder.</p>
<p>In the following example, we redirect the users to the website home page when they access a non-existent page.</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; url=&#39;http://localhost:2368/&#39;&quot; /&gt;
  &lt;/head&gt;
  &lt;body&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>Ghost</tag>
      </tags>
  </entry>
  <entry>
    <title>How to set up etcd cluster</title>
    <url>/blog/how-to-set-up-etcd-cluster/</url>
    <content><![CDATA[<h2 id="Install-etcd-and-etcdctl"><a href="#Install-etcd-and-etcdctl" class="headerlink" title="Install etcd and etcdctl"></a>Install etcd and etcdctl</h2><span id="more"></span>
<p>On vm1, vm2, and vm3, run the following script to install etcd and etcdctl:</p>
<pre><code>$ cat etcd_install
ETCD_VER=v3.4.25

# choose either URL
GOOGLE_URL=https://storage.googleapis.com/etcd
GITHUB_URL=https://github.com/etcd-io/etcd/releases/download
DOWNLOAD_URL=$&#123;GOOGLE_URL&#125;

rm -f /tmp/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz
rm -rf /tmp/etcd-download-test &amp;&amp; mkdir -p /tmp/etcd-download-test

curl -L $&#123;DOWNLOAD_URL&#125;/$&#123;ETCD_VER&#125;/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz -o /tmp/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz
tar xzvf /tmp/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1
rm -f /tmp/etcd-$&#123;ETCD_VER&#125;-linux-amd64.tar.gz

/tmp/etcd-download-test/etcd --version
/tmp/etcd-download-test/etcdctl version
cp /tmp/etcd-download-test/etcd /usr/bin
cp /tmp/etcd-download-test/etcdctl /usr/bin

$ ./etcd_install
</code></pre>
<h2 id="Initialize-the-etcd-cluster"><a href="#Initialize-the-etcd-cluster" class="headerlink" title="Initialize the etcd cluster"></a>Initialize the etcd cluster</h2><p>On each VM, run the following script to initialize etcd cluster:</p>
<pre><code>[root@vm1 ~]# cat etcd_cfg
TOKEN=perf-token-01
CLUSTER_STATE=new
NAME_1=vm1
NAME_2=vm2
NAME_3=vm3
HOST_1=10.13.0.11
HOST_2=10.13.0.12
HOST_3=10.13.0.13
CLUSTER=$&#123;NAME_1&#125;=http://$&#123;HOST_1&#125;:2380,$&#123;NAME_2&#125;=http://$&#123;HOST_2&#125;:2380,$&#123;NAME_3&#125;=http://$&#123;HOST_3&#125;:2380

# For machine 1
THIS_NAME=$&#123;NAME_1&#125;
THIS_IP=$&#123;HOST_1&#125;
etcd --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \
--initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:2380 --listen-peer-urls http://$&#123;THIS_IP&#125;:2380 \
--advertise-client-urls http://$&#123;THIS_IP&#125;:2379 --listen-client-urls http://$&#123;THIS_IP&#125;:2379 \
--initial-cluster $&#123;CLUSTER&#125; \
--initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125;

[root@vm1 ~]# nohup ./etcd_cfg &amp;


[root@vm2 ~]# cat etcd_cfg
TOKEN=perf-token-01
CLUSTER_STATE=new
NAME_1=vm1
NAME_2=vm2
NAME_3=vm3
HOST_1=10.13.0.11
HOST_2=10.13.0.12
HOST_3=10.13.0.13
CLUSTER=$&#123;NAME_1&#125;=http://$&#123;HOST_1&#125;:2380,$&#123;NAME_2&#125;=http://$&#123;HOST_2&#125;:2380,$&#123;NAME_3&#125;=http://$&#123;HOST_3&#125;:2380

# For machine 2
THIS_NAME=$&#123;NAME_2&#125;
THIS_IP=$&#123;HOST_2&#125;
etcd --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \
    --initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:2380 --listen-peer-urls http://$&#123;THIS_IP&#125;:2380 \
    --advertise-client-urls http://$&#123;THIS_IP&#125;:2379 --listen-client-urls http://$&#123;THIS_IP&#125;:2379 \
    --initial-cluster $&#123;CLUSTER&#125; \
    --initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125;
[root@vm2 ~]# nohup ./etcd_cfg &amp;


[root@vm3 ~]# cat etcd_cfg
TOKEN=perf-token-01
CLUSTER_STATE=new
NAME_1=vm1
NAME_2=vm2
NAME_3=vm3
HOST_1=10.13.0.11
HOST_2=10.13.0.12
HOST_3=10.13.0.13
CLUSTER=$&#123;NAME_1&#125;=http://$&#123;HOST_1&#125;:2380,$&#123;NAME_2&#125;=http://$&#123;HOST_2&#125;:2380,$&#123;NAME_3&#125;=http://$&#123;HOST_3&#125;:2380

# For machine 3
THIS_NAME=$&#123;NAME_3&#125;
THIS_IP=$&#123;HOST_3&#125;
etcd --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \
    --initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:2380 --listen-peer-urls http://$&#123;THIS_IP&#125;:2380 \
    --advertise-client-urls http://$&#123;THIS_IP&#125;:2379 --listen-client-urls http://$&#123;THIS_IP&#125;:2379 \
    --initial-cluster $&#123;CLUSTER&#125; \
    --initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125;
[root@vm3 ~]# nohup ./etcd_cfg &amp;


[root@vm1 ~]# etcdctl endpoint status --endpoints=http://10.13.0.11:2379,http://10.13.0.12:2379,http://10.13.0.13:2379 -w table
+--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT         |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|   http://10.13.0.11:2379 | 9e811334a835290e |  3.4.25 |  1.8 MB |      true |      false |         5 |     107103 |             107103 |        |
| http://10.13.0.12:2379 | 73b102ab41e548ec |  3.4.25 |  1.8 MB |     false |      false |         5 |     107103 |             107103 |        |
|  http://10.13.0.13:2379 | a4b2bafd65759baf |  3.4.25 |  1.8 MB |     false |      false |         5 |     107103 |             107103 |        |
+--------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+

[root@vm1 ~]# etcdctl alarm list --endpoints=http://10.13.0.11:2379,http://10.13.0.12:2379,http://10.13.0.13:2379
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Etcd</tag>
      </tags>
  </entry>
  <entry>
    <title>How to modify value in Python 2D list</title>
    <url>/blog/how-to-modify-value-in-python-2d-list/</url>
    <content><![CDATA[<h2 id="Method-one-Be-careful"><a href="#Method-one-Be-careful" class="headerlink" title="Method one(Be careful !!!)"></a>Method one(Be careful !!!)</h2><p>In this method, each row references to the same column. This means, even if we update only one element of the array, it will update same column in our array.</p>
<span id="more"></span>

<pre><code>mylist = [[0] * 4 ] * 3
mylist[1][1] = 9
print(mylist)
</code></pre>
<p>Output:</p>
<pre><code>[[0, 9, 0, 0], [0, 9, 0, 0], [0, 9, 0, 0]]   
</code></pre>
<h2 id="Method-two-Using-List-Comprehension"><a href="#Method-two-Using-List-Comprehension" class="headerlink" title="Method two - Using List Comprehension"></a>Method two - Using List Comprehension</h2><p>Here we use list comprehension by applying loop for a list inside a list and hence creating a 2-D list.</p>
<pre><code>mylist = [[0] * 4 for _ in range(3)]
mylist[1][1] = 9
print(mylist)
</code></pre>
<p>Output:</p>
<pre><code>[[0, 0, 0, 0], [0, 9, 0, 0], [0, 0, 0, 0]]
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.geeksforgeeks.org/python-using-2d-arrays-lists-the-right-way/">https://www.geeksforgeeks.org/python-using-2d-arrays-lists-the-right-way/</a>&gt;</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>How to setup the Grafana and Prometheus dashboard</title>
    <url>/blog/how-to-setup-grafana-and-prometheus/</url>
    <content><![CDATA[<h2 id="Install-node-exporter-on-each-target-nodes"><a href="#Install-node-exporter-on-each-target-nodes" class="headerlink" title="Install node-exporter on each target nodes"></a>Install node-exporter on each target nodes</h2><pre><code>$ docker run --restart=always --name nd-export -d -p 9100:9100 -v /proc:/host/proc -v /sys:/host/sys -v /:/rootfs:ro,rslave --net=host quay.io/prometheus/node-exporter
$ docker ps -a
CONTAINER ID   IMAGE                              COMMAND                CREATED          STATUS          PORTS     NAMES
d8c7fc8a4e40   quay.io/prometheus/node-exporter   &quot;/bin/node_exporter&quot;   32 minutes ago   Up 32 minutes             nd-export
</code></pre>
<h2 id="Deploy-Prometheus-on-a-VM"><a href="#Deploy-Prometheus-on-a-VM" class="headerlink" title="Deploy Prometheus on a VM"></a>Deploy Prometheus on a VM</h2><pre><code>$ vim /etc/prometheus/prometheus.yml
global:
scrape_configs:
  - job_name: &#39;node&#39;
    scrape_interval: 10s
    static_configs:
     - targets: [&#39;localhost:9100&#39;,&#39;host1-ip:9100&#39;,&#39;host2-ip:9100&#39;,&#39;host3-ip:9100&#39;,&#39;host4-ip:9100&#39;]

$ docker run --restart=always --name prometheus -d -p 9090:9090 -v /etc/prometheus:/etc/prometheus prom/prometheus:v1.8.2

$ docker ps -a
CONTAINER ID   IMAGE                              COMMAND                  CREATED          STATUS          PORTS                                       NAMES
3700700b61c9   prom/prometheus:v1.8.2         &quot;/bin/prometheus -co…&quot;   57 minutes ago   Up 22 minutes   0.0.0.0:9090-&gt;9090/tcp, :::9090-&gt;9090/tcp   prometheus
</code></pre>
<p>Verify the Prometheus by accessing <a href="http://vm-ip:9090/targets">http://vm-ip:9090/targets</a>. You should be able to see the target nodes from the web UI.</p>
<h2 id="Deploy-Grafana-on-a-VM"><a href="#Deploy-Grafana-on-a-VM" class="headerlink" title="Deploy Grafana on a VM"></a>Deploy Grafana on a VM</h2><pre><code>$ docker run --restart=always --name grafana -d -p 3000:3000 grafana/grafana

$ docker ps -a
CONTAINER ID   IMAGE                              COMMAND                  CREATED          STATUS          PORTS                                       NAMES
da48a274ecc0   quay.io/prometheus/node-exporter   &quot;/bin/node_exporter&quot;     27 minutes ago   Up 27 minutes                                               nd-export
814511410bff   grafana/grafana                    &quot;/run.sh&quot;                57 minutes ago   Up 57 minutes   0.0.0.0:3000-&gt;3000/tcp, :::3000-&gt;3000/tcp   grafana
3700700b61c9   prom/prometheus:v1.8.2             &quot;/bin/prometheus -co…&quot;   57 minutes ago   Up 22 minutes   0.0.0.0:9090-&gt;9090/tcp, :::9090-&gt;9090/tcp   prometheus
</code></pre>
<h2 id="Configure-the-Promethous-data-source-in-Grafana"><a href="#Configure-the-Promethous-data-source-in-Grafana" class="headerlink" title="Configure the Promethous data source in Grafana"></a>Configure the Promethous data source in Grafana</h2><p>Login to the Grafana web UI by accessing <a href="http://vm-ip:3000/">http://vm-ip:3000/</a>. The default username and password is admin&#x2F;admin.</p>
<p>On the left corner, click “Add sources”.<br><img src="/images/grafana-datasource.png" alt="grafana-dashboard"></p>
<p>Then add “Prometheus” as data source and fill in the prometheus data source name and the URL. Click Save &amp; test. The data source name will be used when you create the Grafana dashboard later.</p>
<p><img src="/images/grafana-datasource-1.png" alt="Image"></p>
<h2 id="Create-the-Grafana-dashboard"><a href="#Create-the-Grafana-dashboard" class="headerlink" title="Create the Grafana dashboard"></a>Create the Grafana dashboard</h2><p>The Grafana dashboard can be created by importing an json template and binding to the previous prometheus data source.</p>
<p><img src="/images/grafana-dashboard.png" alt="Image"></p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Grafana</tag>
        <tag>Prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title>How to setup NFS (Network File System) on RHEL/CentOS</title>
    <url>/blog/how-to-setup-nfs-network-file-system-on-rhel-centos/</url>
    <content><![CDATA[<p>We need to install NFS packages on the NFS Server as well as on NFS Client machine. We can install it via “yum” for RHEL&#x2F;CentOS.</p>
<p>On the NFS server side, run the following commands to setup NFS server.</p>
<pre><code>$ systemctl status nfs
$ mkfs.ext4 /dev/nvme9n1
$ mkdir /mnt/nfsshare

$ vim /etc/exports
/mnt/nfsshare 10.10.10.1(rw,no_root_squash)

$ systemctl restart nfs
$ exportfs -s
/mnt/nfsshare  10.10.10.1(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)
</code></pre>
<p>On the NFS client side, run the following command to mount the exported NFS share.</p>
<pre><code>$ mkdir /mnt/nfs1
$ mount -t nfs nfs-server-hostname:/mnt/nfsshare /mnt/
$ mount  | grep nfs1
fs-server-hostname:/mnt/nfsshare on /mnt/nfs1 type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.10.10.2,local_lock=none,addr=10.10.10.1)
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>NFS</tag>
      </tags>
  </entry>
  <entry>
    <title>How to uninstall ceph storage cluster</title>
    <url>/blog/how-to-uninstall-ceph-storage-cluster/</url>
    <content><![CDATA[<h2 id="Check-the-pools-images-and-OSDs"><a href="#Check-the-pools-images-and-OSDs" class="headerlink" title="Check the pools, images and OSDs"></a>Check the pools, images and OSDs</h2><pre><code>[ceph: root@host1 /]$ ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME             STATUS  REWEIGHT  PRI-AFF
-1         83.83411  root default
-3         27.94470      host host1
 0    ssd   3.49309          osd.0             up   1.00000  1.00000
 1    ssd   3.49309          osd.1             up   1.00000  1.00000
 2    ssd   3.49309          osd.2             up   1.00000  1.00000
 3    ssd   3.49309          osd.3             up   1.00000  1.00000
 4    ssd   3.49309          osd.4             up   1.00000  1.00000
 5    ssd   3.49309          osd.5             up   1.00000  1.00000
 6    ssd   3.49309          osd.6             up   1.00000  1.00000
 7    ssd   3.49309          osd.7             up   1.00000  1.00000
-5         27.94470      host host2
 8    ssd   3.49309          osd.8             up   1.00000  1.00000
 9    ssd   3.49309          osd.9             up   1.00000  1.00000
10    ssd   3.49309          osd.10            up   1.00000  1.00000
11    ssd   3.49309          osd.11            up   1.00000  1.00000
12    ssd   3.49309          osd.12            up   1.00000  1.00000
13    ssd   3.49309          osd.13            up   1.00000  1.00000
14    ssd   3.49309          osd.14            up   1.00000  1.00000
15    ssd   3.49309          osd.15            up   1.00000  1.00000
-7         27.94470      host host3
16    ssd   3.49309          osd.16            up   1.00000  1.00000
17    ssd   3.49309          osd.17            up   1.00000  1.00000
18    ssd   3.49309          osd.18            up   1.00000  1.00000
19    ssd   3.49309          osd.19            up   1.00000  1.00000
20    ssd   3.49309          osd.20            up   1.00000  1.00000
21    ssd   3.49309          osd.21            up   1.00000  1.00000
22    ssd   3.49309          osd.22            up   1.00000  1.00000
23    ssd   3.49309          osd.23            up   1.00000  1.00000

[ceph: root@host1 /]$ ceph osd lspools
1 device_health_metrics
2 datapool

[ceph: root@host1 /]$ rbd showmapped
id  pool      namespace  image    snap  device
0   datapool             rbdvol1  -     /dev/rbd0
1   datapool             rbdvol2  -     /dev/rbd1
2   datapool             rbdvol3  -     /dev/rbd2
3   datapool             rbdvol4  -     /dev/rbd3
</code></pre>
<h2 id="Remove-the-images-and-pools"><a href="#Remove-the-images-and-pools" class="headerlink" title="Remove the images and pools"></a>Remove the images and pools</h2><pre><code>[ceph: root@host1 /]$ rbd unmap /dev/rbd0
[ceph: root@host1 /]$ rbd unmap /dev/rbd1
[ceph: root@host1 /]$ rbd unmap /dev/rbd2
[ceph: root@host1 /]$ rbd unmap /dev/rbd3

[ceph: root@host1 /]$ rbd showmapped

[ceph: root@host1 /]$ rbd rm datapool/rbdvol1
Removing image: 100% complete...done.
[ceph: root@host1 /]$ rbd rm datapool/rbdvol2
Removing image: 100% complete...done.
[ceph: root@host1 /]$ rbd rm datapool/rbdvol3
Removing image: 100% complete...done.
[ceph: root@host1 /]$ rbd rm datapool/rbdvol4
Removing image: 100% complete...done.

[ceph: root@host1 /]$ ceph osd pool rm datapool datapool --yes-i-really-really-mean-it
Error EPERM: pool deletion is disabled; you must first set the mon_allow_pool_delete config option to true before you can destroy a pool

[ceph: root@host1 /]$ ceph tell mon.\* injectargs &#39;--mon-allow-pool-delete=true&#39;
mon.host1: mon_allow_pool_delete = &#39;true&#39;
mon.host1: &#123;&#125;
mon.host3: mon_allow_pool_delete = &#39;true&#39;
mon.host3: &#123;&#125;
mon.host2: mon_allow_pool_delete = &#39;true&#39;
mon.host2: &#123;&#125;
[ceph: root@host1 /]$ ceph osd pool rm datapool datapool --yes-i-really-really-mean-it
pool &#39;datapool&#39; removed
</code></pre>
<h2 id="Remove-the-OSDs"><a href="#Remove-the-OSDs" class="headerlink" title="Remove the OSDs"></a>Remove the OSDs</h2><pre><code>[ceph: root@host1 /]$ for i in `seq 0 23`
&gt; do
&gt; ceph osd down $i &amp;&amp; ceph osd destroy $i --force
&gt; done
marked down osd.0.
destroyed osd.0
[omitted...]

[ceph: root@host1 /]$  ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME             STATUS     REWEIGHT  PRI-AFF
-1         83.83411  root default
-3         27.94470      host host1
 0    ssd   3.49309          osd.0         destroyed   1.00000  1.00000
 1    ssd   3.49309          osd.1         destroyed   1.00000  1.00000
 2    ssd   3.49309          osd.2         destroyed   1.00000  1.00000
 3    ssd   3.49309          osd.3         destroyed   1.00000  1.00000
 4    ssd   3.49309          osd.4         destroyed   1.00000  1.00000
 5    ssd   3.49309          osd.5         destroyed   1.00000  1.00000
 6    ssd   3.49309          osd.6         destroyed   1.00000  1.00000
 7    ssd   3.49309          osd.7         destroyed   1.00000  1.00000
-5         27.94470      host host2
 8    ssd   3.49309          osd.8         destroyed   1.00000  1.00000
 9    ssd   3.49309          osd.9         destroyed   1.00000  1.00000
10    ssd   3.49309          osd.10        destroyed   1.00000  1.00000
11    ssd   3.49309          osd.11        destroyed   1.00000  1.00000
12    ssd   3.49309          osd.12        destroyed   1.00000  1.00000
13    ssd   3.49309          osd.13        destroyed   1.00000  1.00000
14    ssd   3.49309          osd.14        destroyed   1.00000  1.00000
15    ssd   3.49309          osd.15        destroyed   1.00000  1.00000
-7         27.94470      host host3
16    ssd   3.49309          osd.16        destroyed   1.00000  1.00000
17    ssd   3.49309          osd.17        destroyed   1.00000  1.00000
18    ssd   3.49309          osd.18        destroyed   1.00000  1.00000
19    ssd   3.49309          osd.19        destroyed   1.00000  1.00000
20    ssd   3.49309          osd.20        destroyed   1.00000  1.00000
21    ssd   3.49309          osd.21        destroyed   1.00000  1.00000
22    ssd   3.49309          osd.22        destroyed   1.00000  1.00000
23    ssd   3.49309          osd.23               up   1.00000  1.00000
</code></pre>
<h2 id="Remove-the-cluster-hosts"><a href="#Remove-the-cluster-hosts" class="headerlink" title="Remove the cluster hosts"></a>Remove the cluster hosts</h2><pre><code>[ceph: root@host1 /]$ ceph orch host rm host3
Removed host &#39;host3&#39;
[ceph: root@host1 /]$ ceph orch host rm host2
Removed host &#39;host2&#39;
[ceph: root@host1 /]$ ceph orch host rm host1
Removed host &#39;host1&#39;
</code></pre>
<h2 id="Check-if-there-is-ceph-daemon-running"><a href="#Check-if-there-is-ceph-daemon-running" class="headerlink" title="Check if there is ceph daemon running"></a>Check if there is ceph daemon running</h2><pre><code>[ceph: root@host1 /]$ ceph orch ps host3
No daemons reported
[ceph: root@host1 /]$ ceph orch ps host2
No daemons reported
[ceph: root@host1 /]$ ceph orch ps host1
No daemons reported
</code></pre>
<h2 id="Remove-the-ceph-storage-cluster"><a href="#Remove-the-ceph-storage-cluster" class="headerlink" title="Remove the ceph storage cluster"></a>Remove the ceph storage cluster</h2><pre><code>[root@host1 ~]$ cephadm rm-cluster --fsid fec2332e-1b0b-11ec-abbe-ac1f6bc8d268 --force
[root@host1 ~]$ cephadm ls
[]
</code></pre>
<h2 id="Cleanup-the-ceph-configuration-files"><a href="#Cleanup-the-ceph-configuration-files" class="headerlink" title="Cleanup the ceph configuration files"></a>Cleanup the ceph configuration files</h2><pre><code>[root@host1 ~]$ rm -rf /etc/ceph
[root@host1 ~]$ rm -rf /var/lib/ce
ceph/       cephadm/    certmonger/
[root@host1 ~]$ rm -rf /var/lib/ceph*
</code></pre>
<h2 id="Cleanup-the-ceph-block-devices"><a href="#Cleanup-the-ceph-block-devices" class="headerlink" title="Cleanup the ceph block devices"></a>Cleanup the ceph block devices</h2><p>Do the following on each cluster node.</p>
<pre><code>[root@host1 ~]$ lsblk
NAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
nvme0n1                                                                                               259:0    0  3.5T  0 disk
├─nvme0n1p3                                                                                           259:4    0  3.5T  0 part
│ ├─vgroot-lvswap01                                                                                   253:1    0    4G  0 lvm
│ └─vgroot-lvroot                                                                                     253:0    0  3.5T  0 lvm  /
├─nvme0n1p1                                                                                           259:2    0    1G  0 part /boot/efi
└─nvme0n1p2                                                                                           259:3    0  500M  0 part /boot
nvme3n1                                                                                               259:6    0  3.5T  0 disk
└─ceph--ab144c40--73d6--49bc--921b--65025c383bb1-osd--block--2b965e29--b194--4363--8c96--20ab5b97db33 253:3    0  3.5T  0 lvm
nvme2n1                                                                                               259:5    0  3.5T  0 disk
└─ceph--b1ffe76d--1043--43a2--848b--6ba117e71a75-osd--block--0d6ff85d--9c49--43a0--98a3--c519fbb20b9c 253:4    0  3.5T  0 lvm
nvme1n1                                                                                               259:1    0  3.5T  0 disk

[root@host1 ~]$ for i in `seq 2 9`; do dd if=/dev/zero of=/dev/nvme$&#123;i&#125;n1 bs=1M count=1000; done
[root@host1 ~]$ reboot
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use sorted() and sort() in Python</title>
    <url>/blog/how-to-use-sorted-and-sort-in-python/</url>
    <content><![CDATA[<h2 id="Using-sorted"><a href="#Using-sorted" class="headerlink" title="Using sorted()"></a>Using sorted()</h2><p>sorted() is a built-in function in Python.</p>
<ul>
<li><p>The original order of values is unchanged once it’s called.</p>
</li>
<li><p>It returns an ordered list.</p>
<p>  numbers &#x3D; [1, 5, 3, 2, 4]<br>  sorted_numbers &#x3D; sorted(numbers)<br>  print(sorted_numbers)<br>  print(numbers)</p>
<h1 id="Output"><a href="#Output" class="headerlink" title="Output:"></a>Output:</h1><h1 id="1-2-3-4-5"><a href="#1-2-3-4-5" class="headerlink" title="[1, 2, 3, 4, 5]"></a>[1, 2, 3, 4, 5]</h1><h1 id="1-5-3-2-4"><a href="#1-5-3-2-4" class="headerlink" title="[1, 5, 3, 2, 4]"></a>[1, 5, 3, 2, 4]</h1></li>
</ul>
<h2 id="Using-sort"><a href="#Using-sort" class="headerlink" title="Using sort()"></a>Using sort()</h2><p>The sort() is a method of list class. It orders in place and return nothing.</p>
<pre><code>sorted_numbers = numbers.sort()
print(sorted_numbers)
print(numbers)

# Output:
# None
# [1, 2, 3, 4, 5]
</code></pre>
<h2 id="Sort-in-reverse-order"><a href="#Sort-in-reverse-order" class="headerlink" title="Sort in reverse order"></a>Sort in reverse order</h2><pre><code>numbers = [1, 5, 3, 2, 4]
sorted_numbers = sorted(numbers, reverse=True)
print(sorted_numbers)
print(numbers)

# Output:
# [5, 4, 3, 2, 1]
# [1, 5, 3, 2, 4]


numbers.sort(reverse=True)
print(numbers)

# Output:
# [5, 4, 3, 2, 1]
</code></pre>
<h2 id="Sort-dictionary"><a href="#Sort-dictionary" class="headerlink" title="Sort dictionary"></a>Sort dictionary</h2><p>Note that the keys in a dictionary must be unique. In the following example, even if two 5s are initialized in the number list, they will be deduplicated.</p>
<pre><code>numbers_set = &#123;20, 5, 3, 2, 4, 5&#125;
sorted_numbers_set = sorted(numbers_set)
print(numbers_set)
print(sorted_numbers_set)
print(set(sorted_numbers_set))

# Output:
# &#123;2, 3, 4, 5, 20&#125;
# [2, 3, 4, 5, 20]
# &#123;2, 3, 4, 5, 20&#125;
</code></pre>
<h2 id="Sort-tuple"><a href="#Sort-tuple" class="headerlink" title="Sort tuple"></a>Sort tuple</h2><p>Example 1:</p>
<pre><code>numbers_tuple = (10, 5, 3, 2, 4, 5)
sorted_numbers_tuple = sorted(numbers_tuple)
print(numbers_tuple)
print(sorted_numbers_tuple)
print(tuple(sorted_numbers_tuple))

# Output:
# (10, 5, 3, 2, 4, 5)
# [2, 3, 4, 5, 5, 10]
# (2, 3, 4, 5, 5, 10)
</code></pre>
<p>Example 2:</p>
<pre><code>fruits = [(&quot;apple&quot;, 10), (&quot;orange&quot;, 6), (&quot;banana&quot;, 9), (&quot;pear&quot;, 3)]
sorted_fruits = sorted(fruits, key=lambda x: x[1], reverse=True)
print(sorted_fruits)

# Output:
# [(&#39;apple&#39;, 10), (&#39;banana&#39;, 9), (&#39;orange&#39;, 6), (&#39;pear&#39;, 3)]
</code></pre>
<p>Example 3:</p>
<pre><code>from collections import namedtuple

Student = namedtuple(&quot;Student&quot;, &quot;name, age, grade&quot;)
students = [
    Student(&quot;bill&quot;, 11, 97),
    Student(&quot;Andy&quot;, 12, 95),
    Student(&quot;Harry&quot;, 10, 100),
    Student(&quot;Thor&quot;, 11, 96),
]
sorted_students = sorted(students, key=lambda x: getattr(x, &quot;grade&quot;), reverse=True)
print(sorted_students)
print(students)

# Output:
# [Student(name=&#39;Harry&#39;, age=10, grade=100), Student(name=&#39;bill&#39;, age=11, grade=97), Student(name=&#39;Thor&#39;, age=11, grade=96), Student(name=&#39;Andy&#39;, age=12, grade=95)]
# [Student(name=&#39;bill&#39;, age=11, grade=97), Student(name=&#39;Andy&#39;, age=12, grade=95), Student(name=&#39;Harry&#39;, age=10, grade=100), Student(name=&#39;Thor&#39;, age=11, grade=96)]
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Implement CPU burner using Python</title>
    <url>/blog/implement-cpu-burner-using-python/</url>
    <content><![CDATA[<p>For the modern computers, they mostly have multiple CPU cores. We can use Python’s multiprocessing module to automatically get the number of CPU cores. Then we use this module to spawn corresponding processes to generate load on all the CPU cores concurrently.</p>
<span id="more"></span>

<p>On each CPU core, we can perform arithmetic operation for the fraction of a second and then sleep for the rest of a second. For example, if you want to have 85% CPU utilization, each CPU core can do the arithmetic operation for 0.85 second and sleep 0.15 second.</p>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><pre><code>import sys
import math
import time
import multiprocessing

def cpu_burner(duration, util):
    start_time = time.time()
    for i in range(0, duration):
        while time.time() - start_time &lt; util / 100.00:
            math.pow(2, 10)
        time.sleep(1 - util / 100.00)
        start_time += 1


if __name__ == &quot;__main__&quot;:
    if len(sys.argv) == 3:
        processes = []
        for _ in range(multiprocessing.cpu_count()):
            p = multiprocessing.Process(
                target=cpu_burner, args=(int(sys.argv[1]), int(sys.argv[2]))
            )
            p.start()
            processes.append(p)

        for process in processes:
            process.join()
    else:
        print(&quot;Usage:\n python %s duration util&quot; % __file__)
</code></pre>
<h2 id="Burn-the-CPU-cores"><a href="#Burn-the-CPU-cores" class="headerlink" title="Burn the CPU cores"></a>Burn the CPU cores</h2><pre><code># python cpu_burner.py 60 85
</code></pre>
<h2 id="Verify-the-CPU-utilization"><a href="#Verify-the-CPU-utilization" class="headerlink" title="Verify the CPU utilization"></a>Verify the CPU utilization</h2><pre><code># mpstat -P ALL 2
04:46:49 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
04:46:51 AM  all   84.93    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   15.07
04:46:51 AM    0   85.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   15.00
04:46:51 AM    1   84.42    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   15.58
04:46:51 AM    2   85.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   15.00
04:46:51 AM    3   85.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   15.00
04:46:51 AM    4   85.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   15.00
04:46:51 AM    5   85.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   15.00
04:46:51 AM    6   85.43    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   14.57
04:46:51 AM    7   84.92    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   15.08
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://qxf2.com/blog/generate-cpu-load/">https://qxf2.com/blog/generate-cpu-load/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Install and uninstall debuginfo package on CentOS</title>
    <url>/blog/install-and-uninstall-debuginfo-package-on-centos/</url>
    <content><![CDATA[<h2 id="Install-debuginfo-package"><a href="#Install-debuginfo-package" class="headerlink" title="Install debuginfo package"></a>Install debuginfo package</h2><p>On CentOS, we can install debuginfo package as below.</p>
<ul>
<li><p>Modify &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Debuginfo.repo by setting “enabled&#x3D;1”</p>
</li>
<li><p>Run “yum install kernel-debuginfo”</p>
<p>  [root@host1 ~]# uname -r<br>  3.10.0-1160.el7.x86_64</p>
<p>  [root@host1 ~]# vim &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Debuginfo.repo<br>  [base-debuginfo]<br>  name&#x3D;CentOS-7 - Debuginfo<br>  baseurl&#x3D;<a href="http://debuginfo.centos.org/7/$basearch/">http://debuginfo.centos.org/7/$basearch/</a><br>  gpgcheck&#x3D;1<br>  gpgkey&#x3D;file:&#x2F;&#x2F;&#x2F;etc&#x2F;pki&#x2F;rpm-gpg&#x2F;RPM-GPG-KEY-CentOS-Debug-7<br>  enabled&#x3D;1</p>
<p>  [root@host1 ~]# yum install kernel-debuginfo<br>  Installed:<br>kernel-debuginfo.x86_64 0:4.19.113-300.el7<br>  Dependency Installed:<br>kernel-debuginfo-common-x86_64.x86_64 0:4.19.113-300.el7<br>  [root@host1 ~]# rpm -qa | grep debuginfo<br>  kernel-debuginfo-4.19.113-300.el7.x86_64<br>  kernel-debuginfo-common-x86_64-4.19.113-300.el7.x86_64</p>
</li>
</ul>
<p>Notice that the installed kernel-debuginfo package version does not match with the kernel version. We can use the following commands to install the identical version as kernel.</p>
<pre><code>[root@host1 ~]# yum install -y kernel-devel-$(uname -r) \
kernel-debuginfo-$(uname -r) \
kernel-debuginfo-common-$(uname -m)-$(uname -r)

Installed:
  kernel-debuginfo.x86_64 0:3.10.0-1160.el7  kernel-debuginfo-common-x86_64.x86_64 0:3.10.0-1160.el7
</code></pre>
<h2 id="Remove-debuginfo-package"><a href="#Remove-debuginfo-package" class="headerlink" title="Remove debuginfo package"></a>Remove debuginfo package</h2><pre><code>[root@host1 ~]# rpm -qa | grep debuginfo
kernel-debuginfo-4.19.113-300.el7.x86_64
kernel-debuginfo-common-x86_64-4.19.113-300.el7.x86_64

[root@host1 ~]# yum remove kernel-debuginfo
[root@host1 ~]# yum remove kernel-debuginfo-common-x86_64-4.19.113-300.el7.x86_64

[root@host1 ~]# rpm -qa | grep debug
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Dynamic array Implementation in Python</title>
    <url>/blog/implementation-dynamic-array-in-python/</url>
    <content><![CDATA[<p>In languages like Python, array(aka list) is automatially resizable. The array will grow as you append items. You don’t have to resize it manually.</p>
<p>In some languages like Java, array comes with fixed length. The size is defined when you create it. When you need a data structure with dynamic resizing, you can use ArrayList.</p>
<p>In this post, we learn to implement the dynamic array in Python. This is to understand how dynamic array works regardless of any programming language.</p>
<pre><code>class ArrayList:
    def __init__(self, size=10):
        self.capacity = size  # max size
        self.list = [None] * self.capacity
        self.size = 0  # current size

    def add(self, value):
        # check if there is enough capacity
        if self.size &gt;= self.capacity:
            self._increase_capacity()

        # add value to the list
        self.list[self.size] = value
        self.size += 1

        print(&quot;&#123;&#125; is added to list&quot;.format(value))

    def _increase_capacity(self):
        # double the capacity
        new_capacity = self.capacity * 2

        # create a new list with new capacity
        new_list = [None] * new_capacity

        # copy the items from old to new list
        for i in range(self.capacity):
            new_list[i] = self.list[i]

        # update new capacity and list
        self.capacity = new_capacity
        self.list = new_list

        print(&quot;capacity is doubled to &#123;&#125;&quot;.format(self.capacity))

    def get(self, index):
        if index &lt; 0 or index &gt;= self.size:
            raise Exception(&quot;invalid index&quot;)

        return self.list[index]

    def delete(self, index):
        if index &lt; 0 or index &gt;= self.size:
            raise Exception(&quot;invalid index&quot;)

        # check if this is the last item to be deleted
        if index == self.size - 1:
            self.list[index] = None
            return

        # shift items left
        for i in range(index, self.size - 1):
            self.list[i] = self.list[i + 1]

        # update the last item to None
        self.list[self.size - 1] = None

        # update the size
        self.size -= 1
</code></pre>
<p>To test the code:</p>
<pre><code>list1 = ArrayList(5)
list1.add(1)
list1.add(2)
list1.add(3)
list1.add(4)
list1.add(5)
list1.add(6)
list1.add(7)
print(list1.list)
list1.delete(6)
print(list1.list)
</code></pre>
<p>Output:</p>
<pre><code>1 is added to list
2 is added to list
3 is added to list
4 is added to list
5 is added to list
capacity is doubled to 10
6 is added to list
7 is added to list
[1, 2, 3, 4, 5, 6, 7, None, None, None]
[1, 2, 3, 4, 5, 6, None, None, None, None]
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Install GCC from source</title>
    <url>/blog/install-gcc-from-source/</url>
    <content><![CDATA[<p>In this post, we will go through the steps to install <a href="https://gcc.gnu.org/">GCC</a> from source on CentOS 7.</p>
<pre><code>$ cat /etc/centos-release
CentOS Linux release 7.9.2009 (Core)

$ uname -r
5.7.12-1.el7.elrepo.x86_64
</code></pre>
<h2 id="Download-GCC-source"><a href="#Download-GCC-source" class="headerlink" title="Download GCC source"></a>Download GCC <a href="https://gcc.gnu.org/pub/gcc/releases/">source</a></h2><pre><code>$ curl -LO https://gcc.gnu.org/pub/gcc/releases/gcc-12.2.0/gcc-12.2.0.tar.gz
</code></pre>
<p>Untar the source file:</p>
<pre><code>$ tar zxf gcc-12.2.0.tar.gz
</code></pre>
<h2 id="Download-the-dependent-libraries"><a href="#Download-the-dependent-libraries" class="headerlink" title="Download the dependent libraries"></a>Download the dependent libraries</h2><pre><code>$ cd gcc-12.2.0/
$ ./contrib/download_prerequisites
2023-02-01 00:41:01 URL:http://gcc.gnu.org/pub/gcc/infrastructure/gmp-6.2.1.tar.bz2 [2493916/2493916] -&gt; &quot;gmp-6.2.1.tar.bz2&quot; [1]
2023-02-01 00:41:02 URL:http://gcc.gnu.org/pub/gcc/infrastructure/mpfr-4.1.0.tar.bz2 [1747243/1747243] -&gt; &quot;mpfr-4.1.0.tar.bz2&quot; [1]
2023-02-01 00:41:03 URL:http://gcc.gnu.org/pub/gcc/infrastructure/mpc-1.2.1.tar.gz [838731/838731] -&gt; &quot;mpc-1.2.1.tar.gz&quot; [1]
2023-02-01 00:41:04 URL:http://gcc.gnu.org/pub/gcc/infrastructure/isl-0.24.tar.bz2 [2261594/2261594] -&gt; &quot;isl-0.24.tar.bz2&quot; [1]
gmp-6.2.1.tar.bz2: OK
mpfr-4.1.0.tar.bz2: OK
mpc-1.2.1.tar.gz: OK
isl-0.24.tar.bz2: OK
All prerequisites downloaded successfully.
</code></pre>
<h2 id="Configure-and-compile-the-source"><a href="#Configure-and-compile-the-source" class="headerlink" title="Configure and compile the source"></a>Configure and compile the source</h2><pre><code>$ ./configure --enable-languages=c,c++ --disable-multilib
$ make -j$(nproc)
</code></pre>
<h2 id="Install-GCC"><a href="#Install-GCC" class="headerlink" title="Install GCC"></a>Install GCC</h2><pre><code>$ make install
</code></pre>
<h2 id="Verify-GCC-version"><a href="#Verify-GCC-version" class="headerlink" title="Verify GCC version"></a>Verify GCC version</h2><pre><code>$ gcc --version
gcc (GCC) 12.2.0
</code></pre>
<h2 id="Write-a-“hello-world”-C-program"><a href="#Write-a-“hello-world”-C-program" class="headerlink" title="Write a  “hello world” C program"></a>Write a  “hello world” C program</h2><pre><code>$ vi hello.c
#include &lt;stdio.h&gt;
void main()
&#123;
printf(&quot;Hello world!\n&quot;);
&#125;
</code></pre>
<h2 id="Compile-and-run-the-program"><a href="#Compile-and-run-the-program" class="headerlink" title="Compile and run the program"></a>Compile and run the program</h2><pre><code>$ gcc -o hello hello.c
$ ./hello
Hello world!
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Gcc</tag>
      </tags>
  </entry>
  <entry>
    <title>Install git from source</title>
    <url>/blog/install-git-from-source/</url>
    <content><![CDATA[<pre><code>$ wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.38.1.tar.xz
$ tar xvf git-2.38.1.tar.xz
$ cd git-2.38.1/
$ make configure
$ ./configure --prefix=/usr/local
$ make install
$ git --version
git version 2.38.1
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://mirrors.edge.kernel.org/pub/software/scm/git/">https://mirrors.edge.kernel.org/pub/software/scm/git/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Install golang from source</title>
    <url>/blog/install-go-from-source/</url>
    <content><![CDATA[<pre><code>$ wget https://dl.google.com/go/go1.15.5.linux-amd64.tar.gz
$ tar -C /usr/local -xvzf go1.15.5.linux-amd64.tar.gz

$ cat /usr/local/go/VERSION
go1.15.5

$ vim .bash_profile
PATH=$PATH:$HOME/bin:/usr/local/go/bin
export PATH

$ source .bash_profile

$ go version
go version go1.15.5 linux/amd64
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Install Jekyll on MacOS</title>
    <url>/blog/install-jekyll-on-macos/</url>
    <content><![CDATA[<h2 id="Install-Command-Line-Tools"><a href="#Install-Command-Line-Tools" class="headerlink" title="Install Command Line Tools"></a>Install Command Line Tools</h2><p>First, you need to install the command-line tools to be able to compile native extensions, open a terminal and run:</p>
<pre><code>$ xcode-select --install
</code></pre>
<h2 id="Install-Ruby"><a href="#Install-Ruby" class="headerlink" title="Install Ruby"></a>Install Ruby</h2><p>Jekyll requires Ruby &gt; 2.5.0. macOS Catalina 10.15 comes with ruby 2.6.3, so you’re fine. If you’re running a previous macOS system, you’ll have to install a newer version of Ruby.</p>
<h2 id="With-Homebrew"><a href="#With-Homebrew" class="headerlink" title="With Homebrew"></a>With Homebrew</h2><p>To run the latest Ruby version you need to install it through [Homebrew][Homebrew].</p>
<pre><code>$ /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot;

$ ruby -v
ruby 2.3.7p456 (2018-03-28 revision 63024) [universal.x86_64-darwin18]

$  brew install ruby
By default, binaries installed by gem will be placed into:
  /usr/local/lib/ruby/gems/2.7.0/bin

You may want to add this to your PATH.

If you need to have ruby first in your PATH run:
  echo &#39;export PATH=&quot;/usr/local/opt/ruby/bin:$PATH&quot;&#39; &gt;&gt; /Users/skyhawk/.bash_profile

For compilers to find ruby you may need to set:
  export LDFLAGS=&quot;-L/usr/local/opt/ruby/lib&quot;
  export CPPFLAGS=&quot;-I/usr/local/opt/ruby/include&quot;

Then relaunch your terminal and check your updated Ruby setup:

$ ruby -v
ruby 2.7.1p83 (2020-03-31 revision a0c7c23c9c) [x86_64-darwin18]
</code></pre>
<h2 id="With-rbenv"><a href="#With-rbenv" class="headerlink" title="With rbenv"></a>With rbenv</h2><p>People often use rbenv to manage multiple Ruby versions. This is very useful when you need to be able to run a given Ruby version on a project.</p>
<h2 id="Install-Jekyll"><a href="#Install-Jekyll" class="headerlink" title="Install Jekyll"></a>Install Jekyll</h2><p>Now all that is left is installing Bundler and Jekyll.</p>
<pre><code>$ /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot;
$ gem install --user-install bundler jekyll

$ ruby -v
ruby 2.7.1p83 (2020-03-31 revision a0c7c23c9c) [x86_64-darwin18]

$ echo &#39;export PATH=&quot;$HOME/.gem/ruby/2.7.0/bin:$PATH&quot;&#39; &gt;&gt; ~/.bash_profile
</code></pre>
<h2 id="Create-the-First-Website"><a href="#Create-the-First-Website" class="headerlink" title="Create the First Website"></a>Create the First Website</h2><pre><code>$ jekyll new my-awesome-site
$ cd my-awesome-site/

$ bundle exec jekyll serve
Could not find gem &#39;minima (~&gt; 2.5)&#39; in any of the gem sources listed in your Gemfile.
Run `bundle install` to install missing gems.

$ gem install minima
$ gem install jekyll-feed

$ bundle exec jekyll serve
Configuration file: /Users/skyhawk/my-awesome-site/_config.yml
            Source: /Users/skyhawk/my-awesome-site
       Destination: /Users/skyhawk/my-awesome-site/_site
 Incremental build: disabled. Enable with --incremental
      Generating...
       Jekyll Feed: Generating feed for posts
                    done in 0.232 seconds.
 Auto-regeneration: enabled for &#39;/Users/skyhawk/my-awesome-site&#39;
    Server address: http://127.0.0.1:4000/
  Server running... press ctrl-c to stop.
</code></pre>
<p>Check out the [Jekyll on macOS][Jekyll-on-macOS] for more info on how to install Jekyll on macOS.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://brew.sh/">https://brew.sh/</a></li>
<li><a href="https://jekyllrb.com/docs/installation/macos">https://jekyllrb.com/docs/installation/macos</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>Jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to BPF performance tools</title>
    <url>/blog/introduction-to-bpf-performance-tools/</url>
    <content><![CDATA[<h2 id="What-are-BPF-and-eBPF"><a href="#What-are-BPF-and-eBPF" class="headerlink" title="What are BPF and eBPF"></a>What are BPF and eBPF</h2><p>BPF stands for Berkeley Packet Filter, which was firstly developed to improve the performance of packet capture tools. It was rewritten later and turned into a general-purpose execution engine including the creation of performance analysis tools.</p>
<p>BPF allows the kernel to run mini programs on system and application events, such as disk I&#x2F;O. It makes the kernel fully programmable, empowering users(even non-kernel developers) to customize and control the system in order to solve real-world problems.</p>
<p>Extended BPF is abbreviated as eBPF. Since the classic BPF is no longer developed, the kernel contains only one execution engine, BPF(eBPF), to run both extended BPF and classic BPF programs.</p>
<h2 id="What-are-tracing-snooping-sampling-and-profiling"><a href="#What-are-tracing-snooping-sampling-and-profiling" class="headerlink" title="What are tracing, snooping, sampling and profiling"></a>What are tracing, snooping, sampling and profiling</h2><p><strong>Tracing</strong> is event based recording. BPF tools uses this type of instrumentation. You might be familiar with some Linux tracing tools. For example, <em>strace</em> records and prints system calls. <em>tcpdump</em> is another tracing tool for network packets.</p>
<p>There are many ready-to-use BPF tools after you install <a href="https://www.flamingbytes.com/blog/getting-started-with-bcc/">BCC</a>. Some of them are <strong>snooping</strong> tools. It just a differnt naming. It’s actually event based tracing.</p>
<pre><code>$ ls /usr/share/bcc/tools/| grep snoop
bindsnoop biosnoop compactsnoop dcsnoop drsnoop execsnoop exitsnoop killsnoop mountsnoop opensnoop shmsnoop sofdsnoop statsnoop syncsnoop threadsnoop ttysnoop
</code></pre>
<p><strong>Sampling</strong> tools take a subset of measurements which gives a coarse picture of the target system. It is also known as <strong>profiling</strong>. It’s usually timer(frequency) based. For example, 100 samples can be taken every second per CPU. Obviously, the overhead is generally lower than event-based tracing. However, a disadvantage is the sampling would miss events.</p>
<h2 id="What-are-BCC-and-bpftrace"><a href="#What-are-BCC-and-bpftrace" class="headerlink" title="What are BCC and bpftrace"></a>What are BCC and bpftrace</h2><p>Both BCC and bpftrace are front ends which have been developed for tracing based on BPF technology.</p>
<p>BCC was the first tracing framework developed for BPF. It provides C programming environment for writing kernel BPF code and other languages for the user-level interface like python, Lua and C++. BCC is better suited for complex scripts and daemons. It can make use of other libraries. To get started with BCC, refer to this <a href="https://www.flamingbytes.com/blog/getting-started-with-bcc/">post</a>.</p>
<p>bpftrace is a newer front end for developing BPF tools.It is ideal for powerful one-liners and custom short script. To get started with bpftrace, refer to this <a href="https://www.flamingbytes.com/blog/getting-started-with-bpftrace/">post</a>.</p>
<p>BCC and bpftrace live in a Linux Foundation project on github called <strong>IO Visor</strong>.</p>
<ul>
<li><a href="https://github.com/iovisor/bcc">https://github.com/iovisor/bcc</a></li>
<li><a href="https://github.com/iovisor/bpftrace">https://github.com/iovisor/bpftrace</a></li>
</ul>
<h2 id="BPF-tracing-visibility"><a href="#BPF-tracing-visibility" class="headerlink" title="BPF tracing visibility"></a>BPF tracing visibility</h2><p>BPF tracing gives you the visibility across the full software stack and allows new tools to be created on demand.</p>
<p>Thanks to Brendan Gregg who annotated with BPF-based performance tools to observe different components across the system software stack as below.</p>
<p><img src="https://www.brendangregg.com/Perf/bpf_book_tools.png" alt="bpf performance tools and visibility"></p>
<p>In the past, I used traditional tools like perf, strace, ftrace and systemtap for the system tracing and profiling. I’m interested in learning more about these BPF tracing tools in future.</p>
<h2 id="Dynamic-instrumentation-kprobes-and-uprobes"><a href="#Dynamic-instrumentation-kprobes-and-uprobes" class="headerlink" title="Dynamic instrumentation: kprobes and uprobes"></a>Dynamic instrumentation: kprobes and uprobes</h2><p>BPF tracing supports multiple sources of events to provide visibility of the entire software stack.</p>
<p>Dynamic instrumentation(also calledd Dynamic tracing) provides the ability to insert instrumentation points into software. It’s often used by BPF tools to instrument the start and end of the kernel and application functions. BPF tracing tools use both kprobes and uprobes for dynamic instrumentation.</p>
<p>For example,</p>
<ul>
<li>kprobe:vfs_read</li>
<li>kretprobe:vfs_read</li>
<li>uprobe:&#x2F;bin&#x2F;bash:readline</li>
<li>uretprobe:&#x2F;bin&#x2F;bash:readline</li>
</ul>
<h2 id="Static-instrumentation-tracepoints-and-USDT"><a href="#Static-instrumentation-tracepoints-and-USDT" class="headerlink" title="Static instrumentation: tracepoints and USDT"></a>Static instrumentation: tracepoints and USDT</h2><p>A downside for dynamic instrumentation is the instruments functions can be renamed or removed from one software version to the next. This is <em>interface stability issue</em>. Static instrumentation comes with more stable events(functions) which are more well maintained by developers. Of course, this will become a maintenance burden for developers. So, the number of static instrumentation should be limited.</p>
<p>BPF supports <em>tracepoints</em> for kernel static instrumentation and <em>USDT</em> for user-level static instrumentation. For example,</p>
<ul>
<li>tracepoint:syscalls:sys_enter_open</li>
<li>usdt:&#x2F;usr&#x2F;sbin&#x2F;mysqld:mysql:query__start</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>This post introduces BPF tracing technology and its front ends BCC and bpftrace at high level. Dynamic and static instrumentation(tracing) can be used for performance analysis and troubeshooting.</p>
<p>We will dive into these technologies in future posts.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>BPF Performance Tools by Brendan Gregg</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>BPF</tag>
      </tags>
  </entry>
  <entry>
    <title>Interprocess communication(IPC)</title>
    <url>/blog/interprocess-communication-ipc/</url>
    <content><![CDATA[<h2 id="Pipes"><a href="#Pipes" class="headerlink" title="Pipes"></a>Pipes</h2><p>Pipes are the oldest form of UNIX System IPC and are provided by all UNIX systems. Pipes have two limitations.</p>
<ol>
<li>Historically, they have been half duplex (i.e., data flows in only one direction). Some systems now provide full-duplex pipes, but for maximum portability, we should never assume that this is the case.</li>
<li>Pipes can be used only between processes that have a common ancestor. Normally, a pipe is created by a process, that process calls fork, and the pipe is used between the parent and the child.</li>
</ol>
<p>FIFOs get around the second limitation, and UNIX domain sockets get around both limitations.</p>
<h2 id="FIFOs"><a href="#FIFOs" class="headerlink" title="FIFOs"></a>FIFOs</h2><p>FIFOs are sometimes called named pipes. Unnamed pipes can be used only between related processes when a common ancestor has created the pipe. With FIFOs, however, unrelated processes can exchange data.</p>
<p>Creating a FIFO is similar to creating a file. Indeed, the pathname for a FIFO exists in the file system.</p>
<h2 id="Message-Queues"><a href="#Message-Queues" class="headerlink" title="Message Queues"></a>Message Queues</h2><p>A message queue is a linked list of messages stored within the kernel and identified by a message queue identifier. We’ll call the message queue just a queue and its identifier a queue ID.</p>
<p>A new queue is created or an existing queue opened by msgget. New messages are added to the end of a queue by msgsnd. Every message has a positive long integer type field, a non-negative length, and the actual data bytes (corresponding to the length), all of which are specified to msgsnd when the message is added to a queue. Messages are fetched from a queue by msgrcv. We don’t have to fetch the messages in a first-in, first-out order. Instead, we can fetch messages based on their type field.</p>
<h2 id="Semaphores"><a href="#Semaphores" class="headerlink" title="Semaphores"></a>Semaphores</h2><p>A semaphore isn’t a form of IPC similar to the others that we’ve described (pipes, FIFOs, and message queues). A semaphore is a counter used to provide access to a shared data object for multiple processes.</p>
<p>To obtain a shared resource, a process needs to do the following:</p>
<ol>
<li>Test the semaphore that controls the resource.</li>
<li>If the value of the semaphore is positive, the process can use the resource. In this case, the process decrements the semaphore value by 1, indicating that it has used one unit of the resource.</li>
<li>Otherwise, if the value of the semaphore is 0, the process goes to sleep until the semaphore value is greater than 0. When the process wakes up, it returns to step 1.</li>
</ol>
<p>When a process is done with a shared resource that is controlled by a semaphore, the semaphore value is incremented by 1. If any other processes are asleep, waiting for the semaphore, they are awakened.</p>
<p>To implement semaphores correctly, the test of a semaphore’s value and the decrementing of this value must be an atomic operation. For this reason, semaphores are normally implemented inside the kernel.</p>
<h2 id="Shared-Memory"><a href="#Shared-Memory" class="headerlink" title="Shared Memory"></a>Shared Memory</h2><p>Shared memory allows two or more processes to share a given region of memory. This is the fastest form of IPC, because the data does not need to be copied between the client and the server. The only trick in using shared memory is synchronizing access to a given region among multiple processes. If the server is placing data into a shared memory region, the client shouldn’t try to access the data until the server is done. Often, semaphores are used to synchronize shared memory access. But record locking or mutexes can also be used.</p>
<h2 id="Network-IPC-Sockets"><a href="#Network-IPC-Sockets" class="headerlink" title="Network IPC: Sockets"></a>Network IPC: Sockets</h2><p>The socket network IPC interface can be used by processes to communicate with other processes, regardless of where they are running, on the same machine or on different machines. Indeed, this was one of the design goals of the socket interface. The same interfaces can be used for both intermachine communication and intramachine communication.</p>
<p>A socket is an abstraction of a communication endpoint. Just as they would use file descriptors to access files, applications use socket descriptors to access sockets. Socket descriptors are implemented as file descriptors in the UNIX System. Indeed, many of the functions that deal with file descriptors, such as read and write, will work with a socket descriptor.</p>
<p>Normally, the recv functions will block when no data is immediately available. Similarly, the send functions will block when there is not enough room in the socket’s output queue to send the message. This behavior changes when the socket is in nonblocking mode.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>Advanced Programming in the UNIX Environment</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>IPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Iouring - A modern asynchronous I/O interface for Linux</title>
    <url>/blog/iouring-a-modern-asynchronous-i-o-interface-for-linux/</url>
    <content><![CDATA[<p>The Linux kernel has had asynchronous I&#x2F;O since version 2.5, but it was seen as difficult to use and inefficient.</p>
<p>io_uring (previously known as aioring) is a Linux kernel system call interface for storage device asynchronous I&#x2F;O operations addressing performance issues with similar interfaces provided by functions like read()&#x2F;write() or aio_read()&#x2F;aio_write() etc. for operations on data accessed by file descriptors.</p>
<p>It was primarily developed by Jens Axboe at Facebook.</p>
<p>Internally it works by creating two buffers dubbed as “queue rings” (circular buffers) for storage of submission and completion of I&#x2F;O requests (for storage devices, submission queue (SQ) and completion queue (CQ) respectively). Keeping these buffers shared between the kernel and application helps to boost the I&#x2F;O performance by eliminating the need to issue extra and expensive system calls to copy these buffers between the two.According to the io_uring design paper, the SQ buffer is writable only by consumer application, and CQ - by kernel.</p>
<p>The API provided by liburing library for userspace (applications) can be used to interact with the kernel interface more easily.</p>
<p>Both kernel interface and library were adapted in Linux 5.1 kernel version.</p>
<p><img src="/images/iouring.png" alt="Image"></p>
<p>Credit to: Donald Hunter (A visual representation of the io_uring submission and completion queues)</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://developers.redhat.com/articles/2023/04/12/why-you-should-use-iouring-network-io#:~:text=The%20main%20benefit%20of%20io_uring,file%20and%20network%20I%2FO.">Why you should use io_uring for network I&#x2F;O</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3534056.3534945">https://dl.acm.org/doi/pdf/10.1145/3534056.3534945</a></li>
<li><a href="https://kernel-recipes.org/en/2019/talks/faster-io-through-io_uring/">https://kernel-recipes.org/en/2019/talks/faster-io-through-io_uring&#x2F;</a></li>
<li><a href="https://en.wikipedia.org/wiki/Io_uring">https://en.wikipedia.org/wiki/Io_uring</a></li>
<li><a href="https://kernel.dk/io_uring.pdf">https://kernel.dk/io_uring.pdf</a></li>
<li><a href="https://developers.mattermost.com/blog/hands-on-iouring-go/">https://developers.mattermost.com/blog/hands-on-iouring-go/</a></li>
<li><a href="https://lwn.net/Articles/810414/">https://lwn.net/Articles/810414/</a></li>
<li><a href="https://lwn.net/Articles/776703/">https://lwn.net/Articles/776703/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Iouring</tag>
      </tags>
  </entry>
  <entry>
    <title>Iozone - A filesystem benchmark tool</title>
    <url>/blog/iozone-a-filesystem-benchmark-tool/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><a href="https://www.iozone.org/">IOzone</a> is a filesystem benchmark tool. The benchmark generates and measures a variety of file operations.</p>
<p>Iozone is useful for performing a broad filesystem analysis of a vendor’s computer platform. The benchmark tests file I&#x2F;O performance for the following operations:</p>
<ul>
<li>Read, write, re-read, re-write, read backwards, read strided, fread, fwrite, random read, pread ,mmap, aio_read, aio_write</li>
</ul>
<h2 id="Example-for-throughput-benchmark"><a href="#Example-for-throughput-benchmark" class="headerlink" title="Example for throughput benchmark"></a>Example for throughput benchmark</h2><p>In this example, the target is to measure the filesystem throughput(KB&#x2F;s) for different workloads with 4k iosize. The workload operations include sequential read&#x2F;write, random read&#x2F;write and mix random read&#x2F;write.</p>
<p>We run iozone on 6 mounted filesystems and 1GB files are read and written in each filesystem.</p>
<pre><code class="bash">$ /opt/iozone/bin/iozone -h
    -r #  record size in Kb
    -s #  file size in Kb
    -t #  Number of threads or processes to use in throughput test
    -I  Use VxFS VX_DIRECT, O_DIRECT,or O_DIRECTIO for all file operations    
    -F filenames  for each process/thread in throughput test
    -i #  Test to run (0=write/rewrite, 1=read/re-read, 2=random-read/write
        3=Read-backwards, 4=Re-write-record, 5=stride-read, 6=fwrite/re-fwrite
        7=fread/Re-fread, 8=random_mix, 9=pwrite/Re-pwrite, 10=pread/Re-pread
        11=pwritev/Re-pwritev, 12=preadv/Re-preadv)
    [...]    

$ /opt/iozone/bin/iozone -i 0 -i 1 -i 2 -i 8 -r 4k -s 1g -t 6 -I -F /testmnt1/testfile1 /testmnt2/testfile1 /testmnt3/testfile1 /testmnt4/testfile1 /testmnt5testfile1 /testmnt6/testfile1 &gt; iozone.out

$ cat iozone.out
Iozone: Performance Test of File I/O
        Version $Revision: 3.489 $
    Compiled for 64 bit mode.
    Build: linux-AMD64
Contributors:William Norcott, Don Capps, Isom Crawford, Kirby Collins
             Al Slater, Scott Rhine, Mike Wisner, Ken Goss
             Steve Landherr, Brad Smith, Mark Kelly, Dr. Alain CYR,
             Randy Dunlap, Mark Montague, Dan Million, Gavin Brebner,
             Jean-Marc Zucconi, Jeff Blomberg, Benny Halevy, Dave Boone,
             Erik Habbinga, Kris Strecker, Walter Wong, Joshua Root,
             Fabrice Bacchella, Zhenghua Xue, Qin Li, Darren Sawyer,
             Vangel Bojaxhi, Ben England, Vikentsi Lapa,
             Alexey Skidanov, Sudhir Kumar.
Run began: Mon Mar  1 12:25:11 2021
Record Size 4 kB
File size set to 1048576 kB
O_DIRECT feature enabled
Command line used: /opt/iozone/bin/iozone -i 0 -i 1 -i 2 -i 8 -r 4k -s 1g -t 6 -I -F /testmnt1/testfile1 /testmnt2/testfile1 /testmnt3/testfile1 /testmnt4testfile1 /testmnt5/testfile1 /testmnt6/testfile1
Output is in kBytes/sec
Time Resolution = 0.000001 seconds.
Processor cache size set to 1024 kBytes.
Processor cache line size set to 32 bytes.
File stride size set to 17 * record size.
Throughput test with 6 processes
Each process writes a 1048576 kByte file in 4 kByte records
Children see throughput for  6 initial writers 	=  151721.80 kB/sec
Parent sees throughput for  6 initial writers 	=  146366.56 kB/sec
Min throughput per process 			=   24712.40 kB/sec
Max throughput per process 			=   25707.35 kB/sec
Avg throughput per process 			=   25286.97 kB/sec
Min xfer 					= 1007996.00 kB
Children see throughput for  6 rewriters 	=  152089.88 kB/sec
Parent sees throughput for  6 rewriters 	=  152084.55 kB/sec
Min throughput per process 			=   25109.69 kB/sec
Max throughput per process 			=   25674.81 kB/sec
Avg throughput per process 			=   25348.31 kB/sec
Min xfer 					= 1025500.00 kB
Children see throughput for  6 readers 		=    7618.06 kB/sec
Parent sees throughput for  6 readers 		=    7618.04 kB/sec
Min throughput per process 			=    1268.31 kB/sec
Max throughput per process 			=    1270.73 kB/sec
Avg throughput per process 			=    1269.68 kB/sec
Min xfer 					= 1046580.00 kB
Children see throughput for 6 re-readers 	=    7629.77 kB/sec
Parent sees throughput for 6 re-readers 	=    7629.74 kB/sec
Min throughput per process 			=    1270.79 kB/sec
Max throughput per process 			=    1273.63 kB/sec
Avg throughput per process 			=    1271.63 kB/sec
Min xfer 					= 1046240.00 kB
Children see throughput for 6 random readers 	=    7605.91 kB/sec
Parent sees throughput for 6 random readers 	=    7605.89 kB/sec
Min throughput per process 			=    1266.91 kB/sec
Max throughput per process 			=    1268.54 kB/sec
Avg throughput per process 			=    1267.65 kB/sec
Min xfer 					= 1047228.00 kB
Children see throughput for 6 mixed workload 	=   79687.92 kB/sec
Parent sees throughput for 6 mixed workload 	=   78974.22 kB/sec
Min throughput per process 			=    1275.41 kB/sec
Max throughput per process 			=   25449.38 kB/sec
Avg throughput per process 			=   13281.32 kB/sec
Min xfer 					=   52552.00 kB
Children see throughput for 6 random writers 	=  146210.38 kB/sec
Parent sees throughput for 6 random writers 	=  143822.17 kB/sec
Min throughput per process 			=   24206.99 kB/sec
Max throughput per process 			=   24653.06 kB/sec
Avg throughput per process 			=   24368.40 kB/sec
Min xfer 					= 1029604.00 kB
iozone test complete.   
</code></pre>
<h2 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a>iostat</h2><p>In the above test, we use a flash array with single 500TB LUN created. There are 4 active paths from host to the single LUN. Six logical volumes and filesystems are created on the single LUN.</p>
<p>The following is a piece of iostat output for one of the four disks(paths) when the mix random read&#x2F;write workload is running. The read throughput is ~940KB&#x2F;s and the write throughput is 19MB&#x2F;s.</p>
<p>To measure the maximum throughput, we need keep increasing the number of read&#x2F;write threads until the throughput(KB&#x2F;s) is capped. Also, the iosize has big impact on the throughput. We may test with different I&#x2F;O sizes.</p>
<pre><code class="bash">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sdd               0.00     0.00  189.80 4255.20   759.20 17020.80     8.00     1.13    0.25    3.07    0.13   0.18  81.80
sdd               0.00     0.00  237.20 4708.20   948.80 18832.80     8.00     1.35    0.27    3.08    0.13   0.20  96.94
sdd               0.00     0.00  235.40 4898.20   941.60 19592.80     8.00     1.37    0.27    3.06    0.13   0.19  99.54
sdd               0.00     0.00  229.60 4575.60   918.40 18302.40     8.00     1.32    0.27    3.09    0.13   0.20  94.28
sdd               0.00     0.00  229.94 4822.55   919.76 19290.22     8.00     1.32    0.26    3.06    0.13   0.19  96.89
sdd               0.00     0.00  228.80 4512.40   915.20 18049.60     8.00     1.30    0.27    3.07    0.13   0.20  94.40
sdd               0.00     0.00  234.20 4810.20   936.80 19240.80     8.00     1.33    0.26    3.08    0.13   0.19  97.10
sdd               0.00     0.00  246.60 4497.40   986.40 17989.60     8.00     1.35    0.28    3.06    0.13   0.20  97.04
sdd               0.00     0.00  106.60 1665.60   426.40  6661.70     8.00     0.55    0.31    3.05    0.14   0.22  38.26
</code></pre>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://www.iozone.org/">https://www.iozone.org/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Iozone</tag>
      </tags>
  </entry>
  <entry>
    <title>journalctl - Query the systemd journal</title>
    <url>/blog/journalctl-query-the-systemd-journal/</url>
    <content><![CDATA[<p>journalctl may be used to query the contents of the systemd journal as written by systemd-journald.service.</p>
<p>If called without parameters, it will show the full contents of the journal, starting with the oldest entry collected.</p>
<p>The following are some of the common used options:</p>
<ul>
<li>–no-full, –full, -l Ellipsize fields when they do not fit in available columns. The default is to show full fields, allowing them to wrap or be truncated by the pager, if one is used.</li>
<li>-f, –follow Show only the most recent journal entries, and continuously print new entries as they are appended to the journal.</li>
<li>-u, –unit&#x3D;UNIT or PATTERN Show messages for the specified systemd unit UNIT (such as a service unit), or for any of the units matched by PATTERN. This parameter can be specified multiple times.</li>
<li>-S, –since&#x3D;, -U, –until&#x3D; Start showing entries on or newer than the specified date, or on or older than the specified date, respectively.</li>
</ul>
<p>The following commands are to save the journal to a file or query it on the fly with the <em>-f</em> option.</p>
<pre><code>$ systemctl -a | grep ntpd.service
  ntpd.service  loaded    active   running   Network Time Service
$ journalctl -u ntpd &gt; ntpd.journal.out
$ tail ntpd.journal.out

$ journalctl -u ntpd* --since &quot;10 minutes ago&quot;

$ journalctl -lfu ntpd*
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Justice over Loyalty</title>
    <url>/blog/justice-over-loyalty/</url>
    <content><![CDATA[<p>Regulus Aurelius woke up to the clanging of weapons and fierce battle cries of Roman elite soldiers during their practice session for the oncoming battles. Things weren’t going well for Rome when General Hannibal Barca of Carthage stormed into the Italian Peninsula with a raging army of 26000 troops, 6000 horses, and war elephants. Regulus, unlike the fearful Roman citizens, faced a continuous dilemma. Part of him was a loyal Roman general who wanted to defeat Carthage. The other part of him was loyal to Carthage, his homeland. Sometimes, a faint smile tugged at his mouth when frantic Roman scouts reported the losses they suffered.<span id="more"></span></p>
<p>That day, General Scipio Africanus summoned Regulus for a strategic discussion on future battle plans. The Roman army yet again lost a major battle at Cannae. Even though outnumbered, Hannibal used his superior cavalry to encircle and crush the Roman soldiers. This was the worst single day defeat a Roman army ever suffered. Regulus grimly pondered on what to do next. That was when Scipio created his devastating plan to take the war to Carthage, which had a small army left because Hannibal led most of it in Italy. Regulus tensed and sunk deeper into his chair. This flawless plan of General Scipio Africanus would bring upon total annihilation to Carthage. Cold sweat trickled down his neck as he focused on his primary goal: He needed to save Carthage.</p>
<p>Regulus decided his ultimate choice and asked, “General Scipio Africanus, when will our mighty fleets and army set sail for Carthage? Perhaps you would want to first recover from the earlier battles, so our army would be reinforced with fresh troops? The citizens of Romen will surely want a clear and honorable victory over Carthage.”</p>
<p>“Yes, our legions shall leave at dawn in a week. We are eager for a war of revenge that will make the world fear disobeying Roman law again.”</p>
<p>That night, Regulus called his most loyal soldiers to help him prepare the ship, which would take him to Carthage. The trip was peaceful until he arrived in Carthage harbor. At first sight of a Roman ship, the guards readied their swords and waited for the king’s order. Regulus carefully came down and explained the whole situation to king Hanno the Great. He advised the king to withdraw his legions from Rome so they could prepare as much as possible with the remaining time they had. Hanno agreed to his plan and the next day, no Carthaginian soldier remained in the Italian Peninsula. Regulus Aurelius was made the commander of the army. He was pleased to see the effort that his soldiers put in their practice. At one point, he even commented that they were a worthy match for the Roman legions, which was saying a lot.</p>
<p>This newly inspired hope was dampened the moment 1000 Roman triremes docked in the harbor. The ships were so massive and numerous that they appeared to have covered the entire Mediterranean Sea. Dust spread wherever the soldiers moved. The sunlight struck their weapons, giving them a wicked gleam. Their shields and armor were so well-polished that Regulus could see his own reflection on it. The citizens scattered at sight and the only people that remained were Hannibal’s army, which was miniscule compared to the Roman legions. The battle was held at Zama, the final battleground of the war. The armies clashed mercilessly and at first, no victor could be decided. The only sounds came from the general’s commands or the clanging of weapons. However, the battle was soon leaning toward Rome’s favor. Scipio led his legions, which plowed through Carthage’s army and demolished every enemy in their path.</p>
<p>Both armies were weary after a day of fighting, but Rome stood victorious despite Carthage’s efforts. Scipio arranged a peace treaty to be signed and left Carthage with his legions and Regulus in chains. He had already ordered the execution of the soldiers who helped him get to Carthage. Regulus’s fate was to be decided by the judges, who evidently despised him. The vote was unanimous, and they sentenced Regulus to death. Before his execution, he was given permission to say his last words. He looked at the impatient crowd, who seemed eager to witness his demise. Hanno and Hannibal were among the audience, for they wished to honor the hero who willingly gave up his life for the sake of his homeland.</p>
<p>“I have not but two requests to make. Romans, I wish you to view this war from the perspective of justice. You have brought forth this unjust war because you envied Carthage’s wealth. Instead of striving for that goal, you just forced war upon the innocent Carthaginians. Was this rational to invade another’s property because you wanted it? Carthaginians, you must not lose hope because of these defeats and setbacks. If you try again, no matter how many times, you will succeed in the future. But for now, I must depart from this world.”</p>
<p>A week after Regulus’s execution, Carthage still hadn’t signed the peace treaty despite the threats of war from Rome. King Hanno was hesitant and was waiting for an opportunity to spark a new war for independence. This chance arose when Rome faced a civil war, which badly weakened its army. Hanno threatened to attack Rome if it didn’t give Carthage free rein over its trade. At the present, Rome was in no state to refuse, so they agreed to his demands. Finally, Carthage was free of Rome’s control and honored Regulus’s sacrifice by fulfilling his ultimate wish.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Keep a docker container running and not exiting</title>
    <url>/blog/keep-a-docker-container-running-and-not-exiting/</url>
    <content><![CDATA[<p>In the kubernetes environment, we can keep a container(pod) alive and avoid it exits immediately after starting.</p>
<h1 id="Method-one"><a href="#Method-one" class="headerlink" title="Method one"></a>Method one</h1><p>1.<br>Use a long-time-run command in Dockerfile</p>
<pre><code>CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;]
</code></pre>
<p>or</p>
<pre><code>CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep infinity&quot;]
</code></pre>
<p>2.<br>Build the docker image and push to docker repository</p>
<p>3.<br>Launch the container</p>
<pre><code>$ kubectl run mycontainer -it --image=&lt;docker-image-name&gt;
</code></pre>
<h1 id="Method-two"><a href="#Method-two" class="headerlink" title="Method two"></a>Method two</h1><p>When to deploy an application with kubernetes statefulset, we also can add it to the statefulset yaml file instead of adding it to the docker image through Dockerfile.</p>
<pre><code>$ cat myapp.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myapp
spec:
  serviceName: myapp
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: noname/myapp:latest
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;tail -f /dev/null&quot;]
        imagePullPolicy: Always
        volumeMounts:
        - name: myapp-data
          mountPath: /data
        - name: myapp-log
          mountPath: /log
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: myapp-data
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 20Gi
  - metadata:
      name: myapp-log
    spec:
      storageClassName: &lt;storage-class&gt;
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi

$ kubectl apply -f myapp.yaml          
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Key concepts in Elastic Rally</title>
    <url>/blog/key-concepts-in-esrally/</url>
    <content><![CDATA[<h2 id="Pipelines"><a href="#Pipelines" class="headerlink" title="Pipelines"></a>Pipelines</h2><p>A pipeline is a series of steps that are performed to get benchmark results. This is not intended to customize the actual benchmark but rather what happens before and after a benchmark.</p>
<p>An example will clarify the concept: If you want to benchmark a binary distribution of Elasticsearch, Rally has to download a distribution archive, decompress it, start Elasticsearch and then run the benchmark. However, if you want to benchmark a source build of Elasticsearch, it first has to build a distribution using the Gradle Wrapper. So, in both cases, different steps are involved and that’s what pipelines are for.</p>
<p>You can get a list of all pipelines with <em>esrally list pipelines</em></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ esrally --version</span><br><span class="line">esrally 2.6.0</span><br><span class="line"></span><br><span class="line">$ esrally list pipelines</span><br><span class="line"></span><br><span class="line">    ____        ____</span><br><span class="line">   / __ \____ _/ / /_  __</span><br><span class="line">  / /_/ / __ `/ / / / / /</span><br><span class="line"> / _, _/ /_/ / / / /_/ /</span><br><span class="line">/_/ |_|\__,_/_/_/\__, /</span><br><span class="line">                /____/</span><br><span class="line"></span><br><span class="line">Available pipelines:</span><br><span class="line"></span><br><span class="line">Name               Description</span><br><span class="line">-----------------  ---------------------------------------------------------------------------------------------</span><br><span class="line">from-sources       Builds and provisions Elasticsearch, runs a benchmark and reports results.</span><br><span class="line">from-distribution  Downloads an Elasticsearch distribution, provisions it, runs a benchmark and reports results.</span><br><span class="line">benchmark-only     Assumes an already running Elasticsearch instance, runs a benchmark and reports results</span><br><span class="line"></span><br><span class="line">-------------------------------</span><br><span class="line">[INFO] SUCCESS (took 0 seconds)</span><br><span class="line">-------------------------------</span><br></pre></td></tr></table></figure>

<h3 id="benchmark-only"><a href="#benchmark-only" class="headerlink" title="benchmark-only"></a>benchmark-only</h3><p>This is intended if you want to provision a cluster by yourself. Do not use this pipeline unless you are absolutely sure you need to. As Rally has not provisioned the cluster, results are not easily reproducable and it also cannot gather a lot of metrics (like CPU usage).</p>
<p>To benchmark a cluster, you also have to specify the hosts to connect to.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ esrally race --pipeline=benchmark-only --target-host=10.10.10.1:39200,10.10.10.2:39200,10.10.10.3:39200 --track=geonames</span><br></pre></td></tr></table></figure>

<h3 id="from-distribution"><a href="#from-distribution" class="headerlink" title="from-distribution"></a>from-distribution</h3><p>This pipeline allows to benchmark an official Elasticsearch distribution which will be automatically downloaded by Rally. An example invocation:</p>
<pre><code class="bash">$ esrally race --track=geonames --pipeline=from-distribution --distribution-version=7.17.0
</code></pre>
<h3 id="from-sources"><a href="#from-sources" class="headerlink" title="from-sources"></a>from-sources</h3><p>You should use this pipeline when you want to build and benchmark Elasticsearch from sources.</p>
<p>Remember that you also need git installed. You have to specify a revision.</p>
<pre><code class="bash">$ esrally race --track=geonames --pipeline=from-sources --revision=latest
</code></pre>
<h2 id="track"><a href="#track" class="headerlink" title="track"></a>track</h2><p>A track is the description of one or more benchmarking scenarios with a specific document corpus. It defines for example the involved indices, data files and which operations are invoked. List the available tracks with <em>esrally list tracks</em>. Although Rally ships with some tracks out of the box, you should usually create your own track based on your own data.</p>
<ul>
<li>Geonames: for evaluating the performance of structured data.</li>
<li>Geopoint: for evaluating the performance of geopoint queries.</li>
<li>Geopointshape: for evaluating the performance of geopointshape queries.</li>
<li>Geoshape: for evaluating the performance of geoshape data.</li>
<li>Percolator: for evaluating the performance of percolation queries.</li>
<li>PMC: for evaluating the performance of full text search.</li>
<li>NYC taxis: for evaluating the performance for highly structured data.</li>
<li>NYC taxis (ARM): for evaluating the performance for highly structured data and detect ARM-specific regressions.</li>
<li>Nested: for evaluating the performance for nested documents.</li>
<li>HTTP Logs: for evaluating the performance of (Web) server logs.</li>
<li>NOAA: for evaluating the performance of range fields.</li>
<li>EQL: for evaluating the performance of EQL.</li>
<li>SQL: for evaluating the performance of SQL.</li>
<li>SO (Transform): for evaluating the performance of the transform feature.</li>
<li>SO (Frequent Items): for evaluating the performance of the frequent items aggregation.</li>
<li>Logging: for evaluating the performance of the Log Monitoring part of Elastic’s Observability solution.</li>
<li>Logging (Many Shards): for evaluating the performance of the Log Monitoring part of Elastic’s Observability solution with a large amount of shards.</li>
<li>Logging (Snapshots): for evaluating snapshot performance when taking snapshot with a large of amount of shards.</li>
<li>Dense Vector: for evaluating the performance of vectors search.</li>
<li>Security: for evaluating the performance of Elastic’s Security solution.</li>
<li>TSDB: for evaluating the performance of TSDB (time series database).</li>
<li>Logging (CCS): for evaluating the performance of CCS (cross cluster search).</li>
</ul>
<pre><code class="bash">$ esrally list tracks

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available tracks:

Name              Description                                                              Documents    Compressed Size   Uncompressed Size    Default Challenge        All Challenges
----------------  -----------------------------------------------------------------------  -----------  ----------------- -------------------  ----------------------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
elastic/logs      Track for simulating logging workloads                                   14,009,078   N/A                NA                  logging-indexing         logging-indexing-querying,logging-indexing,logging-querying,logging-snapshot-mountlogging-snapshot-restore,logging-snapshot,cross-clusters-search,logging-disk-usage,many-shards-quantitative,many-shards-snapshots
elastic/security  Track for simulating Elastic Security workloads                          77,513,777   N/A                NA                  security-querying        security-indexing-querying,security-indexing,security-querying,index-alert-source-events
elastic/endpoint  Endpoint track                                                           0            0 bytes            0bytes              default                  default
eql               EQL benchmarks based on endgame index of SIEM demo cluster               60,782,211   4.5 GB             109.2GB             default                  default,index-sorting
geonames          POIs from Geonames                                                       11,396,503   252.9 MB           3.3GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflictssignificant-text
geopoint          Point coordinates from PlanetOSM                                         60,844,404   482.1 MB           2.3GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts
geopointshape     Point coordinates from PlanetOSM indexed as geoshapes                    60,844,404   470.8 MB           2.6GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts
geoshape          Shapes from PlanetOSM                                                    84,220,567   17.0 GB            58.7GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-big
http_logs         HTTP server log data                                                     247,249,096  1.2 GB             31.1GB              append-no-conflicts      append-no-conflicts,runtime-fields,append-no-conflicts-index-onlyappend-sorted-no-conflicts,append-index-only-with-ingest-pipeline,update,append-no-conflicts-index-reindex-only
metricbeat        Metricbeat data                                                          1,079,600    87.7 MB            1.2GB               append-no-conflicts      append-no-conflicts
nested            StackOverflow Q&amp;A stored as nested docs                                  11,203,029   663.3 MB           3.4GB               nested-search-challenge  nested-search-challenge,index-only
noaa              Global daily weather measurements from NOAA                              33,659,481   949.4 MB           9.0GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,aggs,filter-aggs
nyc_taxis         Taxi rides in New York in 2015                                           165,346,692  4.5 GB             74.3GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts-index-onlyupdate,append-ml,aggs
percolator        Percolator benchmark based on AOL queries                                2,000,000    121.1 kB           104.9MB             append-no-conflicts      append-no-conflicts
pmc               Full text benchmark with academic papers from PMC                        574,199      5.5 GB             21.7GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflictsappend-fast-with-conflicts,indexing-querying
so                Indexing benchmark using up to questions and answers from StackOverflow  36,062,278   8.9 GB             33.1GB              append-no-conflicts      append-no-conflicts,transform,frequent-items
sql               SQL query performance based on NOAA Weather data                         33,659,481   949.4 MB           9.0GB               sql                      sql
dense_vector      Benchmark for dense vector indexing and search                           10,000,000   7.2 GB             19.5GB              index-and-search         index-and-search
so_vector         Benchmark for vector search with StackOverflow data                      2,000,000    12.3 GB            32.2GB              index-and-search         index-and-search
tsdb              metricbeat information for elastic-app k8s cluster                       116,633,698  N/A                123.0GB             append-no-conflicts      append-no-conflicts,downsample

-------------------------------
[INFO] SUCCESS (took 2 seconds)
-------------------------------

$ esrally info --track=geonames

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Showing details for track [geonames]:

* Description: POIs from Geonames
* Documents: 11,396,503
* Compressed Size: 252.9 MB
* Uncompressed Size: 3.3 GB

================================================
Challenge [append-no-conflicts] (run by default)
================================================

Indexes the whole document corpus using Elasticsearch default settings. We only adjust the number of replicas as we benchmark asingle node cluster and Rally will only start the benchmark if the cluster turns green. Document ids are unique so all indexoperations are append only. After that a couple of queries are run.

Schedule:
----------

1. delete-index
2. create-index
3. check-cluster-health
4. index-append (8 clients)
5. refresh-after-index
6. force-merge
7. refresh-after-force-merge
8. wait-until-merges-finish
9. index-stats
10. node-stats
11. default
12. term
13. phrase
14. country_agg_uncached
15. country_agg_cached
16. scroll
17. expression
18. painless_static
19. painless_dynamic
20. decay_geo_gauss_function_score
21. decay_geo_gauss_script_score
22. field_value_function_score
23. field_value_script_score
24. large_terms
25. large_filtered_terms
26. large_prohibited_terms
27. desc_sort_population
28. asc_sort_population
29. asc_sort_with_after_population
30. desc_sort_geonameid
31. desc_sort_with_after_geonameid
32. asc_sort_geonameid
33. asc_sort_with_after_geonameid

==========================================
Challenge [append-no-conflicts-index-only]
==========================================

Indexes the whole document corpus using Elasticsearch default settings. We only adjust the number of replicas as we benchmark asingle node cluster and Rally will only start the benchmark if the cluster turns green. Document ids are unique so all indexoperations are append only.

Schedule:
----------

1. delete-index
2. create-index
3. check-cluster-health
4. index-append (8 clients)
5. force-merge
6. wait-until-merges-finish

======================================
Challenge [append-fast-with-conflicts]
======================================

Indexes the whole document corpus using a setup that will lead to a larger indexing throughput than the default settings. Rallywill produce duplicate ids in 25% of all documents (not configurable) so we can simulate a scenario with appends most of the timeand some updates in between.

Schedule:
----------

1. delete-index
2. create-index
3. check-cluster-health
4. index-update (8 clients)
5. force-merge
6. wait-until-merges-finish

============================
Challenge [significant-text]
============================

Indexes the whole document corpus using Elasticsearch default settings. We only adjust the number of replicas as we benchmark asingle node cluster and Rally will only start the benchmark if the cluster turns green. Document ids are unique so all indexoperations are append only.

Schedule:
----------

1. delete-index
2. create-index
3. check-cluster-health
4. index-append (8 clients)
5. force-merge
6. wait-until-merges-finish
7. significant_text_selective
8. significant_text_sampled_selective
9. significant_text_unselective
10. significant_text_sampled_unselective


-------------------------------
[INFO] SUCCESS (took 1 seconds)
-------------------------------
</code></pre>
<p>A track is specified in a JSON file. A track JSON file can include the following sections:</p>
<ul>
<li>indices&#x2F;templates: define the relevant indices and index templates</li>
<li>data-streams: define the relevant data streams</li>
<li>composable-templates&#x2F;component-templates: define the relevant composable and component templates</li>
<li>corpora: define all document corpora (i.e. data files) that Rally should use for this track</li>
<li>challenge(s): describe more than one set of operations, in the event your track needs to test more than one set of scenarios</li>
<li>schedule: describe the workload for the benchmark, for example index with two clients at maximum throughput while searching with another two clients with ten operations per second</li>
<li>operations: describe which operations are available for this track and how they are parametrized</li>
<li>dependencies</li>
</ul>
<pre><code class="bash">$ ls .rally/benchmarks/tracks/default/geonames
challenges  files.txt  index.json  operations  __pycache__  README.md  terms.txt  track.json  track.py

$ cat .rally/benchmarks/tracks/default/geonames/track.json
&#123;
  &quot;version&quot;: 2,
  &quot;description&quot;: &quot;POIs from Geonames&quot;,
  &quot;data-url&quot;: &quot;http://benchmarks.elasticsearch.org.s3.amazonaws.com/corpora/geonames&quot;,
  &quot;indices&quot;: [
    &#123;
      &quot;name&quot;: &quot;geonames&quot;,
      &quot;body&quot;: &quot;index.json&quot;
    &#125;
  ],
  &quot;corpora&quot;: [
    &#123;
      &quot;name&quot;: &quot;geonames&quot;,
      &quot;base-url&quot;: &quot;https://rally-tracks.elastic.co/geonames&quot;,
      &quot;documents&quot;: [
        &#123;
          &quot;source-file&quot;: &quot;documents-2.json.bz2&quot;,
          &quot;document-count&quot;: 11396503,
          &quot;compressed-bytes&quot;: 265208777,
          &quot;uncompressed-bytes&quot;: 3547613828
        &#125;
      ]
    &#125;
  ],
  &quot;operations&quot;: [
    &#123;&#123; rally.collect(parts="operations/*.json") &#125;&#125;
  ],
  &quot;challenges&quot;: [
    &#123;&#123; rally.collect(parts="challenges/*.json") &#125;&#125;
  ]
&#125;

$ cat .rally/benchmarks/tracks/default/geonames/index.json
&#123;
  &quot;settings&quot;: &#123;
    &quot;index.number_of_shards&quot;: &#123;&#123;number_of_shards | default(5)&#125;&#125;,
    &quot;index.number_of_replicas&quot;: &#123;&#123;number_of_replicas | default(0)&#125;&#125;,
    &quot;index.store.type&quot;: &quot;&#123;&#123;store_type | default('fs')&#125;&#125;&quot;,
    &quot;index.requests.cache.enable&quot;: false
  &#125;,
  &quot;mappings&quot;: &#123;
    &quot;dynamic&quot;: &quot;strict&quot;,
    &quot;_source&quot;: &#123;
      &quot;enabled&quot;: &#123;&#123; source_enabled | default(true) | tojson &#125;&#125;
    &#125;,
    &quot;properties&quot;: &#123;
    [..]
  &#125;
&#125;
</code></pre>
<h2 id="challenge"><a href="#challenge" class="headerlink" title="challenge"></a>challenge</h2><p>A challenge describes one benchmarking scenario, for example indexing documents at maximum throughput with 4 clients while issuing term and phrase queries from another two clients rate-limited at 10 queries per second each. It is always specified in the context of a track. See the available challenges by listing the corresponding tracks with <em>esrally list tracks</em>.</p>
<h2 id="car"><a href="#car" class="headerlink" title="car"></a>car</h2><p>A car is a specific configuration of an Elasticsearch cluster that is benchmarked, for example the out-of-the-box configuration, a configuration with a specific heap size or a custom logging configuration. List the available cars with <em>esrally list cars</em>.</p>
<pre><code class="bash">$ esrally list cars

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available cars:

Name                     Type    Description
-----------------------  ------  --------------------------------------
16gheap                  car     Sets the Java heap to 16GB
1gheap                   car     Sets the Java heap to 1GB
24gheap                  car     Sets the Java heap to 24GB
2gheap                   car     Sets the Java heap to 2GB
4gheap                   car     Sets the Java heap to 4GB
8gheap                   car     Sets the Java heap to 8GB
defaults                 car     Sets the Java heap to 1GB
basic-license            mixin   Basic License
debug-non-safepoints     mixin   More accurate CPU profiles
ea                       mixin   Enables Java assertions
fp                       mixin   Preserves frame pointers
g1gc                     mixin   Enables the G1 garbage collector
parallelgc               mixin   Enables the Parallel garbage collector
trial-license            mixin   Trial License
unpooled                 mixin   Enables Netty&#39;s unpooled allocator
x-pack-ml                mixin   X-Pack Machine Learning
x-pack-monitoring-http   mixin   X-Pack Monitoring (HTTP exporter)
x-pack-monitoring-local  mixin   X-Pack Monitoring (local exporter)
x-pack-security          mixin   X-Pack Security
zgc                      mixin   Enables the ZGC garbage collector

-------------------------------
[INFO] SUCCESS (took 1 seconds)
-------------------------------
</code></pre>
<h2 id="telemetry"><a href="#telemetry" class="headerlink" title="telemetry"></a>telemetry</h2><p>Telemetry is used in Rally to gather metrics about the car, for example CPU usage or index size.</p>
<pre><code class="bash">$ esrally list telemetry

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available telemetry devices:

Command                     Name                        Description
--------------------------  --------------------------  --------------------------------------------------------------------
jit                         JIT Compiler Profiler       Enables JIT compiler logs.
gc                          GC log                      Enables GC logs.
jfr                         Flight Recorder             Enables Java Flight Recorder (requires an Oracle JDK or OpenJDK 11+)
heapdump                    Heap Dump                   Captures a heap dump.
node-stats                  Node Stats                  Regularly samples node stats
recovery-stats              Recovery Stats              Regularly samples shard recovery stats
ccr-stats                   CCR Stats                   Regularly samples Cross Cluster Replication (CCR) related stats
segment-stats               Segment Stats               Determines segment stats at the end of the benchmark.
transform-stats             Transform Stats             Regularly samples transform stats
searchable-snapshots-stats  Searchable Snapshots Stats  Regularly samples searchable snapshots stats
shard-stats                 Shard Stats                 Regularly samples nodes stats at shard level
data-stream-stats           Data Stream Stats           Regularly samples data stream stats
ingest-pipeline-stats       Ingest Pipeline Stats       Reports Ingest Pipeline stats at the end of the benchmark.
disk-usage-stats            Disk usage of each field    Runs the indices disk usage API after benchmarking

Keep in mind that each telemetry device may incur a runtime overhead which can skew results.

-------------------------------
[INFO] SUCCESS (took 0 seconds)
-------------------------------
</code></pre>
<h2 id="race"><a href="#race" class="headerlink" title="race"></a>race</h2><p>A race is one invocation of the Rally binary. Another name for that is one “benchmarking trial”. During a race, Rally runs one challenge on a track with the given car.</p>
<pre><code class="bash">$ esrally race --pipeline=benchmark-only --target-host=10.10.10.1:39200,10.10.10.2:39200,10.10.10.3:39200 --track=geonames--track-params=&quot;number_of_shards:3,number_of_replicas:1&quot; --challenge=append-no-conflicts --on-error=abort --race-id=$&#123;RACE_ID&#125;

$ esrally list races

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/


Recent races:

Race ID                               Race Timestamp    Track     Track Parameters                          Challenge           Car       User Tags                  Track Revision    Team Revision
------------------------------------  ----------------  --------  ----------------------------------------  ------------------- --------  -------------------------  ----------------  ---------------
066e02fa-a71a-4239-b515-984f705d5f02  20221031T230044Z  geonames  number_of_replicas=1, number_of_shards=3  append-no-conflicts external  intention=3shards1replica  a64a92a
734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f  20221031T214254Z  geonames  number_of_replicas=0, number_of_shards=3  append-no-conflicts external  intention=3shards0replica  a64a92a
</code></pre>
<h2 id="tournament"><a href="#tournament" class="headerlink" title="tournament"></a>tournament</h2><p>A tournament is a comparison of two races. Looks like <a href="https://github.com/elastic/rally/issues/57">Rally</a> doesn’t have the tournament mode support yet. Instead, the comparison can be made as the following command between two races. Note that, we should NOT run the same benchmark multiple times without data cleanup between the benchmarks. It will give us unreproducible results.</p>
<pre><code class="bash">$ esrally compare --baseline=066e02fa-a71a-4239-b515-984f705d5f02 --contender=734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/


Comparing baseline
  Race ID: 066e02fa-a71a-4239-b515-984f705d5f02
  Race timestamp: 2022-10-31 23:00:44
  Challenge: append-no-conflicts
  Car: external
  User tags: intention=3shards1replica

with contender
  Race ID: 734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f
  Race timestamp: 2022-10-31 21:42:54
  Challenge: append-no-conflicts
  Car: external
  User tags: intention=3shards0replica

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                        Metric |                           Task |        Baseline |      Contender |         Diff |    Unit |   Diff % |
|--------------------------------------------------------------:|-------------------------------:|----------------:----------------:|-------------:|--------:|---------:|
|                    Cumulative indexing time of primary shards |                                |    14.3493      |    140125      |     -0.33683 |     min |   -2.35% |
|             Min cumulative indexing time across primary shard |                                |     0           |    0           |      0       |     min |    0.00% |
|          Median cumulative indexing time across primary shard |                                |     0.00696667  |     000696667  |      0       |     min |    0.00% |
|             Max cumulative indexing time across primary shard |                                |     4.954       |     473868     |     -0.21532 |     min |   -4.35% |
|           Cumulative indexing throttle time of primary shards |                                |     0           |    0           |      0       |     min |    0.00% |
|    Min cumulative indexing throttle time across primary shard |                                |     0           |    0           |      0       |     min |    0.00% |
| Median cumulative indexing throttle time across primary shard |                                |     0           |    0           |      0       |     min |    0.00% |
|    Max cumulative indexing throttle time across primary shard |                                |     0           |    0           |      0       |     min |    0.00% |
|                       Cumulative merge time of primary shards |                                |     9.5478      |     486773     |     -4.68007 |     min |  -49.02% |
|                      Cumulative merge count of primary shards |                                |    93           |   70           |    -23       |         |  -24.73% |
|                Min cumulative merge time across primary shard |                                |     0           |    0           |      0       |     min |    0.00% |
|             Median cumulative merge time across primary shard |                                |     0.00276667  |     000276667  |      0       |     min |    0.00% |
|                Max cumulative merge time across primary shard |                                |     3.61232     |     171572     |     -1.8966  |     min |  -52.50% |
|              Cumulative merge throttle time of primary shards |                                |     2.62635     |     0569683    |     -2.05667 |     min |  -78.31% |
|       Min cumulative merge throttle time across primary shard |                                |     0           |    0           |      0       |     min |    0.00% |
|    Median cumulative merge throttle time across primary shard |                                |     0           |    0           |      0       |     min |    0.00% |
|       Max cumulative merge throttle time across primary shard |                                |     1.06415     |     021315     |     -0.851   |     min |  -79.97% |
|                     Cumulative refresh time of primary shards |                                |     1.37218     |     045075     |     -0.92143 |     min |  -67.15% |
|                    Cumulative refresh count of primary shards |                                |   421           |  327           |    -94       |         |  -22.33% |
|              Min cumulative refresh time across primary shard |                                |     0           |    0           |      0       |     min |    0.00% |
|           Median cumulative refresh time across primary shard |                                |     0.0191417   |     00191417   |      0       |     min |    0.00% |
|              Max cumulative refresh time across primary shard |                                |     0.4631      |     0152433    |     -0.31067 |     min |  -67.08% |
|                       Cumulative flush time of primary shards |                                |     0.144633    |     0185383    |      0.04075 |     min |  +28.17% |
|                      Cumulative flush count of primary shards |                                |    14           |   14           |      0       |         |    0.00% |
|                Min cumulative flush time across primary shard |                                |     0           |    0           |      0       |     min |    0.00% |
|             Median cumulative flush time across primary shard |                                |     0.000191667 |     0000191667 |      0       |     min |    0.00% |
|                Max cumulative flush time across primary shard |                                |     0.0575833   |     00674167   |      0.00983 |     min |  +17.08% |
|                                       Total Young Gen GC time |                                |    30.494       |    14511       |    -15.983   |       s |  -52.41% |
|                                      Total Young Gen GC count |                                |  3888           | 2381           |  -1507       |         |  -38.76% |
|                                         Total Old Gen GC time |                                |     7.329       |     3247       |     -4.082   |       s |  -55.70% |
|                                        Total Old Gen GC count |                                |   109           |   48           |    -61       |         |  -55.96% |
|                                                    Store size |                                |     5.78897     |     301044     |     -2.77854 |      GB |  -48.00% |
|                                                 Translog size |                                |     8.19564e-07 |     665896e-07 |     -0       |      GB |  -18.75% |
|                                        Heap used for segments |                                |     0.513947    |     0385025    |     -0.12892 |      MB |  -25.08% |
|                                      Heap used for doc values |                                |     0.037796    |     00160522   |     -0.02174 |      MB |  -57.53% |
|                                           Heap used for terms |                                |     0.383759    |     0296478    |     -0.08728 |      MB |  -22.74% |
|                                           Heap used for norms |                                |     0.0499268   |     00380859   |     -0.01184 |      MB |  -23.72% |
|                                          Heap used for points |                                |     0           |    0           |      0       |      MB |    0.00% |
|                                   Heap used for stored fields |                                |     0.0424652   |     00344086   |     -0.00806 |      MB |  -18.97% |
|                                                 Segment count |                                |    82           |   66           |    -16       |         |  -19.51% |
|                                   Total Ingest Pipeline count |                                |     0           |    0           |      0       |         |    0.00% |
|                                    Total Ingest Pipeline time |                                |     0           |    0           |      0       |      ms |    0.00% |
|                                  Total Ingest Pipeline failed |                                |     0           |    0           |      0       |         |    0.00% |
|                                                    error rate |                   index-append |     0           |    0           |      0       |       % |    0.00% |
|                                                Min Throughput |                    index-stats |    90.001       |    900126      |      0.01165 |   ops/s |   +0.01% |
|                                               Mean Throughput |                    index-stats |    90.0055      |    900241      |      0.01854 |   ops/s |   +0.02% |
|                                             Median Throughput |                    index-stats |    90.0048      |    900219      |      0.01715 |   ops/s |   +0.02% |
|                                                Max Throughput |                    index-stats |    90.0142      |    900422      |      0.02796 |   ops/s |   +0.03% |
|                                       50th percentile latency |                    index-stats |     5.80729     |     540389     |     -0.40339 |      ms |   -6.95% |
|                                       90th percentile latency |                    index-stats |     6.71673     |     622462     |     -0.49211 |      ms |   -7.33% |
|                                       99th percentile latency |                    index-stats |     7.26607     |     68574      |     -0.40867 |      ms |   -5.62% |
|                                     99.9th percentile latency |                    index-stats |    11.3788      |    111477      |     -0.23112 |      ms |   -2.03% |
|                                      100th percentile latency |                    index-stats |    13.6309      |    140192      |      0.38829 |      ms |   +2.85% |
|                                  50th percentile service time |                    index-stats |     4.61601     |     424021     |     -0.37579 |      ms |   -8.14% |
|                                  90th percentile service time |                    index-stats |     5.40477     |     493823     |     -0.46654 |      ms |   -8.63% |
|                                  99th percentile service time |                    index-stats |     5.85565     |     534821     |     -0.50744 |      ms |   -8.67% |
|                                99.9th percentile service time |                    index-stats |     9.8608      |     774023     |     -2.12057 |      ms |  -21.51% |
|                                 100th percentile service time |                    index-stats |    12.6355      |     944686     |     -3.18866 |      ms |  -25.24% |
|                                                    error rate |                    index-stats |     0           |    0           |      0       |
[..]

-------------------------------
[INFO] SUCCESS (took 0 seconds)
-------------------------------
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://esrally.readthedocs.io/en/stable/track.html">https://esrally.readthedocs.io/en/stable/track.html</a></li>
<li><a href="https://esrally.readthedocs.io/en/stable/tournament.html">https://esrally.readthedocs.io/en/stable/tournament.html</a></li>
<li><a href="https://github.com/elastic/rally/issues/57">https://github.com/elastic/rally/issues/57</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Esrally</tag>
      </tags>
  </entry>
  <entry>
    <title>kmalloc and vmalloc</title>
    <url>/blog/kmalloc-and-vmalloc/</url>
    <content><![CDATA[<h2 id="kmalloc"><a href="#kmalloc" class="headerlink" title="kmalloc"></a>kmalloc</h2><p>The function allocates contiguous region in physical memory. It’s fast and doesn’t clear the allocated memory content.</p>
<p>kmalloc function is defined in <a href="https://elixir.bootlin.com/linux/latest/source/include/linux/slab.h#L538">include&#x2F;linux&#x2F;slab.h</a></p>
<pre><code>/**
 * kmalloc - allocate memory
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate.
 *
 * kmalloc is the normal method of allocating memory
 * for objects smaller than page size in the kernel.
 *
 * The allocated object address is aligned to at least ARCH_KMALLOC_MINALIGN
 * bytes. For @size of power of two bytes, the alignment is also guaranteed
 * to be at least to the size.
 *
 * The @flags argument may be one of the GFP flags defined at
 * include/linux/gfp.h and described at
 * :ref:`Documentation/core-api/mm-api.rst &lt;mm-api-gfp-flags&gt;`
 *
 * The recommended usage of the @flags is described at
 * :ref:`Documentation/core-api/memory-allocation.rst &lt;memory_allocation&gt;`
 *
 * Below is a brief outline of the most useful GFP flags
 *
 * %GFP_KERNEL
 *	Allocate normal kernel ram. May sleep.
 *
 * %GFP_NOWAIT
 *	Allocation will not sleep.
 *
 * %GFP_ATOMIC
 *	Allocation will not sleep.  May use emergency pools.
 *
 * %GFP_HIGHUSER
 *	Allocate memory from high memory on behalf of user.
 *
 * Also it is possible to set different flags by OR&#39;ing
 * in one or more of the following additional @flags:
 *
 * %__GFP_HIGH
 *	This allocation has high priority and may use emergency pools.
 *
 * %__GFP_NOFAIL
 *	Indicate that this allocation is in no way allowed to fail
 *	(think twice before using).
 *
 * %__GFP_NORETRY
 *	If memory is not immediately available,
 *	then give up at once.
 *
 * %__GFP_NOWARN
 *	If allocation fails, don&#39;t issue any warnings.
 *
 * %__GFP_RETRY_MAYFAIL
 *	Try really hard to succeed the allocation but fail
 *	eventually.
 */
static __always_inline void *kmalloc(size_t size, gfp_t flags)&#123;
      if (__builtin_constant_p(size)) &#123;
#ifndef CONFIG_SLOB
        unsigned int index;
#endif
        if (size &gt; KMALLOC_MAX_CACHE_SIZE)
            return kmalloc_large(size, flags);
#ifndef CONFIG_SLOB
        index = kmalloc_index(size);

        if (!index)
            return ZERO_SIZE_PTR;

        return kmem_cache_alloc_trace(
                kmalloc_caches[kmalloc_type(flags)][index],
                flags, size);
#endif
    &#125;
    return __kmalloc(size, flags);
&#125;
</code></pre>
<p>The first argument is the size of the blocks to be allocated. The second argument is the memory allocation type flags. It controls the behavior of kmalloc.</p>
<ul>
<li>GFP_KERNEL is the most commonly used flag. It means the allocation is performed on behalf of a process running in kernel space. kmalloc can put the current process to sleep while waiting for the page allocation if the system is in memory starvation. The free memory can be optained either by flushing buffer to disk or swapping out user process memory.</li>
<li>GFP_ATOMIC is used if the kmalloc call is from outside process context. The kernel can use the reserved free pages to serve the allocation. The allocation can fail if there is no last free page.</li>
</ul>
<h2 id="vmalloc"><a href="#vmalloc" class="headerlink" title="vmalloc"></a>vmalloc</h2><p>vmalloc allocates a contiguous memory region in the virtual address space. The allocated pages in physical memory are not consecutive and each page is retrieved with separate call to alloc_page. Thus, the vmalloc function is less efficient than kmalloc.</p>
<p>vmalloc function is defined in <a href="https://elixir.bootlin.com/linux/latest/source/mm/vmalloc.c#L2650">mm&#x2F;vmalloc.c</a></p>
<pre><code>/**
 * vmalloc - allocate virtually contiguous memory
 * @size:    allocation size
 *
 * Allocate enough pages to cover @size from the page level
 * allocator and map them into contiguous kernel virtual space.
 *
 * For tight control over page level allocator and protection flags
 * use __vmalloc() instead.
 *
 * Return: pointer to the allocated memory or %NULL on error
 */
void *vmalloc(unsigned long size)
&#123;
    return __vmalloc_node(size, 1, GFP_KERNEL, NUMA_NO_NODE,
                __builtin_return_address(0));
&#125;
</code></pre>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://www.oreilly.com/library/view/linux-device-drivers/0596005903/ch08.html">https://www.oreilly.com/library/view/linux-device-drivers/0596005903/ch08.html</a></li>
<li><a href="https://elixir.bootlin.com/linux/latest/source/include/linux/slab.h#L538">https://elixir.bootlin.com/linux/latest/source/include/linux/slab.h#L538</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Kernel</tag>
      </tags>
  </entry>
  <entry>
    <title>Kubernetes and Gluster performance</title>
    <url>/blog/kubernetes-and-gluster-performance/</url>
    <content><![CDATA[<h2 id="Kubernetes-and-Gluster-Intro"><a href="#Kubernetes-and-Gluster-Intro" class="headerlink" title="Kubernetes and Gluster Intro"></a>Kubernetes and Gluster Intro</h2><p><a href="https://kubernetes.io/">Kubernetes</a>, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.</p>
<p><a href="https://docs.gluster.org/en/latest/Administrator-Guide/GlusterFS-Introduction/">Gluster</a> is a scalable, distributed file system that aggregates disk storage resources from multiple servers into a single global namespace.</p>
<h2 id="Gluster-performance-study"><a href="#Gluster-performance-study" class="headerlink" title="Gluster performance study"></a>Gluster performance study</h2><p>In this article, we will discuss the Gluster performance in a Docker container environment which is built on Kubernetes.</p>
<h3 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h3><p>We use three Redhat Linux servers to form a Kubernetes cluster in this study. A Kubernetes cluster that handles production traffic should have a minimum of three nodes.</p>
<p>We have three Docker containers(application instances) provisioned within the Kubernetes cluster. Each container instance has its own Gluster storage pool.</p>
<pre><code>[node1:root]~&gt; kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   6d14h
 
[node1:root]~&gt; kubectl get nodes
NAME    STATUS   ROLES    AGE     VERSION
node1   Ready    master   6d14h   v1.19.3
node2   Ready    master   6d14h   v1.19.3
node3   Ready    master   6d14h   v1.19.3
 
[node1:root]~&gt; kubectl get pods --namespace ns-1
NAME         READY   STATUS    RESTARTS   AGE
container1   1/1     Running   0          6d14h
container2   1/1     Running   0          6d14h
container3   1/1     Running   0          6d14h
[...]
 
[node1:root]~&gt; gluster pool list
UUID                    Hostname    State
45d8ec04-4e7a-4442-bbb4-557256b864d6    10.10.1.3 Connected
875be270-ae69-45ea-b38e-2768b7c6ce05    10.10.1.4 Connected
f2696790-b305-4099-8dba-b31d23b0beac    localhost Connected
</code></pre>
<h3 id="Workload-and-performance"><a href="#Workload-and-performance" class="headerlink" title="Workload and performance"></a>Workload and performance</h3><p>We keep increasing the number of workload processes across three container instances and measure the throughput in MB&#x2F;s. For each workload process, it ingests data from multiple clients through 10GbE bonding network and writes the data to the mounted Gluster filesystem.</p>
<p>There are two kinds of workloads. One is very I&#x2F;O intensive and the other is CPU bound.</p>
<p><img src="/images/gluster-perf.png"></p>
<h3 id="Observation"><a href="#Observation" class="headerlink" title="Observation"></a>Observation</h3><ul>
<li>For the CPU bound workload, the performance scales very well.</li>
<li>For the I&#x2F;O bound workload, the performance does not scale when the number of workload processes increases.</li>
</ul>
<h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><p>As we increase the number of workload processes, the workload can be distributed evenly across three instances(on three nodes). Thus, the CPU bandwidth from three nodes are available for the application computing.</p>
<p>Although the storage from three nodes are usable for three container instances, with the default disk allocation for Gluster filesystem, the I&#x2F;O performance may not be optimal. It depends on how the disks are allocated to each Gluster filesystem bricks and how the bricks are assigned to the Gluster filesystems. Also, writing to remote disk would have worse performance due to network latency.</p>
<p>The following is the disk mapping to bricks which are used by one of the three Gluster filesystems. It shows four bricks are created on the same disk <em>&#x2F;dev&#x2F;sde</em> on the node 10.10.1.4. Obviously, the I&#x2F;O performance could be degraded if multiple processes write on it.</p>
<pre><code>brickSize(GB)	device	node
3200	/dev/sdd	10.10.1.2
3200	/dev/sde	10.10.1.2
3200	/dev/sdd	10.10.1.2
3200	/dev/sdd	10.10.1.2
3200	/dev/sdc	10.10.1.3
3200	/dev/sdc	10.10.1.3
3200	/dev/sdb	10.10.1.3
3200	/dev/sdb	10.10.1.3
3200	/dev/sde	10.10.1.4
3200	/dev/sde	10.10.1.4
3200	/dev/sde	10.10.1.4
3200	/dev/sde	10.10.1.4
</code></pre>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>From this case study, we had basic understanding on how the Gluster file system works with storage across multiple nodes. The default Gluster filesystem layout is not fit for all use cases. A custom storage layout would be needed to meet the performance requirement.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>GlusterFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Lambda architecture in big data system</title>
    <url>/blog/lambda-architecture-in-big-data-system/</url>
    <content><![CDATA[<h2 id="What-is-Lambda-Architecture"><a href="#What-is-Lambda-Architecture" class="headerlink" title="What is Lambda Architecture"></a>What is Lambda Architecture</h2><p>When working with very large data sets, it can take a long time to run the sort of queries that clients need. These queries can’t be performed in real time, and often require algorithms such as MapReduce that operate in parallel across the entire data set. The results are then stored separately from the raw data and used for querying.</p>
<p>One drawback to this approach is that it introduces latency — if processing takes a few hours, a query may return results that are several hours old. Ideally, you would like to get some results in real time (perhaps with some loss of accuracy), and combine these results with the results from the batch analytics.</p>
<p>The lambda architecture, first proposed by Nathan Marz, addresses this problem by creating two paths for data flow. All data coming into the system goes through these two paths:</p>
<ul>
<li>A batch layer (cold path) stores all of the incoming data in its raw form and performs batch processing on the data. The result of this processing is stored as a batch view.</li>
<li>A speed layer (hot path) analyzes data in real time. This layer is designed for low latency, at the expense of accuracy.</li>
</ul>
<p>The batch layer feeds into a serving layer that indexes the batch view for efficient querying. The speed layer updates the serving layer with incremental updates based on the most recent data.</p>
<p><img src="/images/lambda.png" alt="Image"></p>
<p><a href="https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/">Source</a></p>
<p>Data that flows into the hot path is constrained by latency requirements imposed by the speed layer, so that it can be processed as quickly as possible. Often, this requires a tradeoff of some level of accuracy in favor of data that is ready as quickly as possible. For example, consider an IoT scenario where a large number of temperature sensors are sending telemetry data. The speed layer may be used to process a sliding time window of the incoming data.</p>
<p>Data flowing into the cold path, on the other hand, is not subject to the same low latency requirements. This allows for high accuracy computation across large data sets, which can be very time intensive.</p>
<p>Eventually, the hot and cold paths converge at the analytics client application. If the client needs to display timely, yet potentially less accurate data in real time, it will acquire its result from the hot path. Otherwise, it will select results from the cold path to display less timely but more accurate data. In other words, the hot path has data for a relatively small window of time, after which the results can be updated with more accurate data from the cold path.</p>
<p>The raw data stored at the batch layer is immutable. Incoming data is always appended to the existing data, and the previous data is never overwritten. Any changes to the value of a particular datum are stored as a new timestamped event record. This allows for recomputation at any point in time across the history of the data collected. The ability to recompute the batch view from the original raw data is important, because it allows for new views to be created as the system evolves.</p>
<h2 id="Batch-Layer"><a href="#Batch-Layer" class="headerlink" title="Batch Layer"></a>Batch Layer</h2><p>New data comes continuously, as a feed to the data system. It gets fed to the batch layer and the speed layer simultaneously. It looks at all the data at once and eventually corrects the data in the stream layer.  Here we can find lots of ETL and a traditional data warehouse. This layer is built using a predefined schedule, usually once or twice a day. The batch layer has two very important functions:</p>
<ul>
<li>To manage the master dataset</li>
<li>To pre-compute the batch views.</li>
</ul>
<h2 id="Serving-Layer"><a href="#Serving-Layer" class="headerlink" title="Serving Layer"></a>Serving Layer</h2><p>The outputs from the batch layer in the form of batch views and those coming from the speed layer in the form of near real-time views get forwarded to the serving.  This layer indexes the batch views so that they can be queried in low-latency on an ad-hoc basis.</p>
<h2 id="Speed-Layer-Stream-Layer"><a href="#Speed-Layer-Stream-Layer" class="headerlink" title="Speed Layer (Stream Layer)"></a>Speed Layer (Stream Layer)</h2><p>This layer handles the data that are not already delivered in the batch view due to the latency of the batch layer. In addition, it only deals with recent data in order to provide a complete view of the data to the user by creating real-time views.</p>
<h2 id="Benefits-of-lambda-architectures"><a href="#Benefits-of-lambda-architectures" class="headerlink" title="Benefits of lambda architectures"></a>Benefits of lambda architectures</h2><p>Here are the main benefits of lambda architectures:</p>
<ul>
<li>No Server Management – you do not have to install, maintain, or administer any software.</li>
<li>Flexible Scaling – your application can be either automatically scaled or scaled by the adjustment of its capacity</li>
<li>Automated High Availability – refers to the fact that serverless applications have already built-in availability and faults tolerance. It represents a guarantee that all requests will get a response about whether they were successful or not.</li>
<li>Business Agility – React in real-time to changing business&#x2F;market scenarios</li>
</ul>
<h2 id="Challenges-with-lambda-architectures"><a href="#Challenges-with-lambda-architectures" class="headerlink" title="Challenges with lambda architectures"></a>Challenges with lambda architectures</h2><ul>
<li>Complexity – lambda architectures can be highly complex. Administrators must typically maintain two separate code bases for batch and streaming layers, which can make debugging difficult.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>&lt;BigData - Priciples and best practices of scalable real-time data systems&gt;</li>
<li><a href="https://databricks.com/glossary/lambda-architecture">https://databricks.com/glossary/lambda-architecture</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/">https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title>Lambda function in Python</title>
    <url>/blog/lambda-function-in-python/</url>
    <content><![CDATA[<p>A lambda function is a small anonymous function. A lambda function can take any number of arguments, but can only have one expression.</p>
<span id="more"></span>
<p>Example 1:</p>
<pre><code># Add 10 to argument a, and return the result
x = lambda a: a + 10
print(x(5))

# Output:
# 15
</code></pre>
<p>Example 2:</p>
<pre><code># Multiply argument a and b and return the result
x = lambda a, b: a * b
print(x(5, 6))

# Output:
# 30
</code></pre>
<p>Example 3:</p>
<pre><code># Reuse a lambda function with unknown parameter
def multipiler(n):
    return lambda a: a * n


m2 = multipiler(2)
print(m2(1000))
m3 = multipiler(3)
print(m3(1000))
m10 = multipiler(10)
print(m10(1000))

# Output:
# 2000
# 3000
# 10000
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Latency numbers</title>
    <url>/blog/latency-numbers/</url>
    <content><![CDATA[<h2 id="Latency-Comparison-Numbers-2012"><a href="#Latency-Comparison-Numbers-2012" class="headerlink" title="Latency Comparison Numbers (~2012)"></a>Latency Comparison Numbers (~2012)</h2><p><img src="/images/latency-numbers.png" alt="Image"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>Disk is much slower than the memory</li>
<li>Avoid the disk seek if possible</li>
<li>Compressing data is worth to consider before sending over network</li>
<li>It takes time to send data between data centers if they are in different regions</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://computers-are-fast.github.io/">https://computers-are-fast.github.io/</a></li>
<li><a href="http://research.google.com/people/jeff/">http://research.google.com/people/jeff/</a> by Jeff Dean</li>
<li><a href="http://norvig.com/21-days.html#answers">http://norvig.com/21-days.html#answers</a> by Peter Norvig</li>
<li>Humanized comparison: <a href="https://gist.github.com/hellerbarde/2843375">https://gist.github.com/hellerbarde/2843375</a></li>
<li>Visual comparison chart: <a href="http://i.imgur.com/k0t1e.png">http://i.imgur.com/k0t1e.png</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
  </entry>
  <entry>
    <title>Level 0 and Level 1 Incremental Backups</title>
    <url>/blog/level-0-and-level-1-incremental-backups/</url>
    <content><![CDATA[<p>Incremental backups can be either level 0 or level 1. A level 0 incremental backup, which is the base for subsequent incremental backups, copies all blocks containing data, backing the datafile up into a backup set just as a full backup would. The only difference between a level 0 incremental backup and a full backup is that a full backup is never included in an incremental strategy.</p>
<p>A level 1 incremental backup can be either of the following types:</p>
<ul>
<li>A differential backup, which backs up all blocks changed after the most recent incremental backup at level 1 or 0</li>
<li>A cumulative backup, which backs up all blocks changed after the most recent incremental backup at level 0</li>
</ul>
<p>Incremental backups are differential by default.</p>
<p>The size of the backup file depends solely upon the number of blocks modified and the incremental backup level.</p>
<p><img src="/images/oracle-incremental-0.png" alt="Image"></p>
<p>In the example shown above, the following occurs:</p>
<ul>
<li>Sunday</li>
</ul>
<p>An incremental level 0 backup backs up all blocks that have ever been in use in this database.</p>
<ul>
<li>Monday - Saturday</li>
</ul>
<p>On each day from Monday through Saturday, a differential incremental level 1 backup backs up all blocks that have changed since the most recent incremental backup at level 1 or 0. So, the Monday backup copies blocks changed since Sunday level 0 backup, the Tuesday backup copies blocks changed since the Monday level 1 backup, and so forth.</p>
<ul>
<li>The cycle is repeated for the next week.</li>
</ul>
<p><img src="/images/oracle-incremental-1.png" alt="Image"></p>
<p>In the example shown above, the following occurs:</p>
<ul>
<li>Sunday</li>
</ul>
<p>An incremental level 0 backup backs up all blocks that have ever been in use in this database.</p>
<ul>
<li>Monday - Saturday</li>
</ul>
<p>A cumulative incremental level 1 backup copies all blocks changed since the most recent level 0 backup. Because the most recent level 0 backup was created on Sunday, the level 1 backup on each day Monday through Saturday backs up all blocks changed since the Sunday backup.</p>
<ul>
<li>The cycle is repeated for the next week.</li>
</ul>
<p><strong>Incremental Backup Algorithm</strong></p>
<p>Each data block in a datafile contains a system change number (SCN), which is the SCN at which the most recent change was made to the block. During an incremental backup, RMAN reads the SCN of each data block in the input file and compares it to the checkpoint SCN of the parent incremental backup. If the SCN in the input data block is greater than or equal to the checkpoint SCN of the parent, then RMAN copies the block.</p>
<p>Note that if you enable the block change tracking feature, RMAN can refer to the change tracking file to identify changed blocks in datafiles without scanning the full contents of the datafile. Once enabled, block change tracking does not alter how you take or use incremental backups, other than offering increased performance.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>Backup Recovery</tag>
      </tags>
  </entry>
  <entry>
    <title>libaio init failed due to resource temporarily unavailable</title>
    <url>/blog/libaio-init-failed-due-to-resource-temporarily-unavailable/</url>
    <content><![CDATA[<h2 id="Issue-Description"><a href="#Issue-Description" class="headerlink" title="Issue Description"></a>Issue Description</h2><pre><code>$ sudo fio --blocksize=64k --directory=/mnt/bench1 --filename=testfile --ioengine=libaio --readwrite=randread --size=10G --name=test --numjobs=512 --group_reporting --direct=1 --iodepth=128 --randrepeat=1 --disable_lat=0 --gtod_reduce=0

test: (g=0): rw=randread, bs=(R) 64.0KiB-64.0KiB, (W) 64.0KiB-64.0KiB, (T) 64.0KiB-64.0KiB, ioengine=libaio, iodepth=128
...
fio-3.7
Starting 512 processes
fio: pid=38868, err=11/file:engines/libaio.c:354, func=io_queue_init, error=Resource temporarily unavailable
...
fio: check /proc/sys/fs/aio-max-nr
fio: io engine libaio init failed. Perhaps try reducing io depth?
</code></pre>
<h2 id="Resolution"><a href="#Resolution" class="headerlink" title="Resolution"></a>Resolution</h2><p>The Linux kernel provides the Asynchronous non-blocking I&#x2F;O (AIO) feature that allows a process to initiate multiple I&#x2F;O operations simultaneously without having to wait for any of them to complete. This helps boost performance for applications that are able to overlap processing and I&#x2F;O.</p>
<p>The performance can be tuned using the &#x2F;proc&#x2F;sys&#x2F;fs&#x2F;aio-max-nr virtual file in the proc file system. The aio-max-nr parameter determines the maximum number of allowable concurrent requests.</p>
<p>To set the aio-max-nr value, add the following line to the &#x2F;etc&#x2F;sysctl.d&#x2F;99-sysctl.conf file:</p>
<pre><code>$  cat  /proc/sys/fs/aio-max-nr
65536
$ echo &quot;fs.aio-max-nr = 1048576&quot; &gt;&gt; /etc/sysctl.d/99-sysctl.conf
</code></pre>
<p>To activate the new setting, run the following command:</p>
<pre><code>$ sysctl -p /etc/sysctl.d/99-sysctl.conf
fs.aio-max-nr = 1048576
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://sort.veritas.com/public/documents/HSO/2.0/linux/productguides/html/hfo_admin_ubuntu/ch04s03.htm">https://sort.veritas.com/public/documents/HSO/2.0/linux/productguides/html/hfo_admin_ubuntu&#x2F;ch04s03.htm</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Fio</tag>
      </tags>
  </entry>
  <entry>
    <title>Limit usable memory size in Linux grub</title>
    <url>/blog/limit-usable-memory-size-in-linux-grub/</url>
    <content><![CDATA[<p>In one case, I need to limit the usable memory size to 128GB from a system which has 1TB memory.</p>
<pre><code>$ cat /proc/meminfo | grep MemTotal
MemTotal:       1056493068 kB
</code></pre>
<h2 id="Edit-etc-default-grub-file"><a href="#Edit-etc-default-grub-file" class="headerlink" title="Edit &#x2F;etc&#x2F;default&#x2F;grub file"></a>Edit &#x2F;etc&#x2F;default&#x2F;grub file</h2><p>Add “mem&#x3D;128G” to the end of line “GRUB_CMDLINE_LINUX”.</p>
<pre><code>$ cat /etc/default/grub
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR=&quot;$(sed &#39;s, release .*$,,g&#39; /etc/system-release)&quot;
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT=&quot;console&quot;
GRUB_CMDLINE_LINUX=&quot;crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet mem=128G&quot;
GRUB_DISABLE_RECOVERY=&quot;true&quot;
</code></pre>
<h2 id="Generate-new-grub-cfg-file"><a href="#Generate-new-grub-cfg-file" class="headerlink" title="Generate new grub.cfg file"></a>Generate new grub.cfg file</h2><pre><code>$ grub2-mkconfig -o /boot/grub2/grub.cfg
$ reboot
</code></pre>
<h2 id="Verify-the-usable-memory"><a href="#Verify-the-usable-memory" class="headerlink" title="Verify the usable memory"></a>Verify the usable memory</h2><pre><code>$ cat /proc/cmdline
BOOT_IMAGE=/vmlinuz-5.7.12-1.el7.elrepo.x86_64 root=/dev/mapper/centos-root ro crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet LANG=en_US.UTF-8 mem=128G

$ cat /proc/meminfo | grep MemTotal
MemTotal:       129486812 kB
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.thegeekdiary.com/centos-rhel-7-how-to-modify-the-kernel-command-line/">CentOS &#x2F; RHEL 7 : How to modify the kernel command line</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Grub</tag>
      </tags>
  </entry>
  <entry>
    <title>Linked List implementation in Python</title>
    <url>/blog/linked-list-implementation-in-python/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>A linked list is a data structure which represents a sequence of nodes. In a singly linked list, each node points to the next node. In a doubly linked list, each node points to both the next and previous node.</p>
<p>To find nth element in the linked list, you have to iterate through n nodes which results in linear time complexity O(n). This is less efficient than array which has constant time complexity O(1) to access an element.</p>
<p>However, the advantage to use linked list is for insert and delete operation since it can be achieved in the constant time without shifting nodes around.</p>
<p>The following are the key methods to be implemented:</p>
<ul>
<li>insert(): add an item to the linked list</li>
<li>remove(): remove a specified item from the linked list</li>
<li>insertAt(index): insert an item at a given index</li>
<li>deleteAt(index): delete an item at a given index</li>
<li>find(): search an item in the linked list</li>
<li>is_empty(): return if the linked list is empty or not</li>
<li>count(): return the number of items in the linked list</li>
</ul>
<h2 id="Construct-a-node-class"><a href="#Construct-a-node-class" class="headerlink" title="Construct a node class"></a>Construct a node class</h2><p>Before we implement a singly linked list, we need to construct a node class. The linked list would contain one or more nodes. Each node has a value and a pointer linked to the next node.</p>
<pre><code>class Node:
    def __init__(self):
        self.val = None
        self.next = None

    def set_val(self, val):
        self.val = val

    def get_val(self):
        return self.val

    def set_next(self, next):
        self.next = next

    def get_next(self):
        return self.next
</code></pre>
<h2 id="Build-a-singly-Linked-List"><a href="#Build-a-singly-Linked-List" class="headerlink" title="Build a singly Linked List"></a>Build a singly Linked List</h2><p>The first function in the linked list class is the constructor, which will be called whenever a linked list is initialized.</p>
<p>It can include three attributes:</p>
<ul>
<li>head: the first node in the linked list</li>
<li>tail: the last node in the linked list</li>
<li>count: the number of nodes in the linked list</li>
</ul>
<p>Then we can build the other required functions according to the node pointer manipulations.</p>
<pre><code>class LinkedList:
    def __init__(self):
        self.head = None
        self.tail = None
        self.count = 0

    def count_nodes(self):
        return self.count

    # insert at the tail of list
    def insert(self, val):
        new_node = Node()
        new_node.set_val(val)

        if self.head == None:
            self.head = new_node
            self.tail = new_node
        else:
            self.tail.next = new_node
            self.tail = new_node

        self.count += 1

    # insert at the given index
    def insertAt(self, val, index):
        new_node = Node()
        new_node.set_val(val)

        current_idx = 0
        current_node = self.head
        previous_node = current_node

        while current_idx &lt; index and current_node.next != None:
            previous_node = current_node
            current_node = current_node.next
            current_idx += 1

        if current_idx == index:
            # found the index
            new_node.next = current_node
            previous_node.next = new_node

            # increase counter
            self.count += 1
        else:
            print(&quot;the specified index &#123;&#125; is invalid&quot;.format(index))

    # delete the specified node
    def deleteAt(self, index):
        current_idx = 0
        current_node = self.head
        previous_node = current_node

        while True:
            if current_idx &lt; index:
                if current_node.next != None:
                    # move on to the next node
                    previous_node = current_node
                    current_node = current_node.next
                    current_idx += 1
                else:
                    print(&quot;invalid index &#123;&#125;&quot;.format(index))
                    break
            else:
                # found the index, delete it
                if current_node.next == None:
                    # this is tail
                    previous_node.next = None
                    self.tail = previous_node
                else:
                    # this is not tail
                    previous_node.next = current_node.next

                # decrease the counter
                self.count -= 1

                break

    def find(self, index):
        current_node = self.head
        current_idx = 0

        while current_node.next != None:
            if current_idx == index:
                return current_node.get_val()
            else:
                current_node = current_node.next

        print(&quot;invalid index &#123;&#125;&quot;.format(index))

    def print_list(self):
        current_node = self.head
        current_idx = 0

        while current_node.next != None:
            print(current_node.get_val())
            current_node = current_node.next

        # print the last node
        print(current_node.get_val())
        print(&quot;count: &#123;&#125;&quot;.format(self.count_nodes()))
        print()
</code></pre>
<h2 id="Test-the-Linked-List-class"><a href="#Test-the-Linked-List-class" class="headerlink" title="Test the Linked List class"></a>Test the Linked List class</h2><pre><code>ll = LinkedList()
ll.insert(1)
ll.insert(2)
ll.insert(5)
ll.print_list()

ll.insertAt(4, 2)
ll.print_list()

ll.insertAt(3, 3)
ll.print_list()

ll.deleteAt(3)
ll.print_list()
</code></pre>
<p>Output:</p>
<pre><code>1
2
5
count: 3

1
2
4
5
count: 4

1
2
4
3
5
count: 5

1
2
4
5
count: 4
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
        <tag>Linked List</tag>
      </tags>
  </entry>
  <entry>
    <title>Linked list in Python</title>
    <url>/blog/linked-list-in-python/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>A <a href="https://www.flamingbytes.com/blog/linked-list-implementation-in-python">linked list</a> is a data structure which represents a sequence of nodes. In a singly linked list, each node points to the next node. In a doubly linked list, each node points to both the next and previous node.<span id="more"></span></p>
<h2 id="Leetcode-25-Reverse-Nodes-in-k-Group"><a href="#Leetcode-25-Reverse-Nodes-in-k-Group" class="headerlink" title="[Leetcode 25] Reverse Nodes in k-Group"></a>[Leetcode 25] Reverse Nodes in k-Group</h2><p>Given the head of a linked list, reverse the nodes of the list k at a time, and return the modified list.</p>
<p>k is a positive integer and is less than or equal to the length of the linked list. If the number of nodes is not a multiple of k then left-out nodes, in the end, should remain as it is.</p>
<p>You may not alter the values in the list’s nodes, only nodes themselves may be changed.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [1,2,3,4,5], k = 2</span><br><span class="line">Output: [2,1,4,3,5]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [1,2,3,4,5], k = 3</span><br><span class="line">Output: [3,2,1,4,5]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>The number of nodes in the list is n.</li>
<li>1 &lt;&#x3D; k &lt;&#x3D; n &lt;&#x3D; 5000</li>
<li>0 &lt;&#x3D; Node.val &lt;&#x3D; 1000</li>
</ul>
<p>Follow-up: Can you solve the problem in O(1) extra memory space?</p>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseKGroup</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode], k: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        <span class="comment"># check if reverse is needed</span></span><br><span class="line">        curr = head</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> curr:</span><br><span class="line">                <span class="keyword">return</span> head <span class="comment"># no need to reverse, thus return the original head</span></span><br><span class="line">            curr = curr.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Reverse the sub-list with k nodes</span></span><br><span class="line">        <span class="comment"># e.g.</span></span><br><span class="line">        <span class="comment"># None  1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6, k = 3</span></span><br><span class="line">        <span class="comment"># ^     ^</span></span><br><span class="line">        <span class="comment"># prev  head(curr) </span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># After reverse the first 3 nodes,</span></span><br><span class="line">        <span class="comment"># 3 -&gt; 2 -&gt; 1 -&gt; 4 -&gt; 5 -&gt; 6</span></span><br><span class="line">        <span class="comment"># ^         ^    ^</span></span><br><span class="line">        <span class="comment"># prev      head curr</span></span><br><span class="line">        prev, curr = <span class="literal">None</span>, head</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            <span class="built_in">next</span> = curr.<span class="built_in">next</span> <span class="comment"># save the next before we break the pointer</span></span><br><span class="line">            curr.<span class="built_in">next</span> = prev <span class="comment"># point to previous node(reverse)</span></span><br><span class="line">            prev = curr <span class="comment"># shift previous pointer</span></span><br><span class="line">            curr = <span class="built_in">next</span> <span class="comment"># shift current pointer </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># after reverse, the curr pointer points to the &quot;head&quot; of next k nodes, </span></span><br><span class="line">        <span class="comment"># and the previous pointer points to the new &quot;head&quot; of the reversed k nodes.</span></span><br><span class="line">        head.<span class="built_in">next</span> = self.reverseKGroup(curr, k) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># the previous pointer points to the head of reversed list</span></span><br><span class="line">        <span class="keyword">return</span> prev</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-82-Remove-Duplicates-from-Sorted-List-II"><a href="#Leetcode-82-Remove-Duplicates-from-Sorted-List-II" class="headerlink" title="[Leetcode 82] Remove Duplicates from Sorted List II"></a>[Leetcode 82] Remove Duplicates from Sorted List II</h2><p>Given the head of a sorted linked list, delete all nodes that have duplicate numbers, leaving only distinct numbers from the original list. Return the linked list sorted as well.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [1,2,3,3,4,4,5]</span><br><span class="line">Output: [1,2,5]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [1,1,1,2,3]</span><br><span class="line">Output: [2,3]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>The number of nodes in the list is in the range [0, 300].</li>
<li>-100 &lt;&#x3D; Node.val &lt;&#x3D; 100</li>
<li>The list is guaranteed to be sorted in ascending order.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deleteDuplicates</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head <span class="keyword">or</span> <span class="keyword">not</span> head.<span class="built_in">next</span>:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line"></span><br><span class="line">        dummy = ListNode()</span><br><span class="line">        dummy.<span class="built_in">next</span> = head</span><br><span class="line">        prev = dummy</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> head <span class="keyword">and</span> head.<span class="built_in">next</span>:</span><br><span class="line">            <span class="keyword">if</span> head.val == head.<span class="built_in">next</span>.val:</span><br><span class="line">                <span class="keyword">while</span> head <span class="keyword">and</span> head.<span class="built_in">next</span> <span class="keyword">and</span> head.val == head.<span class="built_in">next</span>.val:</span><br><span class="line">                    head = head.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">                head = head.<span class="built_in">next</span> <span class="comment"># move to next node which has different value</span></span><br><span class="line">                prev.<span class="built_in">next</span> = head <span class="comment"># point previous to the new node</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prev = prev.<span class="built_in">next</span></span><br><span class="line">                head = head.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-92-Reverse-Linked-List-II"><a href="#Leetcode-92-Reverse-Linked-List-II" class="headerlink" title="[Leetcode 92] Reverse Linked List II"></a>[Leetcode 92] Reverse Linked List II</h2><p>Given the head of a singly linked list and two integers left and right where left &lt;&#x3D; right, reverse the nodes of the list from position left to position right, and return the reversed list.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [1,2,3,4,5], left = 2, right = 4</span><br><span class="line">Output: [1,4,3,2,5]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [5], left = 1, right = 1</span><br><span class="line">Output: [5]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>The number of nodes in the list is n.</li>
<li>1 &lt;&#x3D; n &lt;&#x3D; 500</li>
<li>-500 &lt;&#x3D; Node.val &lt;&#x3D; 500</li>
<li>1 &lt;&#x3D; left &lt;&#x3D; right &lt;&#x3D; n</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseBetween</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode], left: <span class="built_in">int</span>, right: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        dummy = ListNode(<span class="number">0</span>)</span><br><span class="line">        dummy.<span class="built_in">next</span> = head</span><br><span class="line">        prev = dummy</span><br><span class="line"></span><br><span class="line">        mthNode = head</span><br><span class="line"></span><br><span class="line">        i = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; left:</span><br><span class="line">            prev = mthNode</span><br><span class="line">            mthNode = mthNode.<span class="built_in">next</span></span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        nthNode = mthNode</span><br><span class="line">        <span class="keyword">while</span> i &lt; right:</span><br><span class="line">            nthNode = nthNode.<span class="built_in">next</span></span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># disconnect prev and first node</span></span><br><span class="line">        prev.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># disconnect last node and its next</span></span><br><span class="line">        second = nthNode.<span class="built_in">next</span></span><br><span class="line">        nthNode.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># reverse list between first and last node</span></span><br><span class="line">        curr = mthNode</span><br><span class="line">        tempHead = curr</span><br><span class="line">        <span class="keyword">while</span> curr.<span class="built_in">next</span>:</span><br><span class="line">            temp = curr.<span class="built_in">next</span></span><br><span class="line">            curr.<span class="built_in">next</span> = curr.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">            temp.<span class="built_in">next</span> = tempHead</span><br><span class="line">            tempHead = temp</span><br><span class="line"></span><br><span class="line">        <span class="comment"># connect prev and new head of reversed list</span></span><br><span class="line">        prev.<span class="built_in">next</span> = tempHead</span><br><span class="line"></span><br><span class="line">        <span class="comment"># connect the new last node and second half list</span></span><br><span class="line">        curr.<span class="built_in">next</span> = second</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-138-Copy-List-with-Random-Pointer"><a href="#Leetcode-138-Copy-List-with-Random-Pointer" class="headerlink" title="[Leetcode 138] Copy List with Random Pointer"></a>[Leetcode 138] Copy List with Random Pointer</h2><p>A linked list of length n is given such that each node contains an additional random pointer, which could point to any node in the list, or null.</p>
<p>Construct a deep copy of the list. The deep copy should consist of exactly n brand new nodes, where each new node has its value set to the value of its corresponding original node. Both the next and random pointer of the new nodes should point to new nodes in the copied list such that the pointers in the original list and copied list represent the same list state. None of the pointers in the new list should point to nodes in the original list.</p>
<p>For example, if there are two nodes X and Y in the original list, where X.random –&gt; Y, then for the corresponding two nodes x and y in the copied list, x.random –&gt; y.</p>
<p>Return the head of the copied linked list.</p>
<p>The linked list is represented in the input&#x2F;output as a list of n nodes. Each node is represented as a pair of [val, random_index] where:</p>
<ul>
<li>val: an integer representing Node.val</li>
<li>random_index: the index of the node (range from 0 to n-1) that the random pointer points to, or null if it does not point to any node.</li>
</ul>
<p>Your code will only be given the head of the original linked list. </p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [[7,null],[13,0],[11,4],[10,2],[1,0]]</span><br><span class="line">Output: [[7,null],[13,0],[11,4],[10,2],[1,0]]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [[1,1],[2,1]]</span><br><span class="line">Output: [[1,1],[2,1]]</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [[3,null],[3,0],[3,null]]</span><br><span class="line">Output: [[3,null],[3,0],[3,null]]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>0 &lt;&#x3D; n &lt;&#x3D; 1000</li>
<li>-10^4 &lt;&#x3D; Node.val &lt;&#x3D; 10^4</li>
<li>Node.random is null or is pointing to some node in the linked list.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># Definition for a Node.</span></span><br><span class="line"><span class="string">class Node:</span></span><br><span class="line"><span class="string">    def __init__(self, x: int, next: &#x27;Node&#x27; = None, random: &#x27;Node&#x27; = None):</span></span><br><span class="line"><span class="string">        self.val = int(x)</span></span><br><span class="line"><span class="string">        self.next = next</span></span><br><span class="line"><span class="string">        self.random = random</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">copyRandomList</span>(<span class="params">self, head: <span class="string">&#x27;Optional[Node]&#x27;</span></span>) -&gt; <span class="string">&#x27;Optional[Node]&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        oldToNew = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># copy a new node for each original node</span></span><br><span class="line">        curr = head</span><br><span class="line">        <span class="keyword">while</span> curr:</span><br><span class="line">            oldToNew[curr] = Node(curr.val)</span><br><span class="line">            curr = curr.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        curr = head</span><br><span class="line">        <span class="keyword">while</span> curr:</span><br><span class="line">            <span class="comment"># get() returns None if the key doesn&#x27;t exist</span></span><br><span class="line">            oldToNew[curr].<span class="built_in">next</span> = oldToNew.get(curr.<span class="built_in">next</span>) </span><br><span class="line">            oldToNew[curr].random = oldToNew.get(curr.random)</span><br><span class="line">            curr = curr.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> oldToNew[head]</span><br></pre></td></tr></table></figure>

<h2 id="Leetcoe-146-LRU-Cache"><a href="#Leetcoe-146-LRU-Cache" class="headerlink" title="[Leetcoe 146] LRU Cache"></a>[Leetcoe 146] LRU Cache</h2><p>Design a data structure that follows the constraints of a Least Recently Used (LRU) cache.</p>
<p>Implement the LRUCache class:</p>
<ul>
<li>LRUCache(int capacity) Initialize the LRU cache with positive size capacity.</li>
<li>int get(int key) Return the value of the key if the key exists, otherwise return -1.</li>
<li>void put(int key, int value) Update the value of the key if the key exists. Otherwise, add the key-value pair to the cache. If the number of keys exceeds the capacity from this operation, evict the least recently used key.</li>
</ul>
<p>The functions get and put must each run in O(1) average time complexity.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input</span><br><span class="line">[&quot;LRUCache&quot;, &quot;put&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;get&quot;, &quot;get&quot;]</span><br><span class="line">[[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]</span><br><span class="line">Output</span><br><span class="line">[null, null, null, 1, null, -1, null, -1, 3, 4]</span><br><span class="line"></span><br><span class="line">Explanation</span><br><span class="line">LRUCache lRUCache = new LRUCache(2);</span><br><span class="line">lRUCache.put(1, 1); // cache is &#123;1=1&#125;</span><br><span class="line">lRUCache.put(2, 2); // cache is &#123;1=1, 2=2&#125;</span><br><span class="line">lRUCache.get(1);    // return 1</span><br><span class="line">lRUCache.put(3, 3); // LRU key was 2, evicts key 2, cache is &#123;1=1, 3=3&#125;</span><br><span class="line">lRUCache.get(2);    // returns -1 (not found)</span><br><span class="line">lRUCache.put(4, 4); // LRU key was 1, evicts key 1, cache is &#123;4=4, 3=3&#125;</span><br><span class="line">lRUCache.get(1);    // return -1 (not found)</span><br><span class="line">lRUCache.get(3);    // return 3</span><br><span class="line">lRUCache.get(4);    // return 4</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; capacity &lt;&#x3D; 3000</li>
<li>0 &lt;&#x3D; key &lt;&#x3D; 10^4</li>
<li>0 &lt;&#x3D; value &lt;&#x3D; 10^5</li>
<li>At most 2 * 10^5 calls will be made to get and put.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LRUCache</span>:</span><br><span class="line">    <span class="comment"># double-linked list</span></span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">Node</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key=<span class="literal">None</span>, val=<span class="literal">None</span></span>):</span><br><span class="line">            self.key = key</span><br><span class="line">            self.val = val</span><br><span class="line">            self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">            self.prev = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, capacity: <span class="built_in">int</span></span>):</span><br><span class="line">        self.cap = capacity</span><br><span class="line">        self.cache = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dummy head and tail for the doubled-linked list</span></span><br><span class="line">        self.head = self.Node()</span><br><span class="line">        self.tail = self.Node()</span><br><span class="line">        self.head.<span class="built_in">next</span> = self.tail</span><br><span class="line">        self.tail.prev = self.head</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addNode</span>(<span class="params">self, node</span>):</span><br><span class="line">        node1 = self.head.<span class="built_in">next</span></span><br><span class="line">        node.<span class="built_in">next</span> = node1</span><br><span class="line">        node1.prev = node</span><br><span class="line">        self.head.<span class="built_in">next</span> = node</span><br><span class="line">        node.prev = self.head</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deleteNode</span>(<span class="params">self, node</span>):</span><br><span class="line">        prev = node.prev</span><br><span class="line">        <span class="built_in">next</span> = node.<span class="built_in">next</span></span><br><span class="line">        prev.<span class="built_in">next</span> = <span class="built_in">next</span></span><br><span class="line">        <span class="built_in">next</span>.prev = prev</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, key: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> self.cache:</span><br><span class="line">            node = self.cache[key]</span><br><span class="line">            val = node.val</span><br><span class="line">            <span class="keyword">del</span> self.cache[key]</span><br><span class="line">            self.deleteNode(node)</span><br><span class="line">            self.addNode(node)</span><br><span class="line">            self.cache[key] = self.head.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">return</span> val</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">put</span>(<span class="params">self, key: <span class="built_in">int</span>, value: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> self.cache:</span><br><span class="line">            node = self.cache[key]</span><br><span class="line">            <span class="keyword">del</span> self.cache[key]</span><br><span class="line">            self.deleteNode(node)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># delete the least recently used node when the capacity is full</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.cache) == self.cap:</span><br><span class="line">            <span class="keyword">del</span> self.cache[self.tail.prev.key]</span><br><span class="line">            self.deleteNode(self.tail.prev)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># insert the most recent used node to the front</span></span><br><span class="line">        self.addNode(self.Node(key, value))</span><br><span class="line">        self.cache[key] = self.head.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Your LRUCache object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"># obj = LRUCache(capacity)</span></span><br><span class="line"><span class="comment"># param_1 = obj.get(key)</span></span><br><span class="line"><span class="comment"># obj.put(key,value)</span></span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-206-Reverse-Linked-List"><a href="#Leetcode-206-Reverse-Linked-List" class="headerlink" title="[Leetcode 206] Reverse Linked List"></a>[Leetcode 206] Reverse Linked List</h2><p>Given the head of a singly linked list, reverse the list, and return the reversed list.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [1,2,3,4,5]</span><br><span class="line">Output: [5,4,3,2,1]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [1,2]</span><br><span class="line">Output: [2,1]</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = []</span><br><span class="line">Output: []</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>The number of nodes in the list is the range [0, 5000].</li>
<li>-5000 &lt;&#x3D; Node.val &lt;&#x3D; 5000</li>
</ul>
<p>Follow up: A linked list can be reversed either iteratively or recursively. Could you implement both?</p>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># Time: O(n) Space: O(1)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        prev, curr = <span class="literal">None</span>, head</span><br><span class="line">        <span class="keyword">while</span> curr:</span><br><span class="line">            <span class="built_in">next</span> = curr.<span class="built_in">next</span> <span class="comment"># save next node before we break the pointer</span></span><br><span class="line">            curr.<span class="built_in">next</span> = prev <span class="comment"># point to previous node</span></span><br><span class="line">            prev = curr <span class="comment"># shift previous pointer </span></span><br><span class="line">            curr = <span class="built_in">next</span> <span class="comment"># shift current pointer</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> prev</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Time: O(n) Space: O(1)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseList_recursive</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># e.g.</span></span><br><span class="line">        <span class="comment">#   1 -&gt; 2 -&gt; 3 -&gt; 4</span></span><br><span class="line">        <span class="comment">#   ^    ^</span></span><br><span class="line">        <span class="comment"># head  head.next</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#   4  -&gt;  3  -&gt;  2    -&gt;     1   -&gt; None</span></span><br><span class="line">        <span class="comment">#   ^             ^           ^</span></span><br><span class="line">        <span class="comment"># newHead        head.next   head</span></span><br><span class="line">        newHead = head</span><br><span class="line">        <span class="keyword">if</span> head.<span class="built_in">next</span>:</span><br><span class="line">            newHead = self.reverseList(head.<span class="built_in">next</span>)</span><br><span class="line">            head.<span class="built_in">next</span>.<span class="built_in">next</span> = head</span><br><span class="line">        head.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> newHead</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-2130-Maximum-Twin-Sum-of-a-Linked-List"><a href="#Leetcode-2130-Maximum-Twin-Sum-of-a-Linked-List" class="headerlink" title="[Leetcode 2130] Maximum Twin Sum of a Linked List"></a>[Leetcode 2130] Maximum Twin Sum of a Linked List</h2><p>In a linked list of size n, where n is even, the ith node (0-indexed) of the linked list is known as the twin of the (n-1-i)th node, if 0 &lt;&#x3D; i &lt;&#x3D; (n &#x2F; 2) - 1.</p>
<ul>
<li>For example, if n &#x3D; 4, then node 0 is the twin of node 3, and node 1 is the twin of node 2. These are the only nodes with twins for n &#x3D; 4.</li>
</ul>
<p>The twin sum is defined as the sum of a node and its twin.</p>
<p>Given the head of a linked list with even length, return the maximum twin sum of the linked list.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">5--&gt;4--&gt;2--&gt;1</span><br><span class="line">0   1   2   3</span><br><span class="line">Input: head = [5,4,2,1]</span><br><span class="line">Output: 6</span><br><span class="line">Explanation:</span><br><span class="line">Nodes 0 and 1 are the twins of nodes 3 and 2, respectively. All have twin sum = 6.</span><br><span class="line">There are no other nodes with twins in the linked list.</span><br><span class="line">Thus, the maximum twin sum of the linked list is 6. </span><br></pre></td></tr></table></figure>
<p>Constraints:</p>
<ul>
<li>The number of nodes in the list is an even integer in the range [2, 105].</li>
<li>1 &lt;&#x3D; Node.val &lt;&#x3D; 105</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pairSum</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        slow = fast = head</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the middle of linked list</span></span><br><span class="line">        <span class="keyword">while</span> fast <span class="keyword">and</span> fast.<span class="built_in">next</span>:</span><br><span class="line">            slow = slow.<span class="built_in">next</span></span><br><span class="line">            fast = fast.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># reverse the second half of linked list</span></span><br><span class="line">        prev, curr = <span class="literal">None</span>, slow</span><br><span class="line">        <span class="keyword">while</span> curr:</span><br><span class="line">            <span class="built_in">next</span> = curr.<span class="built_in">next</span></span><br><span class="line">            curr.<span class="built_in">next</span> = prev</span><br><span class="line">            prev = curr</span><br><span class="line">            curr = <span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get twin max sum</span></span><br><span class="line">        maxVal = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> prev:</span><br><span class="line">            maxVal = <span class="built_in">max</span>(maxVal, head.val + prev.val)</span><br><span class="line">            head = head.<span class="built_in">next</span></span><br><span class="line">            prev = prev.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> maxVal</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pairSum</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        st = []</span><br><span class="line">        </span><br><span class="line">        slow = fast = head</span><br><span class="line">        st.append(slow.val)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> fast <span class="keyword">and</span> fast.<span class="built_in">next</span> <span class="keyword">and</span> fast.<span class="built_in">next</span>.<span class="built_in">next</span>:   </span><br><span class="line">            slow = slow.<span class="built_in">next</span></span><br><span class="line">            st.append(slow.val)</span><br><span class="line">            fast = fast.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        maxTwinSum = <span class="number">0</span></span><br><span class="line">        slow = slow.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">while</span> st:</span><br><span class="line">            twinSum = slow.val + st.pop()</span><br><span class="line">            maxTwinSum = <span class="built_in">max</span>(maxTwinSum, twinSum)</span><br><span class="line">            slow = slow.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> maxTwinSum</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
        <tag>Linked List</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux performance analysis in 60 seconds</title>
    <url>/blog/linux-performance-analysis-in-60-seconds/</url>
    <content><![CDATA[<h2 id="Performance-tools-checklist"><a href="#Performance-tools-checklist" class="headerlink" title="Performance tools checklist"></a>Performance tools checklist</h2><p>The following tools can be used for the first look at any performance issues. By executing these commands in the poorly performing Linux system, it would help solve the performance issue or reveal the clues for the next step of performance issue analysis.</p>
<ul>
<li>uptime</li>
<li>dmesg | tail</li>
<li>lscpu</li>
<li>free -m</li>
<li>lsblk</li>
<li>vmstat 1</li>
<li>mpstat -P ALL 1</li>
<li>pidstat -u 1</li>
<li>pidstat -dt 1</li>
<li>iostat -ktdx 1</li>
<li>sar -n DEV 1</li>
<li>sar -n TCP,ETCP 1</li>
<li>top</li>
</ul>
<h2 id="uptime"><a href="#uptime" class="headerlink" title="uptime"></a>uptime</h2><p>This is a quick way to view the load averages on the system. It includes the number of processes waiting to run on the CPUs and blocked in uninterruptible I&#x2F;O(usually disk I&#x2F;O). The three numbers are the moving load averages for the last 1-minute, 5-minute, and 15-minutes. In this example, it shows a significant load increase.</p>
<pre><code>$ cat uptime.out
 20:22:25 up 97 days, 46 min,  1 user,  load average: 23.50, 11.97, 5.07
</code></pre>
<h2 id="lscpu"><a href="#lscpu" class="headerlink" title="lscpu"></a>lscpu</h2><p>This command is useful to know about the system CPU information.</p>
<pre><code>$ lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                96
On-line CPU(s) list:   0-95
Thread(s) per core:    2
Core(s) per socket:    24
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 85
Model name:            Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz
Stepping:              7
CPU MHz:               2315.744
CPU max MHz:           4000.0000
CPU min MHz:           1000.0000
BogoMIPS:              4800.00
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              1024K
L3 cache:              36608K
NUMA node0 CPU(s):     0-23,48-71
NUMA node1 CPU(s):     24-47,72-95
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni md_clear flush_l1d arch_capabilities
</code></pre>
<h2 id="free-m"><a href="#free-m" class="headerlink" title="free -m"></a>free -m</h2><p>This shows the total and available memory in MB. Generally, the available memory is the sum of free and buff&#x2F;cache. This is because the buff&#x2F;cache can be flushed to disk as needed.</p>
<pre><code>$ free -m
              total        used        free      shared  buff/cache   available
Mem:          61939        5954        1561         270       54424       52116
Swap:          4095         198        3897
</code></pre>
<h2 id="dmesg-tail"><a href="#dmesg-tail" class="headerlink" title="dmesg | tail"></a>dmesg | tail</h2><p>This shows the last 10 system messages. Sometimes, the error messages shown here could indiate the cause for the performance issue.</p>
<pre><code>$ dmesg | tail
[6832692.919986] bash (27857): drop_caches: 3
[7550537.761563] Device 1009069759715322128 added ffff888248373000 with mode 0x48002 fastpath 0 npath 0
[7550538.267764] removing device 1009069759715322128
[7550544.483191] Device 1009069759715322128 added ffff888801cd6000 with mode 0x48002 fastpath 0 npath 0
[7551342.509806] trace_kprobe: Could not probe notrace function vfs_stat_set_lookup_flags
[7551342.512439] trace_kprobe: Could not probe notrace function vfs_stat_set_lookup_flags
[7551359.071096] trace_kprobe: Could not probe notrace function vfs_stat_set_lookup_flags
[7551359.073699] trace_kprobe: Could not probe notrace function vfs_stat_set_lookup_flags
[8276360.681817] RPC: fragment too large: 612067950
[8281013.520451] nfsd: peername failed (err 107)!
</code></pre>
<h2 id="vmstat-1"><a href="#vmstat-1" class="headerlink" title="vmstat 1"></a>vmstat 1</h2><p>This is a simple but very useful command including a system performance summary for the CPU, memory, swap and I&#x2F;O. In many cases, you could get the cause or clue for the performance issue. In this example, it indicates the number of running processes(or threads) is ~25 and the ~22% CPU spent in user space. Notice that the first report gives the averages since the last reboot. Usually, we look at the report starting from the second sampling period.</p>
<pre><code>$ vmstat 1 3
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
31  0 202752 1599384  45844 55684312    0    0    62   785    0    0  2  1 97  0  0

24  0 202752 1774464  45468 55505764    0   88 328816   116 145768 111575 22  2 75  1  0
27  0 202752 1577296  45476 55693220    0    0 185692    44 148113 108146 23  2 75  0  0
</code></pre>
<h2 id="iostat-xz-1"><a href="#iostat-xz-1" class="headerlink" title="iostat -xz 1"></a>iostat -xz 1</h2><p>This is another useful tool to analyze the disk I&#x2F;O performance. The first report provides statistics since the last system boot.</p>
<p>The metrics include</p>
<ul>
<li><p>merges&#x2F;s - rrqm&#x2F;s, wrqm&#x2F;s</p>
</li>
<li><p>iops - reads&#x2F;s, writes&#x2F;s</p>
</li>
<li><p>throughput - rKB&#x2F;s, wKB&#x2F;s</p>
</li>
<li><p>io size - avgrq-sz</p>
</li>
<li><p>queue size - avgqu-sz</p>
</li>
<li><p>wait time in ms - await, r_await, w_await</p>
</li>
<li><p>service time in ms - svctm</p>
</li>
<li><p>disk bandwidth utilization - %util</p>
<p>  $ iostat -xz 1 3<br>  Linux 5.7.12-1.el7.elrepo.x86_64 (localhost) 03&#x2F;20&#x2F;2023 <em>x86_64</em> (96 CPU)</p>
<p>  avg-cpu:  %user   %nice %system %iowait  %steal   %idle<br>         2.31    0.00    0.52    0.05    0.00   97.11<br>  Device:         rrqm&#x2F;s   wrqm&#x2F;s     r&#x2F;s     w&#x2F;s    rkB&#x2F;s    wkB&#x2F;s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util<br>  nvme4n1           0.09     0.77   13.66   13.87   555.49   681.99    89.89     0.00    0.14    0.22    0.06   0.29   0.81<br>  nvme3n1           1.41    25.85   12.89  269.43   567.87 14516.73   106.86     0.02    0.09    0.23    0.08   0.24   6.75<br>  nvme5n1           1.40    25.13   15.25  261.88   576.08 14394.15   108.04     0.02    0.09    0.20    0.08   0.24   6.72<br>  nvme0n1           0.17     0.47    0.71    0.75    15.99    22.02    52.06     0.00    0.42    0.14    0.69   0.39   0.06<br>  nvme2n1           0.18     0.84   14.35   14.01   559.02   687.26    87.88     0.00    0.14    0.22    0.07   0.30   0.84<br>  nvme1n1           0.00     0.00    0.00    0.00     0.01     0.00    52.12     0.00    0.10    0.10    0.00   0.13   0.00<br>  nvme6n1           1.44    25.28   11.29  259.40   562.39 14368.24   110.32     0.02    0.09    0.25    0.08   0.24   6.60<br>  nvme8n1           0.22     0.80   13.58   13.64   554.13   676.59    90.43     0.00    0.15    0.23    0.07   0.29   0.80<br>  nvme7n1           1.44    25.69   11.51  258.52   556.81 14250.59   109.67     0.02    0.08    0.25    0.08   0.24   6.56<br>  nvme9n1           0.10     0.91   13.86   13.54   550.89   668.71    89.04     0.00    0.14    0.22    0.07   0.29   0.81<br>  md0               0.00     0.00   12.31   15.97   655.15   414.27    75.63     0.00    0.00    0.00    0.00   0.00   0.00</p>
<p>  avg-cpu:  %user   %nice %system %iowait  %steal   %idle<br>        21.63    0.00    2.10    1.23    0.00   75.04<br>  Device:         rrqm&#x2F;s   wrqm&#x2F;s     r&#x2F;s     w&#x2F;s    rkB&#x2F;s    wkB&#x2F;s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util<br>  nvme4n1           8.00     0.00 2264.00    0.00 83248.00     0.00    73.54     0.49    0.22    0.22    0.00   0.29  65.30<br>  nvme0n1           0.00    16.00    0.00    7.00     0.00    92.00    26.29     0.14   19.29    0.00   19.29  15.71  11.00<br>  nvme2n1           3.00     0.00 2264.00    0.00 82724.00     0.00    73.08     0.50    0.22    0.22    0.00   0.30  67.50<br>  nvme8n1           8.00     0.00 2180.00    4.00 80624.00    20.00    73.85     0.47    0.22    0.22    0.00   0.30  64.60<br>  nvme9n1           6.00     0.00 2254.00    0.00 82180.00     0.00    72.92     0.49    0.22    0.22    0.00   0.28  64.10<br>  md0               0.00     0.00 8986.00    8.00 328704.00    20.00    73.10     0.00    0.00    0.00    0.00   0.00   0.00</p>
<p>  avg-cpu:  %user   %nice %system %iowait  %steal   %idle<br>        22.84    0.00    1.78    0.44    0.00   74.94<br>  Device:         rrqm&#x2F;s   wrqm&#x2F;s     r&#x2F;s     w&#x2F;s    rkB&#x2F;s    wkB&#x2F;s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util<br>  nvme4n1           3.00     0.00 1153.00    0.00 47180.00     0.00    81.84     0.25    0.22    0.22    0.00   0.44  50.80<br>  nvme0n1           0.00     0.00    5.00    0.00    40.00     0.00    16.00     0.00    0.20    0.20    0.00   0.40   0.20<br>  nvme2n1           2.00     0.00 1140.00    0.00 47260.00     0.00    82.91     0.26    0.23    0.23    0.00   0.42  48.20<br>  nvme8n1           2.00     0.00 1101.00    8.00 46696.00    32.00    84.27     0.25    0.22    0.22    0.00   0.45  50.10<br>  nvme9n1           5.00     1.00 1112.00    2.00 44516.00    12.00    79.94     0.25    0.22    0.22    0.00   0.43  48.00<br>  md0               0.00     0.00 4516.00   19.00 185620.00    44.00    81.88     0.00    0.00    0.00    0.00   0.00   0.00</p>
</li>
</ul>
<h2 id="sar-n-DEV-1"><a href="#sar-n-DEV-1" class="headerlink" title="sar -n DEV 1"></a>sar -n DEV 1</h2><p>The sar tool has many options for different metrics. Here we use an option to look at the network device metrics. This is very useful to check the current network throughput on each interface.</p>
<pre><code>$ sar -n DEV 1 3
Linux 5.7.12-1.el7.elrepo.x86_64 (localhost) 03/20/2023 _x86_64_ (96 CPU)

08:22:25 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
08:22:26 PM        lo      9.00      9.00      0.73      0.73      0.00      0.00      0.00
08:22:26 PM      eno1     57.00      8.00      3.47      0.47      0.00      0.00      0.00
08:22:26 PM      eno2      0.00      0.00      0.00      0.00      0.00      0.00      0.00
08:22:26 PM enp175s0f0  22353.00  22359.00  13986.21  13454.67     0.00      0.00      0.00
08:22:26 PM enp175s0f1     0.00      0.00      0.00      0.00     0.00      0.00      0.00
08:22:26 PM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00

08:22:26 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
08:22:27 PM        lo      8.00      8.00      0.47      0.47      0.00      0.00      0.00
08:22:27 PM      eno1     60.00      6.00      3.66      0.43      0.00      0.00      0.00
08:22:27 PM      eno2      0.00      0.00      0.00      0.00      0.00      0.00      0.00
08:22:27 PM enp175s0f0  23500.00  23497.00  14913.20  13944.36     0.00      0.00      0.00
08:22:27 PM enp175s0f1     0.00      0.00      0.00      0.00      0.00      0.00      0.00
08:22:27 PM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00

08:22:27 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
08:22:28 PM        lo     12.00     12.00      1.19      1.19      0.00      0.00      0.00
08:22:28 PM      eno1     66.00      8.00      4.11      0.63      0.00      0.00      0.00
08:22:28 PM      eno2      0.00      0.00      0.00      0.00      0.00      0.00      0.00
08:22:28 PM enp175s0f0  23584.00  23590.00  14889.09  14073.68     0.00      0.00      0.00
08:22:28 PM enp175s0f1     0.00      0.00      0.00      0.00      0.00      0.00      0.00
08:22:28 PM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00

Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
Average:           lo      9.67      9.67      0.80      0.80      0.00      0.00      0.00
Average:         eno1     61.00      7.33      3.75      0.51      0.00      0.00      0.00
Average:         eno2      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:    enp175s0f0  23145.67  23148.67  14596.16  13824.24     0.00      0.00      0.00
Average:    enp175s0f1     0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:      docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00
</code></pre>
<h2 id="sar-n-TCP-ETCP-1"><a href="#sar-n-TCP-ETCP-1" class="headerlink" title="sar -n TCP,ETCP 1"></a>sar -n TCP,ETCP 1</h2><p>This command shows the following three useful metrics</p>
<ul>
<li><p>active&#x2F;s - The number of locally initiated TCP connections per second</p>
</li>
<li><p>passive&#x2F;s - The number of remotely initiated TCP connections per second</p>
</li>
<li><p>retrans&#x2F;s - The number of TCP retransmits per second</p>
<p>  $ sar -n TCP,ETCP 1<br>  Linux 5.7.12-1.el7.elrepo.x86_64 (init500-c4) 03&#x2F;20&#x2F;2023 <em>x86_64</em> (96 CPU)</p>
<p>  08:48:02 PM  active&#x2F;s passive&#x2F;s    iseg&#x2F;s    oseg&#x2F;s<br>  08:48:03 PM      1.00      0.00  23194.00  23200.00</p>
<p>  08:48:02 PM  atmptf&#x2F;s  estres&#x2F;s retrans&#x2F;s isegerr&#x2F;s   orsts&#x2F;s<br>  08:48:03 PM      0.00      0.00      0.00      0.00      4.00</p>
<p>  08:48:03 PM  active&#x2F;s passive&#x2F;s    iseg&#x2F;s    oseg&#x2F;s<br>  08:48:04 PM      0.00      0.00  23410.00  23406.00</p>
<p>  08:48:03 PM  atmptf&#x2F;s  estres&#x2F;s retrans&#x2F;s isegerr&#x2F;s   orsts&#x2F;s<br>  08:48:04 PM      0.00      0.00      0.00      0.00      8.00</p>
<p>  08:48:04 PM  active&#x2F;s passive&#x2F;s    iseg&#x2F;s    oseg&#x2F;s<br>  08:48:05 PM      1.00      0.00  23698.00  23706.00</p>
<p>  08:48:04 PM  atmptf&#x2F;s  estres&#x2F;s retrans&#x2F;s isegerr&#x2F;s   orsts&#x2F;s<br>  08:48:05 PM      0.00      0.00      0.00      0.00      4.00</p>
<p>  Average:     active&#x2F;s passive&#x2F;s    iseg&#x2F;s    oseg&#x2F;s<br>  Average:         0.67      0.00  23434.00  23437.33</p>
<p>  Average:     atmptf&#x2F;s  estres&#x2F;s retrans&#x2F;s isegerr&#x2F;s   orsts&#x2F;s<br>  Average:         0.00      0.00      0.00      0.00      5.33</p>
</li>
</ul>
<h2 id="mpstat-P-ALL-1"><a href="#mpstat-P-ALL-1" class="headerlink" title="mpstat -P ALL 1"></a>mpstat -P ALL 1</h2><p>This command helps identify the per-CPU core usage. You may look for if any of the CPU cores has utilization(%usr+%sys) close to 100%.</p>
<pre><code>$ mpstat -P ALL 1 3
Linux 5.7.12-1.el7.elrepo.x86_64 (init500-c4)   03/20/2023  _x86_64_    (96 CPU)

08:22:25 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
08:22:26 PM  all   21.11    0.00    1.79    1.23    0.00    0.20    0.00    0.00    0.00   75.67
08:22:26 PM    0   27.17    0.00    2.17    2.17    0.00    0.00    0.00    0.00    0.00   68.48
08:22:26 PM    1    4.12    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   95.88
08:22:26 PM    2    7.00    0.00    1.00    0.00    0.00    0.00    0.00    0.00    0.00   92.00
08:22:26 PM    3   12.00    0.00    1.00    1.00    0.00    0.00    0.00    0.00    0.00   86.00
08:22:26 PM    4   15.00    0.00    1.00    1.00    0.00    0.00    0.00    0.00    0.00   83.00
08:22:26 PM    5   16.83    0.00    1.98    0.00    0.00    0.00    0.00    0.00    0.00   81.19
08:22:26 PM    6   24.00    0.00    1.00    2.00    0.00    0.00    0.00    0.00    0.00   73.00
[...]

08:22:26 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
08:22:27 PM  all   22.84    0.00    1.53    0.44    0.00    0.28    0.00    0.00    0.00   74.91
08:22:27 PM    0   10.64    0.00    1.06    0.00    0.00    3.19    0.00    0.00    0.00   85.11
08:22:27 PM    1   35.35    0.00    3.03    1.01    0.00    0.00    0.00    0.00    0.00   60.61
08:22:27 PM    2   17.17    0.00    0.00    1.01    0.00    0.00    0.00    0.00    0.00   81.82
08:22:27 PM    3   40.00    0.00    3.00    1.00    0.00    0.00    0.00    0.00    0.00   56.00
08:22:27 PM    4    8.08    0.00    1.01    0.00    0.00    0.00    0.00    0.00    0.00   90.91
08:22:27 PM    5   27.00    0.00    1.00    1.00    0.00    0.00    0.00    0.00    0.00   71.00
08:22:27 PM    6   14.00    0.00    1.00    0.00    0.00    0.00    0.00    0.00    0.00   85.00
[...]

08:22:27 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
08:22:28 PM  all   22.42    0.00    1.57    0.42    0.00    0.20    0.00    0.00    0.00   75.39
08:22:28 PM    0   29.35    0.00    2.17    0.00    0.00    0.00    0.00    0.00    0.00   68.48
08:22:28 PM    1   20.41    0.00    1.02    0.00    0.00    0.00    0.00    0.00    0.00   78.57
08:22:28 PM    2   18.18    0.00    1.01    0.00    0.00    0.00    0.00    0.00    0.00   80.81
08:22:28 PM    3   10.89    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   89.11
08:22:28 PM    4   18.00    0.00    1.00    0.00    0.00    0.00    0.00    0.00    0.00   81.00
08:22:28 PM    5    5.05    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   94.95
08:22:28 PM    6   21.21    0.00    1.01    0.00    0.00    0.00    0.00    0.00    0.00   77.78
[...]

Average:     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
Average:     all   22.12    0.00    1.63    0.69    0.00    0.23    0.00    0.00    0.00   75.32
Average:       0   22.30    0.00    1.80    0.72    0.00    1.08    0.00    0.00    0.00   74.10
Average:       1   20.07    0.00    1.36    0.34    0.00    0.00    0.00    0.00    0.00   78.23
Average:       2   14.09    0.00    0.67    0.34    0.00    0.00    0.00    0.00    0.00   84.90
Average:       3   20.93    0.00    1.33    0.66    0.00    0.00    0.00    0.00    0.00   77.08
Average:       4   13.71    0.00    1.00    0.33    0.00    0.00    0.00    0.00    0.00   84.95
Average:       5   16.33    0.00    1.00    0.33    0.00    0.00    0.00    0.00    0.00   82.33
Average:       6   19.73    0.00    1.00    0.67    0.00    0.00    0.00    0.00    0.00   78.60
[...]
</code></pre>
<h2 id="pidstat-1"><a href="#pidstat-1" class="headerlink" title="pidstat 1"></a>pidstat 1</h2><p>This command shows the CPU usage per process. In this example, the process <em>cockroach</em> consumes 100% CPU. Notice that the system has toally 9600%(96 cores) CPU bandwidth available. There is an option <em>-t</em> which shows the per-thread CPU usage. Also, the option <em>-d</em> shows the disk I&#x2F;O per process(or per thread when to use with option <em>-t</em>).</p>
<pre><code>$ pidstat 1 3
Linux 5.7.12-1.el7.elrepo.x86_64 (localhost) 03/20/2023 _x86_64_ (96 CPU)

08:22:25 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command
08:22:26 PM     0       568    0.00   12.62    0.00   12.62     9  kswapd0
08:22:26 PM     0     27131  100.00  100.00    0.00  100.00    85  cockroach
08:22:26 PM     0     42056    0.97    1.94    0.00    2.91    40  pidstat
08:22:26 PM     0     42060    0.97    1.94    0.00    2.91    89  top
08:22:26 PM     0     53123    0.00    0.97    0.00    0.97    74  kworker/74:0-events
08:22:26 PM     0     94796    0.00    1.94    0.00    1.94    41  kworker/u192:1-mlx5_cmd_0000:af:00.0

08:22:26 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command
08:22:27 PM     0       568    0.00    3.00    0.00    3.00    58  kswapd0
08:22:27 PM     0      3175    0.00    1.00    0.00    1.00    19  containerd
08:22:27 PM     0     17494    0.00    1.00    0.00    1.00    33  kworker/33:2-events
08:22:27 PM     0     27131  100.00  100.00    0.00  100.00    79  cockroach
08:22:27 PM     0     38660    0.00    1.00    0.00    1.00    32  kworker/32:1-mm_percpu_wq
08:22:27 PM     0     39110    0.00    1.00    0.00    1.00    46  kworker/46:0-events
08:22:27 PM     0     41667    0.00    1.00    0.00    1.00    80  kworker/80:2-events
08:22:27 PM     0     42056    1.00    4.00    0.00    5.00    40  pidstat
08:22:27 PM     0     42060    2.00    2.00    0.00    4.00    89  top
08:22:27 PM     0     94796    0.00    1.00    0.00    1.00    81  kworker/u192:1-mlx5_cmd_0000:af:00.0

08:22:27 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command
08:22:28 PM     0        11    0.00    1.00    0.00    1.00    57  rcu_sched
08:22:28 PM     0       568    0.00    7.00    0.00    7.00    58  kswapd0
08:22:28 PM     0     27131  100.00  100.00    0.00  100.00    40  cockroach
08:22:28 PM     0     41616    0.00    1.00    0.00    1.00    23  iostat
08:22:28 PM     0     42056    1.00    2.00    0.00    3.00    41  pidstat
08:22:28 PM     0     53853    0.00    1.00    0.00    1.00    29  kworker/29:1-events
08:22:28 PM     0     87419    0.00    1.00    0.00    1.00     0  kworker/0:0-events

Average:      UID       PID    %usr %system  %guest    %CPU   CPU  Command
Average:        0        11    0.00    0.33    0.00    0.33     -  rcu_sched
Average:        0       568    0.00    7.59    0.00    7.59     -  kswapd0
Average:        0      3175    0.00    0.33    0.00    0.33     -  containerd
Average:        0     17494    0.00    0.33    0.00    0.33     -  kworker/33:2-mm_percpu_wq
Average:        0     27131  100.00  100.00    0.00  100.00     -  cockroach
Average:        0     38660    0.00    0.33    0.00    0.33     -  kworker/32:1-mm_percpu_wq
Average:        0     39110    0.00    0.33    0.00    0.33     -  kworker/46:0-events
Average:        0     41616    0.00    0.33    0.00    0.33     -  iostat
Average:        0     41667    0.00    0.33    0.00    0.33     -  kworker/80:2-events
Average:        0     42056    0.99    2.64    0.00    3.63     -  pidstat
Average:        0     53123    0.00    0.33    0.00    0.33     -  kworker/74:0-events
Average:        0     53853    0.00    0.33    0.00    0.33     -  kworker/29:1-events
Average:        0     87419    0.00    0.33    0.00    0.33     -  kworker/0:0-events
Average:        0     94796    0.00    0.99    0.00    0.99     -  kworker/u192:1-dm-thin
</code></pre>
<h2 id="top"><a href="#top" class="headerlink" title="top"></a>top</h2><p>This command shows the CPU and memory usage per process. It’s very useful when you need to create a CPU or memory profile per process. You may notice that it also shows the load averages. If you want to collect the metrics by saving to file, you can use the command like <em>top -b -d1 -n3</em>.</p>
<pre><code>$ top
top - 20:22:25 up 97 days, 46 min,  1 user,  load average: 23.50, 11.97, 5.07
Tasks: 979 total,   1 running, 405 sleeping,   0 stopped,   0 zombie
%Cpu(s): 23.2 us,  3.0 sy,  0.0 ni, 71.2 id,  2.3 wa,  0.0 hi,  0.3 si,  0.0 st
KiB Mem : 63426556 total,  1962168 free,  6098708 used, 55365680 buff/cache
KiB Swap:  4194300 total,  3991548 free,   202752 used. 53364460 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
27131 root      20   0   19.0g   1.3g  46196 S  2244  2.2   7412:53 cockroach
[...]

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
27131 root      20   0   19.0g   1.3g  46196 R  2177  2.2   7413:15 cockroach
[...]

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
27131 root      20   0   19.0g   1.3g  46208 S  2323  2.2   7413:39 cockroach
[...]
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>RCA</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux Software RAID</title>
    <url>/blog/linux-software-raid/</url>
    <content><![CDATA[<h2 id="What-is-RAID"><a href="#What-is-RAID" class="headerlink" title="What is RAID?"></a>What is RAID?</h2><p>RAID stands for either Redundant Array of Independent Disks, or Redundant Array of Inexpensive Disks. The intention of RAID is to spread your data across several disks, such that a single disk failure will not lose that data.</p>
<p>The current RAID drivers in Linux support the following levels:</p>
<ul>
<li>Linear Mode : JBOD</li>
<li>RAID0&#x2F;Stripe : Two or more disks. No redundancy.</li>
<li>RAID1&#x2F;Mirror : Two or more disks. Redundancy.</li>
<li>RAID-4 : Three or more disks. Redundancy. Not used very often.</li>
<li>RAID-5 : Three or more disks. Redundancy. Allows one disk failure.</li>
<li>RAID-6 : Four or more disks. Redundancy. Allows two disks failure.</li>
<li>RAID-10 : Four or more disks. Combination of RAID-1 and RAID-0.</li>
</ul>
<p>Linux Software RAID (often called mdraid or MD&#x2F;RAID) makes the use of RAID possible without a hardware RAID controller.</p>
<h2 id="mdadm-utility"><a href="#mdadm-utility" class="headerlink" title="mdadm utility"></a>mdadm utility</h2><p>The mdadm utility can be used to create and manage storage arrays using Linux’s software RAID capabilities.</p>
<h3 id="Creating-a-RAID-Array"><a href="#Creating-a-RAID-Array" class="headerlink" title="Creating a RAID Array"></a>Creating a RAID Array</h3><p>The following example shows the creation of a RAID 0 array with 8 NVME disks.</p>
<pre><code>$ mdadm --create /dev/md0 --name=mdvol --level=raid0 --raid-devices=8 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1 /dev/nvme5n1 /dev/nvme6n1 /dev/nvme7n1 /dev/nvme8n1 /dev/nvme9n1
</code></pre>
<h2 id="Check-RAID-array-status"><a href="#Check-RAID-array-status" class="headerlink" title="Check RAID array status"></a>Check RAID array status</h2><pre><code>$ cat /proc/mdstat
    Personalities : [raid0]
    md0 : active raid0 nvme9n1[7] nvme8n1[6] nvme7n1[5] nvme6n1[4] nvme5n1[3] nvme4n1[2] nvme3n1[1] nvme2n1[0]
      30004846592 blocks super 1.2 512k chunks

$  mdadm --detail /dev/md0
    /dev/md0:
               Version : 1.2
         Creation Time : Mon Sep 20 17:39:47 2021
            Raid Level : raid0
            Array Size : 30004846592 (27.94 TiB 30.72 TB)
          Raid Devices : 8
         Total Devices : 8
           Persistence : Superblock is persistent
    
           Update Time : Mon Sep 20 17:39:47 2021
                 State : clean
        Active Devices : 8
       Working Devices : 8
        Failed Devices : 0
         Spare Devices : 0
    
            Chunk Size : 512K
    
    Consistency Policy : none
    
                  Name : host1:mdvol
                  UUID : 5908fc3f:8c8b8851:c0875278:ea274fec
                Events : 0
    
        Number   Major   Minor   RaidDevice State
           0     259        2        0      active sync   /dev/nvme2n1
           1     259        6        1      active sync   /dev/nvme3n1
           2     259        7        2      active sync   /dev/nvme4n1
           3     259        8        3      active sync   /dev/nvme5n1
           4     259       11        4      active sync   /dev/nvme6n1
           5     259       12        5      active sync   /dev/nvme7n1
           6     259       10        6      active sync   /dev/nvme8n1
           7     259        9        7      active sync   /dev/nvme9n1 
</code></pre>
<h3 id="Deleting-a-RAID-Array"><a href="#Deleting-a-RAID-Array" class="headerlink" title="Deleting a RAID Array"></a>Deleting a RAID Array</h3><p>If a RAID volume is no longer required, it can be deactivated using the following commands:</p>
<pre><code>$ mdadm --stop /dev/md0
mdadm: stopped /dev/md0
</code></pre>
<p>A Linux software RAID array stores all of the necessary information about a RAID array in a superblock. The superblock for the individual devices can be deleted by the following commands. By doing this, you can re-use these disks for new RAID arrays.</p>
<pre><code>$ mdadm --zero-superblock /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1 /dev/nvme5n1 /dev/nvme6n1 /dev/nvme7n1 /dev/nvme8n1 /dev/nvme9n1
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://raid.wiki.kernel.org/index.php/Linux_Raid">https://raid.wiki.kernel.org/index.php/Linux_Raid</a></li>
<li><a href="https://www.thomas-krenn.com/en/wiki/Linux_Software_RAID_Information">https://www.thomas-krenn.com/en/wiki/Linux_Software_RAID_Information</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>RAID</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux System Administration</title>
    <url>/blog/linux-system-administration/</url>
    <content><![CDATA[<h2 id="Package-management"><a href="#Package-management" class="headerlink" title="Package management"></a>Package management</h2><p>Install the package:</p>
<pre><code>$ rpm -ivh blktrace
</code></pre>
<p>Upgrade the package:</p>
<pre><code>$ rpm -Uvh blktrace
</code></pre>
<p>Remove the installed package:</p>
<pre><code>$ rpm -ev blktrace
$ rpm -ev --nodeps blktrace
</code></pre>
<p>Display the installed package info:</p>
<pre><code>$ rpm -qi blktrace
Name        : blktrace                     Relocations: (not relocatable)
Version     : 1.0.1                             Vendor: Red Hat, Inc.
Release     : 6.el6                         Build Date: Wed 07 Sep 2011 12:51:04 PM PDT
Install Date: Fri 23 Dec 2016 12:15:57 PM PST      Build Host: x86-006.build.bos.redhat.com
Group       : Development/System            Source RPM: blktrace-1.0.1-6.el6.src.rpm
Size        : 1042319                          License: GPLv2+
Signature   : RSA/8, Fri 23 Sep 2011 04:19:31 AM PDT, Key ID 199e2f91fd431d51
Packager    : Red Hat, Inc. &lt;http://bugzilla.redhat.com/bugzilla&gt;
URL         : http://brick.kernel.dk/snaps
Summary     : Utilities for performing block layer IO tracing in the linux kernel
Description :
blktrace is a block layer IO tracing mechanism which provides detailed
information about request queue operations to user space.  This package
includes both blktrace, a utility which gathers event traces from the kernel;
and blkparse, a utility which formats trace data collected by blktrace.

You should install the blktrace package if you need to gather detailed
information about IO patterns.
</code></pre>
<p>Find out what package a file belongs to:</p>
<pre><code>$ rpm -qf /usr/bin/blktrace
blktrace-1.0.1-6.el6.x86_64
</code></pre>
<p>Display list of configuration files for a package or command:</p>
<pre><code>$ rpm -qc &lt;package-name&gt;
$ rpm -qcf /path/to/file
</code></pre>
<p>Download a package and its dependencies:</p>
<pre><code>$ yum install yum-utils
$ yumdownloader --destdir=./ --resolve blktrace
</code></pre>
<h2 id="Network-bonding"><a href="#Network-bonding" class="headerlink" title="Network bonding"></a>Network bonding</h2><p>Configure LACP bonding without reboot:</p>
<pre><code>$  cat /sys/class/net/bond0/bonding/mode
802.3ad 4
$  cat /sys/class/net/bond0/bonding/xmit_hash_policy
layer2 0
$  echo 1 &gt; /sys/class/net/bond0/bonding/xmit_hash_policy
$  cat /sys/class/net/bond0/bonding/xmit_hash_policy
layer3+4 1
</code></pre>
<p>Configure LACP bonding permanent to reboot:</p>
<pre><code>$ vi /etc/sysconfig/network-scripts/ifcfg-bond0
BONDING_OPTS=&quot;mode=802.3ad xmit_hash_policy=layer3+4&quot;

$ service network restart
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Administration</tag>
      </tags>
  </entry>
  <entry>
    <title>Max open files limit</title>
    <url>/blog/max-open-files-limit/</url>
    <content><![CDATA[<h2 id="System-wide-open-files-limit"><a href="#System-wide-open-files-limit" class="headerlink" title="System wide open files limit"></a>System wide open files limit</h2><p>To check system wide files limit:</p>
<pre><code>$ cat /proc/sys/fs/file-max
4875932
$ sysctl -a | grep file-max
fs.file-max = 4875932
</code></pre>
<p>To change system wide files limit:</p>
<pre><code>$ echo &quot;fs.file-max = 4875932&quot; &gt;&gt; /etc/sysctl.conf
$ sysctl -p /etc/sysctl.conf
</code></pre>
<h2 id="User-level-open-files-limit"><a href="#User-level-open-files-limit" class="headerlink" title="User level open files limit"></a>User level open files limit</h2><p>To check hard&#x2F;soft limits:</p>
<pre><code>$ ulimit -Hn
40960
$ ulimit -Sn
40960
</code></pre>
<p>To change hard&#x2F;soft limits:</p>
<pre><code>$ vi /etc/security/limits.conf
*  hard nofile 40960
*  soft nofile 40960
</code></pre>
<h2 id="Process-level-open-files-limit"><a href="#Process-level-open-files-limit" class="headerlink" title="Process level open files limit"></a>Process level open files limit</h2><p>To check the max open files per process:</p>
<pre><code>$ cat /proc/sys/fs/nr_open
1048576
</code></pre>
<p>To check the specific process max open files limit:</p>
<pre><code>$ cat /proc/`pidof &lt;process-name&gt;`/limits | egrep &quot;Limit |Max open files&quot;
Limit                     Soft Limit           Hard Limit           Units
Max open files            524352               524352               files
</code></pre>
<p>Sometimes, application process may need change the max open files limits on the fly.</p>
<p>In Docker container, the process does not have the permission to do so by default.</p>
<p>Docker provides the following two ways to extend Linux capabilities for the container processes.</p>
<ul>
<li>–cap-add	    Add Linux capabilities</li>
<li>–cap-drop	Drop Linux capabilities</li>
<li>–privileged	Give extended privileges to this container</li>
</ul>
<p>When using the “–privileged” option is not allowed for security reason, we can have fine grain control over the capabilities using –cap-add and –cap-drop.</p>
<p>For example, if we want to grant the container process the permission to change max open files limit on the fly, we can use the following capability option.</p>
<ul>
<li>SYS_RESOURCE - Override resource Limits.</li>
</ul>
<p>We can pass this option to the target docker container.</p>
<pre><code>$ docker run --cap-add=SYS_ADMIN ...
</code></pre>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://docs.docker.com/engine/reference/run/">https://docs.docker.com/engine/reference/run/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>Memory fragmentation</title>
    <url>/blog/memory-fragmentation/</url>
    <content><![CDATA[<h2 id="Memory-fragmentation"><a href="#Memory-fragmentation" class="headerlink" title="Memory fragmentation"></a>Memory fragmentation</h2><p>Memory page availability can be checked from &#x2F;proc&#x2F;buddyinfo as below.</p>
<pre><code>$  cat /proc/buddyinfo
Node 0, zone      DMA      1      0      1      0      1      1      1      0      1      1      3
Node 0, zone    DMA32   3342   2441   2138   5025   1871    236      3      0      0      0      0
Node 0, zone   Normal 143135   6057 150803   4005    330     62      1      0      0      0      0
</code></pre>
<p>If you see the system log message file(&#x2F;var&#x2F;log&#x2F;message) is reporting “kernel: nf_conntrack: falling back to vmalloc”, it highly indicates the memory is being fragmented.</p>
<p>Following command can be used to compact fragmented memory pages.</p>
<pre><code>echo 1 &gt; /proc/sys/vm/compact_memory
</code></pre>
<h2 id="extfrag-threshold"><a href="#extfrag-threshold" class="headerlink" title="extfrag_threshold"></a>extfrag_threshold</h2><p>This parameter affects whether the kernel will compact memory or direct</p>
<p>reclaim to satisfy a high-order allocation. The extfrag&#x2F;extfrag_index file in</p>
<p>debugfs shows what the fragmentation index for each order is in each zone in</p>
<p>the system. Values tending towards 0 imply allocations would fail due to lack</p>
<p>of memory, values towards 1000 imply failures are due to fragmentation and -1</p>
<p>implies that the allocation will succeed as long as watermarks are met.</p>
<p>The kernel will not compact memory in a zone if the fragmentation index is &lt;&#x3D; extfrag_threshold. The default value is 500.</p>
<p>Example output:</p>
<pre><code>$  sysctl -a | grep extfrag_threshold
vm.extfrag_threshold = 500

$  cd /sys/kernel/debug/extfrag/
$  ls
extfrag_index  unusable_index

$  cat unusable_index
Node 0, zone      DMA 0.000 0.000 0.000 0.001 0.001 0.009 0.018 0.035 0.035 0.035 0.173
Node 0, zone    DMA32 0.000 0.000 0.000 0.000 0.000 0.001 0.001 0.003 0.007 0.013 0.028
Node 0, zone   Normal 0.000 0.007 0.020 0.045 0.096 0.192 0.342 0.552 0.803 1.000 1.000
Node 1, zone   Normal 0.000 0.006 0.021 0.051 0.104 0.190 0.325 0.522 0.775 1.000 1.000

$  cat extfrag_index
Node 0, zone      DMA -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
Node 0, zone    DMA32 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
Node 0, zone   Normal -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 0.955 0.978
Node 1, zone   Normal -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 0.956 0.978

$  cat /proc/buddyinfo
Node 0, zone      DMA      1      0      1      0      2      1      1      0      0      1      3
Node 0, zone    DMA32      7     10     13     15     11      8      9     10      8      9    309
Node 0, zone   Normal 1134904 1124019 975094 1027980 973525 755862 528091 316182 123744      0      0
Node 1, zone   Normal 864976 1041246 994760 895916 726438 565284 415625 266573 118000      0      0
</code></pre>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://www.kernel.org/doc/Documentation/sysctl/vm.txt">https://www.kernel.org/doc/Documentation/sysctl/vm.txt</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>Memory-Mapped I/O</title>
    <url>/blog/memory-mapped-i-o/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Memory-mapped I&#x2F;O lets us map a file on disk into a buffer in memory so that, when we fetch bytes from the buffer, the corresponding bytes of the file are read. Similarly, when we store data in the buffer, the corresponding bytes are automatically written to the file. This lets us perform I&#x2F;O without using read or write.</p>
<pre><code>#include &lt;sys/mman.h&gt;

void *mmap(void *addr, size_t len, int prot, int flag, int fd, off_t off );

Returns: starting address of mapped region if OK, MAP_FAILED on error
</code></pre>
<p>The addr argument lets us specify the address where we want the mapped region to start. We normally set this value to 0 to allow the system to choose the starting address. The return value of this function is the starting address of the mapped area.</p>
<p>The fd argument is the file descriptor specifying the file that is to be mapped. We have to open this file before we can map it into the address space. The len argument is the number of bytes to map, and off is the starting offset in the file of the bytes to map.</p>
<p>The prot argument specifies the protection of the mapped region.</p>
<pre><code>prot         Description
PROT_READ    Region can be read.
PROT_WRITE   Region can be written.
PROT_EXEC    Region can be executed.
PROT_NONE    Region cannot be accessed.
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>Advanced Programming in the UNIX Environment</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Memory overcommit</title>
    <url>/blog/memory-overcommit/</url>
    <content><![CDATA[<p>The Linux kernel supports the following overcommit handling modes</p>
<p>0	-	Heuristic overcommit handling. Obvious overcommits of</p>
<p>address space are refused. Used for a typical system. It</p>
<p>ensures a seriously wild allocation fails while allowing</p>
<p>overcommit to reduce swap usage.  root is allowed to</p>
<p>allocate slightly more memory in this mode. This is the</p>
<p>default.</p>
<p>1	-	Always overcommit. Appropriate for some scientific</p>
<p>applications. Classic example is code using sparse arrays</p>
<p>and just relying on the virtual memory consisting almost</p>
<p>entirely of zero pages.</p>
<p>2	-	Don’t overcommit. The total address space commit</p>
<p>for the system is not permitted to exceed swap + a</p>
<p>configurable amount (default is 50%) of physical RAM.</p>
<p>Depending on the amount you use, in most situations</p>
<p>this means a process will not be killed while accessing</p>
<p>pages but will receive errors on memory allocation as</p>
<p>appropriate.</p>
<pre><code>    Useful for applications that want to guarantee their
    memory allocations will be available in the future
    without having to initialize every page.
</code></pre>
<p>The overcommit policy is set via the sysctl &#96;vm.overcommit_memory’.</p>
<p>The overcommit amount can be set via <code>vm.overcommit_ratio&#39; (percentage) or </code>vm.overcommit_kbytes’ (absolute value).</p>
<p>The current overcommit limit and amount committed are viewable in</p>
<p>&#x2F;proc&#x2F;meminfo as CommitLimit and Committed_AS respectively.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
  </entry>
  <entry>
    <title>Mid-Autumn Festival</title>
    <url>/blog/mid-autumn-festival/</url>
    <content><![CDATA[<p>Mid-Autumn Festival is a traditional festival in Chinese culture, having its history dating to 3000 years ago. The festival is held on the 15th day of the 8th month of the lunisolar calendar. The Chinese believe that the moon is at its brightest and largest size that night.</p>
<p>There are multiple legends of the Mid-Autumn Festival, the most popular of them being the story of Hou Yi and Chang’e. The legend says that there once were 10 fiery suns raging in the sky, burning down all the plants and humans. Seeing his people suffering, Hou Yi used his bow to kill 9 of the suns. As a reward, he was given the elixir of immortality, which granted him the ability to live forever. Since he wanted to spend more time with his wife Chang’e, Hou Yi asked her to keep it safe for him. The people asked him to be their master, and he agreed to teach most of them. But one particular greedy disciple Pang Meng wanted the elixir for himself, so one day he pretended to be ill. After Hou Yi left to hunt, Pang Meng went to his house and forced Chang’e to give him the elixir. Since she couldn’t defeat him, Chang’e drank the elixir and was lifted up into the heavens, becoming the moon goddess. After returning, Hou Yi was heartbroken and offered Chang’e some fruit and cakes that she loved to eat. Since then, people have always offered the Moon Goddess fruits and moon cakes to worship the moon.</p>
<p>There are multiple ways to celebrate the Mid-Autumn Festival. The most representative way is to eat moon cakes, which often have red bean, egg yolk, five kernel, or lotus seed. Other ways include making colorful lanterns,  enjoying fire dragon dances, and worshiping the moon. These ways of celebration are believed to bring good luck.</p>
<p>In conclusion, the Mid-Autumn festival is a very important and popular festival that you don’t want to miss. It is a time for family reunion and giving gifts that both children and adults enjoy.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
  </entry>
  <entry>
    <title>Migrate docker data to different directory</title>
    <url>/blog/migrate-docker-data-to-different-directory/</url>
    <content><![CDATA[<p>This post guides us how to migrate the docker data from the existing directory to a target directory in the case that the old directory runs out of space.</p>
<h2 id="Stop-the-docker-daemon"><a href="#Stop-the-docker-daemon" class="headerlink" title="Stop the docker daemon"></a>Stop the docker daemon</h2><pre><code>$ cat /etc/centos-release
CentOS Linux release 7.9.2009 (Core)
    
$ systemctl stop docker.service
$ ps aux | grep -i docker | grep -v grep
</code></pre>
<h2 id="Add-a-configuration-file-to-tell-docker-where-is-the-new-location-of-the-data"><a href="#Add-a-configuration-file-to-tell-docker-where-is-the-new-location-of-the-data" class="headerlink" title="Add a configuration file to tell docker where is the new location of the data"></a>Add a configuration file to tell docker where is the new location of the data</h2><pre><code>$ mkdir /data/var_lib_docker/
$ vim /etc/docker/daemon.json
&#123;
    &quot;data-root&quot;: &quot;/data/var_lib_docker&quot;
&#125;
</code></pre>
<h1 id="Copy-the-docker-data-to-new-directory-It-takes-time"><a href="#Copy-the-docker-data-to-new-directory-It-takes-time" class="headerlink" title="Copy the docker data to new directory(It takes time)"></a>Copy the docker data to new directory(It takes time)</h1><pre><code>sudo rsync -aP /var/lib/docker/ /data/var_lib_docker
</code></pre>
<h1 id="Verify-if-the-migration-works"><a href="#Verify-if-the-migration-works" class="headerlink" title="Verify if the migration works"></a>Verify if the migration works</h1><pre><code>$ mv /var/lib/docker/ /var/lib/docker.old

$ sudo systemctl start docker

$ ps aux | grep -i docker | grep -v grep
root     29227  0.2  0.1 1244076 28448 ?       Ssl  10:43   0:01 /usr/bin/dockerd
root     29243  0.0  0.0 984268  7696 ?        Ssl  10:43   0:00 docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc

$ docker info  | grep Root
    Docker Root Dir: /data/var_lib_docker

$ docker images
$ docker inspect 1cd20ecd897d | grep RootDir
&quot;RootDir&quot;: &quot;/data/var_lib_docker/overlay/90021ce8266c3f717e2d30e258311e850b50e946b7f68d505f504b008378414c/root&quot;
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Min heap and max heap implementation in Python</title>
    <url>/blog/min-heap-and-max-heap-implementation-in-python/</url>
    <content><![CDATA[<h2 id="What-is-min-heap-and-max-heap"><a href="#What-is-min-heap-and-max-heap" class="headerlink" title="What is min-heap and max-heap"></a>What is min-heap and max-heap</h2><p>A heap is a data structure based on binary tree. It’s designed to efficiently access the smallest or largest element in a collection of items. It follows properties of a complete binary tree. So, it’s also known as binary heap.</p>
<p>A min-heap(max-heap) is a binary tree such that</p>
<ul>
<li>The data in each node is less(greater) than or equal to the data in node’s children.</li>
<li>The binary tree is complete. A complete binary tree means all levels are completely filled except the last level and the last level has all nodes as left as possible.</li>
</ul>
<h2 id="How-is-binary-heap-represented"><a href="#How-is-binary-heap-represented" class="headerlink" title="How is binary heap represented"></a>How is binary heap represented</h2><p>A binary heap is typically represented as an array.</p>
<ul>
<li>The root node is arr[0]</li>
<li>The parent node for the ith node is arr[(i-1)&#x2F;2]</li>
<li>the left child node for the ith node is arr[2*i+1]</li>
<li>the right child node for the ith node is arr[2*i+2]</li>
</ul>
<p>Level order traversal is used to achieve the array representation.</p>
<p>Example:</p>
<pre><code>binary tree:
              6(0)
            /      \
          4(1)     8(2)
         /  \     /
       2(3) 9(4) 7(5)
       
array: [6,4,8,2,9,7]
</code></pre>
<h2 id="Application-of-heap"><a href="#Application-of-heap" class="headerlink" title="Application of heap"></a>Application of heap</h2><p>The heap is broadly used to solve the following problems. We will learn more in future posts. Here we will be focus on the heap implementation.</p>
<ul>
<li>Heap sort in O(nlogn) time</li>
<li>Priority queue</li>
<li>Graph algorithms</li>
<li>Many other problems</li>
</ul>
<h2 id="What-is-heapify"><a href="#What-is-heapify" class="headerlink" title="What is heapify"></a>What is heapify</h2><p>Heapify is the process to build the heap data structure using binary tree.</p>
<p>We start the process from the first index of non-leaf node.</p>
<pre><code>     6             6             6             2             2
   /   \         /   \         /   \         /   \         /   \
  4     8  --&gt;  4     7  --&gt;  2     7  --&gt;  6     7  --&gt;  4     7
 / \   /       / \   /       / \   /       / \   /       / \   /
2   9 7       2   9 8       4   9 8       4   9 8       6   9 8
</code></pre>
<h2 id="Implementaion"><a href="#Implementaion" class="headerlink" title="Implementaion"></a>Implementaion</h2><pre><code>def left_child(mylist, k):
    return 2 * k + 1


def right_child(mylist, k):
    return 2 * k + 2


def is_leaf(mylist, k):
    return 2 * k &gt;= len(mylist)


def heapify(mylist, k):
    if is_leaf(mylist, k):
        return

    n = len(mylist)

    # get left and right child index
    left = left_child(mylist, k)
    right = right_child(mylist, k)

    # find the smallest elements among current node, left and right child
    smallest = k
    if left &lt; n and mylist[k] &gt; mylist[left]:
        smallest = left
    if right &lt; n and mylist[smallest] &gt; mylist[right]:
        smallest = right

    # swap current node and the smallest child
    if smallest != k:
        mylist[k], mylist[smallest] = mylist[smallest], mylist[k]
        heapify(mylist, smallest)


def build_min_heap(mylist):
    n = len(mylist) // 2 - 1
    for i in range(n, -1, -1):
        heapify(mylist, i)


def parent(k):
    return (k - 1) // 2


def insert(mylist, val):
    mylist.append(val)

    k = len(mylist) - 1
    while k &gt; 0 and mylist[k] &lt; mylist[parent(k)]:
        mylist[k], mylist[parent(k)] = mylist[parent(k)], mylist[k]
        k = parent(k)


def remove(mylist):
    min = mylist[0]
    mylist[0] = mylist[len(mylist) - 1]
    mylist.pop(len(mylist) - 1)
    heapify(mylist, 0)
    return min


def print_heap(mylist):
    for i in range(len(mylist) // 2):
        left = 2 * i + 1
        right = 2 * i + 2
        if left &lt; len(mylist) and right &lt; len(mylist):
            print(
                f&quot;parent: &#123;mylist[i]&#125; left-child: &#123;mylist[left]&#125; right-child: &#123;mylist[right]&#125;&quot;
            )
        elif left &lt; len(mylist):
            print(f&quot;parent: &#123;mylist[i]&#125; left-child: &#123;mylist[left]&#125; right-child: None&quot;)

test_list = []
insert(test_list, 6)
insert(test_list, 4)
insert(test_list, 8)
insert(test_list, 2)
insert(test_list, 9)
insert(test_list, 7)
print(test_list)
print_heap(test_list)

test_list_1 = [6, 4, 8, 2, 9, 7]
build_min_heap(test_list_1)
print(test_list_1)
print_heap(test_list_1)

remove(test_list_1)
print(test_list_1)
print_heap(test_list_1)
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Monitor RMAN backup and restore progress</title>
    <url>/blog/monitor-rman-backup-and-restore-progress/</url>
    <content><![CDATA[<p>The following script will allow you to monitor progress of an RMAN backup or Restore from 4 different perspectives (channel, session wait events, datafiles, backuppieces). The script is run from SQLPlus and takes a date input value in the format ‘dd-mon-rr hh24:mi:ss’. The date supplied does not have to be precise and can be taken from the rman log of the job that is running.</p>
<pre><code>REM 
REM Script to monitor rman backup/restore operations
REM To run from sqlplus:   @monitor &#39;[dd-mon-rr hh24:mi:ss]&#39; 
REM Example:  
--SQL&gt;spool monitor.out
--SQL&gt;@monitor &#39;06-aug-12 16:38:03&#39;
REM where [date] is the start time of your rman backup or restore job
REM Run monitor script periodically to confirm rman is progessing
REM 

alter session set nls_date_format=&#39;dd-mon-rr hh24:mi:ss&#39;;
set lines 1500
set pages 100
col CLI_INFO format a10
col spid format a5
col ch format a20
col seconds format 999999.99
col filename format a65
col bfc  format 9
col &quot;% Complete&quot; format 999.99
col event format a40
set numwidth 10

select sysdate from dual;

REM gv$session_longops (channel level)

prompt
prompt Channel progress - gv$session_longops:
prompt
select s.inst_id, o.sid, CLIENT_INFO ch, context, sofar, totalwork,
                    round(sofar/totalwork*100,2) &quot;% Complete&quot;
     FROM gv$session_longops o, gv$session s
     WHERE opname LIKE &#39;RMAN%&#39;
     AND opname NOT LIKE &#39;%aggregate%&#39;
     AND o.sid=s.sid
     AND totalwork != 0
     AND sofar &lt;&gt; totalwork;

REM Check wait events (RMAN sessions) - this is for CURRENT waits only
REM use the following for 11G+
prompt
prompt Session progess - CURRENT wait events and time in wait so far:
prompt
select inst_id, sid, CLIENT_INFO ch, seq#, event, state, wait_time_micro/1000000 seconds
from gv$session where program like &#39;%rman%&#39; and
wait_time = 0 and
not action is null;

REM use the following for 10G
--select  inst_id, sid, CLIENT_INFO ch, seq#, event, state, seconds_in_wait secs
--from gv$session where program like &#39;%rman%&#39; and
--wait_time = 0 and
--not action is null;

REM gv$backup_async_io
prompt
prompt Disk (file and backuppiece) progress - includes tape backuppiece 
prompt if backup_tape_io_slaves=TRUE:
prompt
select s.inst_id, a.sid, CLIENT_INFO Ch, a.STATUS,
open_time, round(BYTES/1024/1024,2) &quot;SOFAR Mb&quot; , round(total_bytes/1024/1024,2)
TotMb, io_count,
round(BYTES/TOTAL_BYTES*100,2) &quot;% Complete&quot; , a.type, filename
from gv$backup_async_io a,  gv$session s
where not a.STATUS in (&#39;UNKNOWN&#39;)
and a.sid=s.sid and open_time &gt; to_date(&#39;&amp;1&#39;, &#39;dd-mon-rr hh24:mi:ss&#39;) order by 2,7;

REM gv$backup_sync_io
prompt
prompt Tape backuppiece progress (only if backup_tape_io_slaves=FALSE):
prompt
select s.inst_id, a.sid, CLIENT_INFO Ch, filename, a.type, a.status, buffer_size bsz, buffer_count bfc,
open_time open, io_count
from gv$backup_sync_io a, gv$session s
where
a.sid=s.sid and
open_time &gt; to_date(&#39;&amp;1&#39;, &#39;dd-mon-rr hh24:mi:ss&#39;) ;
REM 
</code></pre>
<p>From RMAN log, you can see the follwoing timestamp.</p>
<p><em>Starting restore at 16-DEC-2019 12:38:04</em></p>
<p>You can run this script at any time if you suspect that rman is taking longer than expected - simply spool the results to a file and rerun the script periodically to check that the job is progressing.</p>
<p>Sample output:</p>
<pre><code>$ sqlplus / as sysdba

SQL*Plus: Release 12.2.0.1.0 Production on Mon Dec 16 14:08:34 2019
Copyright (c) 1982, 2016, Oracle.  All rights reserved.
Connected to:
Oracle Database 12c Enterprise Edition Release 12.2.0.1.0 - 64bit Production

SQL&gt; @monitor &quot;16-DEC-2019 12:38:04&quot;
Session altered.


SYSDATE
16-dec-19 14:08:45


Channel progress - gv$session_longops:
no rows selected

Session progess - CURRENT wait events and time in wait so far:

   INST_ID	  SID CH			 SEQ# EVENT				       STATE		      SECONDS

     1	  217				35823 SQL*Net message from client	       WAITING		       115.75
     1	  225 rman channel=ch00 	 3647 Backup: MML restore backup piece	       WAITING		       115.94
     1	  229 rman channel=ch01 	32765 Backup: MML restore backup piece	       WAITING		       115.88
     1	  233 rman channel=ch02 	51706 Backup: MML restore backup piece	       WAITING		       115.81
     1	  237 rman channel=ch03 	62302 Backup: MML restore backup piece	       WAITING		       115.75

Disk (file and backuppiece) progress - includes tape backuppiece
if backup_tape_io_slaves=TRUE:

old   7: and a.sid=s.sid and open_time &gt; to_date(&#39;&amp;1&#39;, &#39;dd-mon-rr hh24:mi:ss&#39;) order by 2,7
new   7: and a.sid=s.sid and open_time &gt; to_date(&#39;16-DEC-2019 12:38:04&#39;, &#39;dd-mon-rr hh24:mi:ss&#39;) order by 2,7

   INST_ID	  SID CH		   STATUS      OPEN_TIME	    SOFAR Mb	  TOTMB   IO_COUNT % Complete TYPE	FILENAME

     1	  225 rman channel=ch00    FINISHED    16-dec-19 13:03:03	  31	     31 	 9     100.00 OUTPUT	+DATA/tpcc/iitem_0_0
     1	  225 rman channel=ch00    FINISHED    16-dec-19 12:47:56      31900	  31900       7977     100.00 OUTPUT	+DATA/tpcc/hist_0_0
     1	  229 rman channel=ch01    FINISHED    16-dec-19 12:57:30      30600	  30600       7652     100.00 OUTPUT	+DATA/tpcc/stok_0_6
     1	  229 rman channel=ch01    FINISHED    16-dec-19 12:53:37      30890	  30890       7724     100.00 OUTPUT	+DATA/tpcc/ordr_0_9
     1	  229 rman channel=ch01    FINISHED    16-dec-19 12:50:21      31280	  31280       7822     100.00 OUTPUT	+DATA/tpcc/cust_0_6
     1	  233 rman channel=ch02    FINISHED    16-dec-19 13:02:39	  90	     90 	24     100.00 OUTPUT	+DATA/tpcc/ware_0_0
     1	  233 rman channel=ch02    FINISHED    16-dec-19 13:01:49	 933	    933        235     100.00 OUTPUT	+DATA/tpcc/tpccaux
     1	  233 rman channel=ch02    FINISHED    16-dec-19 12:58:16      30600	  30600       7652     100.00 OUTPUT	+DATA/tpcc/stok_0_11
     1	  233 rman channel=ch02    FINISHED    16-dec-19 12:55:07      30890	  30890       7724     100.00 OUTPUT	+DATA/tpcc/ordr_0_14
     1	  233 rman channel=ch02    FINISHED    16-dec-19 12:51:07      30890	  30890       7724     100.00 OUTPUT	+DATA/tpcc/ordr_0_4
     1	  237 rman channel=ch03    FINISHED    16-dec-19 13:02:48      33.21	  33.21 	10     100.00 OUTPUT	+DATA/tpcc/iware_0_0
     1	  237 rman channel=ch03    FINISHED    16-dec-19 13:01:59	 400	    400        102     100.00 OUTPUT	+DATA/tpcc/system_1
     1	  237 rman channel=ch03    FINISHED    16-dec-19 13:00:34    7490.45	7490.45       1874     100.00 OUTPUT	+DATA/tpcc/icust1_0_0
     1	  237 rman channel=ch03    FINISHED    16-dec-19 12:55:15      30600	  30600       7652     100.00 OUTPUT	+DATA/tpcc/stok_0_0
     1	  237 rman channel=ch03    FINISHED    16-dec-19 12:53:02      30890	  30890       7724     100.00 OUTPUT	+DATA/tpcc/ordr_0_8
     1	  237 rman channel=ch03    FINISHED    16-dec-19 12:48:06      31280	  31280       7822     100.00 OUTPUT	+DATA/tpcc/cust_0_1

16 rows selected.

Tape backuppiece progress (only if backup_tape_io_slaves=FALSE):

old   6: open_time &gt; to_date(&#39;&amp;1&#39;, &#39;dd-mon-rr hh24:mi:ss&#39;)
new   6: open_time &gt; to_date(&#39;16-DEC-2019 12:38:04&#39;, &#39;dd-mon-rr hh24:mi:ss&#39;)

   INST_ID	  SID CH		   FILENAME							     TYPE      STATUS		  BSZ BFC OPEN		       IO_COUNT

     1	  225 rman channel=ch00    bk_dTPCC_ucuuj5dpp_s414_p1_t1026733881			     INPUT     FINISHED        262144	4 16-dec-19 12:47:56	  65392
     1	  225 rman channel=ch00    bk_dTPCC_uehuj5udh_s465_p1_t1026750897			     INPUT     FINISHED        262144	4 16-dec-19 13:03:03	      8
     1	  229 rman channel=ch01    bk_dTPCC_ud7uj5i8g_s423_p1_t1026738448			     INPUT     FINISHED        262144	4 16-dec-19 12:50:21	 116172
     1	  229 rman channel=ch01    bk_dTPCC_udtuj5q1d_s445_p1_t1026746413			     INPUT     FINISHED        262144	4 16-dec-19 12:57:30	 113036
     1	  229 rman channel=ch01    bk_dTPCC_udhuj5l9e_s433_p1_t1026741550			     INPUT     FINISHED        262144	4 16-dec-19 12:53:37	  63372
     1	  233 rman channel=ch02    bk_dTPCC_ueeuj5u99_s462_p1_t1026750761			     INPUT     FINISHED        262144	4 16-dec-19 13:02:39	    312
     1	  233 rman channel=ch02    bk_dTPCC_udmuj5mm0_s438_p1_t1026742976			     INPUT     FINISHED        262144	4 16-dec-19 12:55:07	  63776
     1	  233 rman channel=ch02    bk_dTPCC_ue0uj5rik_s448_p1_t1026747988			     INPUT     FINISHED        262144	4 16-dec-19 12:58:16	 113444
     1	  233 rman channel=ch02    bk_dTPCC_ueauj5u3a_s458_p1_t1026750570			     INPUT     FINISHED        262144	4 16-dec-19 13:01:49	   3220
     1	  233 rman channel=ch02    bk_dTPCC_udauj5jb1_s426_p1_t1026739553			     INPUT     FINISHED        262144	4 16-dec-19 12:51:07	  64180
     1	  237 rman channel=ch03    bk_dTPCC_ueguj5uc3_s464_p1_t1026750851			     INPUT     FINISHED        262144	4 16-dec-19 13:02:48	      0
     1	  237 rman channel=ch03    bk_dTPCC_uecuj5u6f_s460_p1_t1026750671			     INPUT     FINISHED        262144	4 16-dec-19 13:01:59	   1184
     1	  237 rman channel=ch03    bk_dTPCC_ue7uj5trl_s455_p1_t1026750325			     INPUT     FINISHED        262144	4 16-dec-19 13:00:34	  28188
     1	  237 rman channel=ch03    bk_dTPCC_udnuj5mut_s439_p1_t1026743261			     INPUT     FINISHED        262144	4 16-dec-19 12:55:15	 113444
     1	  237 rman channel=ch03    bk_dTPCC_udfuj5knj_s431_p1_t1026740979			     INPUT     FINISHED        262144	4 16-dec-19 12:53:02	  63372
     1	  237 rman channel=ch03    bk_dTPCC_ud1uj5f5c_s417_p1_t1026735276			     INPUT     FINISHED        262144	4 16-dec-19 12:48:06	 115768

16 rows selected.
</code></pre>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://www.thegeekdiary.com/script-to-monitor-rman-backup-and-restore-operations/">https://www.thegeekdiary.com/script-to-monitor-rman-backup-and-restore-operations/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Backup Recovery</tag>
      </tags>
  </entry>
  <entry>
    <title>Mulan</title>
    <url>/blog/mulan/</url>
    <content><![CDATA[<p><img src="/images/mulan.jpeg"></p>
<p>In the Northern and Southern Dynasties of china, there lived a heroine named Hua Mulan. Mulan was born in a small village, and she lived with her veteran father, who served in war for the emperor, and her kind hearted mother. Mulan also had an nice little sister. From a very young age, Mulan developed a great interest in the life of a warrior. In his spare time, Mulan’s father trained her in fighting skills at the riverside, everything from riding a horse to shooting an arrow, from swinging a sword to wielding a staff.</p>
<p>One day, in the small village, an shocking event happened. Boai Khan’s army was invading the emperor’s land once again. Guards of the emperor said that one man from every family must volunteer to fight in the war. Mulan knew what this meant. Father was the only man in the family, so he had to join the army. But if he joined and fought, he may never come back. Mulan knew what was necessary. She would take her father’s place in the military. She took his armor, sword, and horse. At dawn, she rode to the military camp.</p>
<p>During her training, Mulan overcame many troubles. She didn’t want to shower with men, so she offered to be on night guard duty. When she wanted to take a bath, she would do it at the lake. But it also made her a tough warrior. Her fighting skills surpassed any men. When the soldiers were forced to carry two buckets of water up a mountain, Mulan was the first to reach the peak.</p>
<p>After a few months of training, the commander said that it was time to enter the war. They were losing the moment they started. Khan’s army even had fireballs. While the rest of the army struggled to defend themselves, mulan targeted on Khan. When Khan saw Mulan coming, he hopped on a horse and ran away. Mulan darted after him but was soon lost in a canyon. Suddenly, the witch came and shot an arrow at Mulan. Mulan died for a lie can only live so long. But the chi inside her wasn’t held back. Like a phoenix, Mulan rebirthed. She revealed her true identity. When she went back, she grabbed a few helmets with her. She put the helmets on a rock so they looked like soldiers. She hid behind the rock and shot an arrow at the enemy. The enemy saw the fake soldiers and launched a fireball at them. This fireball caused a snowstorm down from the mountain. Though Mulan saved her general and the rest of the army, the leader thought she was an imposter. But when Mulan said that Khan was planning a sneak attack on the emperor, her friends believed in her and made her their leader. They rushed back to the palace. Using her chi, Mulan defeated the enemy and saved the emperor. The emperor, who owed his life to Mulan, asked her what reward she wanted. Mulan said that she wanted to apologize to her parents for stealing their armor and sword. After that, she would take care of them. The emperor granted the wish after making Mulan an officer in the army.</p>
<p>People admire Mulan for her bravery, honesty, loyalty to the army, and filial piety to her parents.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Must-read books and articles for Software and Performance Engineering</title>
    <url>/blog/must-read-books-and-articles-for-software-and-performance-engineering/</url>
    <content><![CDATA[<h2 id="Books"><a href="#Books" class="headerlink" title="Books"></a>Books</h2><ul>
<li><p><a href="https://www-cs-faculty.stanford.edu/~knuth/taocp.html">The Art of Computer Programming</a></p>
</li>
<li><p>Understanding the Linux Kernel, 3rd Edition</p>
</li>
<li><p>Linux Device Drivers, 3rd Edition</p>
</li>
<li><p>Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems, 1st Edition</p>
</li>
<li><p>Building Microservices: Designing Fine-Grained Systems 1st Edition</p>
</li>
<li><p>Site Reliability Engineering: How Google Runs Production Systems 1st Editio</p>
</li>
<li><p>Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation (Addison-Wesley Signature Series (Fowler)) 1st Edition</p>
</li>
<li><p>Big Data: Principles and best practices of scalable realtime data systems</p>
</li>
<li><p><a href="https://read.amazon.com/kp/embed?asin=B0090J3SYW&amp;preview=newtab&amp;linkCode=kpe&amp;ref_=cm_sw_r_kb_dp_RT3P75JR498J1QZ0D7KG">NoSQL Distilled: A Brief Guide to the Emerging World of Polyglot Persistence</a></p>
</li>
<li><p><a href="https://www.amazon.com/Professional-NoSQL-Shashank-Tiwari/dp/047094224X/ref=rvi_sccl_4/134-3241400-7217441?pd_rd_w=ZVltv&amp;pf_rd_p=f5690a4d-f2bb-45d9-9d1b-736fee412437&amp;pf_rd_r=GD4HR5BQVYJYBDHMWEGH&amp;pd_rd_r=19ada9b3-5a1c-46d2-a51e-fe3adf0f9f2c&amp;pd_rd_wg=8IpYZ&amp;pd_rd_i=047094224X&amp;psc=1">Professional NoSQL 1st Edition</a></p>
</li>
<li><p>Kubernetes: Up and Running: Dive into the Future of Infrastructure 2nd Edition</p>
</li>
<li><p>Kubernetes in Action 1st Edition</p>
</li>
<li><p>Systems Performance, 2nd Edition</p>
</li>
<li><p>BPF Performance Tools</p>
</li>
<li><p>Understanding Software Dynamics</p>
</li>
</ul>
<h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h2><ul>
<li><a href="https://arxiv.org/abs/1707.08514">Analyzing IO Amplification in Linux File Systems</a></li>
<li><a href="https://courses.cs.duke.edu/fall13/compsci590.4/838-CloudPapers/ycsb.pdf">Benchmarking Cloud Serving Systems with YCSB</a></li>
<li><a href="https://www.researchgate.net/publication/262177144_BTRFS_The_linux_B-tree_filesystem">BTRFS: The linux B-tree filesystem</a></li>
<li><a href="https://www.usenix.org/system/files/login/articles/login_spring16_02_tarasov.pdf">Filebench: A Flexible Framework for File System Benchmarking</a></li>
<li><a href="https://sakisk.me/files/copy-on-write-based-file-systems.pdf">Copy On Write Based File Systems Performance Analysis And Implementation</a></li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf">The Google File System</a></li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf">MapReduce: Simplified Data Processing on Large Clusters</a></li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a></li>
<li><a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo: Amazon’s Highly Available Key-value Store</a></li>
<li><a href="https://patentimages.storage.googleapis.com/d0/f6/e1/2600fa22db7db0/US8190610.pdf">Google patent: MapReduce for distributed database processing</a></li>
</ul>
<h2 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs"></a>Blogs</h2><ul>
<li><a href="https://sites.google.com/site/swperfengg/">Google performance engineering</a></li>
<li><a href="https://engineering.fb.com/tag/performance/">Facebook performance engineering</a></li>
<li><a href="https://netflixtechblog.com/tagged/performance">Netflix performance engineering</a></li>
<li><a href="https://engineering.linkedin.com/blog/topic/performance">Linkedin performance engineering</a></li>
</ul>
<h2 id="Articles"><a href="#Articles" class="headerlink" title="Articles"></a>Articles</h2><ul>
<li><a href="https://engineering.linkedin.com/performance/who-moved-my-99th-percentile-latency">Who moved my 99th percentile latency?</a></li>
<li><a href="https://engineering.fb.com/2009/04/30/core-data/needle-in-a-haystack-efficient-storage-of-billions-of-photos/#:~:text=Storing%20photos%20as%20needles%20in,in%20an%20in%2Dmemory%20index.">Needle in a haystack: efficient storage of billions of photos</a></li>
</ul>
<h2 id="Benchmark-Tools"><a href="#Benchmark-Tools" class="headerlink" title="Benchmark Tools"></a>Benchmark Tools</h2><ul>
<li><a href="https://github.com/filebench/filebench">Filebench</a></li>
<li><a href="https://github.com/brianfrankcooper/YCSB/wiki">YCSB</a></li>
<li><a href="https://www.spec.org/storage2020/">SPECstorage Solution 2020</a></li>
<li><a href="https://www.postgresql.org/docs/10/pgbench.html">pgbench</a></li>
<li><a href="https://github.com/akopytov/sysbench">sysbench</a></li>
<li><a href="https://wiki.lustre.org/VDBench">vdbench</a></li>
<li><a href="https://fio.readthedocs.io/en/latest/fio_doc.html">fio</a></li>
<li><a href="https://github.com/axboe/fio">fio source</a></li>
<li><a href="https://tobert.github.io/post/2014-04-17-fio-output-explained.html">fio output explained</a></li>
<li><a href="https://www.iozone.org/">iozone</a></li>
<li><a href="https://github.com/ukontainer/sqlite-bench">sqlite-bench</a></li>
<li><a href="https://oss.oracle.com/~mason/compilebench/">Compilebench</a></li>
<li><a href="https://openbenchmarking.org/test/pts/fs-mark">FS-Mark</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bonnie%2B%2B">Bonnie++</a></li>
<li><a href="https://www.brendangregg.com/ActiveBenchmarking/bonnie++.html">Active Benchmarking: Bonnie++</a></li>
</ul>
<h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><ul>
<li><a href="https://www.kernel.org/doc/html/latest/filesystems/index.html">Filesystems in the Linux kernel</a></li>
<li><a href="https://btrfs.wiki.kernel.org/index.php/Main_Page">Btrfs Wiki</a></li>
<li><a href="https://btrfs.wiki.kernel.org/index.php/Btrfs_design">Btrfs design</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
  </entry>
  <entry>
    <title>Network bonding</title>
    <url>/blog/network-bonding/</url>
    <content><![CDATA[<p>Network bonding enables the combination of two or more network interfaces into a single-bonded (logical) interface, which increases the bandwidth and provides redundancy. If a specific network interface card (NIC) experiences a problem, communications are not affected significantly as long as the other slave NICs remain active.</p>
<h2 id="Bonding-modes-supported-by-RHEL-and-CentOS-operating-systems"><a href="#Bonding-modes-supported-by-RHEL-and-CentOS-operating-systems" class="headerlink" title="Bonding modes supported by RHEL and CentOS operating systems"></a>Bonding modes supported by RHEL and CentOS operating systems</h2><p>The behavior of the bonded interfaces depends on the mode that is selected. RHEL supports the following common bonding modes:</p>
<ul>
<li>Mode 0 (balance-rr): This mode is also known as round-robin mode. Packets are sequentially transmitted and received through each interface one by one. This mode provides load balancing functionality.</li>
<li>Mode 1 (active-backup): This mode has only one interface set to active, while all other interfaces are in the backup state. If the active interface fails, a backup interface replaces it as the only active interface in the bond. The media access control (MAC) address of the bond interface in mode 1 is visible on only one port (the network adapter), which prevents confusion for the switch. Mode 1 provides fault tolerance.</li>
<li>Mode 2 (balance-xor): The source MAC address uses exclusive or (XOR) logic with the destination MAC address. This calculation ensures that the same slave interface is selected for each destination MAC address. Mode 2 provides fault tolerance and load balancing.</li>
<li>Mode 3 (broadcast): All transmissions are sent to all the slaves. This mode provides fault tolerance.</li>
<li>Mode 4 (802.3ad): This mode creates aggregation groups that share the same speed and duplex settings, and it requires a switch that supports an IEEE 802.3ad dynamic link. Mode 4 uses all interfaces in the active aggregation group. For example, you can aggregate three 1 GB per second (GBPS) ports into a 3 GBPS trunk port. This is equivalent to having one interface with 3 GBPS speed. It provides fault tolerance and load balancing.</li>
<li>Mode 5 (balance-tlb): This mode ensures that the outgoing traffic distribution is set according to the load on each interface and that the current interface receives all the incoming traffic. If the assigned interface fails to receive traffic, another interface is assigned to the receiving role. It provides fault tolerance and load balancing.</li>
<li>Mode 6 (balance-alb): This mode is supported only in x86 environments. The receiving packets are load balanced through Address Resolution Protocol (ARP) negotiation. This mode provides fault tolerance and load balancing.</li>
</ul>
<h2 id="IEEE-802-3ad-Link-Aggregation-Policy-and-LACP"><a href="#IEEE-802-3ad-Link-Aggregation-Policy-and-LACP" class="headerlink" title="IEEE 802.3ad Link Aggregation Policy and LACP"></a>IEEE 802.3ad Link Aggregation Policy and LACP</h2><p>Before we explore LACP configuration, we should understand the IEEE 802.3ad link aggregation policy and LACP bonding, which allows us to aggregate multiple ports into a single group. This process combines the bandwidth into a single connection.</p>
<p>IEEE 802.3ad link aggregation enables us to group Ethernet interfaces at the physical layer to form a single link layer interface, also known as a link aggregation group (LAG) or bundle.</p>
<p>Some users require more bandwidth in their network than a single fast Ethernet link can provide. Using IEEE 802.3ad link aggregation in this situation provides increased port density and bandwidth at a lower cost.</p>
<p>For example, if you need 2 GBPS bandwidth to transmit data and have only 1 GBPS Fast Ethernet links installed on your system, creating a LAG bundle containing two 1 GBPS Fast Ethernet links is more cost-effective than purchasing a single 2 GBPS Ethernet link.</p>
<p>The following diagram illustrates the IEEE 802.3ad link aggregation policy:</p>
<p><img src="/images/lacp.png" alt="Image"></p>
<p>LACP is a mechanism for exchanging port and system information to create and maintain LAG bundles. The LAG bundle distributes MAC clients across the link layer interface and collects traffic from the links to present to the MAC clients of the LAG bundle.</p>
<p>LACP identifies the MAC address of the Ethernet link that has the highest port priority and is of the lowest value, and it assigns that MAC address to the LAG bundle.</p>
<p>This bonding mode requires a switch that supports IEEE 802.3ad dynamic links.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title>Network concepts you should know</title>
    <url>/blog/network-concepts-you-should-know/</url>
    <content><![CDATA[<h2 id="IP-address"><a href="#IP-address" class="headerlink" title="IP address"></a>IP address</h2><p>Networking is about one computer sending a message to another computer. This message is called packet in the IP world. It is just like a postcard in the postal service. The postal service can take a postcard addressed to someone and deliver it. Similarly, it needs to provide a network address when a computer to ask the network to deliver a packet to another computer. The network address is called IP address.</p>
<p>An Internet Protocol address is a numerical label such as 192.0.2.1 that is connected to a computer network that uses the Internet Protocol for communication. An IP address serves two main functions: network interface identification and location addressing. Internet Protocol version 4 (IPv4) defines an IP address as a 32-bit number. However, because of the growth of the Internet and the depletion of available IPv4 addresses, a new version of IP (IPv6) uses 128 bits for the IP address. <a href="https://en.wikipedia.org/wiki/IP_address">Source</a></p>
<h2 id="Network-Routing"><a href="#Network-Routing" class="headerlink" title="Network Routing"></a>Network Routing</h2><p>In order for a souce computer to deliver the addressed packet to the destination computer, it needs to understand the IP address similar to how the postal service understands a mailing address. For example, if the destination address is in the same postal code as sender’s, the mail probably never leaves the same neighbourhood. But if the destination mailing address is in a different postal code, the postal service has to leverage some mechanism to get to the destination. Network routing is very similar. If a packet is sent from 192.168.0.2 to 192.168.0.5, it is likely the two computers are close to each other and the routing is simple, probably just a single hop to get to the target computer. However, if a packet is sent from 192.168.1.2 to 7.7.0.5, it probably requires many hops across complicated network.</p>
<h2 id="Network-Port"><a href="#Network-Port" class="headerlink" title="Network Port"></a>Network Port</h2><p>On a postcard, the name of recipient is usually written. It is because there might be more than one person living at a particular mailing address. In the computer world, there may be many processes on a computer to use the network. The packet can only be sent to the target computer if all we have is a network address. There is no way to decide which process should receive the packet. This is solved by the use of a network port. It’s usually represented with the form of <em>ip:port</em>, for example, 7.7.0.5:80.</p>
<h2 id="Transmission-Control-Protocol-TCP"><a href="#Transmission-Control-Protocol-TCP" class="headerlink" title="Transmission Control Protocol(TCP)"></a>Transmission Control Protocol(TCP)</h2><p>TCP is a layer on top of the Internet Protocol(IP) but they are often used together as “TCP&#x2F;IP”.</p>
<p>Similar to the postal service, networking protocols also set an upper limit to how many bytes can be sent in a single IP packet. TCP can establish a connection to a particular port on the target computer and the desired data can be sent with multiple packets while each packet has a upper limit. TCP will ensure all the data arrives at the destination in the proper order by chopping it up into individually numbered IP packets. In the case of packet loss in transit, it can be resent. TCP also has mechanisms to notice if packets are not getting through for an extended period of time and notifying you of a “broken” connection.</p>
<p>TCP provides reliable, ordered, and error-checked delivery of a stream of octets (bytes) between applications running on hosts communicating via an IP network. Major internet applications such as the World Wide Web, email, remote administration, and file transfer rely on TCP, which is part of the Transport Layer of the TCP&#x2F;IP suite. SSL&#x2F;TLS often runs on top of TCP. TCP is connection-oriented, and a connection between client and server is established before data can be sent. The server must be listening (passive open) for connection requests from clients before a connection is established. Three-way handshake (active open), retransmission, and error detection adds to reliability but lengthens latency. Applications that do not require reliable data stream service may use the User Datagram Protocol (UDP), which provides a connectionless datagram service that prioritizes time over reliability. TCP employs network congestion avoidance. However, there are vulnerabilities to TCP, including denial of service, connection hijacking, TCP veto, and reset attack. <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">Source</a></p>
<h2 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h2><p>In computer networking, the User Datagram Protocol (UDP) is one of the core members of the Internet protocol suite. With UDP, computer applications can send messages, in this case referred to as datagrams, to other hosts on an Internet Protocol (IP) network. Prior communications are not required in order to set up communication channels or data paths. UDP uses a simple connectionless communication model with a minimum of protocol mechanisms. UDP provides checksums for data integrity, and port numbers for addressing different functions at the source and destination of the datagram. It has no handshaking dialogues, and thus exposes the user’s program to any unreliability of the underlying network; there is no guarantee of delivery, ordering, or duplicate protection. If error-correction facilities are needed at the network interface level, an application may instead use Transmission Control Protocol (TCP) or Stream Control Transmission Protocol (SCTP) which are designed for this purpose. UDP is suitable for purposes where error checking and correction are either not necessary or are performed in the application; UDP avoids the overhead of such processing in the protocol stack. Time-sensitive applications often use UDP because dropping packets is preferable to waiting for packets delayed due to retransmission, which may not be an option in a real-time system. The protocol was designed by David P. Reed in 1980 and formally defined in RFC 768. <a href="https://en.wikipedia.org/wiki/User_Datagram_Protocol">Source</a></p>
<h2 id="Domain-Name-System-DNS"><a href="#Domain-Name-System-DNS" class="headerlink" title="Domain Name System(DNS)"></a>Domain Name System(DNS)</h2><p>The destination computer can be addressed with IP in network. However, it’s hard to remember the IP address directly for a user. When the computer and network are upgraded over time, it’s likely to assign a different IP address to a computer. The same destination computer would be unaccessible with the old IP address. Thus, a “phone book” called Domain Name System(DNS) is used to solve these issues.</p>
<p>The <strong>dig</strong> utility can be used to query DNS. In the follow example, the domain name <em>google.com</em> is eventually translated to the IP address <em>142.250.191.46</em>.</p>
<pre><code>$ dig google.com
[..]
;; QUESTION SECTION:
;google.com.			IN	A

;; ANSWER SECTION:
google.com.		222	IN	A	142.250.191.46
[..]
</code></pre>
<h2 id="Hypertext-Transfer-Protocol-HTTP"><a href="#Hypertext-Transfer-Protocol-HTTP" class="headerlink" title="Hypertext Transfer Protocol(HTTP)"></a>Hypertext Transfer Protocol(HTTP)</h2><p>The Hypertext Transfer Protocol is an application layer protocol in the Internet protocol suite model for distributed, collaborative, hypermedia information systems. HTTP is the foundation of data communication for the World Wide Web, where hypertext documents include hyperlinks to other resources that the user can easily access, for example by a mouse click or by tapping the screen in a web browser. <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol">Source</a></p>
<p>When we type a web address, such as <em><a href="http://example.com/hello">http://example.com/hello</a></em>, into a web browser. Firstly, the browser consults DNS to translate the domain name into a IP address. Then, the web browser establishes a TCP connection to the translated IP address of the web server on port 80 which is the default “well known” port for the HTTP. Once a connection is established, the web browser send a GET message to the server to ask for a particular resource. The server will reply with an OK message and the requested content.</p>
<p><strong>curl</strong> is a commonly used CLI tool that you can use to issue GET requests.</p>
<pre><code>$ curl -v http://7.7.0.2/hello
* About to connect() to 7.7.0.2 port 80 (#0)
*   Trying 7.7.0.2...
* Connected to 7.7.0.2 (7.7.0.2) port 80 (#0)
&gt; GET /hello HTTP/1.1
&gt; User-Agent: curl/7.29.0
&gt; Host: 7.7.0.2
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 200 OK
&lt; Server: nginx/1.17.10
&lt; Date: Sun, 13 Mar 2022 01:50:19 GMT
&lt; Content-Type: text/html; charset=utf-8
&lt; Content-Length: 15
&lt; Connection: keep-alive
&lt; 
* Connection #0 to host 7.7.0.2 left intact
Hello world
</code></pre>
<p>Here we use curl utility to issue a simple GET request. In this case, we don’t have DNS setup. We issue the GET request to the target server with IP address directly. It establishes a TCP connection on default port 80 of that address. Then it sends a GET message for the resource <em>&#x2F;hello</em>. Finally, a response of <em>200 OK</em> is received, along with the content <em>Hello world</em>.</p>
<p>In the example output, there are additional lines in both the request and response which look like <em>Name: Value</em>. These are called <strong>headers</strong> which convey additional information about the request and response. For example, the request contains header Accept: <em>&#x2F;</em> which means the client can accept the response in any format. In the response, we see the header <em>Content-Type: text&#x2F;html; charset&#x3D;utf-8</em>, which is the server telling the client the body of response is just a text.</p>
<h2 id="OSI-layer-architecture"><a href="#OSI-layer-architecture" class="headerlink" title="OSI layer architecture"></a>OSI layer architecture</h2><p><img src="/images/osi-layer-arch.png" alt="Image"></p>
<p><a href="https://en.wikipedia.org/wiki/OSI_model">Source</a></p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title>Network ring buffer</title>
    <url>/blog/network-ring-buffer/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Receive ring buffers are shared between the device driver and NIC. The card assigns a transmit (TX) and receive (RX) ring buffer. As the name implies, the ring buffer is a circular buffer where an overflow simply overwrites existing data. It should be noted that there are two ways to move data from the NIC to the kernel, hardware interrupts and software interrupts, also called SoftIRQs.</p>
<p>The RX ring buffer is used to store incoming packets until they can be processed by the device driver. The device driver drains the RX ring, typically via SoftIRQs, which puts the incoming packets into a kernel data structure called an sk_buff or “skb” to begin its journey through the kernel and up to the application which owns the relevant socket. The TX ring buffer is used to hold outgoing packets which are destined for the wire.</p>
<p>These ring buffers reside at the bottom of the stack and are a crucial point at which packet drop can occur, which in turn will adversely affect network performance.</p>
<p>You can increase the size of the Ethernet device RX ring buffer if the packet drop rate causes applications to report:</p>
<ul>
<li>a loss of data</li>
<li>cluster fence</li>
<li>slow performance</li>
<li>timeouts</li>
<li>failed backups</li>
</ul>
<h2 id="Interrupts-and-Interrupt-Handlers"><a href="#Interrupts-and-Interrupt-Handlers" class="headerlink" title="Interrupts and Interrupt Handlers"></a>Interrupts and Interrupt Handlers</h2><p>Interrupts from the hardware are known as “top-half” interrupts. When a NIC receives incoming data, it copies the data into kernel buffers using DMA. The NIC notifies the kernel of this data by raising a hard interrupt. These interrupts are processed by interrupt handlers which do minimal work, as they have already interrupted another task and cannot be interrupted themselves. Hard interrupts can be expensive in terms of CPU usage, especially when holding kernel locks. The hard interrupt handler then leaves the majority of packet reception to a software interrupt, or SoftIRQ, process which can be scheduled more fairly.</p>
<p>Hard interrupts can be seen in &#x2F;proc&#x2F;interrupts where each queue has an interrupt vector in the 1st column assigned to it. These are initialized when the system boots or when the NIC device driver module is loaded. Each RX and TX queue is assigned a unique vector, which informs the interrupt handler as to which NIC&#x2F;queue the interrupt is coming from. The columns represent the number of incoming interrupts as a counter value:</p>
<pre><code>$ egrep “CPU0|eth2” /proc/interrupts
 CPU0 CPU1 CPU2 CPU3 CPU4 CPU5
 105: 141606 0 0 0 0 0 IR-PCI-MSI-edge eth2-rx-0
 106: 0 141091 0 0 0 0 IR-PCI-MSI-edge eth2-rx-1
 107: 2 0 163785 0 0 0 IR-PCI-MSI-edge eth2-rx-2
 108: 3 0 0 194370 0 0 IR-PCI-MSI-edge eth2-rx-3
 109: 0 0 0 0 0 0 IR-PCI-MSI-edge eth2-tx
</code></pre>
<h2 id="SoftIRQs"><a href="#SoftIRQs" class="headerlink" title="SoftIRQs"></a>SoftIRQs</h2><p>Also known as “bottom-half” interrupts, software interrupt requests (SoftIRQs) are kernel routines which are scheduled to run at a time when other tasks will not be interrupted. The SoftIRQ’s purpose is to drain the network adapter receive ring buffers. These routines run in the form of ksoftirqd&#x2F;cpu-number processes and call driver-specific code functions. They can be seen in process monitoring tools such as ps and top.</p>
<p>The following call stack, read from the bottom up, is an example of a SoftIRQ polling a Mellanox card. The functions marked [mlx4_en] are the Mellanox polling routines in the mlx4_en.ko driver kernel module, called by the kernel’s generic polling routines such as net_rx_action. After moving from the driver to the kernel, the traffic being received will then move up to the socket, ready for the application to consume:</p>
<pre><code> mlx4_en_complete_rx_desc [mlx4_en]
 mlx4_en_process_rx_cq [mlx4_en]
 mlx4_en_poll_rx_cq [mlx4_en]
 net_rx_action
 __do_softirq
 run_ksoftirqd
 smpboot_thread_fn
 kthread
 kernel_thread_starter
 kernel_thread_starter
 1 lock held by ksoftirqd
</code></pre>
<p>SoftIRQs can be monitored as follows. Each column represents a CPU:</p>
<pre><code>$ watch -n1 grep RX /proc/softirqs
$ watch -n1 grep TX /proc/softirqs
</code></pre>
<h2 id="Displaying-the-number-of-dropped-packets"><a href="#Displaying-the-number-of-dropped-packets" class="headerlink" title="Displaying the number of dropped packets"></a>Displaying the number of dropped packets</h2><p>The ethtool utility enables administrators to query, configure, or control network driver settings.</p>
<p>The exhaustion of the RX ring buffer causes an increment in the counters, such as “discard” or “drop” in the output of ethtool -S interface_name. The discarded packets indicate that the available buffer is filling up faster than the kernel can process the packets.</p>
<p>To display drop counters for the enp1s0 interface, enter:</p>
<pre><code>$ ethtool -S enp1s0
</code></pre>
<h2 id="Increasing-the-RX-ring-buffer-to-reduce-a-high-packet-drop-rate"><a href="#Increasing-the-RX-ring-buffer-to-reduce-a-high-packet-drop-rate" class="headerlink" title="Increasing the RX ring buffer to reduce a high packet drop rate"></a>Increasing the RX ring buffer to reduce a high packet drop rate</h2><p>The ethtool utility helps to increase the RX buffer to reduce a high packet drop rate.</p>
<ol>
<li><p>To view the maximum RX ring buffer size:</p>
<p> $ ethtool -g nic0<br> Ring parameters for nic0:<br> Pre-set maximums:<br> RX:             4078<br> RX Mini:        0<br> RX Jumbo:       0<br> TX:             4078<br> Current hardware settings:<br> RX:             2048<br> RX Mini:        0<br> RX Jumbo:       0<br> TX:             2048</p>
</li>
<li><p>If the values in the Pre-set maximums section are higher than in the Current hardware settings section, increase RX ring buffer:</p>
</li>
</ol>
<ul>
<li><p>To temporary change the RX ring buffer of the nic0 device to 4078, enter:</p>
<p>  $ ethtool -G nic0 rx 4078</p>
</li>
<li><p>To permanently change the RX ring buffer create a NetworkManager dispatcher script.</p>
</li>
</ul>
<h2 id="Understanding-the-maximum-RX-TX-ring-buffer"><a href="#Understanding-the-maximum-RX-TX-ring-buffer" class="headerlink" title="Understanding the maximum RX&#x2F;TX ring buffer"></a>Understanding the maximum RX&#x2F;TX ring buffer</h2><p>From <a href="https://elixir.bootlin.com/linux/latest/source/include/linux/ethtool.h">ethtool</a> source code, we can find the following function which is used by command “ethtool -g [nic]”</p>
<pre><code>* @get_ringparam: Report ring sizes
</code></pre>
<p>For different NIC vender, the <a href="https://elixir.bootlin.com/linux/latest/C/ident/get_ringparam">driver</a> may be implemented differently.</p>
<p>In this example, the NIC driver is bnx2x.</p>
<pre><code>$ ethtool -i nic0
driver: bnx2x
</code></pre>
<p>So, we can check the <a href="https://elixir.bootlin.com/linux/latest/source/drivers/net/ethernet/broadcom/bnx2x/bnx2x_ethtool.c#L3677">source code</a> as below.</p>
<pre><code>static void bnx2x_get_ringparam(struct net_device *dev,
                struct ethtool_ringparam *ering)
&#123;
    struct bnx2x *bp = netdev_priv(dev);

    ering-&gt;rx_max_pending = MAX_RX_AVAIL;

    /* If size isn&#39;t already set, we give an estimation of the number
     * of buffers we&#39;ll have. We&#39;re neglecting some possible conditions
     * [we couldn&#39;t know for certain at this point if number of queues
     * might shrink] but the number would be correct for the likely
     * scenario.
     */
    if (bp-&gt;rx_ring_size)
        ering-&gt;rx_pending = bp-&gt;rx_ring_size;
    else if (BNX2X_NUM_RX_QUEUES(bp))
        ering-&gt;rx_pending = MAX_RX_AVAIL / BNX2X_NUM_RX_QUEUES(bp);
    else
        ering-&gt;rx_pending = MAX_RX_AVAIL;

    ering-&gt;tx_max_pending = IS_MF_FCOE_AFEX(bp) ? 0 : MAX_TX_AVAIL;
    ering-&gt;tx_pending = bp-&gt;tx_ring_size;
&#125;
</code></pre>
<p>MAX_RX_AVAIL is the place to define the maximum RX ring buffer. We can further check the formula as below.</p>
<pre><code>#define MAX_RX_AVAIL		(MAX_RX_DESC_CNT * NUM_RX_RINGS - 2)

#define NUM_RX_RINGS		8

#define MAX_RX_DESC_CNT		(RX_DESC_CNT - NEXT_PAGE_RX_DESC_CNT)
#define RX_DESC_CNT		(BCM_PAGE_SIZE / sizeof(struct eth_rx_bd))
#define NEXT_PAGE_RX_DESC_CNT	2

#define BCM_PAGE_SIZE		(1 &lt;&lt; BCM_PAGE_SHIFT)
#define BCM_PAGE_SHIFT		12

/*
 * The eth Rx Buffer Descriptor
 */
struct eth_rx_bd &#123;
    __le32 addr_lo;
    __le32 addr_hi;
&#125;;
</code></pre>
<p>So, based on the formula above, we can calculate the maximum RX ring buffer as below.</p>
<pre><code>rx_max = MAX_RX_AVAIL 
       = MAX_RX_DESC_CNT * NUM_RX_RINGS - 2
       = (RX_DESC_CNT - NEXT_PAGE_RX_DESC_CNT) * NUM_RX_RINGS - 2
       = ((BCM_PAGE_SIZE / sizeof(struct eth_rx_bd)) - 2) * 8 - 2
       = (((1 &lt;&lt; BCM_PAGE_SHIFT) / sizeof(struct eth_rx_bd)) - 2) * 8 - 2
       = ((4096 / 8 ) - 2) * 8 - 2
       = 4078
</code></pre>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf">Red Hat Enterprise Linux Network Performance Tuning Guide</a></li>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/monitoring-and-tuning-the-rx-ring-buffer_configuring-and-managing-networking">MONITORING AND TUNING THE RX RING BUFFER</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenZFS - A pooled storage file system</title>
    <url>/blog/openzfs-a-pooled-storage-file-system/</url>
    <content><![CDATA[<h2 id="OpenZFS"><a href="#OpenZFS" class="headerlink" title="OpenZFS"></a>OpenZFS</h2><p>The OpenZFS project is an open source derivative of the Oracle ZFS project. OpenZFS is an outstanding storage platform that encompasses the functionality of traditional filesystems, volume managers, and more, with consistent reliability, functionality and performance across all distributions.</p>
<p><a href="https://en.wikipedia.org/wiki/OpenZFS">Source1</a><br><a href="https://openzfs.github.io/openzfs-docs/Project%20and%20Community/FAQ.html#what-is-openzfs">Source2</a></p>
<h2 id="ZFS-and-ZFS-Pooled-Storage"><a href="#ZFS-and-ZFS-Pooled-Storage" class="headerlink" title="ZFS and ZFS Pooled Storage"></a>ZFS and ZFS Pooled Storage</h2><p>The ZFS file system is a revolutionary new file system that fundamentally changes the way file systems are administered, with features and benefits not found in any other file system available today. ZFS is robust, scalable, and easy to administer.</p>
<p>ZFS uses the concept of storage pools to manage physical storage. Historically, file systems were constructed on top of a single physical device. To address multiple devices and provide for data redundancy, the concept of a volume manager was introduced to provide a representation of a single device so that file systems would not need to be modified to take advantage of multiple devices. This design added another layer of complexity and ultimately prevented certain file system advances because the file system had no control over the physical placement of data on the virtualized volumes.</p>
<p>ZFS eliminates volume management altogether. Instead of forcing you to create virtualized volumes, ZFS aggregates devices into a storage pool. The storage pool describes the physical characteristics of the storage (device layout, data redundancy, and so on) and acts as an arbitrary data store from which file systems can be created. File systems are no longer constrained to individual devices, allowing them to share disk space with all file systems in the pool. You no longer need to predetermine the size of a file system, as file systems grow automatically within the disk space allocated to the storage pool. When new storage is added, all file systems within the pool can immediately use the additional disk space without additional work. In many ways, the storage pool works similarly to a virtual memory system: When a memory DIMM is added to a system, the operating system doesn’t force you to run commands to configure the memory and assign it to individual processes. All processes on the system automatically use the additional memory.</p>
<p><a href="https://docs.oracle.com/cd/E19253-01/819-5461/zfsover-2/index.html">Source</a></p>
<h2 id="Install-ZFS-on-CentOS"><a href="#Install-ZFS-on-CentOS" class="headerlink" title="Install ZFS on CentOS"></a>Install ZFS on CentOS</h2><p>ZFS is not included by default in CentOS. We will learn how to install it on CentOS 7.9 in this post.</p>
<ol>
<li><p>Add ZFS repository</p>
<p> $ cat &#x2F;etc&#x2F;centos-release<br> CentOS Linux release 7.9.2009 (Core)</p>
<p> $ yum install <a href="https://zfsonlinux.org/epel/zfs-release.el7_9.noarch.rpm">https://zfsonlinux.org/epel/zfs-release.el7_9.noarch.rpm</a> -y</p>
</li>
<li><p>DKMS vs. kABI</p>
</li>
</ol>
<p>DKMS and kABI are the two methods ZFS module can be loaded into the kernel. We are going to use kABI since it doesn’t require kernel re-compilation in case of kernel update. We can enable it by editing the ZFS repository as below.</p>
<pre><code>$ cat /etc/yum.repos.d/zfs.repo
[zfs]
name=OpenZFS for EL7 - dkms
baseurl=http://download.zfsonlinux.org/epel/7.9/$basearch/
enabled=1
metadata_expire=7d
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-zfsonlinux

[zfs-kmod]
name=OpenZFS for EL7 - kmod
baseurl=http://download.zfsonlinux.org/epel/7.9/kmod/$basearch/
enabled=0
metadata_expire=7d
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-zfsonlinux

[zfs-source]
name=OpenZFS for EL7 - Source
baseurl=http://download.zfsonlinux.org/epel/7.9/SRPMS/
enabled=0
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-zfsonlinux

[zfs-next]
name=OpenZFS for EL7 - dkms - Next upcoming version
baseurl=http://download.zfsonlinux.org/epel-next/7.9/$basearch/
enabled=0
metadata_expire=7d
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-zfsonlinux

[zfs-testing]
name=OpenZFS for EL7 - dkms - Testing
baseurl=http://download.zfsonlinux.org/epel-testing/7.9/$basearch/
enabled=0
metadata_expire=7d
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-zfsonlinux

[zfs-testing-kmod]
name=OpenZFS for EL7 - kmod - Testing
baseurl=http://download.zfsonlinux.org/epel-testing/7.9/kmod/$basearch/
enabled=0
metadata_expire=7d
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-zfsonlinux

[zfs-testing-source]
name=OpenZFS for EL7 - Testing Source
baseurl=http://download.zfsonlinux.org/epel-testing/7.9/SRPMS/
enabled=0
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-zfsonlinux
</code></pre>
<p>We can disable DKMS and enable KABI by editing the <em>enable&#x3D;</em> in the following two sections.</p>
<pre><code>$ vim /etc/yum.repos.d/zfs.repo
[zfs]
name=OpenZFS for EL7 - dkms
baseurl=http://download.zfsonlinux.org/epel/7.9/$basearch/
enabled=0
metadata_expire=7d
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-zfsonlinux

[zfs-kmod]
name=OpenZFS for EL7 - kmod
baseurl=http://download.zfsonlinux.org/epel/7.9/kmod/$basearch/
enabled=1
metadata_expire=7d
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-zfsonlinux
</code></pre>
<ol start="3">
<li>Install ZFS</li>
</ol>
<p>Install ZFS using the following command:</p>
<pre><code>$ yum install zfs -y
[..]
Installed:
zfs.x86_64 0:2.0.7-1.el7

Dependency Installed:
kmod-zfs.x86_64 0:2.0.7-1.el7      libnvpair3.x86_64 0:2.0.7-1.el7     libuutil3.x86_64 0:2.0.7-1.el7     libzfs4.x86_64 0:2.0.7-1.el7     libzpool4.x86_64 0:2.0.7-1.el7     lm_sensors-libs.x86_64 0:3.4.0-8.20160601gitf9185e5.el7
sysstat.x86_64 0:10.1.5-19.el7
</code></pre>
<p>Reboot the system to load zfs module:</p>
<pre><code>$ reboot
$ lsmod | grep zfs
</code></pre>
<p>Use the following command to load the ZFS kernel module if it’s not loaded after reboot:</p>
<pre><code>$ modprobe zfs

$ lsmod | grep zfs
zfs                  4224878  0
zunicode              331170  1 zfs
zzstd                 460780  1 zfs
zlua                  151526  1 zfs
zcommon                94285  1 zfs
znvpair                94388  2 zfs,zcommon
zavl                   15698  1 zfs
icp                   301775  1 zfs
spl                    96750  6 icp,zfs,zavl,zzstd,zcommon,znvpair

$ modinfo zfs
filename:       /lib/modules/3.10.0-1160.11.1.el7.x86_64/weak-updates/zfs/zfs/zfs.ko
version:        2.0.7-1
license:        CDDL
author:         OpenZFS
description:    ZFS
alias:          devname:zfs
alias:          char-major-10-249
retpoline:      Y
rhelversion:    7.9
srcversion:     CDFB8A7F2D3EE43324CF460
depends:        spl,znvpair,icp,zlua,zzstd,zunicode,zcommon,zavl
vermagic:       3.10.0-1160.49.1.el7.x86_64 SMP mod_unload modversions
[..]

$ zfs version
zfs-2.0.7-1
zfs-kmod-2.0.7-1
</code></pre>
<h2 id="Manage-ZFS-storage-pool-and-file-system"><a href="#Manage-ZFS-storage-pool-and-file-system" class="headerlink" title="Manage ZFS storage pool and file system"></a>Manage ZFS storage pool and file system</h2><ul>
<li><p>Create ZFS storage pool</p>
<p>  $ zpool create zpooldemo &#x2F;dev&#x2F;sdb</p>
</li>
<li><p>Add disk to ZFS storage pool</p>
<p>  $ zpool add zpooldemo &#x2F;dev&#x2F;sdc</p>
</li>
<li><p>Check ZFS pool status</p>
<p>  $ zpool list<br>  NAME        SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT<br>  zpooldemo   119G   174K   119G        -         -     0%     0%  1.00x    ONLINE  -</p>
<p>  $ zpool status<br>  pool: zpooldemo<br>  state: ONLINE<br>  config:</p>
<pre><code>  NAME        STATE     READ WRITE CKSUM
  zpooldemo   ONLINE       0     0     0
  sdb       ONLINE       0     0     0
  sdc       ONLINE       0     0     0
</code></pre>
<p>  errors: No known data errors</p>
</li>
<li><p>Create ZFS file system</p>
</li>
</ul>
<p>When you create a pool, a ZFS file system is created and mounted automatically. The whole file system space can be used as needed.</p>
<pre><code>$ mount | egrep &quot;zfs&quot;
zpooldemo on /zpooldemo type zfs (rw,xattr,noacl)

$ df -h | egrep &quot;Filesystem|zpool&quot;
Filesystem      Size  Used Avail Use% Mounted on
zpooldemo       116G  128K  116G   1% /zpooldemo

$ touch /zpooldemo/testfile
$ ls -la /zpooldemo/
total 6
drwxr-xr-x   3 root root    4 Mar 17 22:49 .
dr-xr-xr-x. 19 root root 4096 Mar 17 22:37 ..
-rw-r--r--   1 root root    0 Mar 17 22:49 testfile
</code></pre>
<p>Within a pool, additional file systems can be created. The new create file systems allow users to manage different sets of data within the same pool.</p>
<pre><code>$ zfs create zpooldemo/zfsdemo

$ mount | egrep &quot;zfs&quot;
zpooldemo on /zpooldemo type zfs (rw,xattr,noacl)
zpooldemo/zfsdemo on /zpooldemo/zfsdemo type zfs (rw,xattr,noacl)

$ df -h | egrep &quot;Filesystem|zpool&quot;
Filesystem         Size  Used Avail Use% Mounted on
zpooldemo          116G  128K  116G   1% /zpooldemo
zpooldemo/zfsdemo  116G  128K  116G   1% /zpooldemo/zfsdemo
</code></pre>
<ul>
<li>Set ZFS file system properties</li>
</ul>
<p>The file system property can be set when the ZFS is created.</p>
<pre><code>$ zfs create -o atime=off zpooldemo/zfsdemo
$ mount | grep zfs
zpooldemo on /zpooldemo type zfs (rw,xattr,noacl)
zpooldemo/zfsdemo on /zpooldemo/zfsdemo type zfs (rw,noatime,xattr,noacl)
</code></pre>
<p>The storage pool and file system properties can be retrieved as below.</p>
<pre><code>$ zpool get all zpooldemo
NAME       PROPERTY                       VALUE                          SOURCE
zpooldemo  size                           59.5G                          -
zpooldemo  capacity                       0%                             -
zpooldemo  altroot                        -                              default
zpooldemo  health                         ONLINE                         -
zpooldemo  guid                           11167503015555961412           -
zpooldemo  version                        -                              default
zpooldemo  bootfs                         -                              default
zpooldemo  delegation                     on                             default
zpooldemo  autoreplace                    off                            default
zpooldemo  cachefile                      -                              default
zpooldemo  failmode                       wait                           default
zpooldemo  listsnapshots                  off                            default
zpooldemo  autoexpand                     off                            default
zpooldemo  dedupratio                     1.00x                          -
zpooldemo  free                           59.5G                          -
zpooldemo  allocated                      106K                           -
zpooldemo  readonly                       off                            -
zpooldemo  ashift                         0                              default
zpooldemo  comment                        -                              default
zpooldemo  expandsize                     -                              -
zpooldemo  freeing                        0                              -
zpooldemo  fragmentation                  0%                             -
zpooldemo  leaked                         0                              -
zpooldemo  multihost                      off                            default
zpooldemo  checkpoint                     -                              -
zpooldemo  load_guid                      10842965729770643306           -
zpooldemo  autotrim                       off                            default
zpooldemo  feature@async_destroy          enabled                        local
zpooldemo  feature@empty_bpobj            enabled                        local
zpooldemo  feature@lz4_compress           active                         local
zpooldemo  feature@multi_vdev_crash_dump  enabled                        local
zpooldemo  feature@spacemap_histogram     active                         local
zpooldemo  feature@enabled_txg            active                         local
zpooldemo  feature@hole_birth             active                         local
zpooldemo  feature@extensible_dataset     active                         local
zpooldemo  feature@embedded_data          active                         local
zpooldemo  feature@bookmarks              enabled                        local
zpooldemo  feature@filesystem_limits      enabled                        local
zpooldemo  feature@large_blocks           enabled                        local
zpooldemo  feature@large_dnode            enabled                        local
zpooldemo  feature@sha512                 enabled                        local
zpooldemo  feature@skein                  enabled                        local
zpooldemo  feature@edonr                  enabled                        local
zpooldemo  feature@userobj_accounting     active                         local
zpooldemo  feature@encryption             enabled                        local
zpooldemo  feature@project_quota          active                         local
zpooldemo  feature@device_removal         enabled                        local
zpooldemo  feature@obsolete_counts        enabled                        local
zpooldemo  feature@zpool_checkpoint       enabled                        local
zpooldemo  feature@spacemap_v2            active                         local
zpooldemo  feature@allocation_classes     enabled                        local
zpooldemo  feature@resilver_defer         enabled                        local
zpooldemo  feature@bookmark_v2            enabled                        local
zpooldemo  feature@redaction_bookmarks    enabled                        local
zpooldemo  feature@redacted_datasets      enabled                        local
zpooldemo  feature@bookmark_written       enabled                        local
zpooldemo  feature@log_spacemap           active                         local
zpooldemo  feature@livelist               enabled                        local
zpooldemo  feature@device_rebuild         enabled                        local
zpooldemo  feature@zstd_compress          enabled                        local

$ zfs get all zpooldemo/zfsdemo
NAME               PROPERTY              VALUE                  SOURCE
zpooldemo/zfsdemo  type                  filesystem             -
zpooldemo/zfsdemo  creation              Thu Mar 17 23:00 2022  -
zpooldemo/zfsdemo  used                  24K                    -
zpooldemo/zfsdemo  available             57.6G                  -
zpooldemo/zfsdemo  referenced            24K                    -
zpooldemo/zfsdemo  compressratio         1.00x                  -
zpooldemo/zfsdemo  mounted               yes                    -
zpooldemo/zfsdemo  quota                 none                   default
zpooldemo/zfsdemo  reservation           none                   default
zpooldemo/zfsdemo  recordsize            128K                   default
zpooldemo/zfsdemo  mountpoint            /zpooldemo/zfsdemo     default
zpooldemo/zfsdemo  sharenfs              off                    default
zpooldemo/zfsdemo  checksum              on                     default
zpooldemo/zfsdemo  compression           off                    default
zpooldemo/zfsdemo  atime                 off                    local
zpooldemo/zfsdemo  devices               on                     default
zpooldemo/zfsdemo  exec                  on                     default
zpooldemo/zfsdemo  setuid                on                     default
zpooldemo/zfsdemo  readonly              off                    default
zpooldemo/zfsdemo  zoned                 off                    default
zpooldemo/zfsdemo  snapdir               hidden                 default
zpooldemo/zfsdemo  aclmode               discard                default
zpooldemo/zfsdemo  aclinherit            restricted             default
zpooldemo/zfsdemo  createtxg             20                     -
zpooldemo/zfsdemo  canmount              on                     default
zpooldemo/zfsdemo  xattr                 on                     default
zpooldemo/zfsdemo  copies                1                      default
zpooldemo/zfsdemo  version               5                      -
zpooldemo/zfsdemo  utf8only              off                    -
zpooldemo/zfsdemo  normalization         none                   -
zpooldemo/zfsdemo  casesensitivity       sensitive              -
zpooldemo/zfsdemo  vscan                 off                    default
zpooldemo/zfsdemo  nbmand                off                    default
zpooldemo/zfsdemo  sharesmb              off                    default
zpooldemo/zfsdemo  refquota              none                   default
zpooldemo/zfsdemo  refreservation        none                   default
zpooldemo/zfsdemo  guid                  10461278007032944398   -
zpooldemo/zfsdemo  primarycache          all                    default
zpooldemo/zfsdemo  secondarycache        all                    default
zpooldemo/zfsdemo  usedbysnapshots       0B                     -
zpooldemo/zfsdemo  usedbydataset         24K                    -
zpooldemo/zfsdemo  usedbychildren        0B                     -
zpooldemo/zfsdemo  usedbyrefreservation  0B                     -
zpooldemo/zfsdemo  logbias               latency                default
zpooldemo/zfsdemo  objsetid              136                    -
zpooldemo/zfsdemo  dedup                 off                    default
zpooldemo/zfsdemo  mlslabel              none                   default
zpooldemo/zfsdemo  sync                  standard               default
zpooldemo/zfsdemo  dnodesize             legacy                 default
zpooldemo/zfsdemo  refcompressratio      1.00x                  -
zpooldemo/zfsdemo  written               24K                    -
zpooldemo/zfsdemo  logicalused           12K                    -
zpooldemo/zfsdemo  logicalreferenced     12K                    -
zpooldemo/zfsdemo  volmode               default                default
zpooldemo/zfsdemo  filesystem_limit      none                   default
zpooldemo/zfsdemo  snapshot_limit        none                   default
zpooldemo/zfsdemo  filesystem_count      none                   default
zpooldemo/zfsdemo  snapshot_count        none                   default
zpooldemo/zfsdemo  snapdev               hidden                 default
zpooldemo/zfsdemo  acltype               off                    default
zpooldemo/zfsdemo  context               none                   default
zpooldemo/zfsdemo  fscontext             none                   default
zpooldemo/zfsdemo  defcontext            none                   default
zpooldemo/zfsdemo  rootcontext           none                   default
zpooldemo/zfsdemo  relatime              off                    default
zpooldemo/zfsdemo  redundant_metadata    all                    default
zpooldemo/zfsdemo  overlay               on                     default
zpooldemo/zfsdemo  encryption            off                    default
zpooldemo/zfsdemo  keylocation           none                   default
zpooldemo/zfsdemo  keyformat             none                   default
zpooldemo/zfsdemo  pbkdf2iters           0                      default
zpooldemo/zfsdemo  special_small_blocks  0                      default
</code></pre>
<p><em>zfs set</em> command can be used to set any dataset property.</p>
<pre><code>$ zfs set checksum=off zpooldemo/zfsdemo
</code></pre>
<p><em>zfs get</em> command can be used to retrieve any dataset property.</p>
<pre><code>$ zfs get checksum zpooldemo
NAME       PROPERTY  VALUE      SOURCE
zpooldemo  checksum  on         default

$ zfs get checksum zpooldemo/zfsdemo
NAME               PROPERTY  VALUE      SOURCE
zpooldemo/zfsdemo  checksum  off        local
</code></pre>
<ul>
<li><p>Destroy ZFS storage pool and file system</p>
<p>  $ zfs list<br>  NAME                USED  AVAIL     REFER  MOUNTPOINT<br>  zpooldemo           194K   115G       25K  &#x2F;zpooldemo<br>  zpooldemo&#x2F;zfsdemo    24K   115G       24K  &#x2F;zpooldemo&#x2F;zfsdemo</p>
<p>  $ zfs destroy zpooldemo&#x2F;zfsdemo</p>
<p>  $ zfs list<br>  NAME        USED  AVAIL     REFER  MOUNTPOINT<br>  zpooldemo   169K   115G       25K  &#x2F;zpooldemo</p>
<p>  $ zpool destroy zpooldemo<br>  $ zpool list<br>  no pools available</p>
</li>
</ul>
<h2 id="Kernel-compatibility"><a href="#Kernel-compatibility" class="headerlink" title="Kernel compatibility"></a>Kernel compatibility</h2><p>When to install zfs on CentOS, it will check if the already installed kernel version matches the specified the release version. In the following example, the required kernel 3.10.0-1160 will be installed automatically during zfs-release.el7_9 installation.</p>
<pre><code>$ yum install https://zfsonlinux.org/epel/zfs-release.el7_9.noarch.rpm -y
$ yum install zfs -y
Installed:
kernel.x86_64 0:3.10.0-1160.59.1.el7                                                                                       
zfs.x86_64 0:2.0.7-1.el7

Dependency Installed:
kmod-zfs.x86_64 0:2.0.7-1.el7                 
libnvpair3.x86_64 0:2.0.7-1.el7                 
libuutil3.x86_64 0:2.0.7-1.el7                 
libzfs4.x86_64 0:2.0.7-1.el7                 
libzpool4.x86_64 0:2.0.7-1.el7

$ reboot
$ uname -r
3.10.0-1160.59.1.el7.x86_64

$ lsmod  |  grep zfs
$ modprobe zfs
$ lsmod  |  grep zfs
zfs                  4224878  0
zunicode              331170  1 zfs
zzstd                 460780  1 zfs
zlua                  151526  1 zfs
zcommon                94285  1 zfs
znvpair                94388  2 zfs,zcommon
zavl                   15698  1 zfs
icp                   301775  1 zfs
spl                    96750  6 icp,zfs,zavl,zzstd,zcommon,znvpair
$ zfs version
zfs-2.0.7-1
zfs-kmod-2.0.7-1
</code></pre>
<h2 id="Uninstall-ZFS"><a href="#Uninstall-ZFS" class="headerlink" title="Uninstall ZFS"></a>Uninstall ZFS</h2><p>Remove the installed rpms and remove the repository as below.</p>
<pre><code>$ rpm -ev &lt;pkg-rpm-name&gt;
$ yum remove zfs-release
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/OpenZFS">https://en.wikipedia.org/wiki/OpenZFS</a></li>
<li><a href="https://openzfs.org/wiki/Main_Page">https://openzfs.org/wiki/Main_Page</a></li>
<li><a href="https://openzfs.github.io/openzfs-docs/index.html">https://openzfs.github.io/openzfs-docs/index.html</a></li>
<li><a href="https://docs.oracle.com/cd/E19253-01/819-5461/index.html">https://docs.oracle.com/cd/E19253-01/819-5461/index.html</a></li>
<li><a href="https://www.symmcom.com/docs/how-tos/storages/how-to-install-zfs-on-centos-7">https://www.symmcom.com/docs/how-tos/storages/how-to-install-zfs-on-centos-7</a></li>
<li><a href="https://zfsonlinux.org/">https://zfsonlinux.org/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>OpenZFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Overwrite the Elastic Rally track parameters</title>
    <url>/blog/overwrite-esrally-track-params/</url>
    <content><![CDATA[<p>Each track allows to overwrite the corresponding parameters using –track-params.</p>
<p>For example, the track <a href="https://github.com/elastic/rally-tracks/tree/master/geonames">geonames</a> allows to overwrite the following parameters.</p>
<ul>
<li>bulk_size (default: 5000)</li>
<li>bulk_indexing_clients (default: 8): Number of clients that issue bulk indexing requests.</li>
<li>ingest_percentage (default: 100): A number between 0 and 100 that defines how much of the document corpus should be ingested.</li>
<li>conflicts (default: “random”): Type of id conflicts to simulate. Valid values are: ‘sequential’ (A document id is replaced with a document id with a sequentially increasing id), ‘random’ (A document id is replaced with a document id with a random other id).</li>
<li>conflict_probability (default: 25): A number between 0 and 100 that defines the probability of id conflicts. This requires to run the respective challenge. Combining conflicts&#x3D;sequential and conflict-probability&#x3D;0 makes Rally generate index ids by itself, instead of relying on Elasticsearch’s automatic id generation <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#_automatic_id_generation">https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#_automatic_id_generation</a>_.</li>
<li>on_conflict (default: “index”): Whether to use an “index” or an “update” action when simulating an id conflict.</li>
<li>recency (default: 0): A number between 0 and 1 that defines whether to bias towards more recent ids when simulating conflicts. See the Rally docs for the full definition of this parameter. This requires to run the respective challenge.</li>
<li>number_of_replicas (default: 0)</li>
<li>number_of_shards (default: 5)</li>
<li>max_num_segments: The maximum number of segments to force-merge to.</li>
<li>source_enabled (default: true): A boolean defining whether the _source field is stored in the index.</li>
<li>index_settings: A list of index settings. Index settings defined elsewhere (e.g. number_of_replicas) need to be overridden explicitly.</li>
<li>cluster_health (default: “green”): The minimum required cluster health.</li>
<li>error_level (default: “non-fatal”): Available for bulk operations only to specify ignore-response-error-level.</li>
</ul>
<p>The default track parameters can be verified in the index.json.</p>
<pre><code>$ cat .rally/benchmarks/tracks/default/geonames/index.json
&#123;
  &quot;settings&quot;: &#123;
    &quot;index.number_of_shards&quot;: &#123;&#123;number_of_shards | default(5)&#125;&#125;,
    &quot;index.number_of_replicas&quot;: &#123;&#123;number_of_replicas | default(0)&#125;&#125;,
    &quot;index.store.type&quot;: &quot;&#123;&#123;store_type | default('fs')&#125;&#125;&quot;,
    &quot;index.requests.cache.enable&quot;: false
  &#125;,
  [..]
&#125;
</code></pre>
<p>You can change the track parameters when you run the race as below.</p>
<pre><code>$ esrally race --pipeline=benchmark-only --target-host=10.10.10.1:39200,10.10.10.2:39200,10.10.10.3:39200 --track=geonames --track-params=&quot;number_of_shards:3,number_of_replicas:1&quot; --challenge=append-no-conflicts --on-error=abort --race-id=$&#123;RACE_ID&#125;

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Race id is [734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f]
[INFO] Racing on track [geonames], challenge [append-no-conflicts] and car [&#39;external&#39;] with version [7.17.0].

[WARNING] indexing_total_time is 19846 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] refresh_total_time is 5970 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [100% done]
Running refresh-after-index                                                    [100% done]
Running force-merge                                                            [100% done]
Running refresh-after-force-merge                                              [100% done]
Running wait-until-merges-finish                                               [100% done]
Running index-stats                                                            [100% done]
Running node-stats                                                             [100% done]
Running default                                                                [100% done]
Running term                                                                   [100% done]
Running phrase                                                                 [100% done]
Running country_agg_uncached                                                   [100% done]
Running country_agg_cached                                                     [100% done]
Running scroll                                                                 [100% done]
Running expression                                                             [100% done]
Running painless_static                                                        [100% done]
Running painless_dynamic                                                       [100% done]
Running decay_geo_gauss_function_score                                         [100% done]
Running decay_geo_gauss_script_score                                           [100% done]
Running field_value_function_score                                             [100% done]
Running field_value_script_score                                               [100% done]
Running large_terms                                                            [100% done]
Running large_filtered_terms                                                   [100% done]
Running large_prohibited_terms                                                 [100% done]
Running desc_sort_population                                                   [100% done]
Running asc_sort_population                                                    [100% done]
Running asc_sort_with_after_population                                         [100% done]
Running desc_sort_geonameid                                                    [100% done]
Running desc_sort_with_after_geonameid                                         [100% done]
Running asc_sort_geonameid                                                     [100% done]
Running asc_sort_with_after_geonameid                                          [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                         Metric |                           Task |           Value |    Unit |
|---------------------------------------------------------------:|-------------------------------:|----------------:|--------:|
|                     Cumulative indexing time of primary shards |                                |    14.3313      |     min |
|             Min cumulative indexing time across primary shards |                                |     0           |     min |
|          Median cumulative indexing time across primary shards |                                |     0.00696667  |     min |
|             Max cumulative indexing time across primary shards |                                |     4.85003     |     min |
|            Cumulative indexing throttle time of primary shards |                                |     0           |     min |
|    Min cumulative indexing throttle time across primary shards |                                |     0           |     min |
| Median cumulative indexing throttle time across primary shards |                                |     0           |     min |
|    Max cumulative indexing throttle time across primary shards |                                |     0           |     min |
|                        Cumulative merge time of primary shards |                                |     9.33973     |     min |
|                       Cumulative merge count of primary shards |                                |    93           |         |
|                Min cumulative merge time across primary shards |                                |     0           |     min |
|             Median cumulative merge time across primary shards |                                |     0.00276667  |     min |
|                Max cumulative merge time across primary shards |                                |     3.20492     |     min |
|               Cumulative merge throttle time of primary shards |                                |     2.39933     |     min |
|       Min cumulative merge throttle time across primary shards |                                |     0           |     min |
|    Median cumulative merge throttle time across primary shards |                                |     0           |     min |
|       Max cumulative merge throttle time across primary shards |                                |     0.8346      |     min |
|                      Cumulative refresh time of primary shards |                                |     1.4791      |     min |
|                     Cumulative refresh count of primary shards |                                |   408           |         |
|              Min cumulative refresh time across primary shards |                                |     0           |     min |
|           Median cumulative refresh time across primary shards |                                |     0.0191417   |     min |
|              Max cumulative refresh time across primary shards |                                |     0.532383    |     min |
|                        Cumulative flush time of primary shards |                                |     0.143       |     min |
|                       Cumulative flush count of primary shards |                                |    14           |         |
|                Min cumulative flush time across primary shards |                                |     0           |     min |
|             Median cumulative flush time across primary shards |                                |     0.000191667 |     min |
|                Max cumulative flush time across primary shards |                                |     0.0622167   |     min |
|                                        Total Young Gen GC time |                                |    30.76        |       s |
|                                       Total Young Gen GC count |                                |  3790           |         |
|                                          Total Old Gen GC time |                                |     7.34        |       s |
|                                         Total Old Gen GC count |                                |   107           |         |
|                                                     Store size |                                |     5.79046     |      GB |
|                                                  Translog size |                                |     8.19564e-07 |      GB |
|                                         Heap used for segments |                                |     0.452423    |      MB |
|                                       Heap used for doc values |                                |     0.0294952   |      MB |
|                                            Heap used for terms |                                |     0.340546    |      MB |
|                                            Heap used for norms |                                |     0.0440674   |      MB |
|                                           Heap used for points |                                |     0           |      MB |
|                                    Heap used for stored fields |                                |     0.0383148   |      MB |
|                                                  Segment count |                                |    74           |         |
|                                    Total Ingest Pipeline count |                                |     0           |         |
|                                     Total Ingest Pipeline time |                                |     0           |       s |
|                                   Total Ingest Pipeline failed |                                |     0           |         |
|                                                 Min Throughput |                   index-append | 80680.2         |  docs/s |
|                                                Mean Throughput |                   index-append | 81105.1         |  docs/s |
|                                              Median Throughput |                   index-append | 81128.5         |  docs/s |
|                                                 Max Throughput |                   index-append | 81286.2         |  docs/s |
|                                        50th percentile latency |                   index-append |   338.821       |      ms |
|                                        90th percentile latency |                   index-append |   843.169       |      ms |
|                                        99th percentile latency |                   index-append |  1132.18        |      ms |
|                                       100th percentile latency |                   index-append |  1158.98        |      ms |
|                                   50th percentile service time |                   index-append |   338.821       |      ms |
|                                   90th percentile service time |                   index-append |   843.169       |      ms |
|                                   99th percentile service time |                   index-append |  1132.18        |      ms |
|                                  100th percentile service time |                   index-append |  1158.98        |      ms |
|                                                     error rate |                   index-append |     0           |       % |
[..]                                                 

----------------------------------
[INFO] SUCCESS (took 4314 seconds)
----------------------------------
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://esrally.readthedocs.io/en/stable/configuration.html">https://esrally.readthedocs.io/en/stable/configuration.html</a></li>
<li><a href="https://github.com/elastic/rally-tracks/tree/master/geonames">https://github.com/elastic/rally-tracks/tree/master/geonames</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Esrally</tag>
      </tags>
  </entry>
  <entry>
    <title>Perf - The official Linux profiler</title>
    <url>/blog/perf-the-official-linux-profiler/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>perf, aka perf_events, is the official Linux profiler and included in the Linux kernel source under tools&#x2F;perf. It can instrument CPU performance counters, tracepoints, kprobes, and uprobes. It is capable of system profiling.</p>
<h2 id="Performance-Monitoring-Counter-PMC"><a href="#Performance-Monitoring-Counter-PMC" class="headerlink" title="Performance Monitoring Counter(PMC)"></a>Performance Monitoring Counter(PMC)</h2><p><strong>Show system wide PMC statistics</strong></p>
<p>The following example shows PMC statistics for the entire system, for 5 seconds.</p>
<pre><code>$ perf stat -a sleep 5

 Performance counter stats for &#39;system wide&#39;:

        399,489.78 msec cpu-clock                 #   79.290 CPUs utilized
            21,795      context-switches          #    0.055 K/sec
               208      cpu-migrations            #    0.001 K/sec
            10,071      page-faults               #    0.025 K/sec
    15,739,163,983      cycles                    #    0.039 GHz
    11,493,495,432      instructions              #    0.73  insn per cycle
     2,091,049,131      branches                  #    5.234 M/sec
        23,987,055      branch-misses             #    1.15% of all branches

       5.038337951 seconds time elapsed
</code></pre>
<p><strong>Show PMC statistics for the specified process</strong></p>
<p>The following example shows PMC statistics for the fio process. The fio job file will be included later on.</p>
<pre><code>$  perf stat fio fio_job_file.ini
Performance counter stats for &#39;fio fio_job_file.ini&#39;:

          9,500.26 msec task-clock                #    0.172 CPUs utilized
           133,844      context-switches          #    0.014 M/sec
            13,540      cpu-migrations            #    0.001 M/sec
           107,703      page-faults               #    0.011 M/sec
    21,039,694,713      cycles                    #    2.215 GHz
     8,896,839,896      instructions              #    0.42  insn per cycle
     1,755,629,641      branches                  #  184.798 M/sec
        34,380,966      branch-misses             #    1.96% of all branches

      55.143040711 seconds time elapsed

       4.520016000 seconds user
       5.921369000 seconds sys
</code></pre>
<h2 id="perf-events"><a href="#perf-events" class="headerlink" title="perf events"></a>perf events</h2><p>perf events can be listed using <em>perf list</em>. I only list a few of them for example. It includes hardware event, hardware cache event, software event, PMU event, tracepoint event and SDT event.</p>
<pre><code>$ perf list
List of pre-defined events (to be used in -e):

  cache-misses                                       [Hardware event]
  cache-references                                   [Hardware event]
  context-switches OR cs                             [Software event]
  page-faults OR faults                              [Software event]
  L1-dcache-load-misses                              [Hardware cache event]
  L1-dcache-loads                                    [Hardware cache event] 
  cache-misses OR cpu/cache-misses/                  [Kernel PMU event]
  cache-references OR cpu/cache-references/          [Kernel PMU event]
  cpu-cycles OR cpu/cpu-cycles/                      [Kernel PMU event]  
  block:block_plug                                   [Tracepoint event]
  block:block_rq_abort                               [Tracepoint event]
  block:block_rq_complete                            [Tracepoint event]
  block:block_rq_insert                              [Tracepoint event]
  block:block_rq_issue                               [Tracepoint event]
  irq:softirq_entry                                  [Tracepoint event]
  kmem:kfree                                         [Tracepoint event]
  kmem:kmalloc                                       [Tracepoint event]
  kmem:kmalloc_node                                  [Tracepoint event]
  kmem:kmem_cache_alloc                              [Tracepoint event]
  kmem:kmem_cache_alloc_node                         [Tracepoint event]
  kmem:kmem_cache_free                               [Tracepoint event]
  kmem:mm_page_alloc                                 [Tracepoint event]
  kmem:mm_page_alloc_extfrag                         [Tracepoint event]
  kmem:mm_page_alloc_zone_locked                     [Tracepoint event]
  kmem:mm_page_free                                  [Tracepoint event]
  kmem:mm_page_free_batched                          [Tracepoint event]
  kmem:mm_page_pcpu_drain                            [Tracepoint event]
  syscalls:sys_enter_write                           [Tracepoint event]
  syscalls:sys_enter_read                            [Tracepoint event]
  sdt_libc:memory_heap_free                          [SDT event]
  sdt_libc:memory_heap_less                          [SDT event]
  sdt_libc:memory_heap_more                          [SDT event]
  sdt_libc:memory_heap_new                           [SDT event]  
[...]  
</code></pre>
<h2 id="PMC-sample-frequency"><a href="#PMC-sample-frequency" class="headerlink" title="PMC sample frequency"></a>PMC sample frequency</h2><p>A default sample frequency is used when using perf record with PMCs. Thus, not all the event are recorded. This is desirable because some PMCs events can occur billions of time per second which causes significant overhead of recording.</p>
<p>The following two examples record the hardware cycle events and software page-faults events. The output shows the frequency sampling is enabled(freq&#x3D;1) and the sample frequency is 4000.</p>
<pre><code>$ perf record -vve cycles -a sleep 1
Using CPUID GenuineIntel-6-55-4
intel_pt default config: tsc,mtc,mtc_period=3,psb_period=3,pt,branch
------------------------------------------------------------
perf_event_attr:
  size                             112
  &#123; sample_period, sample_freq &#125;   4000
  sample_type                      IP|TID|TIME|CPU|PERIOD
  read_format                      ID
  disabled                         1
  inherit                          1
  mmap                             1
  comm                             1
  freq                             1
  task                             1
  sample_id_all                    1
  exclude_guest                    1
  mmap2                            1
  comm_exec                        1
------------------------------------------------------------
[...]
[ perf record: Captured and wrote 2.057 MB perf.data (14549 samples) ]

$ perf record -vve page-faults -a sleep 1
Using CPUID GenuineIntel-6-55-4
intel_pt default config: tsc,mtc,mtc_period=3,psb_period=3,pt,branch
------------------------------------------------------------
perf_event_attr:
  type                             1
  size                             112
  config                           0x2
  &#123; sample_period, sample_freq &#125;   4000
  sample_type                      IP|TID|TIME|CPU|PERIOD
  read_format                      ID
  disabled                         1
  inherit                          1
  mmap                             1
  comm                             1
  freq                             1
  task                             1
  sample_id_all                    1
  exclude_guest                    1
  mmap2                            1
  comm_exec                        1
------------------------------------------------------------
[...]
[ perf record: Captured and wrote 1.434 MB perf.data (615 samples) ]
</code></pre>
<p>The sample frequency can be modified using the -F option.</p>
<pre><code>$ perf record -F 99 -vve cycles -a sleep 1
Using CPUID GenuineIntel-6-55-4
intel_pt default config: tsc,mtc,mtc_period=3,psb_period=3,pt,branch
------------------------------------------------------------
perf_event_attr:
  size                             112
  &#123; sample_period, sample_freq &#125;   99
  sample_type                      IP|TID|TIME|CPU|PERIOD
  read_format                      ID
  disabled                         1
  inherit                          1
  mmap                             1
  comm                             1
  freq                             1
  task                             1
  sample_id_all                    1
  exclude_guest                    1
  mmap2                            1
  comm_exec                        1
------------------------------------------------------------
</code></pre>
<p>In Linux, there is a limit for frequency rate and cpu utilization for perf. These limits can be changed with <em>sysctl</em> if needed.</p>
<pre><code>$ sysctl -a | grep kernel.perf_event_max_sample_rate
kernel.perf_event_max_sample_rate = 16000

$ sysctl -a | grep kernel.perf_cpu_time_max_percent
kernel.perf_cpu_time_max_percent = 25
</code></pre>
<h2 id="CPU-profiling"><a href="#CPU-profiling" class="headerlink" title="CPU profiling"></a>CPU profiling</h2><p>perf is often used for CPU profiling.</p>
<p>In the following example, we have a fio workload running to create 10000 files and 100KB each. We can profile the workload while it’s running.</p>
<p>We increase the open files parameter value from the default 1024 to 10240 so that we can create 10000 files.</p>
<pre><code>$ ulimit -a | grep &quot;open files&quot;
open files                      (-n) 1024

$ ulimit -n 10240

$ ulimit -a | grep &quot;open files&quot;
open files                      (-n) 10240
</code></pre>
<p>We use the following fio job file to run the workload.</p>
<pre><code>$ cat fio_job_file.ini
[job1]
ioengine=libaio
iodepth=8
rw=write
direct=1
nrfiles=10000
filesize=102400
blocksize=4096
dedupe_percentage=30
buffer_compress_percentage=50
buffer_pattern=&quot;0123456789&quot;
numjobs=1
directory=/testdir

$ fio fio_job_file.ini
</code></pre>
<p>We start the perf profiling in a different terminal while the above fio workload is running.</p>
<pre><code>$ perf record -F 99 -p `pidof fio` -a -g -- sleep 5
</code></pre>
<p>Once the profiling is done, a <em>perf.data</em> result file is generated for later analysis. We use <em>perf report –stdio</em> command to summarize the <em>perf.data</em> and generate a text report.</p>
<p>From the text report, we can have a better idea on how the fio works by understanding the system call function graph. This is extremly useful when we need to identify the high latency of functions.</p>
<pre><code>$ ls -la | grep perf.data
-rw-------.  1 root root   143460 Feb 25 02:25 perf.data

$ perf report --stdio &gt; perf.report.stdio.out
# To display the perf.data header info, please use --header/--header-only options.
#
#
# Total Lost Samples: 0
#
# Samples: 335  of event &#39;cycles:ppp&#39;
# Event count (approx.): 3215628508
#
# Children      Self  Command  Shared Object       Symbol
# ........  ........  .......  ..................  ............................................
#
    41.73%     0.00%  :198904  [kernel.kallsyms]   [k] system_call_fastpath
            |
            ---system_call_fastpath
               |
               |--30.99%--sys_io_submit
               |          |
               |          |--29.96%--do_io_submit
               |          |          |
               |          |          |--15.62%--vx_naio_write_v2
               |          |          |          vx_naio_write
               |          |          |          |
               |          |          |          |--9.73%--vx_naio_handoff
               |          |          |          |          |
               |          |          |          |           --8.57%--__wake_up
               |          |          |          |                     __wake_up_common_lock
               |          |          |          |                     __wake_up_common
               |          |          |          |                     vx_wq_wakeup_function
               |          |          |          |                     default_wake_function
               |          |          |          |                     try_to_wake_up
               |          |          |          |                     |
               |          |          |          |                      --0.87%--_raw_spin_lock_irqsave
               |          |          |          |
               |          |          |           --5.89%--vx_naio_checks.isra.5.constprop.6
               |          |          |                     |
               |          |          |                      --5.16%--vx_fel_io_allowed
               |          |          |                                |
               |          |          |                                |--2.86%--vx_irwunlock
               |          |          |                                |          vx_recsmp_rangeunlock
               |          |          |                                |          vx_rwsleep_rec_unlock
               |          |          |                                |
               |          |          |                                 --1.92%--vx_irwlock
               |          |          |                                           vx_irwlock2
               |          |          |                                           vx_recsmp_rangelock
               |          |          |                                           vx_rwsleep_rec_lock
               |          |          |                                           _raw_spin_lock_irqsave
               |          |          |
               |          |          |--7.94%--kmem_cache_alloc
               |          |          |          __slab_alloc
               |          |          |          ___slab_alloc
               |          |          |
               |          |          |--3.83%--rw_verify_area
               |          |          |          security_file_permission
               |          |          |          |
               |          |          |           --3.48%--selinux_file_permission
               |          |          |                     __inode_security_revalidate
               |          |          |
               |          |          |--1.04%--lookup_ioctx
               |          |          |
               |          |           --0.83%--fget
               |          |
               |           --0.86%--kmem_cache_alloc
               |
               |--5.52%--sys_io_getevents
               |          read_events
               |          |
               |          |--3.06%--schedule
               |          |          __schedule
               |          |          |
               |          |           --2.76%--deactivate_task
               |          |                     update_rq_clock.part.76
               |          |
               |          |--1.77%--aio_read_events
               |          |          |
               |          |           --0.58%--_cond_resched
               |          |
               |           --0.64%--prepare_to_wait
               |                     _raw_spin_lock_irqsave
               |
                --5.04%--sys_open
                          do_sys_open
                          |
                          |--4.12%--do_filp_open
                          |          path_openat
                          |          |
                          |          |--2.38%--link_path_walk
                          |          |          |
                          |          |           --0.66%--lookup_fast
                          |          |                     vx_drevalidate
                          |          |                     vx_nmspc_resolve
                          |          |                     _raw_spin_lock_irqsave
                          |          |
                          |          |--0.90%--get_empty_filp
                          |          |          |
                          |          |           --0.72%--kmem_cache_alloc
                          |          |
                          |           --0.81%--do_last
                          |                     |
                          |                      --0.56%--__audit_inode
                          |                                audit_copy_inode
                          |                                get_vfs_caps_from_disk
                          |                                vx_linux_getxattr
                          |                                vx_get_eatype
                          |
                           --0.92%--getname
                                     getname_flags
                                     kmem_cache_alloc
[...]
</code></pre>
<h2 id="Tracepoint-events"><a href="#Tracepoint-events" class="headerlink" title="Tracepoint events"></a>Tracepoint events</h2><p>In the previous profile, we saw a function kmem_cache_alloc. It’s a pre-defined tracepoint. So, we can trace it directly to understand the call graph related.</p>
<pre><code>$ perf list | grep kmem_cache_alloc
  kmem:kmem_cache_alloc                              [Tracepoint event]


$ perf record -F 99 -e kmem:kmem_cache_alloc -p `pidof fio` -a -g -- sleep 5 
</code></pre>
<p>We can check the target tracepoint call graph as below.</p>
<pre><code>$ perf report --stdio
    14.61%    14.61%  (mmap_region+0x38c) call_site=ffffffff877fa39c ptr=0xffff9193252c5f38 bytes_req=216 bytes_alloc=216 gfp_flags=GFP_KERNEL|GFP_ZERO
            |
            ---__mmap64
               system_call_fastpath
               sys_mmap
               sys_mmap_pgoff
               vm_mmap_pgoff
               do_mmap
               mmap_region
               kmem_cache_alloc
[...]               
</code></pre>
<p>Another example is to check the which function issues the disk I&#x2F;Os(e.g. synchronous read&#x2F;write).</p>
<pre><code>$ perf record -e block:block_rq_insert -a -g -- sleep 60
</code></pre>
<p><em>perf script</em> can also be used to get a summary. It’s useful to spot patterns overtime which might be lost in a huge report summary.</p>
<pre><code>$ perf script

fio 264249 [020] 3116264.020748: kmem:kmem_cache_alloc: (getname_flags+0x4f) call_site=ffffffff8785c74f ptr=0xffff9234810d5000 bytes_req=4096 bytes_alloc=4096 gfp_flags=GFP
_KERNEL
    ffffffff8782660d kmem_cache_alloc+0xfd ([kernel.kallsyms])
    ffffffff8785c74f getname_flags+0x4f ([kernel.kallsyms])
    ffffffff8785d8c5 user_path_at_empty+0x45 ([kernel.kallsyms])
    ffffffff8785d951 user_path_at+0x11 ([kernel.kallsyms])
    ffffffff87850633 vfs_fstatat+0x63 ([kernel.kallsyms])
    ffffffff87850a51 SYSC_newlstat+0x31 ([kernel.kallsyms])
    ffffffff87850ebe sys_newlstat+0xe ([kernel.kallsyms])
    ffffffff87d8fede system_call_fastpath+0x25 ([kernel.kallsyms])
        7fd8de3ea365 __lxstat64+0x15 (/usr/lib64/libc-2.17.so)
                8000 [unknown] ([unknown])
[...]
</code></pre>
<p>The following are the reports without using “-g” option during profiling. It summarizes the profile without the detail function graph.</p>
<pre><code>$ perf script
    fio 276212 [047] 3116580.119986: kmem:kmem_cache_alloc: (get_empty_filp+0x5c) call_site=ffffffff8784d10c ptr=0xffff922b52f2ee00 bytes_req=256 bytes_alloc=256 g
    fio 276212 [047] 3116580.120138: kmem:kmem_cache_alloc: (sys_io_setup+0xaa) call_site=ffffffff878a182a ptr=0xffff918f24972f40 bytes_req=576 bytes_alloc=576 gfp
    fio 276212 [008] 3116580.120419: kmem:kmem_cache_alloc: (do_io_submit+0x194) call_site=ffffffff878a1da4 ptr=0xffff91224f606f00 bytes_req=192 bytes_alloc=192 gf
    fio 276212 [008] 3116580.120420: kmem:kmem_cache_alloc: (do_io_submit+0x194) call_site=ffffffff878a1da4 ptr=0xffff91224f606f00 bytes_req=192 bytes_alloc=192 gf
    fio 276212 [008] 3116580.120420: kmem:kmem_cache_alloc: (do_io_submit+0x194) call_site=ffffffff878a1da4 ptr=0xffff91224f606f00 bytes_req=192 bytes_alloc=192 gf
    fio 276212 [008] 3116580.120425: kmem:kmem_cache_alloc: (vx_alloc+0x152) call_site=ffffffffc13ec032 ptr=0xffff91b661b0bd10 bytes_req=88 bytes_alloc=88 gfp_flag
    fio 276212 [008] 3116580.120440: kmem:kmem_cache_alloc: (getname_flags+0x4f) call_site=ffffffff8785c74f ptr=0xffff919470bd5000 bytes_req=4096 bytes_alloc=4096


$ perf report --stdio
    12.93%  (vx_alloc+0x152) call_site=ffffffffc13ec032 ptr=0xffff90e61cd6c840 bytes_req=88 bytes_alloc=88 gfp_flags=GFP_NOFS|GFP_NOWARN|GFP_NORETRY
    12.50%  (do_io_submit+0x194) call_site=ffffffff878a1da4 ptr=0xffff91cbb8190300 bytes_req=192 bytes_alloc=192 gfp_flags=GFP_KERNEL|GFP_ZERO
    12.42%  (do_io_submit+0x194) call_site=ffffffff878a1da4 ptr=0xffff91e20e737980 bytes_req=192 bytes_alloc=192 gfp_flags=GFP_KERNEL|GFP_ZERO
    12.02%  (do_io_submit+0x194) call_site=ffffffff878a1da4 ptr=0xffff919379aea240 bytes_req=192 bytes_alloc=192 gfp_flags=GFP_KERNEL|GFP_ZERO
    11.69%  (vx_alloc+0x152) call_site=ffffffffc13ec032 ptr=0xffff922ef33f91b8 bytes_req=88 bytes_alloc=88 gfp_flags=GFP_NOFS|GFP_NOWARN|GFP_NORETRY
    10.60%  (do_io_submit+0x194) call_site=ffffffff878a1da4 ptr=0xffff9153dad8be00 bytes_req=192 bytes_alloc=192 gfp_flags=GFP_KERNEL|GFP_ZERO
     5.79%  (getname_flags+0x4f) call_site=ffffffff8785c74f ptr=0xffff90edba1c3000 bytes_req=4096 bytes_alloc=4096 gfp_flags=GFP_KERNEL
     5.74%  (vx_alloc+0x152) call_site=ffffffffc13ec032 ptr=0xffff922efbb0b580 bytes_req=88 bytes_alloc=88 gfp_flags=GFP_NOFS|GFP_NOWARN|GFP_NORETRY
     4.88%  (do_io_submit+0x194) call_site=ffffffff878a1da4 ptr=0xffff91224f606a80 bytes_req=192 bytes_alloc=192 gfp_flags=GFP_KERNEL|GFP_ZERO
</code></pre>
<h2 id="perf-stat"><a href="#perf-stat" class="headerlink" title="perf stat"></a>perf stat</h2><p><em>perf stat</em> subcommand can be used to count the target event during the specified seconds.</p>
<p>The following example shows the <em>kmem:kmem_cache_alloc</em> tracepoints fired 146,375 times during 5 seconds.</p>
<pre><code>$ perf stat -e kmem:kmem_cache_alloc -p `pidof fio` -a -- sleep 5

 Performance counter stats for process id &#39;9639&#39;:

           146,375      kmem:kmem_cache_alloc

       5.003282238 seconds time elapsed
</code></pre>
<h2 id="Dynamic-tracing-with-probe-events"><a href="#Dynamic-tracing-with-probe-events" class="headerlink" title="Dynamic tracing with probe events"></a>Dynamic tracing with probe events</h2><p>Except for tracing the predefined perf events which are present in <em>perf list</em> command, <em>perf</em> is capable of creating more probe events dynamically.</p>
<p><strong>kprobes</strong></p>
<p>kprobes(kernel probes) can trace kernel function or instruction.</p>
<p>Noticed that there are lots of function calls of <em>native_apic_mem_write</em></p>
<pre><code>$ perf record -F 99 -p `pidof fio` -a -- sleep 5
$ perf script
  fio 225677 3122981.775077:      42831 cycles:ppp:  ffffffff87663ec0 native_apic_mem_write+0x0 ([kernel.kallsyms])
  fio 225677 3122981.775309:      62731 cycles:ppp:  ffffffff87635b48 native_sched_clock+0x58 ([kernel.kallsyms])
[...]  
</code></pre>
<p>It’s not a pre-defined tracepoint.</p>
<pre><code>$ perf list | grep native_write_msr_safe
</code></pre>
<p>It’s a kernel function.</p>
<pre><code>$ cat /proc/kallsyms | grep native_write_msr_safe
ffffffff8766d5b0 t native_write_msr_safe
</code></pre>
<p>We add the probe event for the kernel function and it will be listed in <em>perf list</em>. Now it’s ready for tracing.</p>
<pre><code>$ perf probe --add native_write_msr_safe
Added new event:
  probe:native_write_msr_safe (on native_write_msr_safe)

You can now use it in all perf tools, such as:

    perf record -e probe:native_write_msr_safe -aR sleep 1

$ perf list | grep native_write_msr_safe
  probe:native_write_msr_safe                        [Tracepoint event]

$ perf record -e probe:native_write_msr_safe -p `pidof fio` -a -g sleep 5
Warning:
PID/TID switch overriding SYSTEM
[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.417 MB perf.data (1564 samples) ]

$ perf script
fio 241552 [020] 3123367.978632: probe:native_write_msr_safe: (ffffffff8766d5b0)
        ffffffff8766d5b1 native_write_msr_safe+0x1 ([kernel.kallsyms])
        ffffffff8770de01 clockevents_program_event+0x71 ([kernel.kallsyms])
        ffffffff8770fb03 tick_program_event+0x23 ([kernel.kallsyms])
        ffffffff876cad42 hrtimer_interrupt+0xf2 ([kernel.kallsyms])
        ffffffff8765c61b local_apic_timer_interrupt+0x3b ([kernel.kallsyms])
        ffffffff87d949d3 smp_apic_timer_interrupt+0x43 ([kernel.kallsyms])
        ffffffff87d90efa apic_timer_interrupt+0x16a ([kernel.kallsyms])
        ffffffffc13a484d vx_log+0x2bd ([kernel.kallsyms])
        ffffffffc14620cb vx_trancommit+0x70b ([kernel.kallsyms])
        ffffffffc1327fb3 vx_growfile+0x683 ([kernel.kallsyms])
        ffffffffc132ecbd vx_tran_extset+0x79d ([kernel.kallsyms])
        ffffffffc132d982 vx_extset+0x4e2 ([kernel.kallsyms])
        ffffffffc13ea139 vx_do_fallocate+0x479 ([kernel.kallsyms])
        ffffffffc13ea330 vx_fallocate+0x60 ([kernel.kallsyms])
        ffffffffc13ea3e6 vx_fallocate_v2+0x16 ([kernel.kallsyms])
        ffffffff87847d82 vfs_fallocate+0x142 ([kernel.kallsyms])
        ffffffff87848dab sys_fallocate+0x5b ([kernel.kallsyms])
        ffffffff87d8fede system_call_fastpath+0x25 ([kernel.kallsyms])
            7fcc1002a730 fallocate64+0x70 (/usr/lib64/libc-2.17.so)
[...]         
</code></pre>
<p>We can remove the kprobe if it’s no longer needed.</p>
<pre><code>$ perf probe --del native_write_msr_safe
Removed event: probe:native_write_msr_safe
</code></pre>
<p>The function variables including arguments are available to perf if the <strong>kernel debuginfo</strong> is available.</p>
<pre><code>$ perf probe --vars native_write_msr_safe
</code></pre>
<p><strong>uprobes</strong></p>
<p>uprobes can trace user-space functions in applications and libraries.</p>
<p>The following example adds user probe for fopen function on the libc library.</p>
<pre><code>$ perf probe -x /usr/lib64/libc.so.6 --add fopen
Added new event:
  probe_libc:fopen     (on fopen in /usr/lib64/libc-2.17.so)

You can now use it in all perf tools, such as:

    perf record -e probe_libc:fopen -aR sleep 1

$ perf probe --del probe_libc:fopen
Removed event: probe_libc:fopen
</code></pre>
<p>The return of the function can be instrumented by adding %return after the function.</p>
<pre><code>$ perf probe -x /usr/lib64/libc.so.6 --add fopen%return
Added new event:
  probe_libc:fopen__return (on fopen%return in /usr/lib64/libc-2.17.so)

You can now use it in all perf tools, such as:

    perf record -e probe_libc:fopen__return -aR sleep 1

$ perf list |grep fopen
  probe_libc:fopen__return                           [Tracepoint event]

$ perf probe --del probe_libc:fopen__return
Removed event: probe_libc:fopen__return
</code></pre>
<p>If the system has debuginfo for the target library, the variable information including arguments might be available.</p>
<pre><code>$ perf probe -x /usr/lib64/libc.so.6 --vars fopen
</code></pre>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://elixir.bootlin.com/linux/latest/source/tools/perf">https://elixir.bootlin.com/linux/latest/source/tools/perf</a></li>
<li><a href="https://www.kernel.org/doc/html/latest/trace/">https://www.kernel.org/doc/html/latest/trace/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Perf</tag>
        <tag>Profiling Tracing</tag>
      </tags>
  </entry>
  <entry>
    <title>Priority Queue implementation in Python</title>
    <url>/blog/priority-queue-implementation-in-python/</url>
    <content><![CDATA[<p>Priority Queue is an extension of Queue with following properties</p>
<ul>
<li>each item has a priority associated with it</li>
<li>the item with high priority is dequeued before other item with low priority</li>
</ul>
<p>Binary heap is generally preferred implementation for Priority Queue. It takes time O(log n) to re-heapify the remaining items if the min&#x2F;max item is removed.The same amount of time O(log n) is taken to insert an new item in the heap.</p>
<p>The key functions to implement in Priority Queue include:</p>
<ul>
<li>insert(val)</li>
<li>extractMin()&#x2F;extractMax()</li>
<li>remove(i)</li>
<li>getMin()&#x2F;getMax()</li>
<li>changePriority(i, val)</li>
</ul>
<p>Implementation in Python:</p>
<pre><code>def left_child(k):
    return 2 * k + 1


def right_child(k):
    return 2 * k + 2


def is_leaf(k):
    return 2 * k &gt;= len(mylist)


def parent(k):
    return (k - 1) // 2


def getMin():
    return mylist[0]


def heapify(k):
    if is_leaf(k):
        return

    n = len(mylist)

    # get left and right child index
    left = left_child(k)
    right = right_child(k)

    # find the smallest elements among current node, its left child and right child if exists
    smallest = k
    if left &lt; n and mylist[k] &gt; mylist[left]:
        smallest = left
    if right &lt; n and mylist[smallest] &gt; mylist[right]:
        smallest = right

    # swap current node and the smallest child
    if smallest != k:
        mylist[k], mylist[smallest] = mylist[smallest], mylist[k]
        heapify(smallest)


def shift_up(i):
    while i &gt; 0 and mylist[i] &lt; mylist[parent(i)]:
        mylist[i], mylist[parent(i)] = mylist[parent(i)], mylist[i]
        i = parent(i)


def insert(val):
    mylist.append(val)
    shift_up(len(mylist) - 1)


def extractMin():
    min = mylist[0]
    mylist[0] = mylist[len(mylist) - 1]
    mylist.pop(len(mylist) - 1)
    heapify(0)
    return min


def remove(i):
    mylist[i] = getMin() - 1
    shift_up(i)
    extractMin()


def print_heap():
    for i in range(len(mylist) // 2):
        left = 2 * i + 1
        right = 2 * i + 2
        if left &lt; len(mylist) and right &lt; len(mylist):
            print(
                f&quot;parent: &#123;mylist[i]&#125; left-child: &#123;mylist[left]&#125; right-child: &#123;mylist[right]&#125;&quot;
            )
        elif left &lt; len(mylist):
            print(f&quot;parent: &#123;mylist[i]&#125; left-child: &#123;mylist[left]&#125; right-child: None&quot;)
            
mylist = []            
</code></pre>
<p>Build the binary heap:</p>
<pre><code>insert(6)
insert(4)
insert(8)
insert(2)
insert(9)
insert(7)
print(mylist)

Output:
[2,4,7,6,9,8]


         4        4         2         2          2
4  --&gt;  /   --&gt;  / \  --&gt;  / \  --&gt;  / \  --&gt;  /    \ 
       6        6   8     4   8     4   8     4     7
                         /         / \       / \   /
                        6         6   9     6   9 8
</code></pre>
<p>Insert a new item:</p>
<pre><code>insert(1)
print(mylist)

Output:
[1,4,2,6,9,8,7]

      2             2             1
    /   \         /   \         /   \
   4     7  --&gt;  4     1  --&gt;  4     2
  / \   / \     / \   / \     / \   / \
 6   9 8   1   6   9 8   7   6   9 8   7
</code></pre>
<p>Extract the min item:</p>
<pre><code>extractMin()
print(mylist)

Output:
[2,4,7,6,9,8]

      1             7             2
    /   \         /   \         /   \
   4     2  --&gt;  4     2  --&gt;  4     7
  / \   / \     / \   /       / \   / 
 6   9 8   7   6   9  8      6   9 8
</code></pre>
<p>Remove the item at specified position:</p>
<pre><code>remove(1)
print(mylist)

Output:
[2,6,7,8,9]

       2                1(2-1)         8              2              2
     /   \            /   \          /   \          /   \          /   \
    4     7  --&gt;     2     7  --&gt;   2     7  --&gt;   8     7  --&gt;   6     7
   / \   /          / \   /        / \            / \            / \
  6   9 8          6   9 8        6   9          6   9          8   9
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Proc filesystem - diskstats</title>
    <url>/blog/proc-filesystem-diskstats/</url>
    <content><![CDATA[<p>The &#x2F;proc&#x2F;diskstats file displays the I&#x2F;O statistics of block devices.</p>
<p>Here we have a system which has one disk <em>sda</em> used by Linux operating system and two disks <em>sdb sdc</em> for other purpose.</p>
<pre><code>$ cat /etc/centos-release
CentOS Linux release 7.9.2009 (Core)

$ uname -r
3.10.0-1160.11.1.el7.x86_64

$ lsblk
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda      8:0    0   128G  0 disk
├─sda1   8:1    0   3.7G  0 part /boot
└─sda2   8:2    0 124.3G  0 part /
sdb      8:16   0    60G  0 disk
sdc      8:32   0    60G  0 disk
</code></pre>
<p>The following is the content of &#x2F;proc&#x2F;diskstats file.</p>
<pre><code>$ cat /proc/diskstats
   8       0 sda 20076 3 798556 75864 23573766 4732398 253002144 24887356 0 9814563 24947775
   8       1 sda1 143 0 22434 681 44 21 21664 588 0 397 1269
   8       2 sda2 19905 3 774042 75034 23573722 4732377 252980480 24886768 0 9814404 24946360
   8      32 sdc 88 0 4160 4 0 0 0 0 0 4 4
   8      16 sdb 88 0 4160 2 0 0 0 0 0 2 2
</code></pre>
<p>Taking the <em>sda</em> line for example, the following explains the meaning of each column.</p>
<ul>
<li>8          - major number</li>
<li>0          - minor number</li>
<li>sda        - device name</li>
<li>20076      - reads completed successfully</li>
<li>3          - reads merged</li>
<li>798556     - sectors read</li>
<li>75864      - time spent reading (ms)</li>
<li>23573766   - writes completed</li>
<li>4732398    - writes merged</li>
<li>253002144  - sectors written</li>
<li>24887356   - time spent writing (ms)</li>
<li>0          - I&#x2F;Os currently in progress</li>
<li>9814563    - time spent doing I&#x2F;Os (ms)</li>
<li>24947775   - weighted time spent doing I&#x2F;Os (ms)</li>
</ul>
<p>You may see more columns from the &#x2F;proc&#x2F;diskstats file if the kernel version is 4.18+ or 5.5+. For the detailed explanation, please refer to the <a href="https://www.kernel.org/doc/Documentation/ABI/testing/procfs-diskstats">documentation</a>.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Proc</tag>
      </tags>
  </entry>
  <entry>
    <title>Protests against Children’s Right to have Cell Phones</title>
    <url>/blog/protests-against-childrens-right-to-have-cell-phones/</url>
    <content><![CDATA[<p>Modern technology has changed how people communicate with each other, and helped them connect with their friends and family. Children of age group 9-15 should not be allowed to have cell phones because they aren’t responsible enough. If they call an unknown number, they could talk with scammers and hackers, which could be dangerous. Also, it could easily distract them and give them the urge to waste time by playing video games. This makes the use of cell phones quite unnecessary.</p>
<p>Cell phones are commonly used for communication, but sometimes this could lead to danger for the children. They could misuse it and lose integrity. Children might call unknown strangers, who might ask them for personal information. Also, they might pick up scamming phone calls. They could easily get disturbed and get distracted to relax by playing games. Children aren’t responsible enough to handle a phone, and they start the trust of their parents by lying to them about what they’re doing with the phone.</p>
<p>In other people’s opinion, children should be given a phone. They think children are responsible enough and can control their time. They need it to contact parents if there’s an emergency, or to keep in touch with their friends. They can also use it for research on projects. The parents think they can teach kids to have self-control and self-maintenance. Some children, who are responsible, might be able to use their time wisely and use the cell phones to enhance their life.</p>
<p>Children are not old enough to be able to manage their time. They shouldn’t be trusted with a phone because they might lose their integrity. Cell phones are expensive items, so it would not be good if children accidentally lost them. If they really need to call a friend, they could borrow their parent’s phone. In times of emergency, they could borrow someone else’s phone to contact their parents. If they needed to do research, they could use the computer instead of a phone. Looking at a cell phone’s small screen could damage their vision, so they should use computers, which have a large screen. Overall, it is not a rational decision to let children have their own phones.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
  </entry>
  <entry>
    <title>Pull an image from a private docker registry in Kubernetes Pod</title>
    <url>/blog/pull-an-image-from-a-private-docker-registry-in-kubernetes-pod/</url>
    <content><![CDATA[<h2 id="Log-in-to-Docker-Hub"><a href="#Log-in-to-Docker-Hub" class="headerlink" title="Log in to Docker Hub"></a>Log in to Docker Hub</h2><p>In order to pull a image from Docker Hub, you must authenticate with a registry. Use <strong>docker</strong> tool to log in to the Docker Hub as below. A username and password is needed to log in.</p>
<pre><code>$ docker login
</code></pre>
<p>The login process creates or updates the config.json file which holds an authorization token.</p>
<pre><code>$ cat /root/.docker/config.json
&#123;
    &quot;auths&quot;: &#123;
        &quot;https://index.docker.io/v1/&quot;: &#123;
            &quot;auth&quot;: &quot;xxx=&quot;
        &#125;
    &#125;
&#125;
</code></pre>
<h2 id="Create-a-Secret-based-on-existing-credentials"><a href="#Create-a-Secret-based-on-existing-credentials" class="headerlink" title="Create a Secret based on existing credentials"></a>Create a Secret based on existing credentials</h2><p>A Kubernetes cluster uses the Secret of kubernetes.io&#x2F;dockerconfigjson type to authenticate with a container registry to pull a private image.</p>
<p>If you already ran docker login, you can copy that credential into Kubernetes:</p>
<pre><code>$ kubectl create secret generic regcred --from-file=.dockerconfigjson=/root/.docker/config.json --type=kubernetes.io/dockerconfigjson
</code></pre>
<p>You can inspect the Secret as below.</p>
<pre><code>$ kubectl get secret regcred --output=yaml

apiVersion: v1
data:
  .dockerconfigjson: &lt;base64-formatted-docker-credentials&gt;
kind: Secret
metadata:
  creationTimestamp: &quot;2022-02-28T22:25:43Z&quot;
  name: regcred
  namespace: default
  resourceVersion: &quot;1503624&quot;
  uid: yyy
type: kubernetes.io/dockerconfigjson
</code></pre>
<p>The value of the .dockerconfigjson field is a base64 representation of your Docker credentials. To understand what is in the .dockerconfigjson field, convert the secret data to a readable format:</p>
<pre><code>$ kubectl get secret regcred --output=&quot;jsonpath=&#123;.data.\.dockerconfigjson&#125;&quot; | base64 --decode
&#123;
    &quot;auths&quot;: &#123;
        &quot;https://index.docker.io/v1/&quot;: &#123;
            &quot;auth&quot;: &quot;xxx=&quot;
        &#125;
    &#125;
&#125;
</code></pre>
<h2 id="Create-a-Pod-that-uses-the-Secret-to-pull-image"><a href="#Create-a-Pod-that-uses-the-Secret-to-pull-image" class="headerlink" title="Create a Pod that uses the Secret to pull image"></a>Create a Pod that uses the Secret to pull image</h2><pre><code>$ vi my-private-reg-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: &lt;your-private-image&gt;
  imagePullSecrets:
  - name: regcred

$ kubectl apply -f my-private-reg-pod.yaml
$ kubectl get pod private-reg  
</code></pre>
<p>Note that the <strong>imagePullSecrets</strong> field specifies that Kubernetes should get the credentials from a Secret named <em>regcred</em> in order to pull a container image from Docker Hub.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Queue implementation in Python</title>
    <url>/blog/queue-implementation-in-python/</url>
    <content><![CDATA[<h2 id="What-is-queue"><a href="#What-is-queue" class="headerlink" title="What is queue"></a>What is queue</h2><p>Like stack, queue is also a linear data structure in which the items are stored in a First-In-First-Out(FIFO)manner.</p>
<p>The queue may include the following key functions.</p>
<ul>
<li>enqueue() - add an item to the end of the queue</li>
<li>dequeue() - remove the front item</li>
<li>size() - return the number of items in queue</li>
<li>peek() - return the front item</li>
<li>is_empty() - check if the queue is empty</li>
</ul>
<h2 id="How-to-implement"><a href="#How-to-implement" class="headerlink" title="How to implement"></a>How to implement</h2><p>There are various ways to implement a queue in Python. In this post, we will learn how to implement it with the following ways.</p>
<ul>
<li>list</li>
<li>collections.deque</li>
<li>queue.Queue</li>
<li>linked list</li>
</ul>
<h3 id="using-list"><a href="#using-list" class="headerlink" title="using list"></a>using list</h3><p>In Python, list is like dynamic array. The items in list are stored contiguously in memory. Thus, the random access to list is as fast as array. However, inserting&#x2F;deleting item in front of the queue is slow because it requires shifting other items with time complexity O(n).</p>
<pre><code>st = []
st.append(1)
st.append(2)
st.append(3)
print(&quot;stack size: &#123;&#125;&quot;.format(len(st)))
print(st.pop(0))
print(st.pop(0))
print(st.pop(0))
print(&quot;stack size: &#123;&#125;&quot;.format(len(st)))
</code></pre>
<h3 id="using-collections-deque"><a href="#using-collections-deque" class="headerlink" title="using collections.deque"></a>using collections.deque</h3><p>deque is pronounced “deck” and stands for “double-ended queue”. It is built upon a doubly linked list which allows insert&#x2F;remove nodes from both ends with constant time O(1). Deque is the prefered method to implement a queue in Python. Note that deque is not thread-safe.</p>
<pre><code>from collections import deque

st = deque()
st.append(1)
st.append(2)
st.append(3)
print(&quot;stack size: &#123;&#125;&quot;.format(len(st)))
print(st.popleft())
print(st.popleft())
print(st.popleft())
print(&quot;stack size: &#123;&#125;&quot;.format(len(st)))
</code></pre>
<h3 id="using-queue-Queue"><a href="#using-queue-Queue" class="headerlink" title="using queue.Queue"></a>using queue.Queue</h3><p>If you need multi-threading, you can use queue.Queue module. However, it comes at a performance cost to achieve the thread-safety.</p>
<pre><code>from queue import Queue

st = Queue()
st.put(1)
st.put(2)
st.put(3)
print(&quot;stack size: &#123;&#125;&quot;.format(st.qsize()))
print(st.get())
print(st.get())
print(st.get())
print(&quot;stack size: &#123;&#125;&quot;.format(st.qsize()))
</code></pre>
<h3 id="using-linked-list"><a href="#using-linked-list" class="headerlink" title="using linked list"></a>using linked list</h3><p>You also can implement your own queue with the linked list data structure.</p>
<pre><code>class Node:
    def __init__(self, val):
        self.val = val
        self.next = None


class MyQueue:
    def __init__(self):
        self.front = self.rear = None
        self.size = 0

    def get_size(self):
        return self.size

    def is_empty(self):
        return self.size == 0

    def enqueue(self, val):
        n = Node(val)
        if self.rear == None:
            self.front = n
            self.rear = n
        else:
            self.rear.next = n
            self.rear = n

        self.size += 1

    def dequeue(self):
        if self.is_empty():
            raise Exception(&quot;empty stack&quot;)

        val = self.front.val
        self.front = self.front.next
        self.size -= 1
        return val

    def __str__(self):
        curr = self.front
        st_str = &quot;&quot;
        while curr:
            st_str += str(curr.val) + &quot;-&gt;&quot;
            curr = curr.next
        return st_str[:-2]


qu = MyQueue()
qu.enqueue(1)
qu.enqueue(2)
qu.enqueue(3)
print(&quot;queue: &#123;&#125;&quot;.format(qu))
print(&quot;queue size: &#123;&#125;&quot;.format(qu.size))
print(qu.dequeue())
print(qu.dequeue())
print(qu.dequeue())
print(&quot;queue size: &#123;&#125;&quot;.format(qu.size))
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title>RAID - Redundant Array of Inexpensive Disks</title>
    <url>/blog/raid-redundant-array-of-inexpensive-disks/</url>
    <content><![CDATA[<p>RAID stands for Redundant Array of Inexpensive (Independent) Disks.</p>
<p>On most situations you will be using one of the following four levels of RAIDs.</p>
<ul>
<li>RAID 0</li>
<li>RAID 1</li>
<li>RAID 5(6)</li>
<li>RAID 10 (also known as RAID 1+0)</li>
</ul>
<h2 id="RAID-0"><a href="#RAID-0" class="headerlink" title="RAID 0"></a>RAID 0</h2><p><img src="/images/raid0.svg" alt="Image"></p>
<p>Following are the key points to remember for RAID level 0.</p>
<ul>
<li>Minimum 2 disks.</li>
<li>Excellent performance (as blocks are striped).</li>
<li>No redundancy (no mirror, no parity).</li>
<li>Don’t use this for any critical system.</li>
</ul>
<h2 id="RAID-1"><a href="#RAID-1" class="headerlink" title="RAID 1"></a>RAID 1</h2><p><img src="/images/raid1.svg" alt="Image"></p>
<p>Following are the key points to remember for RAID level 1.</p>
<ul>
<li>Minimum 2 disks.</li>
<li>Good performance (no striping. no parity).</li>
<li>Excellent redundancy (as blocks are mirrored).</li>
</ul>
<h2 id="RAID-5-6"><a href="#RAID-5-6" class="headerlink" title="RAID 5(6)"></a>RAID 5(6)</h2><p><img src="/images/raid5.svg" alt="Image"></p>
<p>Following are the key points to remember for RAID level 5.</p>
<ul>
<li>Minimum 3 disks.</li>
<li>Good performance (as blocks are striped).</li>
<li>Good redundancy (distributed parity).</li>
<li>Best cost-effective option providing both performance and redundancy. Use this for DB that is heavily read oriented. Write operations will be slow.</li>
</ul>
<p>RAID6 allows two disks failure while RAID5 allows only one disk failure.</p>
<h2 id="RAID-10"><a href="#RAID-10" class="headerlink" title="RAID 10"></a>RAID 10</h2><p><img src="/images/raid10.svg" alt="Image"></p>
<p>Following are the key points to remember for RAID level 10.</p>
<ul>
<li>Minimum 4 disks.</li>
<li>This is also called as “stripe of mirrors”</li>
<li>Excellent redundancy (as blocks are mirrored)</li>
<li>Excellent performance (as blocks are striped)</li>
<li>If you can afford the dollar, this is the BEST option for any mission critical applications (especially databases).</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://www.prepressure.com/library/technology/raid">https://www.prepressure.com/library/technology/raid</a></li>
<li><a href="https://www.thegeekstuff.com/2010/08/raid-levels-tutorial/">https://www.thegeekstuff.com/2010/08/raid-levels-tutorial/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>RAID</tag>
      </tags>
  </entry>
  <entry>
    <title>Rationality Island</title>
    <url>/blog/rationality-island/</url>
    <content><![CDATA[<p>Rationality Island is a city where the citizens choose to live a happy and successful life. The city earned its name based on the mission statement, “Using rationality to solve your problems is the best way to enhance your life.” The citizens demonstrate the values of rationality, self-reliance, and determination. The people solve their own problems by analyzing and finding the cause of it. Instead of feeling disheartened when they face challenges, the citizens rationally find ways to overcome the problems. They persevere through the hardships and help each other. The city of Rationality Island is progressing every day and people work hard to live happily.</p>
<p>Because Rationality Island has a variety of landforms on it, there is a wide range of natural resources available.  The city is surrounded by water and there are exotic fish, which traders from all over the world come to buy. To the North of the main city lies the Crystal Mountains, where miners mine an abundant amount of gold, silver, and other precious metals. These metals are either crafted into jewelry or traded in the market. On the opposite side of the island, woodchoppers get a large amount of lumber, which is used for building ships for trading or used for houses and furniture. Trade is also an important factor in the economy. Goods are imported in the East Harbor and exported in the North Harbor. Hundreds of ships arrive every day and merchants trade spices and extraordinary furniture like Persian rugs for the supreme jewelry that craftsmen produce. The citizens  of Rationality Island are productive entrepreneurs who use their resources to enhance their life.</p>
<p>To live in Rationality Island, the citizens must follow one guiding principle: Respect everyone’s natural rights. Everyone is equal regardless of race and skin color because they enjoy the same natural rights. Also the citizens must be honest and honor their contract. Those who don’t show integrity and steal other’s property will be sentenced to jail with a fair trial. Even if anyone is caught doing a dishonorable act, the judges must listen to the defendant’s argument before giving the judgement. The citizens of Rationality Island believe that rationality should be used to enhance your life. Therefore, everyone must go to school to gain valuable knowledge. The citizens are free to produce and trade in a  place where violence is unacceptable and contracts are reliable.</p>
<p>The flag of Rationality Island is a symbol of the values that the citizens exhibit. The gold bar on the island represents the discoveries and opportunities that people make and receive. The purple mountains indicate the precious metals that are mined in the Crystal Mountains. The lush, green tree and bushes symbolize the large amount of lumber that is chopped. The sun symbolizes hope for citizens to improve and enhance their lives. The fish stands for the exotic salmon and tuna available. The blue ocean represents the peace and freedom that people enjoy. Finally, the light blue sky symbolizes the trade and success in the city.  Rationality Island is a prosperous city, for people demonstrate the values of rationality and independence.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Reach the Stars!</title>
    <url>/blog/reachthestars/</url>
    <content><![CDATA[<h2 id="Peace-Pals-International-Art-Exhibition-and-Awards-2022"><a href="#Peace-Pals-International-Art-Exhibition-and-Awards-2022" class="headerlink" title="Peace Pals International Art Exhibition and Awards - 2022"></a>Peace Pals International Art Exhibition and Awards - 2022</h2><p>Finalist for Age category 11-13</p>
<p><a href="https://peacepalsinternational.org/">https://peacepalsinternational.org/</a></p>
<p><img src="/images/Peace-Pals-Art-Contest-2022.jpg"></p>
<h2 id="Celebrating-Art-Contest-2021-Summer"><a href="#Celebrating-Art-Contest-2021-Summer" class="headerlink" title="Celebrating Art Contest - 2021 Summer"></a>Celebrating Art Contest - 2021 Summer</h2><p>Theme: My Summer Life</p>
<p><a href="https://www.celebratingart.com/">https://www.celebratingart.com/</a></p>
<p><img src="/images/IMG_20210919_133128.jpg"></p>
<h2 id="Google-Doodles-Participation-2021"><a href="#Google-Doodles-Participation-2021" class="headerlink" title="Google Doodles Participation - 2021"></a>Google Doodles Participation - 2021</h2><p><img src="/images/doodles_2021.jpg"></p>
<h2 id="Cupertino-Young-Artist-Award-2020"><a href="#Cupertino-Young-Artist-Award-2020" class="headerlink" title="Cupertino Young Artist Award - 2020"></a>Cupertino Young Artist Award - 2020</h2><p>Theme: New Normal(Covid 19)</p>
<p>Platinum Award, 1st Place (age 6 to 10)</p>
<p><a href="https://www.cupertino.org/our-city/commissions/arts-and-culture-commission/artist-awards">https://www.cupertino.org/our-city/commissions/arts-and-culture-commission/artist-awards</a></p>
<p><img src="/images/New-Normal.jpg"></p>
<h2 id="2018-2019-California-International-Student-Art-Contest"><a href="#2018-2019-California-International-Student-Art-Contest" class="headerlink" title="2018-2019 California International Student Art Contest"></a>2018-2019 California International Student Art Contest</h2><p>Honorable Mention</p>
<p><img src="/images/IMG_0382.jpeg"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>RMAN backup set, pieces, and datafiles</title>
    <url>/blog/rman-backup-set-pieces-and-datafiles/</url>
    <content><![CDATA[<p>RMAN backs up the datafiles, control file, archived log files, and server parameter files in a RMAN specific format called a backup piece. In a nutshell, a backup set is a bundle of dbf, ctl and redo file that can restore a database. A set of one or more such backup pieces makes up a backup set. A backup set is created using the BACKUP command.</p>
<h2 id="Useful-views-for-backup-query"><a href="#Useful-views-for-backup-query" class="headerlink" title="Useful views for backup query"></a>Useful views for backup query</h2><ul>
<li><p>V$BACKUP_SET displays information about backup sets from the control file. A backup set record is inserted after the backup set is successfully completed. The backup set stamp (SET_STAMP) and count (SET_COUNT) uniquely identify the backup set.</p>
</li>
<li><p>V$BACKUP_PIECE displays information about backup pieces from the control file. Each backup set consists of one or more backup pieces.</p>
</li>
<li><p>V$BACKUP_DATAFILE displays information about control files and datafiles in backup sets from the control file.</p>
</li>
<li><p>V$BACKUP_FILES displays information about all RMAN backups (both image copies and backup sets) and archived logs.This view simulates the LIST BACKUP and LIST COPY RMAN commands. This view requires that the database be set using the DBMS_RCVMAN.SETDATABASE procedure.</p>
</li>
</ul>
<h2 id="Useful-SQL-statements"><a href="#Useful-SQL-statements" class="headerlink" title="Useful SQL statements"></a>Useful SQL statements</h2><ul>
<li><p>Query backup set information. The very last record(e.g. RECID&#x3D;360) could be used as the start point for future queries.</p>
<p>  SQL&gt; select recid, set_stamp, set_count, backup_type,incremental_level,pieces,start_time,completion_time,elapsed_seconds,block_size from v$backup_set;</p>
</li>
</ul>
<p><img src="/images/rman01.png" alt="Image"></p>
<ul>
<li><p>Query backup pieces for certain backup set.</p>
<p>  SQL&gt; select piece#,compressed,start_time,completion_time,elapsed_seconds,bytes&#x2F;1024&#x2F;1024 from v$backup_piece where set_stamp&#x3D;1029939968 and set_count&#x3D;372;</p>
</li>
</ul>
<p><img src="/images/rman02.png" alt="Image"></p>
<ul>
<li><p>Query backup files for certain backup set.</p>
<p>  SQL&gt; select pkey,fname,file_type,bytes&#x2F;1024&#x2F;1024,bs_key,bs_type,bs_pieces,bs_incr_type,bs_bytes&#x2F;1024&#x2F;1024, bs_completion_time from v$backup_files where FILE_TYPE&#x3D;’PIECE’order by bs_key;</p>
</li>
</ul>
<p><img src="/images/rman03.png" alt="Image"></p>
<pre><code>SQL&gt; select pkey,file_type, fname, bytes/1024/1024,bs_key,bs_type,bs_incr_type,bs_bytes/1024/1024 from v$backup_files where bs_key=360;
</code></pre>
<p><img src="/images/rman04.png" alt="Image"></p>
<ul>
<li><p>Query total blocks and read blocks for certain backup set.</p>
<p>  SQL&gt; select set_stamp,set_count,incremental_level,datafile_blocks,blocks,block_size, CREATION_TIME ,completion_time,used_change_tracking,blocks_read from v$backup_datafile where set_count&gt;365;</p>
</li>
</ul>
<p><img src="/images/rman05.png" alt="Image"></p>
<ul>
<li><p>Query total blocks and read blocks for certain backup. The following example checks the read and write ratio when BCT is enabled and a full backup is done.</p>
<p>  SQL&gt; select sum(BLOCKS_READ*BLOCK_SIZE)&#x2F;1024&#x2F;1024&#x2F;1024 read_GB from v$backup_datafile where USED_CHANGE_TRACKING &#x3D; ‘YES’;</p>
<p>  READ_GB<br>  1195.7244</p>
<p>  SQL&gt; select sum(BLOCKS*BLOCK_SIZE)&#x2F;1024&#x2F;1024&#x2F;1024 write_GB from v$backup_datafile where USED_CHANGE_TRACKING &#x3D; ‘YES’;</p>
<pre><code>WRITE_GB
</code></pre>
<p>  898.733315</p>
<p>  SQL&gt; select sum(DATAFILE_BLOCKS*BLOCK_SIZE)&#x2F;1024&#x2F;1024&#x2F;1024 datafile_GB from v$backup_datafile where USED_CHANGE_TRACKING &#x3D; ‘YES’;</p>
<p>  DATAFILE_GB<br>  1195.7244</p>
<p>  SQL&gt; select sum(BLOCKS_READ)&#x2F;sum(DATAFILE_BLOCKS) from v$backup_datafile where USED_CHANGE_TRACKING &#x3D; ‘YES’;</p>
<p>  SUM(BLOCKS_READ)&#x2F;SUM(DATAFILE_BLOCKS)<br>1<br>  SQL&gt; select sum(BLOCKS)&#x2F;sum(DATAFILE_BLOCKS) from v$backup_datafile where USED_CHANGE_TRACKING &#x3D; ‘YES’;</p>
<p>  SUM(BLOCKS)&#x2F;SUM(DATAFILE_BLOCKS)<br>.751622457</p>
</li>
<li><p>The following example checks the read and write blocks ratio after incremental backup is done which is based on previous full backup. The write blocks ratio can be treated as data change ratio.</p>
<p>  $ cat bct_check.sql<br>  set linesize 500;</p>
<p>  select set_stamp,set_count,incremental_level, used_change_tracking,blocks_read,blocks,datafile_blocks,block_size from v$backup_datafile where set_count&gt;372 order by set_count;</p>
<p>  select sum(BLOCKS_READ*BLOCK_SIZE)&#x2F;1024&#x2F;1024&#x2F;1024 read_GB from v$backup_datafile where set_count&gt;372;</p>
<p>  select sum(BLOCKS*BLOCK_SIZE)&#x2F;1024&#x2F;1024&#x2F;1024 write_GB from v$backup_datafile where set_count&gt;372;</p>
<p>  select sum(DATAFILE_BLOCKS*BLOCK_SIZE)&#x2F;1024&#x2F;1024&#x2F;1024 total_GB from v$backup_datafile where set_count&gt;372;</p>
<p>  select sum(BLOCKS_READ)&#x2F;sum(DATAFILE_BLOCKS) “%READ” from v$backup_datafile where set_count&gt;372;</p>
<p>  select sum(BLOCKS)&#x2F;sum(DATAFILE_BLOCKS) “%WRITE” from v$backup_datafile where set_count&gt;372;</p>
<p>  exit</p>
</li>
</ul>
<p>$ sqlplus &#x2F; as sysdba @bct_check.sql</p>
<p><img src="/images/rman06.png" alt="Image"></p>
<pre><code>READ_GB
662.855164

WRITE_GB
103.244957

TOTAL_GB
1195.75419

%READ
.554337382

%WRITE
.086336245
</code></pre>
<ul>
<li><p>The following command also checks total write data during backup.</p>
<p>  SQL&gt; select sum(BYTES)&#x2F;1024&#x2F;1024&#x2F;1024 from v$backup_piece where set_count&gt;372;</p>
<p>  SUM(BYTES)&#x2F;1024&#x2F;1024&#x2F;1024<br>  130.938721</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Backup Recovery</tag>
      </tags>
  </entry>
  <entry>
    <title>Run pgbench on PostgreSQL</title>
    <url>/blog/run-pgbench-on-postgresql/</url>
    <content><![CDATA[<h2 id="Install-PostgreSQL"><a href="#Install-PostgreSQL" class="headerlink" title="Install PostgreSQL"></a>Install PostgreSQL</h2><p>Refer to this <a href="https://www.flamingbytes.com/blog/getting-started-with-postgresql/">post</a> to install PostgreSQL.</p>
<h2 id="Intro-to-pgbench"><a href="#Intro-to-pgbench" class="headerlink" title="Intro to pgbench"></a>Intro to pgbench</h2><p><a href="https://www.postgresql.org/docs/current/pgbench.html">pgbench</a> is a simple program for running benchmark tests on PostgreSQL. It runs the same sequence of SQL commands over and over, possibly in multiple concurrent database sessions, and then calculates the average transaction rate (transactions per second). By default, pgbench tests a scenario that is loosely based on TPC-B, involving five SELECT, UPDATE, and INSERT commands per transaction. However, it is easy to test other cases by writing your own transaction script files.</p>
<p>Available built-in scripts are: tpcb-like, simple-update and select-only.</p>
<span id="more"></span>

<h2 id="Initialize-PostgreSQL"><a href="#Initialize-PostgreSQL" class="headerlink" title="Initialize PostgreSQL"></a>Initialize PostgreSQL</h2><p>You need to create a database prior to initialization. You can adjust the scale of database as needed. For example, -s 5000 will create 500,000,000 rows in the pgbench_accounts table. Roughly, the size is ~63GB.</p>
<pre><code># pg_server=10.10.10.243
# psql --host=$pg_server --port=5432 --username=postgres -w -c &quot;create database testdb&quot;

# scale=5000; pgbench -h $pg_server -p 5432 -U postgres -i -s $scale testdb


# psql --host=$pg_server --port=5432 --username=postgres -w -c &quot;\l&quot;
# psql --host=$pg_server --port=5432 --username=postgres -w -d testdb -c &quot;\dt+&quot;
                                        List of relations
 Schema |       Name       | Type  |  Owner   | Persistence | Access method |  Size   | Description
--------+------------------+-------+----------+-------------+---------------+---------+-------------
 public | pgbench_accounts | table | postgres | permanent   | heap          | 63 GB   |
 public | pgbench_branches | table | postgres | permanent   | heap          | 216 kB  |
 public | pgbench_history  | table | postgres | permanent   | heap          | 0 bytes |
 public | pgbench_tellers  | table | postgres | permanent   | heap          | 2200 kB |
(4 rows)
</code></pre>
<h2 id="Pgbench-options"><a href="#Pgbench-options" class="headerlink" title="Pgbench options"></a>Pgbench options</h2><p>The following are commonly used pgbench options and you can specify <a href="https://www.postgresql.org/docs/current/pgbench.html">more</a> as needed.</p>
<p>-<br>-c clients</p>
<p>–client&#x3D;clients</p>
<p>Number of clients simulated, that is, number of concurrent database sessions. Default is 1.</p>
<p>-<br>-j threads</p>
<p>–jobs&#x3D;threads</p>
<p>Number of worker threads within pgbench. Using more than one thread can be helpful on multi-CPU machines. Clients are distributed as evenly as possible among available threads. Default is 1.</p>
<p>-<br>-t transactions</p>
<p>–transactions&#x3D;transactions</p>
<p>Number of transactions each client runs. Default is 10.</p>
<h2 id="Run-pgbench-test"><a href="#Run-pgbench-test" class="headerlink" title="Run pgbench test"></a>Run pgbench test</h2><p>You should adjust the option values based on the available system resource. Usually, you have to experiment by gradually changing the values until you get the sweet spot.</p>
<pre><code># clients=192; threads=96; transactions=50000
# pgbench -h $pg_server -p 5432 -U postgres -c $clients -j $threads -t $transactions testdb -b simple-update
pgbench (15.3)
starting vacuum...end.
transaction type: &lt;builtin: simple update&gt;
scaling factor: 5000
query mode: simple
number of clients: 192
number of threads: 96
maximum number of tries: 1
number of transactions per client: 50000
number of transactions actually processed: 9600000/9600000
number of failed transactions: 0 (0.000%)
latency average = 18.814 ms
initial connection time = 97.392 ms
tps = 10204.933020 (without initial connection time)
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>PostgreSQL</tag>
        <tag>Pgbench</tag>
      </tags>
  </entry>
  <entry>
    <title>Scale a system to support millions of users</title>
    <url>/blog/scale-a-system-to-support-millions-of-users/</url>
    <content><![CDATA[<h2 id="Start-from-single-server"><a href="#Start-from-single-server" class="headerlink" title="Start from single server"></a>Start from single server</h2><p>Building a large scale system is not one time effort, it should be an iterative process as the user workload increases. The journey could just start with single server and single user request.</p>
<p><img src="/images/scale-million-users-1.png" alt="Image"></p>
<ol>
<li>User access a website through domain name, such as example.com</li>
<li>IP address is returned to the browser or the mobile app.</li>
<li>HTTP requests are sent to the web server.</li>
<li>The web server returns the requested resources, such as a HTML page or JSON response.</li>
</ol>
<h2 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h2><p>With the growth of the user base, we can separate the web server and database server to allow them scale independently.</p>
<p><img src="/images/scale-million-users-2.png" alt="Image"></p>
<h3 id="SQL-vs-NoSQL"><a href="#SQL-vs-NoSQL" class="headerlink" title="SQL vs. NoSQL"></a>SQL vs. NoSQL</h3><p>We can choose between relational database and non-relational database.</p>
<p>Relational database is also called relational database management system(RDBMS) or SQL database. The popular relational databases are MySQL, Oracle database, MS-SQL database, PostgreSQL and so on. Relational database stores data in tables and rows. Join operations can be performed across different database tables.</p>
<p>Non-relational database is also called NoSQL database. The popular ones are MongoDB, CouchDB, Cassandra, HBase, Neo4j, Amazon DynamoDB and so on. They can be grouped into four categories: key-value stores, graph stores, column stores, and document stores. Generally, join operations are not supported in non-relational database.</p>
<p>Non-relational database might be the choice if:</p>
<ul>
<li>very low latency is required</li>
<li>data is unstructured or not relational</li>
<li>the data needs to be serialized and deserialied(JSON, XML, YAML, etc)</li>
<li>massive amount of data needs to be stored</li>
</ul>
<h2 id="Vertical-scaling-vs-horizontal-scaling"><a href="#Vertical-scaling-vs-horizontal-scaling" class="headerlink" title="Vertical scaling vs. horizontal scaling"></a>Vertical scaling vs. horizontal scaling</h2><p>Vertical scaling, also called “scale up”, means adding more compute power(CPU&#x2F;Memory), disk bandwidth, and network bandwidth to the servers. Vertical scaling is simple but it’s impossible to add unlimited resource to a single server. It doesn’t provide failover and redundancy and it may cause single point of failure.</p>
<p>Horizontal scaling, also called “scale out”, means adding more servers to the resouce pool. It’s suitable for large scale applications.</p>
<h2 id="Load-balancer"><a href="#Load-balancer" class="headerlink" title="Load balancer"></a>Load balancer</h2><p>With the single server setup, users won’t be able to access the website if the web server goes offline. Users may experience slow response(high latency) or failed connection if there are too many user requests to the server simultaneously.</p>
<p>A load balancer can help evenly distribute incoming traffic among web servers.</p>
<p><img src="/images/scale-million-users-3.png" alt="Image"></p>
<p>Users connect to the public IP of the load balancer. The web servers sitting behind the load balancer are unreachable directly by the users anymore. The private IPs can be used for communications between load balancer and web servers.</p>
<p>After the load balancer and more web servers are added, we improved the high availability of the web servers. For example, if web server 1 goes offline, the traffic can be routed to other web servers which are still healthy. As the website traffic grows rapidly, we can always add more web servers to scale and handle the incoming traffic.</p>
<h2 id="Database-replication"><a href="#Database-replication" class="headerlink" title="Database replication"></a>Database replication</h2><p>Now that we can scale the web tier as needed, how about the data tier? Database replication is a common technique to address the problems like failover and redundancy.</p>
<p>“Database replication can be used on many database management systems (DBMS), usually with a primary&#x2F;replica relationship between the original and the copies.” <a href="https://en.wikipedia.org/wiki/Replication_(computing)">Source</a></p>
<p>A master database generally supports write operations only. A slave database replicates data from master and only supports read operations.</p>
<p><img src="/images/scale-million-users-4.png" alt="Image"></p>
<p>The master-slave model may not be the only method. We just use this model to learn about how to scale the database tier. We can achieve much better performance by spearating the write and read operations to master and slave databases. It also provides the data reliability in the case of natural disaster like earthquake.  The data is highly available even if a database is offline.</p>
<ul>
<li>If the master database goes offline, a slave database will be promoted as the new master. A new slave database will replace the old one for data replication. Promoting a master is not that simple process because the data in slave database may not be up to date. The missing data have to be recovered by some recovery mechanism.</li>
<li>If the slave databases are go offline, read operations can be directed to master database temporarily until a new slave database is in place.</li>
</ul>
<h2 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h2><p>After we have the design of scalable web and data tier, it’s time to improve the system performance like response time. Adding a cache layer can help serve the user requests more quickly.</p>
<ol>
<li>If data exists in cache, read data from cache</li>
<li>If data doesn’t exist in cache, fetch data from database and save data to cache. Then the cached data are returned to web servers. This is so called read-through cache.</li>
</ol>
<p><img src="/images/scale-million-users-5.png" alt="Image"></p>
<p>Here are some considerations for using a cache system:</p>
<ul>
<li>Consider using cache when the data is read frequently but write(modification) infrequently</li>
<li>Use an expiration policy to reload the data from data store</li>
<li>Use an eviction policy to add data to the already full cache. Least-recently-used(LRU) or First in First Out(FIFO)?</li>
<li>Maintain the consistency between data store and cache. MemCache solution?</li>
<li>Avoid single point of failure of a cache system</li>
</ul>
<h2 id="Content-Delivery-Network-CDN"><a href="#Content-Delivery-Network-CDN" class="headerlink" title="Content Delivery Network(CDN)"></a>Content Delivery Network(CDN)</h2><p>A CDN is a network of geographially dispersed servers used to deliver static content. CDN servers cache static content like images, videos, CSS, Javascript files, etc.</p>
<p>When a user visits a website, a CDN server closest to the user will deliver the static content. The website loads faster from a closer CDN server.</p>
<p><img src="/images/scale-million-users-6.png" alt="Image"></p>
<ol>
<li>If the content to be loaded is not in CDN, get it from the origin server and store it in CDN server.</li>
<li>If the content is in CDN, return it from CDN.</li>
</ol>
<p><img src="/images/scale-million-users-7.png" alt="Image"></p>
<h2 id="Stateless-web-tier"><a href="#Stateless-web-tier" class="headerlink" title="Stateless web tier"></a>Stateless web tier</h2><p>In order to scale the web tier horizontally, we need to move the state(e.g. user session data) out of the web tier. A practice is to store the session data in the persistent storage such as SQL or NoSQL database. The web servers in the cluster can access the state data from the database. This is called stateless web tier.</p>
<p>In the stateless architecture, the HTTP requests from many users can be sent to any web servers. The session data will be fetched from a shared data store which is separated from web servers.</p>
<p><img src="/images/scale-million-users-8.png" alt="Image"></p>
<p>After the session data is stored out of web servers, auto-scaling of the web tier becomes a lot more easier.</p>
<p><img src="/images/scale-million-users-9.png" alt="Image"></p>
<h2 id="Data-centers"><a href="#Data-centers" class="headerlink" title="Data centers"></a>Data centers</h2><p>As the website access grows internationally, supporting multiple data centers globally is crucial. In the event of any data center outage, the traffic can be directed to a healthy data center as well.</p>
<p><img src="/images/scale-million-users-10.png" alt="Image"></p>
<h2 id="Message-queue"><a href="#Message-queue" class="headerlink" title="Message queue"></a>Message queue</h2><p>To further scale the system, we can decouple the system components so that they can be scaled independently.</p>
<p>A message queue is a durable component, stored in memory, that supports asynchronous communication. It serves as a buffer and distributes asynchronous requests. Typically, a message queue has an input service, also called producers&#x2F;publishers, to create message, and publish them to the message queue. The consumer services connect to the queue and process the messages asynchronously.</p>
<p><img src="/images/scale-million-users-11.png" alt="Image"></p>
<h2 id="Logging-monitoring-automation"><a href="#Logging-monitoring-automation" class="headerlink" title="Logging, monitoring, automation"></a>Logging, monitoring, automation</h2><p>As the system scale for a very large business, it’s necessary to invest in the logging, monitoring and automation. These tools can help troubleshoot issues, understand system insight and system health, and improve productivity.</p>
<h2 id="Database-scaling"><a href="#Database-scaling" class="headerlink" title="Database scaling"></a>Database scaling</h2><p>Vertical scaling and horizontal scaling are the two approaches for database scaling.</p>
<p>Vertical scaling, also known as scaling up, is to scale by adding more hardware components(CPU, Memory, Disk, Network) to the existing server. But it has a hardware limit and the cost can be high. Also there is risk of single point of failures.</p>
<p>Horizontal scaling, also known as sharding, is a practice of adding more database servers. Sharding separates large databases to smaller ones which are called shards. A hash function(e.g. user_id % n) can be used to determine which shard should be accessed. In this example of hash function, the user_id is the sharding key which is an important factor to consider when to implement a sharding strategy. The goal is to choose a key which can evenly distribute the data across shards.</p>
<p>With database sharding, it may introduce complexities and new challenges to the system.</p>
<ul>
<li>resharding data</li>
<li>hotspot key</li>
<li>join operations across database shards</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Scaling a system is an iterative process. As the user requests increase and system scales, more fine-tuning and new strategies are needed. In summary, we can scale the system to support millions of users by addressing the following areas.</p>
<ul>
<li>Keep web tier stateless by storing session data out of web servers</li>
<li>Have redundancy for each tier</li>
<li>Cache data as much as possible to improve response time</li>
<li>Build multiple data centers</li>
<li>Use CDN to serve static content</li>
<li>Sharding the data tier if needed</li>
<li>Decouple the components to increase throughput(e.g. message queue)</li>
<li>Add tools to improve ease of use</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>System Design</tag>
      </tags>
  </entry>
  <entry>
    <title>Semaphore</title>
    <url>/blog/semaphore/</url>
    <content><![CDATA[<h2 id="What-is-semaphore"><a href="#What-is-semaphore" class="headerlink" title="What is semaphore"></a>What is semaphore</h2><p>A semaphore is a very relaxed type of lockable object. A given semaphore has a predefined maximum count, and a current count. You take ownership of a semaphore with a wait operation, also referred to as decrementing the semaphore, or even just abstractly called P. You release ownership with a signal operation, also referred to as incrementing the semaphore, a post operation, or abstractly called V. The single-letter operation names are from Dijkstra’s original paper on semaphores.</p>
<p>Every time you wait on a semaphore, you decrease the current count. If the count was greater than zero then the decrement just happens, and the wait call returns. If the count was already zero then it cannot be decremented, so the wait call will block until another thread increases the count by signaling the semaphore.</p>
<h2 id="Semaphore-tuning"><a href="#Semaphore-tuning" class="headerlink" title="Semaphore tuning"></a>Semaphore tuning</h2><p>To display the semaphore limits:</p>
<pre><code>$ cat /proc/sys/kernel/sem
300	307200	32	1024

$ sysctl -a | grep sem
kernel.sem = 300	307200	32	1024

$ ipcs -l | grep -i &quot;sem&quot;
------ Semaphore Limits --------
max semaphores per array = 300
max semaphores system wide = 307200
max ops per semop call = 32
semaphore max value = 32767
</code></pre>
<p>The values of the semaphore parameters are displayed in the following order.</p>
<ul>
<li>SEMMSL - The maximum number of semaphores in a sempahore set.</li>
<li>SEMMNS - A system-wide limit on the number of semaphores in all semaphore sets. The maximum number of sempahores in the system.</li>
<li>SEMOPM - The maximum number of operations in a single semop call</li>
<li>SEMMNI - A system-wide limit on the maximum number of semaphore identifiers (sempahore sets)</li>
</ul>
<p>To display the current semaphore status:</p>
<pre><code>$ ipcs -u | egrep -i &quot;used arrays|sem&quot;
------ Semaphore Status --------
used arrays = 3
allocated semaphores = 3
</code></pre>
<p>To display the active semaphore sets info:</p>
<pre><code>$ ipcs -s
------ Semaphore Arrays --------
key        semid      owner      perms      nsems
0x00000000 0          root       600        1
0x00000000 32769      root       600        1
0x00005653 229380     root       666        1
</code></pre>
<p>To adjust the semaphore values on the fly:</p>
<pre><code>$ echo 300   307200   32   1024 &gt; /proc/sys/kernel/sem
</code></pre>
<p>To modify system semaphore values permanently:</p>
<pre><code>$ echo &quot;kernel.sem = 300  307200  32  1024&quot; &gt;&gt; /etc/sysctl.conf
$ sysctl -p 
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Semaphore</tag>
      </tags>
  </entry>
  <entry>
    <title>Separating from Pangaea</title>
    <url>/blog/separating-from-pangaea/</url>
    <content><![CDATA[<p>All the continents of Planet Earth rest on giant slabs of rock called plate tectonics. Due to the movement of the plates, the continents drifted around until they completely separated from the supercontinent “Pangaea”. Over the past millions of years, the continents moved to where they are now. They are still moving today and the location of the continents will be very different after millions of years.</p>
<p>Alfred Wegener created the theory of Continental Drift. He thought that the continents were kind of like a jigsaw puzzle. Mountain ranges seemed to start on one continent and continue on another. For example, the Appalachian Mountains in North America matched up neatly with the Scottish Highlands. Fossils in various places showed that the climate there a long time ago had been different. Fossils of fresh water reptile Mesosaurus were discovered in both Africa and South America. It could not have swum across the salty ocean so scientists conclude that the two continents were once joined.</p>
<p>This theory was supported by the theory of plate tectonics. This theory explains how forces deep within Earth caused ocean floors to spread and continents to move. It describes how the lithosphere is made up of huge plates of solid rock. The continents rest on these plates. The asthenosphere is made up of almost melted rock and acts as a slippery surface for the plates to move on. Magma is pushed from the mantle toward the movement when the plates move. Tension is caused by the upward movement. It moves the ocean floor apart and separates the plates. The continents that rest on these plates also move apart.</p>
<p>Based on fossils, rocks, and other geological evidence, scientists concluded that the continents were once part of the supercontinents “Pangaea”. Over time, the continents spread apart due to the movement of plate tectonics. Even now, the continents are still moving and North America is drifting closer to Asia and Australia.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
  </entry>
  <entry>
    <title>Set in golang</title>
    <url>/blog/set-in-golang/</url>
    <content><![CDATA[<p>Go does not have set by default but there is way to imitate it in Go.</p>
<p>The following is an example code piece to create “set” in Go by setting a map with empty values.</p>
<pre><code>$ cat test.go
package main

import (
    &quot;fmt&quot;
)

// Main function
func main() &#123;
    set := make(map[string]struct&#123;&#125;)
    set[&quot;cat&quot;] = struct&#123;&#125;&#123;&#125;
    set[&quot;dog&quot;] = struct&#123;&#125;&#123;&#125;
    set[&quot;rabbit&quot;] = struct&#123;&#125;&#123;&#125;

    if _, ok := set[&quot;rabbit&quot;]; ok &#123;
        fmt.Println(&quot;rabbit exists&quot;)
    &#125;

    delete(set,&quot;rabbit&quot;)

    if _, ok := set[&quot;rabbit&quot;]; ok &#123;
        fmt.Println(&quot;exist&quot;)
    &#125;else&#123;
        fmt.Println(&quot;rabbit doesn&#39;t exist&quot;)
    &#125;
&#125;

$ go run test.go
rabbit exists
rabbit doesn&#39;t exist
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Setting up LVM volumes on a mdraid array</title>
    <url>/blog/setting-up-lvm-volumes-on-a-mdraid-array/</url>
    <content><![CDATA[<h2 id="Setting-up-mdraid-array"><a href="#Setting-up-mdraid-array" class="headerlink" title="Setting up mdraid array"></a>Setting up mdraid array</h2><pre><code>$ mdadm --create /dev/md0 --name=mdvol --level=raid0 --raid-devices=2 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.

$ cat /proc/mdstat
Personalities : [raid0] [linear]
md0 : active raid0 nvme4n1[3] nvme3n1[2] nvme2n1[1] nvme1n1[0]
      15002423296 blocks super 1.2 512k chunks

unused devices: &lt;none&gt;

$ mdadm --detail /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Tue Feb  1 19:07:32 2022
        Raid Level : raid0
        Array Size : 15002423296 (13.97 TiB 15.36 TB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Tue Feb  1 19:07:32 2022
             State : clean
    Active Devices : 4
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 0

        Chunk Size : 512K

Consistency Policy : none

              Name : mdvol
              UUID : aa6fe868:3aa2391d:37a21dc8:d0f4c5f1
            Events : 0

    Number   Major   Minor   RaidDevice State
       0     259        8        0      active sync   /dev/nvme1n1
       1     259        9        1      active sync   /dev/nvme2n1
       2     259       10        2      active sync   /dev/nvme3n1
       3     259       11        3      active sync   /dev/nvme4n1
</code></pre>
<h2 id="Settup-LVM-volumes-on-mdraid-array"><a href="#Settup-LVM-volumes-on-mdraid-array" class="headerlink" title="Settup LVM volumes on mdraid array"></a>Settup LVM volumes on mdraid array</h2><pre><code>$ pvcreate /dev/md0
$ pvs | egrep &quot;PV|md&quot;
  PV             VG     Fmt  Attr PSize  PFree
  /dev/md0              lvm2 ---  13.97t 13.97t
$ vgcreate testvg /dev/md0  

$ vgs | egrep &quot;VG|testvg&quot;
  VG     #PV #LV #SN Attr   VSize  VFree
  testvg   1   0   0 wz--n- 13.97t 13.97t

$ lvcreate -n testlv11 -L 500G testvg -Wy --yes

$ lvs | egrep &quot;LV|testvg&quot;
  LV       VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  testlv11 testvg -wi-a----- 500.00g  

$ lvs -ao name,size,stripesize,chunksize,metadata_percent | egrep &quot;LV|testlv&quot;
  LV       LSize   Stripe Chunk Meta%
  testlv11 500.00g     0     0  

$ mkfs.ext4 /dev/testvg/testlv11

$ mkdir -p /mnt/testmnt11

$ mount /dev/testvg/testlv11 /mnt/testmnt11

$ lsblk
NAME                MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
&lt;omitted...&gt;
nvme1n1             259:8    0  3.5T  0 disk
└─md0                 9:0    0   14T  0 raid0
  └─testvg-testlv11 253:3    0  500G  0 lvm   /mnt/testmnt11
nvme2n1             259:11   0  3.5T  0 disk
└─md0                 9:0    0   14T  0 raid0
  └─testvg-testlv11 253:3    0  500G  0 lvm   /mnt/testmnt11
nvme3n1             259:10   0  3.5T  0 disk
└─md0                 9:0    0   14T  0 raid0
  └─testvg-testlv11 253:3    0  500G  0 lvm   /mnt/testmnt11
nvme4n1             259:9    0  3.5T  0 disk
└─md0                 9:0    0   14T  0 raid0
  └─testvg-testlv11 253:3    0  500G  0 lvm   /mnt/testmnt11  
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Mdraid</tag>
        <tag>LVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Setup CockroachDB cluster with HAProxy load balancing</title>
    <url>/blog/setup-cockroachdb-cluster-with-haproxy-load-balancing/</url>
    <content><![CDATA[<p>Each CockroachDB node is an equally suitable SQL gateway to your cluster, but to ensure client performance and reliability, it’s important to use load balancing.</p>
<ul>
<li><p>Performance: Load balancers spread client traffic across nodes. This prevents any one node from being overwhelmed by requests and improves overall cluster performance (queries per second).</p>
</li>
<li><p>Reliability: Load balancers decouple client health from the health of a single CockroachDB node. In cases where a node fails, the load balancer redirects client traffic to available nodes.</p>
</li>
</ul>
<p>HAProxy is one of the most popular open-source TCP load balancers, and CockroachDB includes a built-in command for generating a configuration file that is preset to work with your running cluster.</p>
<p>With a single load balancer, client connections are resilient to node failure, but the load balancer itself is a point of failure. It’s therefore best to make load balancing resilient as well by using multiple load balancing instances, with a mechanism like floating IPs or DNS to select load balancers for clients.</p>
<p>For performance and availability reasons, it is not recommended to run the HAproxy on the same node as the cockroachDB.</p>
<p>This post shows a detailed steps how to deploy a HAProxy load balancer for CockroachDB cluster.</p>
<h2 id="Deploy-the-CockroachDB-cluster"><a href="#Deploy-the-CockroachDB-cluster" class="headerlink" title="Deploy the CockroachDB cluster"></a>Deploy the CockroachDB cluster</h2><pre><code>[root@host1 ~]# ps -ef | grep cockroach | grep -v grep
root      4783     1 99 18:59 ?        2-17:35:59 cockroach start --log-dir=/var/log/cockroachdb_logs/1 --store=/mnt/cockroachdb_mnt1 --insecure --listen-addr=host1:26257 --http-addr=host1:8080 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      4876     1 95 18:59 ?        03:33:42 cockroach start --log-dir=/var/log/cockroachdb_logs/2 --store=/mnt/cockroachdb_mnt2 --insecure --listen-addr=host1:26258 --http-addr=host1:8081 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      4932     1 77 18:59 ?        02:53:13 cockroach start --log-dir=/var/log/cockroachdb_logs/3 --store=/mnt/cockroachdb_mnt3 --insecure --listen-addr=host1:26259 --http-addr=host1:8082 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      5049     1 99 18:59 ?        06:02:17 cockroach start --log-dir=/var/log/cockroachdb_logs/4 --store=/mnt/cockroachdb_mnt4 --insecure --listen-addr=host1:26260 --http-addr=host1:8083 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      5131     1 94 18:59 ?        03:31:51 cockroach start --log-dir=/var/log/cockroachdb_logs/5 --store=/mnt/cockroachdb_mnt5 --insecure --listen-addr=host1:26261 --http-addr=host1:8084 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      5220     1 72 18:59 ?        02:42:39 cockroach start --log-dir=/var/log/cockroachdb_logs/6 --store=/mnt/cockroachdb_mnt6 --insecure --listen-addr=host1:26262 --http-addr=host1:8085 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      5309     1 94 18:59 ?        03:30:22 cockroach start --log-dir=/var/log/cockroachdb_logs/7 --store=/mnt/cockroachdb_mnt7 --insecure --listen-addr=host1:26263 --http-addr=host1:8086 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      5358     1 58 18:59 ?        02:11:20 cockroach start --log-dir=/var/log/cockroachdb_logs/8 --store=/mnt/cockroachdb_mnt8 --insecure --listen-addr=host1:26264 --http-addr=host1:8087 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB

[root@host1 ~]# cockroach node status --insecure --host=host1:26257
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+----------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-17 18:59:37.989384 | 2022-09-17 22:43:39.507057 |          | true         | true
   2 | host1:26258 | host1:26258 | v22.1.6 | 2022-09-17 18:59:39.128398 | 2022-09-17 22:43:40.640431 |          | true         | true
   3 | host1:26259 | host1:26259 | v22.1.6 | 2022-09-17 18:59:39.374127 | 2022-09-17 22:43:40.885678 |          | true         | true
   4 | host1:26260 | host1:26260 | v22.1.6 | 2022-09-17 18:59:39.726667 | 2022-09-17 22:43:36.73931  |          | true         | true
   5 | host1:26261 | host1:26261 | v22.1.6 | 2022-09-17 18:59:39.971549 | 2022-09-17 22:43:36.983538 |          | true         | true
   6 | host1:26262 | host1:26262 | v22.1.6 | 2022-09-17 18:59:40.225358 | 2022-09-17 22:43:37.235768 |          | true         | true
   7 | host1:26263 | host1:26263 | v22.1.6 | 2022-09-17 18:59:40.447688 | 2022-09-17 22:43:37.459962 |          | true         | true
   8 | host1:26264 | host1:26264 | v22.1.6 | 2022-09-17 18:59:40.677217 | 2022-09-17 22:43:37.690883 |          | true         | true
   9 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-17 18:59:41.604348 | 2022-09-17 22:43:38.613662 |          | true         | true
  10 | host2:26258 | host2:26258 | v22.1.6 | 2022-09-17 18:59:41.904071 | 2022-09-17 22:43:38.912773 |          | true         | true
  11 | host2:26259 | host2:26259 | v22.1.6 | 2022-09-17 18:59:42.194446 | 2022-09-17 22:43:39.202688 |          | true         | true
  12 | host2:26260 | host2:26260 | v22.1.6 | 2022-09-17 18:59:42.512201 | 2022-09-17 22:43:39.519727 |          | true         | true
  13 | host2:26261 | host2:26261 | v22.1.6 | 2022-09-17 18:59:42.850974 | 2022-09-17 22:43:39.860063 |          | true         | true
  14 | host2:26262 | host2:26262 | v22.1.6 | 2022-09-17 18:59:43.177162 | 2022-09-17 22:43:40.191082 |          | true         | true
  15 | host2:26263 | host2:26263 | v22.1.6 | 2022-09-17 18:59:43.543973 | 2022-09-17 22:43:40.553146 |          | true         | true
  16 | host2:26264 | host2:26264 | v22.1.6 | 2022-09-17 18:59:43.858993 | 2022-09-17 22:43:40.86705  |          | true         | true
  17 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-17 18:59:44.807641 | 2022-09-17 22:43:37.315927 |          | true         | true
  18 | host3:26258 | host3:26258 | v22.1.6 | 2022-09-17 18:59:45.119533 | 2022-09-17 22:43:37.628497 |          | true         | true
  19 | host3:26259 | host3:26259 | v22.1.6 | 2022-09-17 18:59:45.508535 | 2022-09-17 22:43:38.015714 |          | true         | true
  20 | host3:26260 | host3:26260 | v22.1.6 | 2022-09-17 18:59:45.901631 | 2022-09-17 22:43:38.409443 |          | true         | true
  21 | host3:26261 | host3:26261 | v22.1.6 | 2022-09-17 18:59:46.191724 | 2022-09-17 22:43:38.700531 |          | true         | true
  22 | host3:26262 | host3:26262 | v22.1.6 | 2022-09-17 18:59:46.540504 | 2022-09-17 22:43:39.048942 |          | true         | true
  23 | host3:26263 | host3:26263 | v22.1.6 | 2022-09-17 18:59:46.944353 | 2022-09-17 22:43:39.453175 |          | true         | true
  24 | host3:26264 | host3:26264 | v22.1.6 | 2022-09-17 18:59:47.274458 | 2022-09-17 22:43:39.782378 |          | true         | true
(24 rows)
</code></pre>
<h2 id="Generate-HAProxy-configuration-file"><a href="#Generate-HAProxy-configuration-file" class="headerlink" title="Generate HAProxy configuration file"></a>Generate HAProxy configuration file</h2><p>Install HAProxy on one of the CockroachDB nodes:</p>
<pre><code>[root@host1 ~]# yum install haproxy
[root@host1 ~]# haproxy -v
HA-Proxy version 1.5.18 2016/05/10
Copyright 2000-2016 Willy Tarreau &lt;willy@haproxy.org&gt;
</code></pre>
<p>Generate the HAProxy configuration file:</p>
<pre><code>[root@host1 ~]# cockroach gen haproxy --host=host1:26257 --insecure

[root@host1 ~]# cat haproxy.cfg

global
  maxconn 4096

defaults
    mode                tcp

    # Timeout values should be configured for your specific use.
    # See: https://cbonte.github.io/haproxy-dconv/1.8/configuration.html#4-timeout%20connect

    # With the timeout connect 5 secs,
    # if the backend server is not responding, haproxy will make a total
    # of 3 connection attempts waiting 5s each time before giving up on the server,
    # for a total of 15 seconds.
    retries             2
    timeout connect     5s

    # timeout client and server govern the maximum amount of time of TCP inactivity.
    # The server node may idle on a TCP connection either because it takes time to
    # execute a query before the first result set record is emitted, or in case of
    # some trouble on the server. So these timeout settings should be larger than the
    # time to execute the longest (most complex, under substantial concurrent workload)
    # query, yet not too large so truly failed connections are lingering too long
    # (resources associated with failed connections should be freed reasonably promptly).
    timeout client      10m
    timeout server      10m

    # TCP keep-alive on client side. Server already enables them.
    option              clitcpka

listen psql
    bind :26257
    mode tcp
    balance roundrobin
    option httpchk GET /health?ready=1
    server cockroach1 host1:26257 check port 8080
    server cockroach2 host1:26258 check port 8081
    server cockroach3 host1:26259 check port 8082
    server cockroach4 host1:26260 check port 8083
    server cockroach5 host1:26261 check port 8084
    server cockroach6 host1:26262 check port 8085
    server cockroach7 host1:26263 check port 8086
    server cockroach8 host1:26264 check port 8087
    server cockroach9 host2:26257 check port 8080
    server cockroach10 host2:26258 check port 8081
    server cockroach11 host2:26259 check port 8082
    server cockroach12 host2:26260 check port 8083
    server cockroach13 host2:26261 check port 8084
    server cockroach14 host2:26262 check port 8085
    server cockroach15 host2:26263 check port 8086
    server cockroach16 host2:26264 check port 8087
    server cockroach17 host3:26257 check port 8080
    server cockroach18 host3:26258 check port 8081
    server cockroach19 host3:26259 check port 8082
    server cockroach20 host3:26260 check port 8083
    server cockroach21 host3:26261 check port 8084
    server cockroach22 host3:26262 check port 8085
    server cockroach23 host3:26263 check port 8086
    server cockroach24 host3:26264 check port 8087
</code></pre>
<h2 id="Start-HAProxy"><a href="#Start-HAProxy" class="headerlink" title="Start HAProxy"></a>Start HAProxy</h2><p>It’s not recommended to deploy HAProxy on the same node as CockroachDB for performance and availability reasons. Thus, we are going to start the HAProxy  on a dedicated node.</p>
<p>Connect to the dedicated HAProxy node and install the proxy on it:</p>
<pre><code>[root@host4 ~]# yum install haproxy
</code></pre>
<p>Start HAproxy with the generated configuration file:</p>
<pre><code>[root@host4 ~]# scp host1:/root/haproxy.cfg ./
[root@host4 ~]# haproxy -f haproxy.cfg
</code></pre>
<h2 id="Verify-the-HAProxy"><a href="#Verify-the-HAProxy" class="headerlink" title="Verify the HAProxy"></a>Verify the HAProxy</h2><p>The Load Balancer distributes the requests sent to the cluster equally between the different nodes.</p>
<p>From primary node, we can check which node is being connected as below:</p>
<pre><code>[root@host1 ~]# cockroach sql --host=host4 --insecure
root@host4:26257/defaultdb&gt; SHOW node_id;
  node_id
-----------
  1

root@host4:26257/defaultdb&gt; exit
[root@host1 ~]# cockroach sql --host=host4 --insecure
root@host4:26257/defaultdb&gt; SHOW node_id;
  node_id
-----------
  2

[root@host1 ~]# cockroach sql --host=host4 --insecure
root@host4:26257/defaultdb&gt; SHOW node_id;
  node_id
-----------
  3
</code></pre>
<h2 id="Run-TPCC-through-the-HAProxy-load-balancer"><a href="#Run-TPCC-through-the-HAProxy-load-balancer" class="headerlink" title="Run TPCC through the HAProxy load balancer"></a>Run TPCC through the HAProxy load balancer</h2><pre><code>[root@host4 cockroachdb]# cockroach workload run tpcc --warehouses=5000 --split --scatter --ramp=30s --duration=5m &#39;postgresql://root@host4:26257/tpcc?sslmode=disable&#39;
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.cockroachlabs.com/docs/v22.1/deploy-cockroachdb-on-premises-insecure">https://www.cockroachlabs.com/docs/v22.1/deploy-cockroachdb-on-premises-insecure</a></li>
<li><a href="https://www.scaleway.com/en/docs/tutorials/setup-cockroachdb-cluster/">Install a multi-node Cockroach database with HA Proxy</a></li>
<li><a href="http://www.haproxy.org/">http://www.haproxy.org/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>CockroachDB</tag>
      </tags>
  </entry>
  <entry>
    <title>Setup CockroachDB in docker container</title>
    <url>/blog/setup-cockroachdb-in-docker-container/</url>
    <content><![CDATA[<h2 id="Create-docker-volume"><a href="#Create-docker-volume" class="headerlink" title="Create docker volume"></a>Create docker volume</h2><p>Create docker volume on each host:</p>
<pre><code>[root@host1 ~]# docker volume create --driver local --opt type=ext4 --opt device=/dev/nvme2n1 vol1
[root@host1 ~]# mkfs.ext4 /dev/nvme2n1
[root@host1 ~]# docker inspect vol1
[
    &#123;
        &quot;CreatedAt&quot;: &quot;2022-09-13T01:10:50Z&quot;,
        &quot;Driver&quot;: &quot;local&quot;,
        &quot;Labels&quot;: &#123;&#125;,
        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/vol1/_data&quot;,
        &quot;Name&quot;: &quot;vol1&quot;,
        &quot;Options&quot;: &#123;
            &quot;device&quot;: &quot;/dev/nvme2n1&quot;,
            &quot;type&quot;: &quot;ext4&quot;
        &#125;,
        &quot;Scope&quot;: &quot;local&quot;
    &#125;
]
</code></pre>
<h2 id="Start-CockroachDB-instance"><a href="#Start-CockroachDB-instance" class="headerlink" title="Start CockroachDB instance"></a>Start CockroachDB instance</h2><p>Start the CockroachDB instance on each host:</p>
<pre><code>[root@host1 ~]# docker run -d --cpus=&quot;0.25&quot; --memory=&quot;32g&quot; --name=roach1 --hostname=host1 -p 26257:26257 -p 8080:8080 -v &quot;vol1:/cockroach/cockroach-data&quot; cockroachdb/cockroach:v22.1.6 start --insecure --join=host1,host2,host3

[root@host2 ~]# docker run -d --cpus=&quot;0.25&quot; --memory=&quot;32g&quot; --name=roach2 --hostname=host2 -p 26257:26257 -p 8080:8080 -v &quot;vol2:/cockroach/cockroach-data&quot; cockroachdb/cockroach:v22.1.6 start --insecure --join=host1,host2,host3

[root@host3 ~]# docker run -d --cpus=&quot;0.25&quot; --memory=&quot;32g&quot; --name=roach3 --hostname=host3 -p 26257:26257 -p 8080:8080 -v &quot;vol3:/cockroach/cockroach-data&quot; cockroachdb/cockroach:v22.1.6 start --insecure --join=host1,host2,host3
</code></pre>
<h2 id="Initialize-the-CockroachDB-cluster"><a href="#Initialize-the-CockroachDB-cluster" class="headerlink" title="Initialize the CockroachDB cluster"></a>Initialize the CockroachDB cluster</h2><p>Initialize the cluster on any of the cluster nodes:</p>
<pre><code>[root@host1 ~]# docker exec -it roach1 ./cockroach init --insecure
</code></pre>
<p>Check the cluster status:</p>
<pre><code>[root@host1 ~]# docker exec -it roach1 ./cockroach node ls --insecure
  id
------
   1
   2
   3
(3 rows)

[root@host1 ~]# docker exec -it roach1 ./cockroach node status --insecure
  id |     address      |   sql_address    |  build  |         started_at         |         updated_at         | locality | is_available | is_live
-----+------------------+------------------+---------+----------------------------+----------------------------+----------+--------------+----------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-13 01:09:19.857864 | 2022-09-13 01:10:22.870301 |          | true         | true
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-13 01:09:20.668787 | 2022-09-13 01:10:23.671303 |          | true         | true
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-13 01:09:20.910602 | 2022-09-13 01:10:19.415254 |          | true         | true
(3 rows)

[root@host1 ~]# docker exec -it roach1 df -h
Filesystem               Size  Used Avail Use% Mounted on
overlay                   50G  7.1G   43G  15% /
tmpfs                     64M     0   64M   0% /dev
tmpfs                    504G     0  504G   0% /sys/fs/cgroup
shm                       64M     0   64M   0% /dev/shm
/dev/nvme2n1             1.5T  1.6G  1.4T   1% /cockroach/cockroach-data
/dev/mapper/centos-root   50G  7.1G   43G  15% /etc/hosts
tmpfs                    504G     0  504G   0% /proc/acpi
tmpfs                    504G     0  504G   0% /proc/scsi
tmpfs                    504G     0  504G   0% /sys/firmware

[root@host1 ~]# mount | grep nvme2n1
/dev/nvme2n1 on /var/lib/docker/volumes/vol1/_data type ext4 (rw,relatime)
</code></pre>
<h2 id="Check-the-cluster-startup-logs"><a href="#Check-the-cluster-startup-logs" class="headerlink" title="Check the cluster startup logs"></a>Check the cluster startup logs</h2><pre><code>[root@host1 ~]# docker exec -it roach1 grep &#39;node starting&#39; cockroach-data/logs/cockroach.log -A 11
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +CockroachDB node starting at 2022-09-13 00:57:22.532397342 +0000 UTC (took 110.6s)
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +build:               CCL v22.1.6 @ 2022/08/23 17:05:04 (go1.17.11)
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +webui:               ‹http://host1:8080›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +sql:                 ‹postgresql://root@host1:26257/defaultdb?sslmode=disable›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +sql (JDBC):          ‹jdbc:postgresql://host1:26257/defaultdb?sslmode=disable&amp;user=root›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +RPC client flags:    ‹/cockroach/cockroach &lt;client cmd&gt; --host=host1:26257 --insecure›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +logs:                ‹/cockroach/cockroach-data/logs›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +temp dir:            ‹/cockroach/cockroach-data/cockroach-temp1471904785›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +external I/O path:   ‹/cockroach/cockroach-data/extern›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +store[0]:            ‹path=/cockroach/cockroach-data›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +storage engine:      pebble
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +clusterID:           ‹8276f8fb-30f2-4712-ad4e-f008d382372d›
</code></pre>
<h2 id="Load-the-tpcc-dataset"><a href="#Load-the-tpcc-dataset" class="headerlink" title="Load the tpcc dataset"></a>Load the tpcc dataset</h2><pre><code>[root@host1 ~]# docker exec -it roach1 ./cockroach workload init tpcc &quot;postgresql://root@host1:26257?sslmode=disable&quot; --warehouses 2500 --drop

[root@host2 ~]# iostat -ktdx 2 nvme2n1
09/13/2022 01:14:00 AM
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00   239.50    0.00 2112.00     0.00 167118.00   158.26     0.27    0.13    0.00    0.13   0.25  53.00

09/13/2022 01:14:02 AM
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00   268.00    0.00 2369.50     0.00 185266.00   156.38     0.30    0.13    0.00    0.13   0.24  56.50
</code></pre>
<p>The following are the elapsed time to load dataset for 2500 warehouses.</p>
<pre><code>I220913 04:28:20.633547 1 workload/workloadsql/dataload.go:146  [-] 1  imported warehouse (0s, 2500 rows)
I220913 04:28:20.900607 1 workload/workloadsql/dataload.go:146  [-] 2  imported district (0s, 25000 rows)
I220913 04:46:27.360882 7 1@util/log/log_flush.go:99  [-] 3  hangup received, flushing logs
I220913 04:59:37.468628 1 workload/workloadsql/dataload.go:146  [-] 4  imported customer (31m17s, 75000000 rows)
I220913 05:06:54.166160 1 workload/workloadsql/dataload.go:146  [-] 5  imported history (7m17s, 75000000 rows)
I220913 09:17:27.487420 1 workload/workloadsql/dataload.go:146  [-] 6  imported order (4h10m33s, 75000000 rows)
I220913 09:18:29.376933 1 workload/workloadsql/dataload.go:146  [-] 7  imported new_order (1m2s, 22500000 rows)
I220913 09:18:29.975510 1 workload/workloadsql/dataload.go:146  [-] 8  imported item (1s, 100000 rows)
I220913 10:07:29.447874 1 workload/workloadsql/dataload.go:146  [-] 9  imported stock (48m59s, 250000000 rows)
I220913 11:18:52.140400 1 workload/workloadsql/dataload.go:146  [-] 10  imported order_line (1h11m23s, 750022630 rows)
</code></pre>
<p>The following shows the used file system size for the 2500 warehouses.</p>
<pre><code>[root@host1 ~]# docker exec -it roach1 df -h
Filesystem               Size  Used Avail Use% Mounted on
overlay                   50G  7.1G   43G  15% /
tmpfs                     64M     0   64M   0% /dev
tmpfs                    504G     0  504G   0% /sys/fs/cgroup
shm                       64M     0   64M   0% /dev/shm
/dev/nvme2n1             1.5T   49G  1.4T   4% /cockroach/cockroach-data
/dev/mapper/centos-root   50G  7.1G   43G  15% /etc/hosts
tmpfs                    504G     0  504G   0% /proc/acpi
tmpfs                    504G     0  504G   0% /proc/scsi
tmpfs                    504G     0  504G   0% /sys/firmware
</code></pre>
<h2 id="Run-the-tpcc-workload"><a href="#Run-the-tpcc-workload" class="headerlink" title="Run the tpcc workload"></a>Run the tpcc workload</h2><p>Run tpcc workload from remote host:</p>
<pre><code>[root@host4 ~]# cat addr
postgres://root@host1:26257?sslmode=disable postgres://root@host2:26257?sslmode=disable postgres://root@host3:26257?sslmode=disable
[root@host4 ~]# cockroach workload run tpcc --warehouses=2500 --ramp=5m --duration=30m $(cat addr)
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.cockroachlabs.com/docs/v22.1/start-a-local-cluster-in-docker-linux">https://www.cockroachlabs.com/docs/v22.1/start-a-local-cluster-in-docker-linux</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>CockroachDB</tag>
      </tags>
  </entry>
  <entry>
    <title>Shadow copy and deep copy in Python</title>
    <url>/blog/shadow-copy-and-deep-copy-in-python/</url>
    <content><![CDATA[<p>In this post, we’ll try to understand the difference between shadow copy and deep copy in Python. We study the copy behavior with list data structure in Python.</p>
<h2 id="Problem-statement"><a href="#Problem-statement" class="headerlink" title="Problem statement"></a>Problem statement</h2><p>Assuming we have the following list:</p>
<pre><code>test_list = [1, 2, [3, 6], 4, 5]
</code></pre>
<p>We want to create a duplicate list so that we can modify the contents independently without affecting the original list.</p>
<pre><code>test_list_clone = [1, 2, [3, 6], 4, 5]
test_list_clone[0] = 11
test_list_clone[2].append(11)
print(test_list)
print(test_list_clone)
</code></pre>
<p>The expected output should be:</p>
<p>[1, 2, [3, 6], 4, 5]</p>
<p>[11, 2, [3, 6, 11], 4, 5]</p>
<p>How should we copy the original list?</p>
<h2 id="Duplicate-reference"><a href="#Duplicate-reference" class="headerlink" title="Duplicate reference"></a>Duplicate reference</h2><p>It’s not uncommon that you may want to copy the list as below.</p>
<pre><code>test_list_clone = test_list
test_list_clone[0] = 11
test_list_clone[2].append(11)
print(test_list)
print(test_list_clone)
</code></pre>
<p>Output:</p>
<p>[11, 2, [3, 6, 11], 4, 5]</p>
<p>[11, 2, [3, 6, 11], 4, 5]</p>
<p>This doesn’t work since it simply stores the list reference to another variable. When the duplciate list is modified, the original list is also modified.</p>
<h2 id="Copy-list-by-brute-force"><a href="#Copy-list-by-brute-force" class="headerlink" title="Copy list by brute force"></a>Copy list by brute force</h2><p>In this solution, we append each item in the original list to the duplicate list by using a list comprehensions(a short form of for loop).</p>
<pre><code>test_list_clone = [item for item in test_list]
test_list_clone[0] = 22
test_list_clone[2][-1] = 22
print(test_list)
print(test_list_clone)
</code></pre>
<p>Output:</p>
<p>[11, 2, [3, 6, 22], 4, 5]</p>
<p>[22, 2, [3, 6, 22], 4, 5]</p>
<p>This works partially but not for the nested list. This is because it only duplicates the outer list. But the inner list is still duplicated as a reference of original object. That’s why the number 22 is appended to both the duplciate list and original list.</p>
<h2 id="Copy-list-with-slice"><a href="#Copy-list-with-slice" class="headerlink" title="Copy list with slice"></a>Copy list with slice</h2><p>We can get a subset of the list with slice in Python. The colon means to get the whole copy as original. Again, this only duplicates the outer list.</p>
<pre><code>test_list_clone = test_list[:]
test_list_clone[0] = 33
test_list_clone[2][-1] = 33
print(test_list)
print(test_list_clone)
</code></pre>
<p>Output:</p>
<p>[11, 2, [3, 6, 33], 4, 5]</p>
<p>[33, 2, [3, 6, 33], 4, 5]</p>
<h2 id="Copy-list-with-list-constructor"><a href="#Copy-list-with-list-constructor" class="headerlink" title="Copy list with list constructor"></a>Copy list with list constructor</h2><p>A more readable form might be using list constructor to duplciate the list. But it has the same issue without deep copy of inner list.</p>
<pre><code>test_list_clone = list(test_list)
test_list_clone[0] = 44
test_list_clone[2][-1] = 44
print(test_list)
print(test_list_clone)
</code></pre>
<p>Output:</p>
<p>[11, 2, [3, 6, 44], 4, 5]</p>
<p>[44, 2, [3, 6, 44], 4, 5]</p>
<h2 id="Copy-list-with-starred-expression"><a href="#Copy-list-with-starred-expression" class="headerlink" title="Copy list with starred expression"></a>Copy list with starred expression</h2><p>Another way is to use starred expression to duplciate the list.</p>
<pre><code>test_list_clone = [*test_list]
test_list_clone[0] = 55
test_list_clone[2][-1] = 55
print(test_list)
print(test_list_clone)
</code></pre>
<p>Output:</p>
<p>[11, 2, [3, 6, 55], 4, 5]</p>
<p>[55, 2, [3, 6, 55], 4, 5]</p>
<h2 id="Copy-list-with-copy-function-python-3-3"><a href="#Copy-list-with-copy-function-python-3-3" class="headerlink" title="Copy list with copy function, python 3.3+"></a>Copy list with copy function, python 3.3+</h2><p>In Python 3.3+, a copy function of the list is available. But it doesn’t do the deep copy either.</p>
<pre><code>test_list_clone = test_list.copy()
test_list_clone[0] = 66
test_list_clone[2][-1] = 66
print(test_list)
print(test_list_clone)
</code></pre>
<p>Output:</p>
<p>[11, 2, [3, 6, 66], 4, 5]</p>
<p>[66, 2, [3, 6, 66], 4, 5]</p>
<h2 id="Copy-list-with-multiplication"><a href="#Copy-list-with-multiplication" class="headerlink" title="Copy list with multiplication"></a>Copy list with multiplication</h2><p>A tricky way to duplciate the list is to use multiplication style.</p>
<pre><code>test_list_clone = test_list * 1
test_list_clone[0] = 77
test_list_clone[2][-1] = 77
print(test_list)
print(test_list_clone)
</code></pre>
<p>Output:</p>
<p>[11, 2, [3, 6, 77], 4, 5]</p>
<p>[77, 2, [3, 6, 77], 4, 5]</p>
<h2 id="Copy-list-with-copy-package"><a href="#Copy-list-with-copy-package" class="headerlink" title="Copy list with copy package"></a>Copy list with copy package</h2><p>Fortunately, a copy package is built in Python to do the shadow copy and deep copy for us.</p>
<h3 id="Shadow-copy"><a href="#Shadow-copy" class="headerlink" title="Shadow copy"></a>Shadow copy</h3><pre><code>import copy

test_list_clone = copy.copy(test_list)
test_list_clone[0] = 88
test_list_clone[2][-1] = 88
print(test_list)
print(test_list_clone)
</code></pre>
<p>Output:</p>
<p>[11, 2, [3, 6, 88], 4, 5]</p>
<p>[88, 2, [3, 6, 88], 4, 5]</p>
<h3 id="Deep-copy"><a href="#Deep-copy" class="headerlink" title="Deep copy"></a>Deep copy</h3><pre><code>test_list_clone = copy.deepcopy(test_list)
test_list_clone[0] = 99
test_list_clone[2][-1] = 99
print(test_list)
print(test_list_clone)
</code></pre>
<p>Output:</p>
<p>[11, 2, [3, 6, 88], 4, 5]</p>
<p>[99, 2, [3, 6, 99], 4, 5]</p>
<p>Finally, we got a way to do the deep copy for the nested list. Now, the nested list is modified without affecting the original list content.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell, sed and awk scripting</title>
    <url>/blog/shell-sed-and-awk-scripting/</url>
    <content><![CDATA[<h1 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h1><h2 id="Special-variables-in-Shell"><a href="#Special-variables-in-Shell" class="headerlink" title="Special variables in Shell:"></a>Special variables in Shell:</h2><ul>
<li>$0 - The filename of the current script.</li>
<li>$# - The number of arguments supplied to a script.</li>
<li>$* - All the arguments are double quoted. If a script receives two arguments, $* is equivalent to $1 $2.</li>
<li>$@ - All the arguments are individually double quoted. If a script receives two arguments, $@ is equivalent to $1 $2.</li>
<li>$? - The exit status of the last command executed.</li>
<li>$$ - The process number of the current shell. For shell scripts, this is the process ID under which they are executing.</li>
<li>$! - The process number of the last background command.</li>
</ul>
<h2 id="Enable-ssh-passwordless-login-for-multiple-servers"><a href="#Enable-ssh-passwordless-login-for-multiple-servers" class="headerlink" title="Enable ssh passwordless login for multiple servers:"></a>Enable ssh passwordless login for multiple servers:</h2><pre><code>$ rpm -ivh sshpass-1.06-1.el7.x86_64.rpm

$ for i in `seq 0 239`
do
    sshpass -p &quot;password&quot; ssh-copy-id -i /root/.ssh/id_rsa.pub -o StrictHostKeyChecking=no server$i
done
</code></pre>
<h1 id="Sed"><a href="#Sed" class="headerlink" title="Sed"></a>Sed</h1><h2 id="Print-specific-line-from-a-file"><a href="#Print-specific-line-from-a-file" class="headerlink" title="Print specific line from a file:"></a>Print specific line from a file:</h2><pre><code>$ sed -n 5p &lt;file&gt;
</code></pre>
<h2 id="Replace-specific-line-with-new-string"><a href="#Replace-specific-line-with-new-string" class="headerlink" title="Replace specific line with new string:"></a>Replace specific line with new string:</h2><pre><code>$ sed -i &#39;2s/.*/aa/&#39; /file/path
</code></pre>
<h2 id="Add-line-after-string-match"><a href="#Add-line-after-string-match" class="headerlink" title="Add line after string match:"></a>Add line after string match:</h2><pre><code>$ sed -i &#39;/SERVER = str1/a SERVER = &quot;str2&quot;&#39; /file/path
</code></pre>
<h2 id="Remove-strings-from-beginning-of-lines"><a href="#Remove-strings-from-beginning-of-lines" class="headerlink" title="Remove strings from beginning of lines:"></a>Remove strings from beginning of lines:</h2><pre><code># Remove 40 characters from beginning of lines
$ cat file | sed -r &#39;s/.&#123;40&#125;//&#39; 
</code></pre>
<h2 id="Remove-leading-and-tailing-spaces"><a href="#Remove-leading-and-tailing-spaces" class="headerlink" title="Remove leading and tailing spaces:"></a>Remove leading and tailing spaces:</h2><pre><code>$ echo $diskstats
252       2 dev/dev1029144966969016657 150 0 1200 8031 2297170 2606 165332568 46106742 0 684017 44948515 5 0 72 4
$ echo $diskstats | awk &#39;&#123;$1=&quot;&quot;;$2=&quot;&quot;;$3=&quot;&quot;;print&#125;&#39; | sed &#39;s/^[ \t]*//;s/[ \t]*$//;&#39;
150 0 1200 8031 2297170 2606 165332568 46106742 0 684017 44948515 5 0 72 4
</code></pre>
<h2 id="Replace-multiple-digits-in-a-string"><a href="#Replace-multiple-digits-in-a-string" class="headerlink" title="Replace multiple digits in a string"></a>Replace multiple digits in a string</h2><pre><code>$ echo fio.read_16k_8jobs_20210927_103626.out | sed &#39;s/fio\.//g;s/\.out//g;&#39; | awk &#39;&#123;gsub(&quot;_[0-9]&#123;5,&#125;&quot;,&quot;&quot;,$1); print&#125;&#39;
read_16k_8jobs
</code></pre>
<h1 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h1><h2 id="Use-awk-regex-to-match-variable-passed-from-shell"><a href="#Use-awk-regex-to-match-variable-passed-from-shell" class="headerlink" title="Use awk regex to match variable passed from shell"></a>Use awk regex to match variable passed from shell</h2><pre><code>str=abc
awk  &#39;/&#39;$str&#39;/ &#123;print $0 &#125;&#39; filename
</code></pre>
<p>Or</p>
<pre><code>str=abc
awk  -v pattern=$str &#39;$0 ~ pattern&#123;print $0 &#125;&#39; filename
</code></pre>
<h1 id="Misc"><a href="#Misc" class="headerlink" title="Misc"></a>Misc</h1><h2 id="download-a-file"><a href="#download-a-file" class="headerlink" title="download a file"></a>download a file</h2><pre><code>$ sudo curl -vL -o testfile.zip &#39;https://example.com/remote_testfile.zip&#39;
</code></pre>
<p>wget directory:</p>
<pre><code>$ wget -r -np -R &quot;index.html*&quot; http://example.com/configs/.vim/
</code></pre>
<h2 id="Sync-Mac-Address-during-boot"><a href="#Sync-Mac-Address-during-boot" class="headerlink" title="Sync Mac Address during boot"></a>Sync Mac Address during boot</h2><pre><code>$ cat /etc/rc.d/rc.local
new_mac=`cat /sys/class/net/eno16780032/address`
sed -i &quot;s/HWADDR=.*/HWADDR=$new_mac/&quot; /etc/sysconfig/network-scripts/ifcfg-eno16780032
service network restart
</code></pre>
<h2 id="Check-command-exit-status"><a href="#Check-command-exit-status" class="headerlink" title="Check command exit status"></a>Check command exit status</h2><pre><code>for pool in $pools 
do 
    for vol in $vols
    do 
        pxctl v i $vol | grep $pool &gt; /dev/null 2&gt;&amp;1
        if [ $? -eq 0 ]; then echo $vol; break; fi
    done
done
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Sliding window</title>
    <url>/blog/sliding-window/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p><a href="https://www.geeksforgeeks.org/window-sliding-technique/">Window Sliding Technique</a> is a computational technique that aims to reduce the use of nested loops and replace it with a single loop, thereby reducing the time complexity.<span id="more"></span></p>
<h2 id="Leetcode-3-Longest-Substring-Without-Repeating-Characters"><a href="#Leetcode-3-Longest-Substring-Without-Repeating-Characters" class="headerlink" title="[Leetcode 3] Longest Substring Without Repeating Characters"></a>[Leetcode 3] Longest Substring Without Repeating Characters</h2><p>Given a string s, find the length of the longest substring without repeating characters.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;abcabcbb&quot;</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: The answer is &quot;abc&quot;, with the length of 3.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;bbbbb&quot;</span><br><span class="line">Output: 1</span><br><span class="line">Explanation: The answer is &quot;b&quot;, with the length of 1.</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;pwwkew&quot;</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: The answer is &quot;wke&quot;, with the length of 3.</span><br><span class="line">Notice that the answer must be a substring, &quot;pwke&quot; is a subsequence and not a substring.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>0 &lt;&#x3D; s.length &lt;&#x3D; 5 * 10^4</li>
<li>s consists of English letters, digits, symbols and spaces.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> s:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">        longestLen = <span class="number">0</span></span><br><span class="line">        left, currLen = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        idxMap = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> right <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            c = s[right]</span><br><span class="line">            <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> idxMap:</span><br><span class="line">                <span class="comment"># extend right pointer</span></span><br><span class="line">                idxMap[c] = right</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># calculate current length</span></span><br><span class="line">                currLen += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># calculate longest length</span></span><br><span class="line">                longestLen = <span class="built_in">max</span>(longestLen, currLen)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># reset left pointer</span></span><br><span class="line">                <span class="keyword">if</span> left &lt;= idxMap[c]:</span><br><span class="line">                    left = idxMap[c] + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># update index for the duplicate item</span></span><br><span class="line">                idxMap[c] = right</span><br><span class="line"></span><br><span class="line">                <span class="comment"># reset current length</span></span><br><span class="line">                currLen = right - left + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(longestLen, currLen)</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-42-Trapping-Rain-Water"><a href="#Leetcode-42-Trapping-Rain-Water" class="headerlink" title="[Leetcode 42] Trapping Rain Water"></a>[Leetcode 42] Trapping Rain Water</h2><p>Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it can trap after raining.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: height = [0,1,0,2,1,0,1,3,2,1,2,1]</span><br><span class="line">Output: 6</span><br><span class="line">Explanation: The above elevation map (black section) is represented by array [0,1,0,2,1,0,1,3,2,1,2,1]. In this case, 6 units of rain water (blue section) are being trapped.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: height = [4,2,0,3,2,5]</span><br><span class="line">Output: 9</span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>n &#x3D;&#x3D; height.length</li>
<li>1 &lt;&#x3D; n &lt;&#x3D; 2 * 10^4</li>
<li>0 &lt;&#x3D; height[i] &lt;&#x3D; 10^5</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># e.g.</span></span><br><span class="line">    <span class="comment"># height:  3 2 4, water trapped: (3 - 2) = 1</span></span><br><span class="line">    <span class="comment">#              |</span></span><br><span class="line">    <span class="comment">#          |   |</span></span><br><span class="line">    <span class="comment">#          | | |</span></span><br><span class="line">    <span class="comment">#          | | |</span></span><br><span class="line">    <span class="comment"># The water to be trapped is determined by the lower bar</span></span><br><span class="line">    <span class="comment"># height:  3 2 1 4, water trapped: (3 - 2) + (3 - 1) = 3</span></span><br><span class="line">    <span class="comment">#                |</span></span><br><span class="line">    <span class="comment">#          |     |</span></span><br><span class="line">    <span class="comment">#          | |   |</span></span><br><span class="line">    <span class="comment">#          | | | |</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">trap</span>(<span class="params">self, height: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        leftMax, rightMax = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        i, j = <span class="number">0</span>, <span class="built_in">len</span>(height) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> i &lt;= j:</span><br><span class="line">            leftMax = <span class="built_in">max</span>(leftMax, height[i])</span><br><span class="line">            rightMax = <span class="built_in">max</span>(rightMax, height[j])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># trapped water determined by highest bar</span></span><br><span class="line">            <span class="keyword">if</span> leftMax &lt; rightMax:</span><br><span class="line">                ans += leftMax - height[i]</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ans += rightMax - height[j]</span><br><span class="line">                j -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-209-Minimum-Size-Subarray-Sum"><a href="#Leetcode-209-Minimum-Size-Subarray-Sum" class="headerlink" title="[Leetcode 209] Minimum Size Subarray Sum"></a>[Leetcode 209] Minimum Size Subarray Sum</h2><p>Given an array of positive integers nums and a positive integer target, return the minimal length of a subarray whose sum is greater than or equal to target. If there is no such subarray, return 0 instead.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: target = 7, nums = [2,3,1,2,4,3]</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: The subarray [4,3] has the minimal length under the problem constraint.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: target = 4, nums = [1,4,4]</span><br><span class="line">Output: 1</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: target = 11, nums = [1,1,1,1,1,1,1,1]</span><br><span class="line">Output: 0</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; target &lt;&#x3D; 10^9</li>
<li>1 &lt;&#x3D; nums.length &lt;&#x3D; 10^5</li>
<li>1 &lt;&#x3D; nums[i] &lt;&#x3D; 10^4</li>
</ul>
<p>Follow up: If you have figured out the O(n) solution, try coding another solution of which the time complexity is O(n log(n)).</p>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minSubArrayLen_bf</span>(<span class="params">self, target: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        minLen = <span class="built_in">len</span>(nums) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            currSum = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i, <span class="built_in">len</span>(nums)):</span><br><span class="line">                currSum += nums[j]</span><br><span class="line">                <span class="keyword">if</span> currSum &gt;= target:</span><br><span class="line">                    minLen = <span class="built_in">min</span>(minLen, j - i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> minLen <span class="keyword">if</span> minLen &lt; <span class="built_in">len</span>(nums) + <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># sliding window(with two pointers)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minSubArrayLen</span>(<span class="params">self, target: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        minLen = <span class="built_in">len</span>(nums) + <span class="number">1</span>  <span class="comment"># length of min subarray</span></span><br><span class="line">        left = <span class="number">0</span>  <span class="comment"># left pointer</span></span><br><span class="line">        currSum = <span class="number">0</span>  <span class="comment"># sum of the current window</span></span><br><span class="line">        currLen = <span class="number">0</span>  <span class="comment"># window length</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># right pointer</span></span><br><span class="line">        <span class="keyword">for</span> right <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="comment"># extend the current window until the sum is &gt;= target</span></span><br><span class="line">            currSum += nums[right]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> currSum &gt;= target:</span><br><span class="line">                currLen = right - left + <span class="number">1</span> </span><br><span class="line">                minLen = <span class="built_in">min</span>(minLen, currLen) </span><br><span class="line"></span><br><span class="line">                <span class="comment"># shink the window from left side</span></span><br><span class="line">                currSum -= nums[left]</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> minLen <span class="keyword">if</span> minLen &lt; <span class="built_in">len</span>(nums) + <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-1004-Max-Consecutive-Ones-III"><a href="#Leetcode-1004-Max-Consecutive-Ones-III" class="headerlink" title="[Leetcode 1004] Max Consecutive Ones III"></a>[Leetcode 1004] Max Consecutive Ones III</h2><p>Given a binary array nums and an integer k, return the maximum number of consecutive 1’s in the array if you can flip at most k 0’s.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,1,1,0,0,0,1,1,1,1,0], k = 2</span><br><span class="line">Output: 6</span><br><span class="line">Explanation: [1,1,1,0,0,1,1,1,1,1,1]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [0,0,1,1,0,0,1,1,1,0,1,1,0,0,0,1,1,1,1], k = 3</span><br><span class="line">Output: 10</span><br><span class="line">Explanation: [0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; nums.length &lt;&#x3D; 105</li>
<li>nums[i] is either 0 or 1.</li>
<li>0 &lt;&#x3D; k &lt;&#x3D; nums.length</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># e.g. [1 1 1 0 0 0 1 1 1 1 0], k = 2</span></span><br><span class="line">    <span class="comment">#       ^         ^             l = 0, r = 5, k = -1 keep moving right pointer until k &lt; 0</span></span><br><span class="line">    <span class="comment">#         ^       ^             l = 1, r = 5, k = -1 increment k if nums[left]==0, and move left pointer forward </span></span><br><span class="line">    <span class="comment">#           ^       ^           l = 2, r = 6, k = -1</span></span><br><span class="line">    <span class="comment">#             ^       ^         l = 3, r = 7, k = -1</span></span><br><span class="line">    <span class="comment">#               ^       ^       l = 4, r = 8, k = 0</span></span><br><span class="line">    <span class="comment">#               ^         ^     l = 4, r = 9, k = 0</span></span><br><span class="line">    <span class="comment">#                 ^          ^  l = 5, r = 10,k = -1</span></span><br><span class="line">    <span class="comment"># result = r - l + 1 = 10 - 5 + 1 = 6</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestOnes</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        l = r = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="comment"># use flip opportunity if the right side of window is 0</span></span><br><span class="line">            <span class="keyword">if</span> nums[r] == <span class="number">0</span>:</span><br><span class="line">                k -= <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> k &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># recover the flip opportunity if the left side is 0</span></span><br><span class="line">                <span class="keyword">if</span> nums[l] == <span class="number">0</span>:</span><br><span class="line">                    k += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># shrink the window from left side</span></span><br><span class="line">                l += <span class="number">1</span></span><br><span class="line">            <span class="comment">#print(l,r,k)</span></span><br><span class="line">         </span><br><span class="line">        <span class="keyword">return</span> r - l + <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-1456-Maximum-Number-of-Vowels-in-a-Substring-of-Given-Length"><a href="#Leetcode-1456-Maximum-Number-of-Vowels-in-a-Substring-of-Given-Length" class="headerlink" title="[Leetcode 1456] Maximum Number of Vowels in a Substring of Given Length"></a>[Leetcode 1456] Maximum Number of Vowels in a Substring of Given Length</h2><p>Given a string s and an integer k, return the maximum number of vowel letters in any substring of s with length k.</p>
<p>Vowel letters in English are ‘a’, ‘e’, ‘i’, ‘o’, and ‘u’.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;abciiidef&quot;, k = 3</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: The substring &quot;iii&quot; contains 3 vowel letters.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;aeiou&quot;, k = 2</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: Any substring of length 2 contains 2 vowels.</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;leetcode&quot;, k = 3</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: &quot;lee&quot;, &quot;eet&quot; and &quot;ode&quot; contain 2 vowels.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; s.length &lt;&#x3D; 105</li>
<li>s consists of lowercase English letters.</li>
<li>1 &lt;&#x3D; k &lt;&#x3D; s.length</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxVowels</span>(<span class="params">self, s: <span class="built_in">str</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        vowels = &#123;<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;i&#x27;</span>,<span class="string">&#x27;o&#x27;</span>,<span class="string">&#x27;u&#x27;</span>&#125;</span><br><span class="line">        maxLen = <span class="number">0</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            <span class="comment"># first window with size k</span></span><br><span class="line">            <span class="keyword">if</span> i &lt; k :</span><br><span class="line">                <span class="keyword">if</span> s[i] <span class="keyword">in</span> vowels:</span><br><span class="line">                    count += <span class="number">1</span></span><br><span class="line">            <span class="comment"># moving out of window</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># calculate max length from previous window </span></span><br><span class="line">                maxLen = <span class="built_in">max</span>(maxLen, count)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># recalculate for the current new window</span></span><br><span class="line">                <span class="keyword">if</span> s[i - k] <span class="keyword">in</span> vowels:</span><br><span class="line">                    count -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> s[i] <span class="keyword">in</span> vowels:</span><br><span class="line">                    count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate the last window</span></span><br><span class="line">        maxLen = <span class="built_in">max</span>(maxLen, count)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> maxLen    </span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-1493-Longest-Subarray-of-1’s-After-Deleting-One-Element"><a href="#Leetcode-1493-Longest-Subarray-of-1’s-After-Deleting-One-Element" class="headerlink" title="[Leetcode 1493] Longest Subarray of 1’s After Deleting One Element"></a>[Leetcode 1493] Longest Subarray of 1’s After Deleting One Element</h2><p>Given a binary array nums, you should delete one element from it.</p>
<p>Return the size of the longest non-empty subarray containing only 1’s in the resulting array. Return 0 if there is no such subarray.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,1,0,1]</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: After deleting the number in position 2, [1,1,1] contains 3 numbers with value of 1&#x27;s.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [0,1,1,1,0,1,1,0,1]</span><br><span class="line">Output: 5</span><br><span class="line">Explanation: After deleting the number in position 4, [0,1,1,1,1,1,0,1] longest subarray with value of 1&#x27;s is [1,1,1,1,1].</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,1,1]</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: You must delete one element.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; nums.length &lt;&#x3D; 105</li>
<li>nums[i] is either 0 or 1</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestSubarray</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        l = r = <span class="number">0</span></span><br><span class="line">        zeros = <span class="number">0</span></span><br><span class="line">        longest = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="comment"># count zeros</span></span><br><span class="line">            <span class="keyword">if</span> nums[r] == <span class="number">0</span>:</span><br><span class="line">                zeros += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># move left pointer forward until one zero left</span></span><br><span class="line">            <span class="keyword">while</span> zeros &gt; <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> nums[l] == <span class="number">0</span>:</span><br><span class="line">                    zeros -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                l += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># calcualte longest subarray between left and right</span></span><br><span class="line">            longest = <span class="built_in">max</span>(longest, r - l + <span class="number">1</span> - zeros)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># note that both 0 and 1 can be deleted</span></span><br><span class="line">        <span class="keyword">return</span>  longest - <span class="number">1</span> <span class="keyword">if</span> longest == <span class="built_in">len</span>(nums) <span class="keyword">else</span> longest</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
        <tag>Algorithm</tag>
        <tag>Sliding Window</tag>
      </tags>
  </entry>
  <entry>
    <title>Solve LeetCode problems in Visual Studio Code</title>
    <url>/blog/solve-leetcode-problems-in-visual-studio-code/</url>
    <content><![CDATA[<h2 id="Install-the-leetcode-extension-in-VS-Code"><a href="#Install-the-leetcode-extension-in-VS-Code" class="headerlink" title="Install the leetcode extension in VS Code"></a>Install the leetcode extension in VS Code</h2><p><img src="/images/leetcode-extension1.png" alt="Image"></p>
<h2 id="Login-to-LeetCode-endpoint-with-cookie"><a href="#Login-to-LeetCode-endpoint-with-cookie" class="headerlink" title="Login to LeetCode endpoint with cookie"></a>Login to LeetCode endpoint with cookie</h2><ol>
<li>Login to leetcode from Google Chrome</li>
<li>In Chrome, Inspect -&gt; Network -&gt; Fetch&#x2F;XHR</li>
<li>Click on any button in leetcode page, and in Inspector to the right under the “Name” tab find and select bottom “graphql” and under Headers tab and in “Request Headers” portion, select and copy the entire cookie string starting from “__cfduid” and ending with “_gat&#x3D;1”</li>
<li>Paste the cookie string in VS Code leetcode login prompt.</li>
</ol>
<p><img src="/images/leetcode-extension2.png" alt="Image"></p>
<h2 id="Enjoy-coding-in-VS-Code"><a href="#Enjoy-coding-in-VS-Code" class="headerlink" title="Enjoy coding in VS Code"></a>Enjoy coding in VS Code</h2><p><img src="/images/leetcode-extension3.png" alt="Image"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/LeetCode-OpenSource/vscode-leetcode/issues/478#issuecomment-564757098">https://github.com/LeetCode-OpenSource/vscode-leetcode/issues/478#issuecomment-564757098</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>Sorting algorithm in Python</title>
    <url>/blog/sorting-algorithm-in-python/</url>
    <content><![CDATA[<h2 id="Selection-sort"><a href="#Selection-sort" class="headerlink" title="Selection sort"></a>Selection sort</h2><p>The selection sort algorithm sorts array by repeatedly finding the minimum element from the unsorted subarray and moving it to the sorted subarray.</p>
<p>For ascending order, we can implement the algorithm as below.</p>
<pre><code>def selection_sort(my_list):
    for i in range(len(my_list) - 1):
        min_idx = i
        for j in range(i + 1, len(my_list)):
            if my_list[j] &lt; my_list[min_idx]:
                min_idx = j

        # swap the minimum element with the current element
        temp = my_list[i]
        my_list[i] = my_list[min_idx]
        my_list[min_idx] = temp


test_list_1 = [4, 3, 2, 10, 12, 1, 5, 6]
selection_sort(test_list_1)
print(test_list_1)

# Time complexity: O(n^2)
# Space complexity: O(1)
</code></pre>
<p>The following steps elaborate how it works. The left arrow indicates the current element position and the right arrow indicates the minimum element position in the unsorted sublist.</p>
<pre><code>i=0: [1, 3, 2, 10, 12, 4, 5, 6]
      ^                ^
i=1: [1, 2, 3, 10, 12, 4, 5, 6]
         ^  ^
i=2: [1, 2, 3, 10, 12, 4, 5, 6]
            ^^
i=3: [1, 2, 3, 4, 12, 10, 5, 6]
               ^      ^
i=4: [1, 2, 3, 4, 5, 10, 12, 6]
                  ^      ^
i=5: [1, 2, 3, 4, 5, 6, 12, 10]
                     ^      ^
i=6: [1, 2, 3, 4, 5, 6, 10, 12]
                        ^   ^
</code></pre>
<h2 id="Insertion-sort"><a href="#Insertion-sort" class="headerlink" title="Insertion sort"></a>Insertion sort</h2><p>Just like playing a card game, to re-organize the cards, we pick an element from the unsorted sublist(on the right side), search a spot backwards from the sorted sublist(on the left side), and insert the element by shifting greater elements to the right.</p>
<pre><code>def insert_sort(my_list):
    # pick an element from the unsorted sublist [i:n-1]
    for i in range(1, len(my_list)):
        current = my_list[i]

        # search a spot in the sorted sublist [0:i-1]
        j = i - 1
        while j &gt;= 0:
            if my_list[j] &gt; current:
                my_list[j + 1] = my_list[j]
                j -= 1
            else:
                break

        # insert the current element to the spot
        my_list[j + 1] = current


test_list_2 = [4, 3, 2, 10, 12, 1, 5, 6]
insert_sort(test_list_2)
print(test_list_2)

# Time complexity: O(n^2)
# Space complexity: O(1)
</code></pre>
<p>The following steps elaborate how it works. The left arrow indicates where to insert and the right arrow indicates the current picked unsorted element.</p>
<pre><code>i=1: [3, 4, 2, 10, 12, 1, 5, 6]
      ^  ^
i=2: [2, 3, 4, 10, 12, 1, 5, 6]
      ^     ^
i=3: [2, 3, 4, 10, 12, 1, 5, 6]
               ^^
i=4: [2, 3, 4, 10, 12, 1, 5, 6]
                   ^^ 
i=5: [1, 2, 3, 4, 10, 12, 5, 6]
      ^               ^
i=6: [1, 2, 3, 4, 5, 10, 12, 6]
                  ^      ^
i=7: [1, 2, 3, 4, 5, 6, 10, 12]
                     ^      ^
</code></pre>
<h2 id="Bubble-sort"><a href="#Bubble-sort" class="headerlink" title="Bubble sort"></a>Bubble sort</h2><p>Bubble sort repeatedly swapping the adjacent elements if they are in wrong order.</p>
<pre><code>def bubble_sort(my_list):
    n = len(my_list)

    # bubble up the maximum element for each round, totally (n-1) rounds
    for i in range(n - 1):
        swapped = False
        # walk through the unsorted sublist [0:n-i-2]
        for j in range(n - i - 1):
            # bubble up if the current item is greater
            if my_list[j] &gt; my_list[j + 1]:
                temp = my_list[j]
                my_list[j] = my_list[j + 1]
                my_list[j + 1] = temp
                swapped = True

        # already sorted
        if not swapped:
            break


test_list_3 = [4, 3, 2, 10, 12, 1, 5, 6]
bubble_sort(test_list_3)
print(test_list_3)

# Time complexity: O(n^2)
# Space complexity: O(1)
</code></pre>
<p>The following steps elaborate how it works.</p>
<pre><code>i=0: [3, 4, 2, 10, 12, 1, 5, 6]
     [3, 2, 4, 10, 12, 1, 5, 6]
     [3, 2, 4, 10, 12, 1, 5, 6]
     [3, 2, 4, 10, 12, 1, 5, 6]
     [3, 2, 4, 10, 1, 12, 5, 6]
     [3, 2, 4, 10, 1, 5, 12, 6]
     [3, 2, 4, 10, 1, 5, 6, 12]
                            ^
i=1: [2, 3, 4, 10, 1, 5, 6, 12]
     [2, 3, 4, 10, 1, 5, 6, 12]
     [2, 3, 4, 10, 1, 5, 6, 12]
     [2, 3, 4, 1, 10, 5, 6, 12]
     [2, 3, 4, 1, 5, 10, 6, 12]
     [2, 3, 4, 1, 5, 6, 10, 12]
                        ^
i=2: [2, 3, 4, 1, 5, 6, 10, 12]
     [2, 3, 4, 1, 5, 6, 10, 12]
     [2, 3, 1, 4, 5, 6, 10, 12]
     [2, 3, 1, 4, 5, 6, 10, 12]
     [2, 3, 1, 4, 5, 6, 10, 12]
                     ^
i=3: [2, 3, 1, 4, 5, 6, 10, 12]
     [2, 3, 1, 4, 5, 6, 10, 12]
     [2, 1, 3, 4, 5, 6, 10, 12]
     [2, 1, 3, 4, 5, 6, 10, 12]
                  ^
i=4: [2, 1, 3, 4, 5, 6, 10, 12]
     [1, 2, 3, 4, 5, 6, 10, 12]
     [1, 2, 3, 4, 5, 6, 10, 12]
               ^
i=5: [1, 2, 3, 4, 5, 6, 10, 12]
     [1, 2, 3, 4, 5, 6, 10, 12]
            ^
For loop ends early since there is no swap in this round!
</code></pre>
<h2 id="Merge-sort"><a href="#Merge-sort" class="headerlink" title="Merge sort"></a>Merge sort</h2><p>Merge sort is Divide-and-Conquer algorithm. It divides list into two halves, recursively calls itself, and then merges the two sorted halves. The merge function is the key process for the merging two halves.</p>
<pre><code>def merge_sort(my_list, low, high):
    if low &lt; high:
        mid = int((low + high) / 2)

        # sort each half of list
        merge_sort(my_list, low, mid)
        merge_sort(my_list, mid + 1, high)

        # merge the sorted two halves
        merge(my_list, low, mid, high)


def merge(my_list, low, mid, high):
    n1 = mid - low + 1
    n2 = high - mid

    # temporary list to store each half of list
    my_list1 = [None] * n1
    my_list2 = [None] * n2
    for i in range(n1):
        my_list1[i] = my_list[low + i]
    for i in range(n2):
        my_list2[i] = my_list[mid + 1 + i]

    # merge the two halves using running pointers on each half
    i = j = 0
    k = low
    while i &lt; n1 and j &lt; n2:
        if my_list1[i] &lt; my_list2[j]:
            my_list[k] = my_list1[i]
            i += 1
        else:
            my_list[k] = my_list2[j]
            j += 1
        k += 1

    # merge the remaining items from each sorted list
    while i &lt; n1:
        my_list[k] = my_list1[i]
        i += 1
        k += 1
    while j &lt; n2:
        my_list[k] = my_list2[j]
        j += 1
        k += 1


test_list_4 = [4, 3, 2, 10, 12, 1, 5, 6]
merge_sort(test_list_4, 0, len(test_list_4) - 1)
print(test_list_4)
</code></pre>
<p>The following process simulates how it works.</p>
<pre><code>1.      [4, 3, 2, 10, 12, 1, 5, 6]
2.     [4,3,2,10]       [12,1,15,6]
3.   [4,3]   [2,10]   [12,1]   [15,6]
4.  [4] [3] [2] [10] [12] [1] [15] [6]
5.   [3,4]   [2,10]   [1,12]   [6,15]
6.     [2,3,4,10]       [1,6,12,15]
7.        [1,2,3,4,6,10,12,15]

# Time complexity: O(n*logn)
# Space complexity: O(n)
</code></pre>
<h2 id="Quick-sort"><a href="#Quick-sort" class="headerlink" title="Quick sort"></a>Quick sort</h2><p>Like merge sort, quick sort is also a Divide-and-Conquer algorithm. It picks an element as pivot and partitions the list around the pivot. The pivot can be picked in different ways(first, last, random, or median position). The partition function is the key process. Its target is to put the pivot in the correct position, put all smaller elements to the left, and put all greater elements to the right of it.</p>
<pre><code>def partition(my_list, low, high):
    pivot = my_list[high]
    curr = low - 1

    # move smaller items to the very left
    for i in range(low, high):
        if my_list[i] &lt; pivot:
            curr += 1
            temp = my_list[curr]
            my_list[curr] = my_list[i]
            my_list[i] = temp

    # move pivot to the final place
    temp = my_list[curr + 1]
    my_list[curr + 1] = pivot
    my_list[high] = temp

    # return pivot index
    return curr + 1


def quick_sort(my_list, low, high):
    if low &lt; high:
        pivot = partition(my_list, low, high)

        quick_sort(my_list, low, pivot - 1)
        quick_sort(my_list, pivot + 1, high)


test_list_5 = [4, 3, 2, 10, 12, 1, 5, 6]
quick_sort(test_list_5, 0, len(test_list_5) - 1)
print(test_list_5)

# Time complexity: O(n*logn)
# Space complexity: O(n)
</code></pre>
<p>The following process simulates how it works.</p>
<pre><code>1. [4, 3, 2, 1, 5] 6 [12, 10]
                   ^ 
2. [4, 3, 2, 1] 5  6 [12, 10]
                ^
3. 1 [3, 2, 4]  5  6 [12, 10]
   ^
4. 1 [3, 2] 4   5  6 [12, 10]
            ^
5. 1 2 [3]  4   5  6 [12, 10]
     ^
6. 1 2 [3]  4   5  6 10  [12]
                     ^
</code></pre>
<h2 id="Heap-sort"><a href="#Heap-sort" class="headerlink" title="Heap sort"></a>Heap sort</h2><p>Heap sort is a comparison based sorting algorithm based on binary heap. It follows the below idea:</p>
<ol>
<li>build a max heap with the given list</li>
<li>repeat the following steps until the heap contains only one item:</li>
</ol>
<p>2.1 swap the root of heap with last item in the heap</p>
<p>2.2 remove the last item of the heap which is already sorted to expected position</p>
<p>2.3 heapify the remaining items in the heap</p>
<pre><code>def heapify(my_List, n, i):
    largest = i

    # get left and right child
    l = 2 * i + 1
    r = 2 * i + 2

    # check if left child exists and greater than parent
    if l &lt; n and my_List[largest] &lt; my_List[l]:
        largest = l

    # check if right child exist and greater than parent
    if r &lt; n and my_List[largest] &lt; my_List[r]:
        largest = r

    # change parent if needed
    if largest != i:
        # swap parent with largest child
        my_List[i], my_List[largest] = my_List[largest], my_List[i]

        # heapify the subtree of largest child
        heapify(my_List, n, largest)


def heap_sort(my_list):
    n = len(my_list)

    # 1. build max heap
    for i in range(n // 2 - 1, -1, -1):
        heapify(my_list, n, i)

    # extract one by one
    for i in range(n - 1, 0, -1):
        # 2.1 swap the root of heap with the last item in the heap
        my_list[0], my_list[i] = my_list[i], my_list[0]

        # 2.3 heapify the remaining items
        heapify(my_list, i, 0)


test_list_6 = [4, 3, 2, 10, 12, 1, 5, 6]
heap_sort(test_list_6)
print(test_list_6)

# Time complexity: O(n*logn)
# Space complexity: O(1)
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Sorting algorithm</title>
    <url>/blog/sorting-algorithm/</url>
    <content><![CDATA[<p>In this post, it includes the following sorting algorithms. The code is self explained.</p>
<ul>
<li><p>selection sort</p>
</li>
<li><p>insert sort</p>
</li>
<li><p>bubble sort</p>
</li>
<li><p>quick sort</p>
</li>
<li><p>merge sort</p>
<p>  import java.util.Arrays;</p>
<p>  public class Sort {<br>  public static void selection_sort(int[] arr) {<br>      for (int i &#x3D; 0; i &lt; arr.length - 1; i++) {<br>          int min_idx &#x3D; i;<br><br>          &#x2F;&#x2F; find the minimum value from right of arr[i] and swap with arr[i]<br>          for (int j &#x3D; i + 1; j &lt; arr.length; j++) {<br>              if (arr[j] &lt; arr[min_idx]) {<br>                  min_idx &#x3D; j;<br>              }<br>          }<br><br>          &#x2F;&#x2F; move the min value to the beginning of array<br>          int temp &#x3D; arr[i];<br>          arr[i] &#x3D; arr[min_idx];<br>          arr[min_idx] &#x3D; temp;<br>      }<br>  }<br><br>  public static void insert_sort(int[] arr) {<br>      &#x2F;&#x2F; sort from the second element since the first one is already sorted for itself<br>      for (int i &#x3D; 1; i &lt; arr.length; i++) {<br>          int curr &#x3D; arr[i];<br>          int j &#x3D; i - 1;<br><br>          &#x2F;&#x2F; move all the greater values(than curr) to one position right<br>          while (j &gt;&#x3D; 0 &amp;&amp; arr[j] &gt; curr) {<br>              arr[j + 1] &#x3D; arr[j];<br>              j–;<br>          }<br><br>          arr[j + 1] &#x3D; curr;<br>      }<br>  }<br><br>  public static void bubble_sort(int[] arr) {<br>      &#x2F;&#x2F; bubble sort for n - 1 rounds<br>      for (int i &#x3D; 0; i &lt; arr.length - 1; i++) {<br>          boolean swapped &#x3D; false;<br>          &#x2F;&#x2F; For each round, bubble up the maximum element to the right<br>          for (int j &#x3D; 0; j &lt; arr.length - i - 1; j++) {<br>              if (arr[j] &gt; arr[j + 1]) {<br>                  int temp &#x3D; arr[j];<br>                  arr[j] &#x3D; arr[j + 1];<br>                  arr[j + 1] &#x3D; temp;<br>                  swapped &#x3D; true;<br>              }<br>          }<br><br>          if (!swapped)<br>              break;<br>      }<br>  }<br><br>  public static void quick_sort(int[] arr, int left, int right) {<br>      if (left &lt; right) {<br>          int pivot &#x3D; partition(arr, left, right);<br>          quick_sort(arr, left, pivot - 1);<br>          quick_sort(arr, pivot + 1, right);<br>      }<br>  }<br><br>  private static int partition(int[] arr, int left, int right) {<br>      int pivot &#x3D; arr[right];<br>      int curr &#x3D; left - 1;<br><br>      for (int i &#x3D; left; i &lt; right; i++) {<br>          if (arr[i] &lt;&#x3D; pivot) {<br>              curr++;<br>              int temp &#x3D; arr[curr];<br>              arr[curr] &#x3D; arr[i];<br>              arr[i] &#x3D; temp;<br>          }<br>      }<br><br>      curr++;<br>      int temp &#x3D; arr[curr];<br>      arr[curr] &#x3D; pivot;<br>      arr[right] &#x3D; temp;<br>      return curr;<br>  }<br><br>  public static void merge_sort(int[] arr, int left, int right) {<br>      if (left &lt; right) {<br>          &#x2F;&#x2F;int mid &#x3D; (left + right) &#x2F; 2;<br>          int mid &#x3D; left + (right - left) &#x2F; 2; &#x2F;&#x2F; avoid overflow<br>          merge_sort(arr, left, mid);<br>          merge_sort(arr, mid + 1, right);<br>          merge(arr, left, mid, right);<br>      }<br>  }<br><br>  private static void merge(int[] arr, int left, int mid, int right) {<br>      int l1 &#x3D; mid - left + 1;<br>      int l2 &#x3D; right - mid;<br><br>      &#x2F;&#x2F; copy left and right to the temporary array<br>      int[] a1 &#x3D; new int[l1];<br>      int[] a2 &#x3D; new int[l2];<br><br><br>      for (int i &#x3D; 0; i &lt; l1; i++) {<br>          a1[i] &#x3D; arr[left + i];<br>      }<br><br>      for (int i &#x3D; 0; i &lt; l2; i++) {<br>          a2[i] &#x3D; arr[mid + 1 + i];<br>      }<br><br>      &#x2F;&#x2F; merge back to the original array<br>      int i &#x3D; 0, j &#x3D; 0, k &#x3D; left;<br>      while (i &lt; l1 &amp;&amp; j &lt; l2) {<br>          if (a1[i] &lt;&#x3D; a2[j]) {<br>              arr[k++] &#x3D; a1[i++];<br>          } else {<br>              arr[k++] &#x3D; a2[j++];<br>          }<br>      }<br><br>      while (i &lt; l1) {<br>          arr[k++] &#x3D; a1[i++];<br>      }<br>      while (j &lt; l2) {<br>          arr[k++] &#x3D; a2[j++];<br>      }<br>  }<br><br>  public static void main(String[] args) {<br>      &#x2F;&#x2F; TODO Auto-generated method stub<br>      int[] arr &#x3D; { 11, 25, 12, 22, 64 };<br>      selection_sort(arr);<br>      System.out.println(Arrays.toString(arr));<br><br>      int[] arr1 &#x3D; { 11, 25, 12, 22, 64 };<br>      insert_sort(arr1);<br>      System.out.println(Arrays.toString(arr1));<br><br>      int[] arr2 &#x3D; { 11, 25, 12, 22, 64 };<br>      bubble_sort(arr2);<br>      System.out.println(Arrays.toString(arr2));<br><br>      int[] arr3 &#x3D; { 11, 25, 12, 22, 64 };<br>      quick_sort(arr3, 0, arr3.length - 1);<br>      System.out.println(Arrays.toString(arr3));<br><br>      int[] arr4 &#x3D; { 11, 25, 12, 22, 64 };<br>      merge_sort(arr4, 0, arr4.length - 1);<br>      System.out.println(Arrays.toString(arr4));<br>  }<br>  }</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Stack and queue in Python</title>
    <url>/blog/stack-and-queue-in-python/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>A <a href="https://www.flamingbytes.com/blog/stack-implementation-in-python/">stack</a> is a linear data structure that stores items in a Last-In&#x2F;First-Out (LIFO) or First-In&#x2F;Last-Out (FILO) manner. In stack, a new element is added at one end and an element is removed from that end only. The insert and delete operations are often called push and pop.</p>
<p><img src="/images/stack.png"></p>
<span id="more"></span>
<p>Like stack, <a href="https://www.flamingbytes.com/blog/queue-implementation-in-python/">queue</a> is a linear data structure that stores items in First In First Out (FIFO) manner. With a queue the least recently added item is removed first. A good example of queue is any queue of consumers for a resource where the consumer that came first is served first.</p>
<p><img src="/images/queue.png"></p>
<h2 id="Stack-practices"><a href="#Stack-practices" class="headerlink" title="Stack practices"></a>Stack practices</h2><h3 id="Leetcode-150-Evaluate-Reverse-Polish-Notation"><a href="#Leetcode-150-Evaluate-Reverse-Polish-Notation" class="headerlink" title="[Leetcode 150] Evaluate Reverse Polish Notation"></a>[Leetcode 150] Evaluate Reverse Polish Notation</h3><p>You are given an array of strings tokens that represents an arithmetic expression in a Reverse Polish Notation.</p>
<p>Evaluate the expression. Return an integer that represents the value of the expression.</p>
<p>Note that:</p>
<ul>
<li>The valid operators are ‘+’, ‘-‘, ‘*’, and ‘&#x2F;‘.</li>
<li>Each operand may be an integer or another expression.</li>
<li>The division between two integers always truncates toward zero.</li>
<li>There will not be any division by zero.</li>
<li>The input represents a valid arithmetic expression in a reverse polish notation.</li>
<li>The answer and all the intermediate calculations can be represented in a 32-bit integer.</li>
</ul>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: tokens = [&quot;2&quot;,&quot;1&quot;,&quot;+&quot;,&quot;3&quot;,&quot;*&quot;]</span><br><span class="line">Output: 9</span><br><span class="line">Explanation: ((2 + 1) * 3) = 9</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: tokens = [&quot;4&quot;,&quot;13&quot;,&quot;5&quot;,&quot;/&quot;,&quot;+&quot;]</span><br><span class="line">Output: 6</span><br><span class="line">Explanation: (4 + (13 / 5)) = 6</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: tokens = [&quot;10&quot;,&quot;6&quot;,&quot;9&quot;,&quot;3&quot;,&quot;+&quot;,&quot;-11&quot;,&quot;*&quot;,&quot;/&quot;,&quot;*&quot;,&quot;17&quot;,&quot;+&quot;,&quot;5&quot;,&quot;+&quot;]</span><br><span class="line">Output: 22</span><br><span class="line">Explanation: ((10 * (6 / ((9 + 3) * -11))) + 17) + 5</span><br><span class="line">= ((10 * (6 / (12 * -11))) + 17) + 5</span><br><span class="line">= ((10 * (6 / -132)) + 17) + 5</span><br><span class="line">= ((10 * 0) + 17) + 5</span><br><span class="line">= (0 + 17) + 5</span><br><span class="line">= 17 + 5</span><br><span class="line">= 22</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; tokens.length &lt;&#x3D; 10^4</li>
<li>tokens[i] is either an operator: “+”, “-“, “*”, or “&#x2F;“, or an integer in the range [-200, 200].</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evalRPN</span>(<span class="params">self, tokens: <span class="type">List</span>[<span class="built_in">str</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        st = []</span><br><span class="line">        op = [<span class="string">&#x27;+&#x27;</span>, <span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;*&#x27;</span>, <span class="string">&#x27;/&#x27;</span>]</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> tokens:</span><br><span class="line">            <span class="keyword">if</span> t <span class="keyword">in</span> op:</span><br><span class="line">                x , y = <span class="built_in">int</span>(st.pop()), <span class="built_in">int</span>(st.pop())</span><br><span class="line">                temp = <span class="literal">None</span></span><br><span class="line">                <span class="keyword">if</span> t == <span class="string">&#x27;+&#x27;</span>:</span><br><span class="line">                    temp = x + y</span><br><span class="line">                <span class="keyword">elif</span> t == <span class="string">&#x27;-&#x27;</span>:</span><br><span class="line">                    temp = y - x</span><br><span class="line">                <span class="keyword">elif</span> t == <span class="string">&#x27;*&#x27;</span>:</span><br><span class="line">                    temp = x * y</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    temp = <span class="built_in">int</span>(y / x)</span><br><span class="line">                st.append(temp)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                st.append(t)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">int</span>(st[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-155-Min-Stack"><a href="#Leetcode-155-Min-Stack" class="headerlink" title="[Leetcode 155] Min Stack"></a>[Leetcode 155] Min Stack</h3><p>Design a stack that supports push, pop, top, and retrieving the minimum element in constant time.</p>
<p>Implement the MinStack class:</p>
<ul>
<li>MinStack() initializes the stack object.</li>
<li>void push(int val) pushes the element val onto the stack.</li>
<li>void pop() removes the element on the top of the stack.</li>
<li>int top() gets the top element of the stack.</li>
<li>int getMin() retrieves the minimum element in the stack.</li>
<li>*ou must implement a solution with O(1) time complexity for each function.</li>
</ul>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input</span><br><span class="line">[&quot;MinStack&quot;,&quot;push&quot;,&quot;push&quot;,&quot;push&quot;,&quot;getMin&quot;,&quot;pop&quot;,&quot;top&quot;,&quot;getMin&quot;]</span><br><span class="line">[[],[-2],[0],[-3],[],[],[],[]]</span><br><span class="line"></span><br><span class="line">Output</span><br><span class="line">[null,null,null,null,-3,null,0,-2]</span><br><span class="line"></span><br><span class="line">Explanation</span><br><span class="line">MinStack minStack = new MinStack();</span><br><span class="line">minStack.push(-2);</span><br><span class="line">minStack.push(0);</span><br><span class="line">minStack.push(-3);</span><br><span class="line">minStack.getMin(); // return -3</span><br><span class="line">minStack.pop();</span><br><span class="line">minStack.top();    // return 0</span><br><span class="line">minStack.getMin(); // return -2</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>-2^31 &lt;&#x3D; val &lt;&#x3D; 2^31 - 1</li>
<li>Methods pop, top and getMin operations will always be called on non-empty stacks.</li>
<li>At most 3 * 104 calls will be made to push, pop, top, and getMin.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MinStack</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.vals = []</span><br><span class="line">        self.minVals = []</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">push</span>(<span class="params">self, val: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.vals.append(val)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.minVals:</span><br><span class="line">            self.minVals.append(val)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.minVals[-<span class="number">1</span>] &gt;= val:</span><br><span class="line">                self.minVals.append(val)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pop</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> self.vals[-<span class="number">1</span>] == self.minVals[-<span class="number">1</span>]:</span><br><span class="line">            self.minVals.pop()</span><br><span class="line">        self.vals.pop()</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">top</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> self.vals[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getMin</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> self.minVals[-<span class="number">1</span>]  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Your MinStack object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"># obj = MinStack()</span></span><br><span class="line"><span class="comment"># obj.push(val)</span></span><br><span class="line"><span class="comment"># obj.pop()</span></span><br><span class="line"><span class="comment"># param_3 = obj.top()</span></span><br><span class="line"><span class="comment"># param_4 = obj.getMin()</span></span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-224-Basic-Calculator"><a href="#Leetcode-224-Basic-Calculator" class="headerlink" title="[Leetcode 224] Basic Calculator"></a>[Leetcode 224] Basic Calculator</h3><p>Given a string s representing a valid expression, implement a basic calculator to evaluate it, and return the result of the evaluation.</p>
<p>Note: You are not allowed to use any built-in function which evaluates strings as mathematical expressions, such as eval().</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;1 + 1&quot;</span><br><span class="line">Output: 2</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot; 2 - (3 + 4) &quot;</span><br><span class="line">Output: -5</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;(1+(4+5+2)-3)+(6+8)&quot;</span><br><span class="line">Output: 23</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; s.length &lt;&#x3D; 3 * 10^5</li>
<li>s consists of digits, ‘+’, ‘-‘, ‘(‘, ‘)’, and ‘ ‘.</li>
<li>s represents a valid expression.</li>
<li>‘+’ is not used as a unary operation (i.e., “+1” and “+(2 + 3)” is invalid).</li>
<li>‘-‘ could be used as a unary operation (i.e., “-1” and “-(2 + 3)” is valid).</li>
<li>There will be no two consecutive operators in the input.</li>
<li>Every number and running calculation will fit in a signed 32-bit integer.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calculate</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Refer to https://leetcode.com/problems/basic-calculator/solutions/546092/simple-python-solution-using-stack-with-explanation-inline/?envType=study-plan-v2&amp;envId=top-interview-150</span></span><br><span class="line"><span class="string">        1. Take 3 containers:</span></span><br><span class="line"><span class="string">        num -&gt; to store current num value only</span></span><br><span class="line"><span class="string">        sign -&gt; to store sign value, initially +1</span></span><br><span class="line"><span class="string">        res -&gt; to store sum</span></span><br><span class="line"><span class="string">        When ( comes these containers used for calculate sum of intergers within () brackets.</span></span><br><span class="line"><span class="string">        --------------------</span></span><br><span class="line"><span class="string">        2. When c is + or -</span></span><br><span class="line"><span class="string">        Move num to res, because we need to empty num for next integer value.</span></span><br><span class="line"><span class="string">        set num = 0</span></span><br><span class="line"><span class="string">        sign = update with c</span></span><br><span class="line"><span class="string">        --------------------</span></span><br><span class="line"><span class="string">        3. When c is &#x27;(&#x27;</span></span><br><span class="line"><span class="string">        Here, we need num, res, sign to calculate sum of integers within ()</span></span><br><span class="line"><span class="string">        So, move num and sign to stack =&gt; [num, sign]</span></span><br><span class="line"><span class="string">        Now reset - res = 0, num = 0, sign = 1 (default)</span></span><br><span class="line"><span class="string">        --------------------</span></span><br><span class="line"><span class="string">        4. When c is &#x27;)&#x27; -&gt; 2-(3+4), Here res=3, num=4, sign=1 stack [2, -] </span></span><br><span class="line"><span class="string">        res +=sign*num -&gt; calculate sum for num first, then pop items from stack, res=7</span></span><br><span class="line"><span class="string">        res *=stack.pop() - &gt; Pop sign(+ or -) to multiply with res, res = 7*(-1)</span></span><br><span class="line"><span class="string">        res +=stack.pop() - &gt; Pop integer and add with prev. sum, res = -7 + 2 - 5</span></span><br><span class="line"><span class="string">        --------------------</span></span><br><span class="line"><span class="string">        Simple Example: 2 - 3</span></span><br><span class="line"><span class="string">        Initially res will have 2,i.e. res = 2</span></span><br><span class="line"><span class="string">        then store &#x27;-&#x27; in sign. it will be used when 3 comes. ie. sign = -1</span></span><br><span class="line"><span class="string">        Now 3 comes =&gt; res = res + num*sign</span></span><br><span class="line"><span class="string">        Return statement: res+num*sign =&gt; res = 2 + 3(-1) = 2 - 3 = -1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num = <span class="number">0</span></span><br><span class="line">        sign = <span class="number">1</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        stack = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)): <span class="comment"># iterate characters</span></span><br><span class="line">            c = s[i]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> c.isdigit(): </span><br><span class="line">                <span class="comment"># parse the consecutive digits. e.g. 23 -&gt; 2 * 10 + 3</span></span><br><span class="line">                num = num*<span class="number">10</span> + <span class="built_in">int</span>(c) </span><br><span class="line">            </span><br><span class="line">            <span class="keyword">elif</span> c <span class="keyword">in</span> <span class="string">&#x27;-+&#x27;</span>: <span class="comment"># check for - and +</span></span><br><span class="line">                <span class="comment"># calculate the temporary result before &#x27;+&#x27; or &#x27;-&#x27;</span></span><br><span class="line">                res += num*sign</span><br><span class="line">                <span class="comment"># save the new sign</span></span><br><span class="line">                sign = -<span class="number">1</span> <span class="keyword">if</span> c == <span class="string">&#x27;-&#x27;</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">                <span class="comment"># reset num to 0 for parsing the next operand</span></span><br><span class="line">                num = <span class="number">0</span></span><br><span class="line">            <span class="keyword">elif</span> c == <span class="string">&#x27;(&#x27;</span>:</span><br><span class="line">                <span class="comment"># we need res and sign to save the temporary result within &#x27;()&#x27;. Reset sign to default 1.</span></span><br><span class="line">                stack.append(res)</span><br><span class="line">                stack.append(sign)</span><br><span class="line">                res = <span class="number">0</span></span><br><span class="line">                sign = <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> c == <span class="string">&#x27;)&#x27;</span>:</span><br><span class="line">                <span class="comment"># calculate the result within &#x27;()&#x27;</span></span><br><span class="line">                <span class="comment"># e.g. 2 -(3 + 4) -&gt; res = 3, num = 4, sign = 1, stack = [2, -]</span></span><br><span class="line">                res +=sign*num <span class="comment"># res = 3 * 1 + 4 = 7</span></span><br><span class="line">                res *=stack.pop() <span class="comment"># res = 7 * (-1) = -7</span></span><br><span class="line">                res +=stack.pop() <span class="comment"># res = -7 + 2 = -5</span></span><br><span class="line">                num = <span class="number">0</span> <span class="comment"># reset num to 0</span></span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> res + num*sign</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calculate_recursive</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">i</span>):</span><br><span class="line">            res, digits, sign = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(s):</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> s[i].isdigit():</span><br><span class="line">                    <span class="comment"># parse the operand</span></span><br><span class="line">                    digits = digits * <span class="number">10</span> + <span class="built_in">int</span>(s[i])</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">elif</span> s[i] <span class="keyword">in</span> <span class="string">&#x27;+-&#x27;</span>:</span><br><span class="line">                    <span class="comment"># evaluate the subresult before the next operator</span></span><br><span class="line">                    res += digits * sign</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># reset digits to 0</span></span><br><span class="line">                    digits = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># reset operator sign</span></span><br><span class="line">                    sign = <span class="number">1</span> <span class="keyword">if</span> s[i] == <span class="string">&#x27;+&#x27;</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">elif</span> s[i] == <span class="string">&#x27;(&#x27;</span>:</span><br><span class="line">                    <span class="comment"># evalute the sub results in &#x27;()&#x27; and return the index of &#x27;)&#x27;</span></span><br><span class="line">                    subres, i = evaluate(i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># accumulate the result</span></span><br><span class="line">                    res += subres * sign</span><br><span class="line"></span><br><span class="line">                <span class="keyword">elif</span> s[i] == <span class="string">&#x27;)&#x27;</span>:</span><br><span class="line">                    <span class="comment"># evaluate the subresult before &#x27;)&#x27;</span></span><br><span class="line">                    res += digits * sign</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># return the result and index</span></span><br><span class="line">                    <span class="keyword">return</span> res, i</span><br><span class="line">                </span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> res + digits * sign        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> evaluate(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-394-Decode-String"><a href="#Leetcode-394-Decode-String" class="headerlink" title="[Leetcode 394] Decode String"></a>[Leetcode 394] Decode String</h3><p>Given an encoded string, return its decoded string.</p>
<p>The encoding rule is: k[encoded_string], where the encoded_string inside the square brackets is being repeated exactly k times. Note that k is guaranteed to be a positive integer.</p>
<p>You may assume that the input string is always valid; there are no extra white spaces, square brackets are well-formed, etc. Furthermore, you may assume that the original data does not contain any digits and that digits are only for those repeat numbers, k. For example, there will not be input like 3a or 2[4].</p>
<p>The test cases are generated so that the length of the output will never exceed 105.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;3[a]2[bc]&quot;</span><br><span class="line">Output: &quot;aaabcbc&quot;</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;3[a2[c]]&quot;</span><br><span class="line">Output: &quot;accaccacc&quot;</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;2[abc]3[cd]ef&quot;</span><br><span class="line">Output: &quot;abcabccdcdcdef&quot;</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; s.length &lt;&#x3D; 30</li>
<li>s consists of lowercase English letters, digits, and square brackets ‘[]’.</li>
<li>s is guaranteed to be a valid input.</li>
<li>All the integers in s are in the range [1, 300].</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decodeString</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        st = []</span><br><span class="line">        num = <span class="number">0</span></span><br><span class="line">        res = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> s:</span><br><span class="line">            <span class="comment"># parse the repeat number</span></span><br><span class="line">            <span class="keyword">if</span> ch.isnumeric():</span><br><span class="line">                num = num * <span class="number">10</span> + <span class="built_in">int</span>(ch)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#  save the temporary string and repeat number</span></span><br><span class="line">            <span class="keyword">elif</span> ch == <span class="string">&#x27;[&#x27;</span>:</span><br><span class="line">                st.append(res)</span><br><span class="line">                st.append(num)</span><br><span class="line">                res = <span class="string">&quot;&quot;</span></span><br><span class="line">                num = <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># create the repeated strings</span></span><br><span class="line">            <span class="keyword">elif</span> ch == <span class="string">&#x27;]&#x27;</span>:</span><br><span class="line">                repeat = st.pop()</span><br><span class="line">                temp = st.pop()</span><br><span class="line">                res = temp + res * repeat</span><br><span class="line"></span><br><span class="line">            <span class="comment"># parse the string to repeat</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res += ch</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-735-Asteroid-Collision"><a href="#Leetcode-735-Asteroid-Collision" class="headerlink" title="[Leetcode 735] Asteroid Collision"></a>[Leetcode 735] Asteroid Collision</h3><p>We are given an array asteroids of integers representing asteroids in a row.</p>
<p>For each asteroid, the absolute value represents its size, and the sign represents its direction (positive meaning right, negative meaning left). Each asteroid moves at the same speed.</p>
<p>Find out the state of the asteroids after all collisions. If two asteroids meet, the smaller one will explode. If both are the same size, both will explode. Two asteroids moving in the same direction will never meet.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: asteroids = [5,10,-5]</span><br><span class="line">Output: [5,10]</span><br><span class="line">Explanation: The 10 and -5 collide resulting in 10. The 5 and 10 never collide.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: asteroids = [8,-8]</span><br><span class="line">Output: []</span><br><span class="line">Explanation: The 8 and -8 collide exploding each other.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>2 &lt;&#x3D; asteroids.length &lt;&#x3D; 104</li>
<li>-1000 &lt;&#x3D; asteroids[i] &lt;&#x3D; 1000</li>
<li>asteroids[i] !&#x3D; 0</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">asteroidCollision</span>(<span class="params">self, asteroids: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        st = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> asteroid <span class="keyword">in</span> asteroids:</span><br><span class="line">            <span class="comment"># only case to collide: -&gt; &lt;-</span></span><br><span class="line">            <span class="keyword">if</span> asteroid &lt; <span class="number">0</span>:</span><br><span class="line">                exploded = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">while</span> st <span class="keyword">and</span> st[-<span class="number">1</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># explode both</span></span><br><span class="line">                    <span class="keyword">if</span> st[-<span class="number">1</span>] == <span class="built_in">abs</span>(asteroid):</span><br><span class="line">                        exploded = <span class="literal">True</span></span><br><span class="line">                        st.pop()</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># explode the smaller asteroid on the left</span></span><br><span class="line">                    <span class="keyword">elif</span> st[-<span class="number">1</span>] &lt; <span class="built_in">abs</span>(asteroid):     </span><br><span class="line">                        st.pop()</span><br><span class="line">                        </span><br><span class="line">                    <span class="comment"># explode the asteroid and continue check the next asteroid</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        exploded = <span class="literal">True</span></span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> exploded:</span><br><span class="line">                    st.append(asteroid)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># cases not to collide:</span></span><br><span class="line">            <span class="comment"># -&gt; -&gt;</span></span><br><span class="line">            <span class="comment"># &lt;- -&gt;</span></span><br><span class="line">            <span class="comment"># &lt;- &lt;-</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                st.append(asteroid)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> st</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-2390-Removing-Stars-From-a-String"><a href="#Leetcode-2390-Removing-Stars-From-a-String" class="headerlink" title="[Leetcode 2390] Removing Stars From a String"></a>[Leetcode 2390] Removing Stars From a String</h3><p>You are given a string s, which contains stars *.</p>
<p>In one operation, you can:</p>
<ul>
<li>Choose a star in s.</li>
<li>Remove the closest non-star character to its left, as well as remove the star itself.</li>
</ul>
<p>Return the string after all stars have been removed.</p>
<p>Note:</p>
<ul>
<li>The input will be generated such that the operation is always possible.</li>
<li>It can be shown that the resulting string will always be unique.</li>
</ul>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;leet**cod*e&quot;</span><br><span class="line">Output: &quot;lecoe&quot;</span><br><span class="line">Explanation: Performing the removals from left to right:</span><br><span class="line">- The closest character to the 1st star is &#x27;t&#x27; in &quot;leet**cod*e&quot;. s becomes &quot;lee*cod*e&quot;.</span><br><span class="line">- The closest character to the 2nd star is &#x27;e&#x27; in &quot;lee*cod*e&quot;. s becomes &quot;lecod*e&quot;.</span><br><span class="line">- The closest character to the 3rd star is &#x27;d&#x27; in &quot;lecod*e&quot;. s becomes &quot;lecoe&quot;.</span><br><span class="line">There are no more stars, so we return &quot;lecoe&quot;.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; s.length &lt;&#x3D; 105</li>
<li>s consists of lowercase English letters and stars *.</li>
<li>The operation above can be performed on s.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeStars</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        st = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> c != <span class="string">&#x27;*&#x27;</span>:</span><br><span class="line">                st.append(c)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                st.pop()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(st)</span><br><span class="line">        </span><br></pre></td></tr></table></figure>

<h2 id="Queue-practices"><a href="#Queue-practices" class="headerlink" title="Queue practices"></a>Queue practices</h2><h3 id="Leetcode-933-Number-of-Recent-Calls"><a href="#Leetcode-933-Number-of-Recent-Calls" class="headerlink" title="[Leetcode 933] Number of Recent Calls"></a>[Leetcode 933] Number of Recent Calls</h3><p>You have a RecentCounter class which counts the number of recent requests within a certain time frame.</p>
<p>Implement the RecentCounter class:</p>
<ul>
<li>RecentCounter() Initializes the counter with zero recent requests.</li>
<li>int ping(int t) Adds a new request at time t, where t represents some time in milliseconds, and returns the number of requests that has happened in the past 3000 milliseconds (including the new request). Specifically, return the number of requests that have happened in the inclusive range [t - 3000, t].</li>
</ul>
<p>It is guaranteed that every call to ping uses a strictly larger value of t than the previous call.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input</span><br><span class="line">[&quot;RecentCounter&quot;, &quot;ping&quot;, &quot;ping&quot;, &quot;ping&quot;, &quot;ping&quot;]</span><br><span class="line">[[], [1], [100], [3001], [3002]]</span><br><span class="line">Output</span><br><span class="line">[null, 1, 2, 3, 3]</span><br><span class="line"></span><br><span class="line">Explanation</span><br><span class="line">RecentCounter recentCounter = new RecentCounter();</span><br><span class="line">recentCounter.ping(1);     // requests = [1], range is [-2999,1], return 1</span><br><span class="line">recentCounter.ping(100);   // requests = [1, 100], range is [-2900,100], return 2</span><br><span class="line">recentCounter.ping(3001);  // requests = [1, 100, 3001], range is [1,3001], return 3</span><br><span class="line">recentCounter.ping(3002);  // requests = [1, 100, 3001, 3002], range is [2,3002], return 3</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; t &lt;&#x3D; 109</li>
<li>Each test case will call ping with strictly increasing values of t.</li>
<li>At most 104 calls will be made to ping.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RecentCounter</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.qu = deque()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ping</span>(<span class="params">self, t: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        self.qu.append(t)</span><br><span class="line">        <span class="keyword">while</span> self.qu:</span><br><span class="line">            <span class="keyword">if</span> self.qu[<span class="number">0</span>] &lt; t - <span class="number">3000</span>:</span><br><span class="line">                self.qu.popleft()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.qu)</span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-649-Dota2-Senate"><a href="#Leetcode-649-Dota2-Senate" class="headerlink" title="[Leetcode 649] Dota2 Senate"></a>[Leetcode 649] Dota2 Senate</h3><p>In the world of Dota2, there are two parties: the Radiant and the Dire.</p>
<p>The Dota2 senate consists of senators coming from two parties. Now the Senate wants to decide on a change in the Dota2 game. The voting for this change is a round-based procedure. In each round, each senator can exercise one of the two rights:</p>
<ul>
<li>Ban one senator’s right: A senator can make another senator lose all his rights in this and all the following rounds.<br>Announce the victory: If this senator found the senators who still have rights to vote are all from the same party, he can announce the victory and decide on the change in the game.</li>
<li>Given a string senate representing each senator’s party belonging. The character ‘R’ and ‘D’ represent the Radiant party and the Dire party. Then if there are n senators, the size of the given string will be n.</li>
</ul>
<p>The round-based procedure starts from the first senator to the last senator in the given order. This procedure will last until the end of voting. All the senators who have lost their rights will be skipped during the procedure.</p>
<p>Suppose every senator is smart enough and will play the best strategy for his own party. Predict which party will finally announce the victory and change the Dota2 game. The output should be “Radiant” or “Dire”.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: senate = &quot;RD&quot;</span><br><span class="line">Output: &quot;Radiant&quot;</span><br><span class="line">Explanation: </span><br><span class="line">The first senator comes from Radiant and he can just ban the next senator&#x27;s right in round 1. </span><br><span class="line">And the second senator can&#x27;t exercise any rights anymore since his right has been banned. </span><br><span class="line">And in round 2, the first senator can just announce the victory since he is the only guy in the senate who can vote.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: senate = &quot;RDD&quot;</span><br><span class="line">Output: &quot;Dire&quot;</span><br><span class="line">Explanation: </span><br><span class="line">The first senator comes from Radiant and he can just ban the next senator&#x27;s right in round 1. </span><br><span class="line">And the second senator can&#x27;t exercise any rights anymore since his right has been banned. </span><br><span class="line">And the third senator comes from Dire and he can ban the first senator&#x27;s right in round 1. </span><br><span class="line">And in round 2, the third senator can just announce the victory since he is the only guy in the senate who can vote.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>n &#x3D;&#x3D; senate.length</li>
<li>1 &lt;&#x3D; n &lt;&#x3D; 104</li>
<li>senate[i] is either ‘R’ or ‘D’.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predictPartyVictory</span>(<span class="params">self, senate: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        radiant = deque()</span><br><span class="line">        dire = deque()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># add senate to separate queues</span></span><br><span class="line">        <span class="keyword">for</span> i, s <span class="keyword">in</span> <span class="built_in">enumerate</span>(senate):</span><br><span class="line">            <span class="keyword">if</span> s == <span class="string">&#x27;R&#x27;</span>:</span><br><span class="line">                radiant.append(i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dire.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># senate votes to ban the opponent</span></span><br><span class="line">        n = <span class="built_in">len</span>(senate)</span><br><span class="line">        <span class="keyword">while</span> radiant <span class="keyword">and</span> dire:</span><br><span class="line">            <span class="comment"># senator stays if he/she can vote earlier</span></span><br><span class="line">            <span class="keyword">if</span> radiant[<span class="number">0</span>] &lt; dire[<span class="number">0</span>]:</span><br><span class="line">                <span class="comment"># front dire is eliminated from queue</span></span><br><span class="line">                dire.popleft()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># front radiant moves to the end of queue for next round</span></span><br><span class="line">                radiant.popleft()</span><br><span class="line">                radiant.append(n)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># front radiant is eliminated</span></span><br><span class="line">                radiant.popleft()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># front dire moves to the end of queue for next round</span></span><br><span class="line">                dire.popleft()</span><br><span class="line">                dire.append(n)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># increment the position in the queue</span></span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Radiant&quot;</span> <span class="keyword">if</span> radiant <span class="keyword">else</span> <span class="string">&quot;Dire&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
        <tag>Queue</tag>
        <tag>Stack</tag>
      </tags>
  </entry>
  <entry>
    <title>Stack implementation in Python</title>
    <url>/blog/stack-implementation-in-python/</url>
    <content><![CDATA[<h2 id="What-is-stack"><a href="#What-is-stack" class="headerlink" title="What is stack"></a>What is stack</h2><p>Stack is a linear data structure in which the items are in a Last-In-First-Out(LIFO)manner.</p>
<p>The stack may include the following key functions.</p>
<ul>
<li>push() - add an item to the top of the stack</li>
<li>pop() - remove the item at the top</li>
<li>size() - return the number of items in stack</li>
<li>peek() - return the item from top of the stack</li>
<li>is_empty() - check if the stack is empty</li>
</ul>
<h2 id="How-to-implement"><a href="#How-to-implement" class="headerlink" title="How to implement"></a>How to implement</h2><p>There are various ways to implement a stack in Python. In this post, we will learn how to implement it with the following ways.</p>
<ul>
<li>list</li>
<li>collections.deque</li>
<li>queue.LifoQueue</li>
<li>linked list</li>
</ul>
<h3 id="using-list"><a href="#using-list" class="headerlink" title="using list"></a>using list</h3><p>In Python, list is like dynamic array. The items in list are stored contiguously in memory. Thus, the random access to list is as fast as array. However, as it grows bigger than the allocated memory it currently holds, it needs to have more memory allocation to increase the capacity dynamically. In such case, the stack append call could take longer time.</p>
<pre><code>st = []
st.append(1)
st.append(2)
st.append(3)
print(&quot;stack size: &#123;&#125;&quot;.format(len(st)))
print(st.pop())
print(st.pop())
print(st.pop())
print(&quot;stack size: &#123;&#125;&quot;.format(len(st)))
</code></pre>
<h3 id="Using-deque"><a href="#Using-deque" class="headerlink" title="Using deque"></a>Using deque</h3><p>deque is pronounced “deck” and stands for “double-ended queue”. It is built upon a doubly linked list which allows insert&#x2F;remove nodes from both ends with constant time(O(1)). Deque is the prefered method to implement a stack in Python. Note that deque is not thread-safe.</p>
<pre><code>from collections import deque

st = deque()
st.append(1)
st.append(2)
st.append(3)
print(&quot;stack size: &#123;&#125;&quot;.format(len(st)))
print(st.pop())
print(st.pop())
print(st.pop())
print(&quot;stack size: &#123;&#125;&quot;.format(len(st)))
</code></pre>
<h3 id="Using-LifoQueue"><a href="#Using-LifoQueue" class="headerlink" title="Using LifoQueue"></a>Using LifoQueue</h3><p>If you need multi-threading, you can use queue.LifoQueue module. However, it comes at a performance cost to achieve the thread-safety.</p>
<pre><code>from queue import LifoQueue

st = LifoQueue()
st.put(1)
st.put(2)
st.put(3)
print(&quot;stack size: &#123;&#125;&quot;.format(st.qsize()))
print(st.get())
print(st.get())
print(st.get())
print(&quot;stack size: &#123;&#125;&quot;.format(st.qsize()))
</code></pre>
<h3 id="Using-linked-list"><a href="#Using-linked-list" class="headerlink" title="Using linked list"></a>Using linked list</h3><p>You also can implement your own stack with the linked list data structure.</p>
<pre><code>class Node:
    def __init__(self, val):
        self.val = val
        self.next = None


class Stack:
    def __init__(self):
        self.top = None
        self.size = 0

    def get_size(self):
        return self.size

    def is_empty(self):
        return self.size == 0

    def push(self, val):
        n = Node(val)
        if self.top == None:
            self.top = n
        else:
            n.next = self.top
            self.top = n

        self.size += 1

    def pop(self):
        if self.is_empty():
            raise Exception(&quot;empty stack&quot;)

        val = self.top.val
        self.top = self.top.next
        self.size -= 1
        return val

    def peek(self):
        if self.is_empty():
            raise Exception(&quot;empty stack&quot;)

        return self.top.val

    def __str__(self):
        curr = self.top
        st_str = &quot;&quot;
        while curr:
            st_str += str(curr.val) + &quot;-&gt;&quot;
            curr = curr.next
        return st_str[:-2]


st = Stack()
st.push(1)
st.push(2)
st.push(3)
print(&quot;Stack: &#123;&#125;&quot;.format(st))
print(&quot;stack size: &#123;&#125;&quot;.format(st.size))
print(&quot;stack top: &#123;&#125;&quot;.format(st.peek()))
print(st.pop())
print(st.pop())
print(st.pop())
print(&quot;stack size: &#123;&#125;&quot;.format(st.size))
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
        <tag>Stack</tag>
      </tags>
  </entry>
  <entry>
    <title>Start jekyll with systemd service</title>
    <url>/blog/start-jekyll-with-systemd-service/</url>
    <content><![CDATA[<h2 id="Create-systemd-file"><a href="#Create-systemd-file" class="headerlink" title="Create systemd file"></a>Create systemd file</h2><pre><code>$ cat /etc/systemd/system/jekyll.service
# Description: to make `jekyll serve` a system service and start on boot
#
# Usage: place this file at `/etc/systemd/system/jekyll.service`
# then run
#  sudo systemctl start jekyll.service
#  sudo systemctl enable jekyll.service

[Unit]
Description=Jekyll service
After=syslog.target network.target

[Service]
User=username
Type=simple
WorkingDirectory=/home/username/www
ExecStart=/home/username/gems/bin/bundle exec jekyll serve --host 192.168.0.16
ExecStop=/usr/bin/pkill -f jekyll
Restart=always
TimeoutStartSec=60
RestartSec=60
StandardOutput=syslog
StandardError=syslog
SyslogIdentifier=jekyll

[Install]
WantedBy=multi-user.target network-online.target
</code></pre>
<h2 id="Start-jekyll-service"><a href="#Start-jekyll-service" class="headerlink" title="Start jekyll service"></a>Start jekyll service</h2><pre><code>$ sudo systemctl daemon-reload
$ sudo systemctl start jekyll
$ sudo systemctl enable jekyll
$ sudo systemctl status jekyll
● jekyll.service - Jekyll service
     Loaded: loaded (/etc/systemd/system/jekyll.service; disabled; vendor preset: enabled)
     Active: active (running) since Wed 2020-08-19 21:14:18 PDT; 7s ago
   Main PID: 35228 (ruby2.7)
      Tasks: 4 (limit: 13599)
     Memory: 83.8M
     CGroup: /system.slice/jekyll.service
             └─35228 ruby2.7 /home/[username]/www/vendor/bundle/ruby/2.7.0/bin/jekyll serve --host 192.168.0.16

Aug 19 21:14:19 skyhawk-01 jekyll[35228]: Configuration file: /home/username/www/_config.yml
Aug 19 21:14:19 skyhawk-01 jekyll[35228]:             Source: /home/username/www
Aug 19 21:14:19 skyhawk-01 jekyll[35228]:        Destination: /home/username/www/_site
Aug 19 21:14:19 skyhawk-01 jekyll[35228]:  Incremental build: disabled. Enable with --incremental
Aug 19 21:14:19 skyhawk-01 jekyll[35228]:       Generating...
Aug 19 21:14:19 skyhawk-01 jekyll[35228]:        Jekyll Feed: Generating feed for posts
Aug 19 21:14:21 skyhawk-01 jekyll[35228]:                     done in 2.455 seconds.
Aug 19 21:14:21 skyhawk-01 jekyll[35228]:  Auto-regeneration: enabled for &#39;/home/username/www&#39;
Aug 19 21:14:21 skyhawk-01 jekyll[35228]:     Server address: http://192.168.0.16:4000
Aug 19 21:14:21 skyhawk-01 jekyll[35228]:   Server running... press ctrl-c to stop.
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>Jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title>Storage concepts you should know</title>
    <url>/blog/storage-concepts-you-should-know/</url>
    <content><![CDATA[<h2 id="Object-Storage"><a href="#Object-Storage" class="headerlink" title="Object Storage"></a>Object Storage</h2><p>Object storage, also known as object-based storage, is a strategy that manages and manipulates data storage as distinct units, called objects. These objects are kept in a single storehouse and are not ingrained in files inside other folders. Instead, object storage combines the pieces of data that make up a file, adds all its relevant metadata to that file, and attaches a custom identifier.</p>
<p>Object storage adds comprehensive metadata to the file, eliminating the tiered file structure used in file storage, and places everything into a flat address space, called a storage pool. This metadata is key to the success of object storage in that it provides deep analysis of the use and function of data in the storage pool.</p>
<p><strong>Object storage vs. file storage vs. block storage</strong></p>
<p>Object storage takes each piece of data and designates it as an object. Data is kept in separate storehouses versus files in folders and is bundled with associated metadata and a unique identifier to form a storage pool.</p>
<p>File storage stores data as a single piece of information in a folder to help organize it among other data. This is also called hierarchical storage, imitating the way that paper files are stored. When you need access to data, your computer system needs to know the path to find it.</p>
<p>Block storage takes a file apart into singular blocks of data and then stores these blocks as separate pieces of data. Each piece of data has a different address, so they don’t need to be stored in a file structure.</p>
<p><strong>Benefits of object storage</strong></p>
<p>Now that we’ve described what object storage is, what are its benefits?</p>
<ul>
<li>Greater data analytics. Object storage is driven by metadata, and with this level of classification for every piece of data, the opportunity for analysis is far greater.</li>
<li>Infinite scalability. Keep adding data, forever. There’s no limit.</li>
</ul>
<p>Faster data retrieval. Due to the categorization structure of object storage, and the lack of folder hierarchy, you can retrieve your data much faster.</p>
<ul>
<li>Reduction in cost. Due to the scale-out nature of object storage, it’s less costly to store all your data.</li>
<li>Optimization of resources. Because object storage does not have a filing hierarchy, and the metadata is completely customizable, there are far fewer limitations than with file or block storage.</li>
</ul>
<p><strong>Object storage use cases</strong></p>
<p>There are multiple use cases for object storage. For example, it can assist you in the following ways:</p>
<ul>
<li>Deliver rich media. Define workflows by leveraging industry-leading solutions for managing unstructured data. Reduce your costs for globally distributed rich media.</li>
<li>Manage distributed content. Optimize the value of your data throughout its lifecycle and deliver competitive storage services.</li>
<li>Embrace the Internet of Things (IoT). Manage machine-to-machine data efficiently, support artificial intelligence and analytics, and compress the cost and time of the design process.</li>
</ul>
<p><a href="https://www.netapp.com/data-storage/storagegrid/what-is-object-storage/">Source</a></p>
<h2 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h2><p>Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache, and message broker. Redis provides data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes, and streams. Redis has built-in replication, Lua scripting, LRU eviction, transactions, and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster.</p>
<p><a href="https://redis.io/">Source</a></p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
  </entry>
  <entry>
    <title>StringBuilder implementation in Python</title>
    <url>/blog/stringbuilder-implementation-in-python/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>A string is a collection of characters. In Java, StringBuilder class is used to create mutable string. But there isn’t a built-in StringBuilder in Python. In Python, string is an immutable object. New memory has to be allocated for every string.</p>
<p>In Pythin, there are following ways to implement a StringBuilder.</p>
<ol>
<li>string concatenation(append)</li>
<li>join() function</li>
<li>string IO module</li>
</ol>
<h2 id="String-concatenation-append"><a href="#String-concatenation-append" class="headerlink" title="String concatenation(append)"></a>String concatenation(append)</h2><pre><code>def method1():
    string = &quot;&quot;
    for i in range(count):
        string += str(i)
    return string
</code></pre>
<h2 id="join-function"><a href="#join-function" class="headerlink" title="join() function"></a>join() function</h2><pre><code>def method2():
    strings = []
    for i in range(count):
        strings.append(str(i))
    return &quot;&quot;.join(strings)
</code></pre>
<h2 id="StringIO-module"><a href="#StringIO-module" class="headerlink" title="StringIO module"></a>StringIO module</h2><pre><code>def method3():
    si = StringIO()
    for i in range(count):
        si.write(str(i))

    return si.getvalue()
</code></pre>
<h2 id="Efficiency-measurement"><a href="#Efficiency-measurement" class="headerlink" title="Efficiency measurement"></a>Efficiency measurement</h2><p>I’m interested to see which method is more efficient. So, we are building a long string with one million numbers in order to measure the run time of each method.</p>
<pre><code>from io import StringIO
import time
from array import array


# using string concatenation(append)
def method1():
    string = &quot;&quot;
    for i in range(count):
        string += str(i)
    return string


# using join()
def method2():
    strings = []
    for i in range(count):
        strings.append(str(i))
    return &quot;&quot;.join(strings)


# using StringIO module
def method3():
    si = StringIO()
    for i in range(count):
        si.write(str(i))

    return si.getvalue()


# implement a StringBuilder in Python
class StringBuilder:
    def __init__(self):
        self.sb = StringIO()

    def add(self, str):
        self.sb.write(str)

    def toString(self):
        return self.sb.getvalue()


# using array
def method4():
    char_array = array(&quot;L&quot;)
    for i in range(count):
        char_array.append(i)
    return char_array


# test function
def run_test(method):
    start_time = time.perf_counter()
    eval(&quot;method&quot; + str(method))()
    end_time = time.perf_counter()
    print(&quot;elapsed time &#123;&#125;&quot;.format(round(end_time - start_time, 2)))
</code></pre>
<p>We run the tests for the four different methods twice to measure the efficiency.</p>
<pre><code># tests
count = 1000000
run_test(1)
run_test(2)
run_test(3)
run_test(4)


run_test(4)
run_test(3)
run_test(2)
run_test(1)

# Output:
# elapsed time 0.29
# elapsed time 0.16
# elapsed time 0.19
# elapsed time 0.09

# elapsed time 0.08
# elapsed time 0.19
# elapsed time 0.15
# elapsed time 0.31
</code></pre>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>The string appending method is the slowest. The join() function is relative efficient. Using an array seems efficient as well. However, we don’t count the array to string convention time yet.</p>
<p>In general, the join() functions seems fast enough for long string construction.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Sublime Jekyll Package</title>
    <url>/blog/sublime-jekyll-package/</url>
    <content><![CDATA[<p>sublime-jekyll is a Sublime Text package for Jekyll static sites. This package is aimed at streamlining the process of managing and creating new Jekyll posts. It does this by providing quick and easy access to new post and draft commands, snippets and completions for Liquid template tags and filters, as well as some handy commands for adding dates and upload links to your posts.</p>
<p>If you use Jekyll as your static site generator, and Sublime Text as your text editor, you should absolutely install and use this package!</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://sublime-jekyll.readthedocs.io/en/latest/index.html">https://sublime-jekyll.readthedocs.io/en/latest/index.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>Jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title>Surprised! How can the write IOPS be 10x faster than the cloud native disk performance</title>
    <url>/blog/surprised-how-can-the-write-throughput-be-10x-faster-than-the-cloud-native-disk-performance/</url>
    <content><![CDATA[<p>In a competitive fio write performance benchmark between 3rd party storage solution and cloud native disk, we noticed that the write performance for the 3rd party storage solution is 10x faster than the cloud native. This usually seems impossible since we would argue that no one can beat the raw disk performance. The following case shows an interesting write performance optimization from the storage solution under test.</p>
<h2 id="Cloud-native-raw-disk-performance-fio-4k-write"><a href="#Cloud-native-raw-disk-performance-fio-4k-write" class="headerlink" title="Cloud native raw disk performance(fio,4k,write)"></a>Cloud native raw disk performance(fio,4k,write)</h2><p>From the benchmark result, it shows the IOPS limit of the cloud drive is ~7500. This aligns with the cloud storage SPEC in use.</p>
<p><img src="/images/10xfast_1.jpg" alt="Image"><br><img src="/images/10xfast_2.jpg" alt="Image"><br><img src="/images/10xfast_3.jpg" alt="Image"></p>
<h2 id="10x-faster-IOPS-is-observed"><a href="#10x-faster-IOPS-is-observed" class="headerlink" title="10x faster IOPS is observed!!!"></a>10x faster IOPS is observed!!!</h2><p>In the output of fio, the IOPS(&gt;75k) is 10x faster than the result from cloud native raw disk(~7500). This can be verified from the iostat output. The IOPS at logic volume layer is aligned with the fio output.</p>
<p><img src="/images/10xfast_4.jpg" alt="Image"><br><img src="/images/10xfast_5.jpg" alt="Image"><br><img src="/images/10xfast_6.jpg" alt="Image"></p>
<h2 id="Iosize-matters"><a href="#Iosize-matters" class="headerlink" title="Iosize matters!!!"></a>Iosize matters!!!</h2><p>When the write request comes to the physical disk from logic volume, the I&#x2F;O size is changed from 4k to 400k. It indicates the smaller writes got merged to larger writes.</p>
<p><img src="/images/10xfast_7.jpg" alt="Image"><br><img src="/images/10xfast_8.jpg" alt="Image"><br><img src="/images/10xfast_9.jpg" alt="Image"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>With limited IOPS on the cloud drive, larger IO size really help improve the write performance by reducing the write requests to disk. With proper write performance optimization at logic volume layer, the write performance can be boosted 10x faster with no surprise!!!</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>RCA</tag>
      </tags>
  </entry>
  <entry>
    <title>SysRq - Linux Magic System Request Key Hacks</title>
    <url>/blog/sysrq-linux-magic-system-request-key-hacks/</url>
    <content><![CDATA[<h2 id="What-is-the-magic-SysRq-key"><a href="#What-is-the-magic-SysRq-key" class="headerlink" title="What is the magic SysRq key?"></a>What is the magic SysRq key?</h2><p>It is a ‘magical’ key combo you can hit which the kernel will respond to regardless of whatever else it is doing, unless it is completely locked up.</p>
<h2 id="How-do-I-enable-the-magic-SysRq-key"><a href="#How-do-I-enable-the-magic-SysRq-key" class="headerlink" title="How do I enable the magic SysRq key?"></a>How do I enable the magic SysRq key?</h2><p>You need to say “yes” to ‘Magic SysRq key (CONFIG_MAGIC_SYSRQ)’ when configuring the kernel. When running a kernel with SysRq compiled in, &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;sysrq controls the functions allowed to be invoked via the SysRq key. The default value in this file is set by the CONFIG_MAGIC_SYSRQ_DEFAULT_ENABLE config symbol, which itself defaults to 1. Here is the list of possible values in &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;sysrq:</p>
<ul>
<li><p>0 - disable sysrq completely</p>
</li>
<li><p>1 - enable all functions of sysrq</p>
</li>
<li><p>$&gt;$1 - bitmask of allowed sysrq functions (see below for detailed function description):</p>
<pre><code>2 =   0x2 - enable control of console logging level
4 =   0x4 - enable control of keyboard (SAK, unraw)
8 =   0x8 - enable debugging dumps of processes etc.
</code></pre>
<p>   16 &#x3D;  0x10 - enable sync command<br>   32 &#x3D;  0x20 - enable remount read-only<br>   64 &#x3D;  0x40 - enable signalling of processes (term, kill, oom-kill)<br>  128 &#x3D;  0x80 - allow reboot&#x2F;poweroff<br>  256 &#x3D; 0x100 - allow nicing of all RT tasks</p>
</li>
</ul>
<p>You can set the value in the file by the following command:</p>
<pre><code>$ echo &quot;number&quot; &gt;/proc/sys/kernel/sysrq
</code></pre>
<p>The number may be written here either as decimal or as hexadecimal with the 0x prefix. CONFIG_MAGIC_SYSRQ_DEFAULT_ENABLE must always be written in hexadecimal.</p>
<p>Note that the value of &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;sysrq influences only the invocation via a keyboard. Invocation of any operation via &#x2F;proc&#x2F;sysrq-trigger is always allowed (by a user with admin privileges).</p>
<h2 id="How-do-I-use-the-magic-SysRq-key"><a href="#How-do-I-use-the-magic-SysRq-key" class="headerlink" title="How do I use the magic SysRq key?"></a>How do I use the magic SysRq key?</h2><p>Write a character to &#x2F;proc&#x2F;sysrq-trigger. e.g.:</p>
<pre><code>$ echo &quot;command key&quot; &gt; /proc/sysrq-trigger
</code></pre>
<p>The “command key” is case sensitive.</p>
<h2 id="What-are-the-command-keys"><a href="#What-are-the-command-keys" class="headerlink" title="What are the command keys?"></a>What are the command keys?</h2><p><img src="/images/sysrq-command-key.png" alt="Image"></p>
<h2 id="What-can-I-use-them-for"><a href="#What-can-I-use-them-for" class="headerlink" title="What can I use them for?"></a>What can I use them for?</h2><p>If we want to check the backtrace of all active CPUs, we can do following.</p>
<pre><code>$ echo l &gt; /proc/sysrq-trigger


$ tail -f /var/log/messages
May 21 17:49:41 host1 kernel: sysrq: Show backtrace of all active CPUs
May 21 17:49:41 host1 kernel: NMI backtrace for cpu 34
May 21 17:49:41 host1 kernel: CPU: 34 PID: 27101 Comm: bash Tainted: G           O      5.7.12-1.el7.elrepo.x86_64 #1
May 21 17:49:41 host1 kernel: Hardware name: Supermicro SYS-1029U-TN12RV/X11DPU-V, BIOS 3.4 11/03/2020
May 21 17:49:41 host1 kernel: Call Trace:
May 21 17:49:41 host1 kernel: dump_stack+0x6d/0x9a
May 21 17:49:41 host1 kernel: ? lapic_can_unplug_cpu.cold+0x40/0x40
May 21 17:49:41 host1 kernel: nmi_cpu_backtrace.cold+0x14/0x53
May 21 17:49:41 host1 kernel: nmi_trigger_cpumask_backtrace+0xd9/0xdb
May 21 17:49:41 host1 kernel: arch_trigger_cpumask_backtrace+0x19/0x20
May 21 17:49:41 host1 kernel: sysrq_handle_showallcpus+0x17/0x20
May 21 17:49:41 host1 kernel: __handle_sysrq.cold+0x48/0x111
May 21 17:49:41 host1 kernel: write_sysrq_trigger+0x28/0x37
May 21 17:49:41 host1 kernel: proc_reg_write+0x66/0x90
May 21 17:49:41 host1 kernel: __vfs_write+0x1b/0x40
May 21 17:49:41 host1 kernel: vfs_write+0xb9/0x1b0
May 21 17:49:41 host1 kernel: ksys_write+0x67/0xe0
May 21 17:49:41 host1 kernel: __x64_sys_write+0x1a/0x20
May 21 17:49:41 host1 kernel: do_syscall_64+0x60/0x1e0
May 21 17:49:41 host1 kernel: entry_SYSCALL_64_after_hwframe+0x44/0xa9
&lt;...&gt;
</code></pre>
<p>The same message can also be checked from the following command.</p>
<pre><code>$ dmesg -T
$ journalctl --since &quot;5 minutes ago&quot;
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.kernel.org/doc/html/latest/admin-guide/sysrq.html">https://www.kernel.org/doc/html/latest/admin-guide/sysrq.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Administration</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP 3-way handshake (SYN, SYN-ACK,ACK)</title>
    <url>/blog/tcp-3-way-handshake-syn-syn-ack-ack/</url>
    <content><![CDATA[<h2 id="What-is-TCP-Three-Way-HandShake"><a href="#What-is-TCP-Three-Way-HandShake" class="headerlink" title="What is TCP Three-Way HandShake"></a>What is TCP Three-Way HandShake</h2><p>TCP 3-way handshake is a process which is used in a TCP&#x2F;IP network to make a connection between the client and server. It is a three-step process that requires both the client and server to exchange synchronization and acknowledgment packets before the real data communication process starts.</p>
<span id="more"></span>

<h2 id="TCP-message-types"><a href="#TCP-message-types" class="headerlink" title="TCP message types"></a>TCP message types</h2><ul>
<li>Syn - initiate and establish a connection. It helps synchronize sequence numbers between devices</li>
<li>ACK - help confirm to the other side that it has received the SYN</li>
<li>SYN-ACK - SYN message from local device and ACK of the earlier packet</li>
<li>FIN - terminate a connection</li>
</ul>
<h2 id="TCP-3-Way-handshake-process"><a href="#TCP-3-Way-handshake-process" class="headerlink" title="TCP 3-Way handshake process"></a>TCP 3-Way handshake process</h2><p>TCP traffic begins with a 3-way handshake. In this handshake process, a client needs to initiate the conversation by requesting a communication session with the Server:</p>
<p><img src="/images/tcp-handshake.png" alt="tcp-handshake"></p>
<ol>
<li>Client establishes a connection with a server. It sends a segment with SYN and informs the server about the client should start communication, and with what should be its sequence number.</li>
<li>Server responds to the client request with SYN-ACK signal set. ACK helps to signify the response of segment that is received and SYN signifies what sequence number it should able to start with the segments.</li>
<li>Client acknowledges the response of the Server, and they both create a stable connection will begin the actual data transfer process.</li>
</ol>
<p>Let’s delve into to see how it works:</p>
<p><img src="/images/tcp-handshake-1.png" alt="tcp-handshake-1"></p>
<ol>
<li>SYN - the client wants to establish a connection with a server, so it sends a segment with SYN(Synchronize Sequence Number) which informs the server that the client is likely to start communication and with what sequence number it starts segments with</li>
<li>SYN + ACK - Server responds to the client request with SYN-ACK signal bits set. Acknowledgement(ACK) signifies the response of the segment it received and SYN signifies with what sequence number it is likely to start the segments with</li>
<li>ACK - In the final part client acknowledges the response of the server and they both establish a reliable connection with which they will start the actual data transfer</li>
</ol>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title>The Civil and Revolutionary War</title>
    <url>/blog/the-civil-and-revolutionary-war/</url>
    <content><![CDATA[<p>The Revolutionary and Civil War were two of the most well known wars in American history. They greatly impacted America in different ways. The Civil War abolished slavery while the Revolutionary War created our nation.<span id="more"></span></p>
<p>The Civil War started when America’s North and South had different ideas about slavery. The people of Northern America, which was called Union, thought that slavery was unjust. They wanted to abolish it. However, the South wanted slavery. The South left the United States and created their own nation called the Confederacy. The first battle that started the Civil War was the attack on Fort Sumter. After the huge loss of Fort Sumter, President Lincoln announced that the Union would attempt to keep supplies from leaving or entering the Confederate nation. This blockade would later weaken the Confederacy. President Lincoln did not want his nation to be divided, so he tried his best to win the war. He chose General Grant as leader for the Union army. Under Grant’s leadership, the Union defeated the Confederate army and slavery was abolished. The Confederacy later reunited with the Union and America was a united nation again.</p>
<p>The Revolutionary War started when the colonists thought that they were not represented in the British government. The British government was making laws and raising taxes constantly which made the colonists feel that they had no freedom. They wanted to make choices and speak without restraint. With the leadership of General Washington, they went through many hardships and battles. Eventually, they were allied with the French and Spanish armies. Together, they trapped the British, who were led by General Cornwallis, and made them surrender. Having no choice, the British signed the Treaty of Paris and the people of America earned their freedom. The Americans had another problem though. How would they govern themselves? They wanted to be different from England, their home country. They did not want a monarch. Instead, they created several branches which were called the Legislative, Judicial, and Executive. These branches made laws and advised the president. The Americans made sure no one in the government would be too powerful or strong enough to become king.</p>
<p>The Revolutionary War and Civil War both had a heavy impact on our nation. If we had not won both , nobody knows what may have happened to America today. Both wars involved a lot of battles and the colonies had to constantly use up their supplies to support the war. The colonists before the Revolutionary War were left in the New World to govern themselves. Later on, the British wished to gain control over them and have monopoly over their products. The colonists rebelled and began the war. However, in the Civil War, it wasn’t about rebelling for freedom. Two parts of America had different ideas about slavery. The Northern part thought slavery was unjust but the South wanted slavery. The South eventually decided to separate from America and create their own nation called the Confederacy. Both of these wars were fought in the United States and were fought for the freedom of our citizens’ rights. The major difference was that the Revolutionary War was fought against Britain while the Civil War was fought against ourselves. Also, the weaponry of the patriots were knives and bayonets or hand combat, while the Civil War troops used deadlier weapons such as cannonballs. Due to the deadlier weapons, about 620,000 Americans died in the Civil War, and 25,000 Americans died in the Revolutionary War. But since both wars were won by the Americans, the result was the unity of our country.</p>
<p>The effects of the Civil and Revolutionary War, two of the most important wars in American history, led to what America is right now. Though they had serious impacts on our land, the Revolutionary War led to the establishment of our land and freedom from Britain, while the result of the Civil War was that slavery was demolished. The site Ducksters U.S. History said, “The Revolutionary War was fought for the Americans’ freedom, while the Civil War was fought for the freedom of slaves.” So both wars affected the rights of citizens.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>The complete guide to System Design</title>
    <url>/blog/the-complete-guide-to-system-design/</url>
    <content><![CDATA[<p><a href="https://www.educative.io/blog/complete-guide-to-system-design?eid=5082902844932096">The complete guide to System Design in 2022</a> covers:</p>
<ul>
<li><p>What is System Design?</p>
</li>
<li><p>System Design fundamentals</p>
</li>
<li><p>Horizontal and vertical scaling</p>
</li>
<li><p>Microservices</p>
</li>
<li><p>Proxy servers</p>
</li>
<li><p>CAP theorem</p>
</li>
<li><p>Redundancy and replication</p>
</li>
<li><p>Storage</p>
</li>
<li><p>Block storage</p>
</li>
<li><p>File storage</p>
</li>
<li><p>Object storage</p>
</li>
<li><p>Redundant Disk Arrays (RAID)</p>
</li>
<li><p>Message queues</p>
</li>
<li><p>Kafka</p>
</li>
<li><p>File systems</p>
</li>
<li><p>Google File System (GFS)</p>
</li>
<li><p>Hadoop Distributed File System (HDFS)</p>
</li>
<li><p>System Design patterns</p>
</li>
<li><p>Bloom filters</p>
</li>
<li><p>Consistent hashing</p>
</li>
<li><p>Quorum</p>
</li>
<li><p>Checksum</p>
</li>
<li><p>Merkle trees</p>
</li>
<li><p>Leader election</p>
</li>
<li><p>Databases</p>
</li>
<li><p>Relational databases</p>
</li>
<li><p>MySQL</p>
</li>
<li><p>PostgreSQL</p>
</li>
<li><p>SQL joins</p>
</li>
<li><p>Non-relational databases</p>
</li>
<li><p>MongoDB</p>
</li>
<li><p>How to choose a database</p>
</li>
<li><p>Database schemas</p>
</li>
<li><p>Database queries</p>
</li>
<li><p>ACID properties</p>
</li>
<li><p>Database sharding and partitioning</p>
</li>
<li><p>Database indexing</p>
</li>
<li><p>What are distributed systems?</p>
</li>
<li><p>Distributed system failures</p>
</li>
<li><p>Distributed system fundamentals</p>
</li>
<li><p>MapReduce</p>
</li>
<li><p>Stateless and stateful systems</p>
</li>
<li><p>Raft</p>
</li>
<li><p>Distibuted system design patterns</p>
</li>
<li><p>Scalable web applications</p>
</li>
<li><p>DNS and load balancing</p>
</li>
<li><p>N-tier applications</p>
</li>
<li><p>HTTP and REST</p>
</li>
<li><p>Stream processing</p>
</li>
<li><p>Caching</p>
</li>
<li><p>Cache invalidation</p>
</li>
<li><p>Cache eviction</p>
</li>
<li><p>Machine learning and System Design</p>
</li>
<li><p>Containerization and System Design</p>
</li>
<li><p>The cloud and System Design</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.educative.io/blog/complete-guide-to-system-design?eid=5082902844932096">The complete guide to System Design in 2022</a></li>
<li><a href="https://www.educative.io/blog/complete-guide-system-design-interview">The complete guide to the System Design Interview in 2022</a></li>
<li><a href="https://www.codinginterview.com/?eid=5082902844932096">Company-specific interview guides</a></li>
<li><a href="https://www.geeksforgeeks.org/system-design-tutorial/?ref=ghm">System Design Tutorial</a></li>
<li><a href="https://www.geeksforgeeks.org/software-design-patterns/?ref=ghm">Software Design Patterns</a></li>
<li><a href="https://www.educative.io/blog/uber-backend-system-design?eid=5082902844932096">Design the Uber backend: System design walkthrough</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>System Design</tag>
      </tags>
  </entry>
  <entry>
    <title>The Dark Ages</title>
    <url>/blog/the-dark-ages/</url>
    <content><![CDATA[<p>After the Dark Ages ended, Europe began to get prosperous. It was time for the rebirth of greek ideas. Many people wanted to improve their lives and create inventions. Many famous artists like Leonardo da Vinci and Michelangelo were there at the time of the Renaissance. They created sculptures and art and other things to help Europe become prosperous. However, citizens thought about being free from kings the first time in many years. They started to get the idea of rebelling and ear their freedom. They thought it was unjust to obey the king and listen to him. As for the kings of Europe, they were having trouble keeping the people under their control. To prevent losing their power, they built walls around their land so the people could not leave to some other kindoms.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>The Detective</title>
    <url>/blog/the-detective/</url>
    <content><![CDATA[<p>Young Author Gold Award, 4th grade, 2021</p>
<p>It was a fine summer morning and Zachory Taylor was enjoying the fresh air and sunshine. He was on summer vacation to Azogia, one of the richest countries which controlled most of the gold mines in the world. Azogia was located next to Zemonia, which controlled most of the silver mines. Azog, who was the ruler of Azogia, was a cousin of Zemo, the ruler of Zemonia. The two cousins had been competing against each other for a few decades on whose country was the most powerful. Azog was building golden walls to protect Azogopolis, the capital of Azogia, in case Zemo tried to launch a surprise attack on Azogia.<span id="more"></span></p>
<p>Suddenly, the door burst open and Sally, Zachory’s boastful and greedy mom, rushed in. She was holding a newspaper and shoved it in Zachory’s face. Zachory took one look and knew what she was up to. Someone had stolen Azog’s crown the night before, and the old king was demanding that someone must find his crown or else he would unleash his full fury on Zemonia.</p>
<p>“Well?” Sally demanded, “What are you waiting for? Let’s go! Hurry!”</p>
<p>Zachory knew this was coming and could not reject it.Sally wanted him to volunteer to solve an impossible case. He also couldn’t blame her for thinking a little boy of eleven years old could solve a real case like this. Sally Taylor named him Zachory after the president of the U.S.A and the hero of the Mexican American War. This made people think he could accomplish heroic deeds like solving mysteries. So far, the only things Zachory succeeded in doing was acing his classes and finding kids’ stolen toys. He had considered himself intelligent and rational, but never did he consider this case solving thing serious.</p>
<p>Sally impatiently pushed Zachory out of the way and grabbed her phone. She searched up the location of Azog’s palace and stated that it was only four blocks from their apartment. Zachory silently went downstairs and whistled for his loyal dogs Spot and Sniff. He never went anywhere without them. At school, they would sit at the corner of the classroom quietly. Once the two dogs were at his side, Zachory, Sally, Spot, and Sniff headed for the king’s palace. The building was pretty easy for Zachory to identify because it was made of solid gold and stood in the middle of Azogopolis. Zachory could have swore that anyone who looked at the palace for more than five minutes would be blinded. Once he got to the gate, he was stopped by two muscular guards wearing golden suits.</p>
<p>“What do you think you are doing here, young man? If you don’t have any business here, we insist that you leave immediately or we may have to force you out,” recited the first guard, as if he was practicing this line for a few years.</p>
<p>Zachory tried to walk away with Sniff and Spot but Sally blocked the way and replied that Zachory had volunteered to solve the case of the missing crown. The two guards searched Sally and Zachory in case they were going to attempt to assassinate Azog and were spies from Zemonia. When they finally confirmed that the Taylors didn’t have any deadly weapons besides a magnifying glass, which Zachory assured them that it didn’t shoot laser beams or could explode like a bomb, they allowed them to enter the palace. The palace was made up of five major rooms. There were three bathrooms on the bottom floor. Each one had a golden bathtub and toilet. On the top floor, there were two bedrooms for the king and queen. But Zachory was mainly focused on the throne room. There was a golden throne decorated with emeralds and rubies in the center of the room. And on the throne sat an elegant person in full battle armor.</p>
<p>“Are you a warlord or general? I need to see your king and tell him Zachory has eagerly volunteered to find the golden crown of yours,” Sally said, pushing Zachory forward, who smiled weakly.</p>
<p>The person laughed and said ,“Why, I am the king! Since you look like a runt to me, I guess you require all the help you can get. My royal detective is one of the greatest detectives in the world. I’m sure he will be of great survive to you. First, tell me your plan.”</p>
<p>Zachory saw the royal detective, who was looking at him with curiosity. He gathered up his courage to say that he was going to search every room in the palace in case the thief hid the crown inside. Then, he muttered that he didn’t need any help besides his dogs and Sally. Without saying another word, he quickly walked away with his team. He told Sally to check the first bathroom, Spot the second, and Sniff the third. He was hoping his plan would work since the crown had a pleasantly sweet aroma, which was because Azog’s father commanded his blacksmiths to dip the crown in the Sweet River. He thought this was an easy task for Sniff who was an expert at sniffing out things. Zachory himself entered the bedroom and found a large wardrobe. It was old fashioned, with images of the victories of Azog the Great. When he opened it, he was dazzled by the shiny jewels in it. A sudden idea came to his mind. Maybe the thief was a greedy person and would attempt to steal more riches from Azog. He grabbed some rubies and set them on the couch. He made the mistake of saying his plan out loud, not noticing that a spy was staring at him that very moment. The royal detective was looking at him with a smirk on face. He laughed at how easily Zachory thought the thief was to catch. Hearing this,  Zachory narrowed his eyes.</p>
<p>Meanwhile, Sally and the dogs were having no luck, so they went back to Zachory. Zachory himself was sitting on the couch, thinking about his little chat with the royal detective. The royal detective had asked him what was the point of finding the crown if Zemo could declare war anyway and Azogia would suffer heavy losses. At first, Zachory did not understand what he was saying. Now, he realized  that while Azog was spending his time building defenses around the city, Zemo was stocking up his weapons. No matter how sturdy the walls around Azogopolis seemed, it was still no match against Zemonia.  This was like the Civil War, and Azogia was the Confederacy, who played on the defensive and lost against the invading Union.</p>
<p>While thinking about this, he did not notice the constant knocking on the door. When he opened it, Sally rushed in and started scolding him for keeping them waiting for ten long minutes. Zachory hushed her up and grimly told her about his conversation with the sneaky detective. When he finished, Sally’s scowl changed to a grin. She triumphantly exclaimed that the royal detective had to be the thief. Zachory agreed, but he was thinking about a bigger problem. What if Zemonia would launch a attack so great on Azogopolis that the city would collapse? He knew that should not happen and he needed to put a to it. But first, he needed to find the royal detective. Zachory and Sally ran to the throne room and told Azog that they wanted to know where the sneaky detective was.</p>
<p>Azog, upon hearing this, was outraged and growled ,“We have little time. My royal detective went down to the stables and is probably going to leave with my precious crown!”</p>
<p>Zachory whistled and motioned for Sniff and Spot to head to the stables so they might corner the detective there. Then, Azog got his golden chariot ready and headed to the stables with Zachory. It appeared that the detective was having a fierce argument with the owner of the horses, who said that the horses weren’t for sale that day. Suddenly, Spot and Sniff leaped on the thief and temporarily knocked him out. Luckily, he had the crown with him. It was in a suitcase with a lock on it. Azog sliced through the lock as if it was made of butter and took out his crown. But something wasn’t right. The crown was not shiny, and was bendy. Azog threw it on the ground  and gently touched it with his golden sword. On contact, the crown shattered.</p>
<p>“My real crown was made in the forges of my ancestors. This is supposed to be unbreakable ,”the king snapped ,“You tricked me!”</p>
<p>Azog ordered his two guards to lock the spy in the dungeons and looked at Zachory, who had one last plan down his sleeve. Zachory rushed to the palace and up the staircase to Azog’s bedroom. He found himself face to face with a person dressed in a baker’s uniform, who was busily shoving jewels into his black bag. Zachory positioned his dogs to guard the door and entered the room. He pleasantly questioned the baker if he was done stealing. The baker, who didn’t notice him until now, made a desperate attempt to flee and ran at Zachory with his dagger, but was charged by Spot and Sniff. With the thief pinned to the floor, Azog, who saw Zachory from the first floor, came in and poured out the precious contents of the bag. There was rubies and emeralds of many sizes, and most importantly, the real crown. Azog placed it on Zachory’s head with huge satisfaction. Zachory had not suspected him to do this, but gladly accepted this offer as heir to the Azogian throne.</p>
<p>“I was looking for a heir for a long time since I am getting old,”declared the king ,“You have proven yourself worthy and from this day onwards, you shall be known as the king of Azogia!”</p>
<p>The next day, parades were held and Zachory formally signed a treaty with Zemo, who also did not support the idea of going to war against Azog. Zachory sighed and finally accepted the fact that he was really more than just an eleven years kid.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>The Highway to Fantasy Land</title>
    <url>/blog/the-highway-to-fantasy-land/</url>
    <content><![CDATA[<p>It was ten o’clock at night when James heard a violent knock on his glass door. He was currently resting on his couch watching my TV when this happened. His first thought was that it was the mailman. But then, the mailman only came in the morning and had already delivered mail that very same day. James looked through his window and saw a man wearing a black hoodie. He hesitated and then opened the door. At first, the man did not say anything. He just stared at him. He creeped James out without even talking. Then, he held up a red envelope.<span id="more"></span></p>
<p>James took it very slowly because he was expecting it to explode. The stranger said, “The king of Fantasy awaits your reply.” Then, with one last glare, he was gone in a puff of black smoke.</p>
<p>That night, James thought about what happened. The envelope stated that the king of Fantasy would like to meet him. That was weird. “How did this king from a land he had not even heard of before know him?” James thought. Then, when he put the letter back in the envelope, he found a key at the bottom. He took it out, bewildered. On it, there were engraved words: “Welcome to the Kingdom of Fantasy!” James at first thought this was some kind of joke. Why would someone give him a key, if they did not tell him which door it would fit on? With that thought, he fell to sleep.</p>
<p>That night, James dreamed that he was in the kingdom of Fantasy. There were farms, fields of candy corn and sugar canes, flying cars, and even buildings that were taller than the clouds. The building that James noticed first was made out of solid gold. There was a neon light on the door flashing shiny red, white, and blue. James walked in and found himself in what seemed like the biggest lobby he had ever been in before. He saw people rushing in and out.</p>
<p>Then, a man came towards him. He looked oddly familiar. “ I suppose you are here to meet the king, a newcomer. I am Luke, the messenger of the king. Follow me,” said Luke. And then what happened was perhaps the most embarrassing moment of James’s life. He followed Luke and that proved to be a fatal mistake. Luke led him up to the three hundred thirty-ninth floor. James, a person who was afraid of heights, saw himself in a glass room. He looked under the window and could see the whole of the kingdom of Fantasy. Suddenly, Luke pushed him and James toppled off the building. The window seemed to have suddenly disappeared when Luke pushed him. “AHHHHHHHHH!”, James screamed. He heard the sound of roaring laughter and then all was pitch black.</p>
<p> James woke up with a start. He was sweating badly. But that was not all. He saw three people wearing bronze armor. They were muttering among themselves. Then, James whispered, “What are you people doing in my house? First, a black hooded stranger sends me a weird letter, then three people in full battle armor look at me asleep. This is breaking and entering you know!” Then the anger in him exploded. “ GET OUT OF MY HOUSE OR I WILL FORCE YOU OUT!”  Then, he got out a remote control from under his pillow. He pressed the red button and an invisible door appeared in front of him. He got out a key and opened the door. Before the stunned warriors could stop him, James dived into the door.</p>
<p>When James was conscious again, he found himself in the same place he dreamed about: The Land of Fantasy. He suddenly remembered what had happened. This door, an invention of his, teleported him anywhere the key that unlocked it wanted it to go. So the strange key from the Land of Fantasy must have teleported him here.</p>
<p>James looked around. It was exactly like the city he dreamed about. He saw the gold building which had neon lights flashing on the door. He hesitated and then ran to the door.  He spotted the staircase leading to the three hundred thirty-ninth floor. He dashed up the stairs and after an hour, he arrived at the glass building. But there was no one there.</p>
<p>James decided to explore a bit. Finally, he got to a room with paintings hanging on the walls. The pictures were moving. Then he heard footsteps. He quickly hid behind the curtains. It was the three warriors he saw in his bedroom. They said an incantation and the painting of a person wearing full battle armor flung open. James decided to follow them. He jumped in a moment before the painting closed. He tumbled into a room. But when he looked around, he figured out it wasn’t a room at all. He was sitting on a statue. “Where to?” said a deep voice.</p>
<p>“Umm, the king?” James replied. Then he found himself dropping down. BOOM! He landed in a crash. When he opened his eyes, he saw a thousand eyes staring at him.</p>
<p>“Why hello, it’s a pleasant surprise to meet you, James,” said the king.</p>
<p>“How do you know my name?” James demanded.</p>
<p>The king smiled. “I was an old family friend. The reason I summoned you here on this fine spring evening was to let you know that you will command my army from now on.”</p>
<p>“Army? You want me to command an army. I mean, I know how to fight, but why me?” James asked. He was suffering from his lack of confidence.</p>
<p>The king said, “ It’s in your blood. You are a descendant of Merlin.”</p>
<p>James gulped and said in a tiny voice, “ Ok, I start training tomorrow.”</p>
<p>The next day, at dawn, he was greeted by a man dressed in golden robes and a sword in his hand. “Hey, you are Luke!” James said.</p>
<p>Luke grinned. “How did you like my little dream I sent you? That is my specialty.” Then he straightened up and looked more businesslike. “Luke, lord of dreams, magic, and mischief, at your service. I used to be a general in the army, but now that you are in charge, I became your personal guard and mentor. Now, let’s start with something that is crucial for your training curriculum: sword Fighting. Here is your sword.” He pulled out a silver blade. “Here, try this one out.” James swung it and found that it was light and fits perfectly well.</p>
<p>That day, James learned to fight with a sword. Luke said that he was a natural and that he should participate in the annual Sword Festival. James turned it down, however, and made an excuse that he needed more practice. But he could not fool Luke. Luke knew he was lacking self-confidence. The next day, James had to admit the Sword Festival was sort of fun. The winner was a warrior called Alex. Alex glanced at the crowd and said, “I would like to face you, James.” James faltered. He wanted to fight, but he was afraid he would make a fool out of himself. The crowd, on the other hand, cheered him on. James could not control himself. His legs drove him up the stadium. Moments later, Alex made his first strike. Maybe some normal warrior would be wounded in seconds if they were fighting against Alex. But James was not a normal warrior. He could see each of Alex’s strikes clearly. He was the first one to actually beat Alex. The crowd was shocked that the best warrior they had lost against a newcomer like James.</p>
<p>Alex smiled, however. “Nice skill.” James grinned at this compliment. Luke decided to let James partner up with Alex for his training. Each time, they would fight to a stalemate. At the end of the year, James excelled in every kind of training, sword fighting, archery, hand-to-hand combat, and many other techniques. But no one knew about his weakness: Real life experience in a war.</p>
<p>But he was about to get more experience than James ever wanted. Every warrior knew that they were about to start a war very soon against Chen, king of the nearby city, Aegis. During one of James and Alex’s training sessions, they were summoned by their king. He said that the war had started and they needed to gather the entire army quickly. James pulled out his horn and blew it. Soon, the whole of the Kingdom of Fantasy was at his service. James told Alex to lead part of the army around the enemies and attack from the back. He told Luke to lead the other part of the army up the hill. Then James himself would guard the gate to the Kingdom of Fantasy with five soldiers: Jack, Chase, Nathan, Percy, and Fred.</p>
<p>The plan worked out perfectly. Chen’s army was trapped. But James found himself face to face with Lord Chen. James charged Chen and motioned the others to stay where they were. He followed Chen into the mountains. James slashed at him but Lord Chen dodged. He unsheathed his sword and struck James. He ducked but then he heard a tiny groan behind him.</p>
<p>It was Alex. He used his body to intercept an arrow aimed at James. The arrow wounded him in the arm. Outraged, James slashed Chen’s armor with all the might he could muster. The sword made a lash in his armor. Chen’s shock gave James the perfect chance to strike him. James slammed Chen with his shield and shot an arrow directly at Chen’s heart. Chen, with horror on his face, fell down and gave a last twitch. He was defeated. James blew his horn one last time and fainted.</p>
<p>When he woke up, he was in the hospital wing. He tried to sit up but quickly sat back down because his arm was still bandaged. He called for the nurse and asked her if Alex was alright. The nurse nodded. The next Monday, James was able to get out of bed. The moment his fellow soldiers saw him in the hallway, they all cheered for him. James then knew that he was awarded for his act of bravery and finally gained self-confidence.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Top 50 Linux Commands You should Know</title>
    <url>/blog/top-50-linux-commands-you-should-know/</url>
    <content><![CDATA[<h2 id="aa"><a href="#aa" class="headerlink" title="aa"></a>aa</h2><ol>
<li>ls - The most frequently used command in Linux to list directories</li>
<li>pwd - Print working directory command in Linux</li>
<li>cd - Linux command to navigate through directories</li>
</ol>
<p>mkdir - Command used to create directories in Linux</p>
<p>mv - Move or rename files in Linux</p>
<p>cp - Similar usage as mv but for copying files in Linux</p>
<p>rm - Delete files or directories</p>
<p>touch - Create blank&#x2F;empty files</p>
<p>ln - Create symbolic links (shortcuts) to other files</p>
<p>cat - Display file contents on the terminal</p>
<p>clear - Clear the terminal display</p>
<p>echo - Print any text that follows the command</p>
<p>less - Linux command to display paged outputs in the terminal</p>
<p>man - Access manual pages for all Linux commands</p>
<p>uname - Linux command to get basic information about the OS</p>
<p>whoami - Get the active username</p>
<p>tar - Command to extract and compress files in Linux</p>
<p>grep - Search for a string within an output</p>
<p>head - Return the specified number of lines from the top</p>
<p>tail - Return the specified number of lines from the bottom</p>
<p>diff - Find the difference between two files</p>
<p>cmp - Allows you to check if two files are identical</p>
<p>comm - Combines the functionality of diff and cmp</p>
<p>sort - Linux command to sort the content of a file while outputting</p>
<p>export - Export environment variables in Linux</p>
<p>zip - Zip files in Linux</p>
<p>unzip - Unzip files in Linux</p>
<p>ssh - Secure Shell command in Linux</p>
<p>service - Linux command to start and stop services</p>
<p>ps - Display active processes</p>
<p>kill and killall - Kill active processes by process ID or name</p>
<p>df - Display disk filesystem information</p>
<p>mount - Mount file systems in Linux</p>
<p>chmod - Command to change file permissions</p>
<p>chown - Command for granting ownership of files or folders</p>
<p>ifconfig - Display network interfaces and IP addresses</p>
<p>traceroute - Trace all the network hops to reach the destination</p>
<p>wget - Direct download files from the internet</p>
<p>ufw - Firewall command</p>
<p>iptables - Base firewall for all other firewall utilities to interface with</p>
<p>apt, pacman, yum, rpm - Package managers depending on the distro</p>
<p>sudo - Command to escalate privileges in Linux</p>
<p>cal - View a command-line calendar</p>
<p>alias - Create custom shortcuts for your regularly used commands</p>
<p>dd - Majorly used for creating bootable USB sticks</p>
<p>whereis - Locate the binary, source, and manual pages for a command</p>
<p>whatis - Find what a command is used for</p>
<p>top - View active processes live with their system usage</p>
<p>useradd and usermod - Add new user or change existing users data</p>
<p>passwd - Create or update passwords for existing users</p>
<p><a href="https://www.digitalocean.com/community/tutorials/linux-commands">https://www.digitalocean.com/community/tutorials/linux-commands</a></p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Administration</tag>
      </tags>
  </entry>
  <entry>
    <title>Tracking stock financial metrics in tradingview</title>
    <url>/blog/tracking-stock-financial-metrics-in-tradingview/</url>
    <content><![CDATA[<p>In this post, we learn how to use tradingview.com to track financial metrics of the interested stock. It helps understand how the financial metrics support the stock price under the hood.</p>
<h2 id="Display-the-stock-price-in-year-view"><a href="#Display-the-stock-price-in-year-view" class="headerlink" title="Display the stock price in year view"></a>Display the stock price in year view</h2><p><img src="/images/12m_price.png"></p>
<h2 id="Add-the-revenue-metric"><a href="#Add-the-revenue-metric" class="headerlink" title="Add the revenue metric"></a>Add the revenue metric</h2><p><img src="/images/revenue.png"></p>
<h2 id="Add-the-operating-income-metric"><a href="#Add-the-operating-income-metric" class="headerlink" title="Add the operating income metric"></a>Add the operating income metric</h2><p><img src="/images/operating_income.png"></p>
<h2 id="Add-the-current-ratio-metric"><a href="#Add-the-current-ratio-metric" class="headerlink" title="Add the current ratio metric"></a>Add the current ratio metric</h2><p><img src="/images/quick_ratio.png"></p>
<h2 id="Add-the-P-E-ratio"><a href="#Add-the-P-E-ratio" class="headerlink" title="Add the P&#x2F;E ratio"></a>Add the P&#x2F;E ratio</h2><p><img src="/images/pe.png"></p>
<h2 id="Check-the-trend-for-the-added-metrics"><a href="#Check-the-trend-for-the-added-metrics" class="headerlink" title="Check the trend for the added metrics"></a>Check the trend for the added metrics</h2><p>In the case of Tesla stock, the prive surged in 2020 and 2021. Notice that the revenue and operating income increased tremendously at the same time.<br><img src="/images/indicators.png"></p>
]]></content>
      <categories>
        <category>Investment</category>
      </categories>
  </entry>
  <entry>
    <title>Transform Plain Text Into Static Websites And Blogs</title>
    <url>/blog/transform-plain-text-into-static-websites-and-blogs/</url>
    <content><![CDATA[<h2 id="Quick-start-instructions"><a href="#Quick-start-instructions" class="headerlink" title="Quick start instructions"></a>Quick start instructions</h2><pre><code>$ gem install bundler jekyll
$ jekyll new my-awesome-site
$ cd my-awesome-site
$ bundle exec jekyll serve
# =&gt; Now browse to http://localhost:4000
</code></pre>
<p>Check out the [Jekyll][jekyll] for more info on how to translate your plain text into static websites and blogs.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://jekyllrb.com/">https://jekyllrb.com/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Blog</category>
      </categories>
  </entry>
  <entry>
    <title>Tree traversal algorithm in Python</title>
    <url>/blog/tree-traversal-algorithm-in-python/</url>
    <content><![CDATA[<p>Tree traversal (also known as tree search and walking the tree) is a form of graph traversal and refers to the process of visiting (e.g. retrieving, updating, or deleting) each node in a tree data structure, exactly once. Such traversals are classified by the order in which the nodes are visited. - <a href="https://en.wikipedia.org/wiki/Tree_traversal">Wikipedia</a></p>
<p>Tree traversal algorithms can be classified in the following two categories:</p>
<ul>
<li>Depth-First Search (DFS): It starts at the root node (selecting some arbitrary node as the root node in the case of a graph) and explores as far as possible along each branch before backtracking.</li>
<li>Breadth-First Search (BFS): It starts at the tree’s root (selecting some arbitrary node as the root node in the case of a graph) and searches all nodes at the current depth level before moving on to the nodes at the next depth level.</li>
</ul>
<p>Depth-First Search (DFS) algorithms have three variants:</p>
<ul>
<li>Preorder Traversal (current-left-right): Visit the current node before visiting any nodes inside left or right subtrees.</li>
<li>Inorder Traversal (left-current-right): Visit the current node after visiting all nodes inside left subtree but before visiting any node within the right subtree.</li>
<li>Postorder Traversal (left-right-current): Visit the current node after visiting all the nodes of left and right subtrees.</li>
</ul>
<p>Breadth-First Search (BFS) Algorithm has one variant:</p>
<ul>
<li>Level Order Traversal: Visit nodes level-by-level and left-to-right fashion at the same level.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Node</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val</span>):</span><br><span class="line">        self.val = val</span><br><span class="line">        self.left = <span class="literal">None</span></span><br><span class="line">        self.right = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pre_order_traverse</span>(<span class="params">node</span>):</span><br><span class="line">    <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;node.val&#125;</span> &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    pre_order_traverse(node.left)</span><br><span class="line">    pre_order_traverse(node.right)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">in_order_traverse</span>(<span class="params">node</span>):</span><br><span class="line">    <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    in_order_traverse(node.left)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;node.val&#125;</span> &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    in_order_traverse(node.right)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">post_order_traverse</span>(<span class="params">node</span>):</span><br><span class="line">    <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    post_order_traverse(node.left)</span><br><span class="line">    post_order_traverse(node.right)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;node.val&#125;</span> &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">level_order_traverse</span>(<span class="params">node</span>):</span><br><span class="line">    <span class="keyword">if</span> node == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    queue = []</span><br><span class="line">    queue.append(node)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(queue) &gt; <span class="number">0</span>:</span><br><span class="line">        first = queue.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;first.val&#125;</span> &quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> first.left != <span class="literal">None</span>:</span><br><span class="line">            queue.append(first.left)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> first.right != <span class="literal">None</span>:</span><br><span class="line">            queue.append(first.right)</span><br><span class="line"></span><br><span class="line">root = Node(<span class="number">0</span>)</span><br><span class="line">n1 = Node(<span class="number">1</span>)</span><br><span class="line">n2 = Node(<span class="number">2</span>)</span><br><span class="line">root.left = n1</span><br><span class="line">root.right = n2</span><br><span class="line">n3 = Node(<span class="number">3</span>)</span><br><span class="line">n4 = Node(<span class="number">4</span>)</span><br><span class="line">n1.left = n3</span><br><span class="line">n1.right = n4</span><br><span class="line">n5 = Node(<span class="number">5</span>)</span><br><span class="line">n6 = Node(<span class="number">6</span>)</span><br><span class="line">n2.left = n5</span><br><span class="line">n2.right = n6</span><br><span class="line">n7 = Node(<span class="number">7</span>)</span><br><span class="line">n8 = Node(<span class="number">8</span>)</span><br><span class="line">n3.left = n7</span><br><span class="line">n3.right = n8</span><br><span class="line"></span><br><span class="line">pre_order_traverse(root)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line">in_order_traverse(root)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line">post_order_traverse(root)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line">level_order_traverse(root)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># 0 1 3 7 8 4 2 5 6</span></span><br><span class="line"><span class="comment"># 7 3 8 1 4 0 5 2 6</span></span><br><span class="line"><span class="comment"># 7 8 3 4 1 5 6 2 0</span></span><br><span class="line"><span class="comment"># 0 1 2 3 4 5 6 7 8</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Trie (Prefix Tree)</title>
    <url>/blog/trie/</url>
    <content><![CDATA[<h2 id="What-is-Trie"><a href="#What-is-Trie" class="headerlink" title="What is Trie"></a>What is Trie</h2><p>A trie (pronounced as “try”) or prefix tree is a tree data structure used to efficiently store and retrieve keys in a dataset of strings. There are various applications of this data structure, such as autocomplete and spellchecker.</p>
<span id="more"></span>

<h2 id="Leetcode-208-Implement-Trie-Prefix-Tree"><a href="#Leetcode-208-Implement-Trie-Prefix-Tree" class="headerlink" title="[Leetcode 208] Implement Trie (Prefix Tree)"></a>[Leetcode 208] Implement Trie (Prefix Tree)</h2><p>Implement the Trie class:</p>
<ul>
<li>Trie() Initializes the trie object.</li>
<li>void insert(String word) Inserts the string word into the trie.</li>
<li>boolean search(String word) Returns true if the string word is in the trie (i.e., was inserted before), and false otherwise.</li>
<li>boolean startsWith(String prefix) Returns true if there is a previously inserted string word that has the prefix prefix, and false otherwise.</li>
</ul>
<p>Example 1:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Input</span><br><span class="line">[<span class="string">&quot;Trie&quot;</span>, <span class="string">&quot;insert&quot;</span>, <span class="string">&quot;search&quot;</span>, <span class="string">&quot;search&quot;</span>, <span class="string">&quot;startsWith&quot;</span>, <span class="string">&quot;insert&quot;</span>, <span class="string">&quot;search&quot;</span>]</span><br><span class="line">[[], [<span class="string">&quot;apple&quot;</span>], [<span class="string">&quot;apple&quot;</span>], [<span class="string">&quot;app&quot;</span>], [<span class="string">&quot;app&quot;</span>], [<span class="string">&quot;app&quot;</span>], [<span class="string">&quot;app&quot;</span>]]</span><br><span class="line">Output</span><br><span class="line">[null, null, true, false, true, null, true]</span><br><span class="line"></span><br><span class="line">Explanation</span><br><span class="line">Trie trie = new Trie();</span><br><span class="line">trie.insert(<span class="string">&quot;apple&quot;</span>);</span><br><span class="line">trie.search(<span class="string">&quot;apple&quot;</span>);   // <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">trie.search(<span class="string">&quot;app&quot;</span>);     // <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">trie.startsWith(<span class="string">&quot;app&quot;</span>); // <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">trie.insert(<span class="string">&quot;app&quot;</span>);</span><br><span class="line">trie.search(<span class="string">&quot;app&quot;</span>);     // <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; word.length, prefix.length &lt;&#x3D; 2000</li>
<li>word and prefix consist only of lowercase English letters.</li>
<li>At most 3 * 104 calls in total will be made to insert, search, and startsWith.</li>
</ul>
<p><strong>Solution:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trie</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.root = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># e.g. &#123;&#x27;a&#x27;: &#123;&#x27;p&#x27;: &#123;&#x27;p&#x27;: &#123;&#x27;l&#x27;: &#123;&#x27;e&#x27;: &#123;&#x27;*&#x27;: &#x27;&#x27;&#125;&#125;&#125;&#125;&#125;&#125;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">insert</span>(<span class="params">self, word: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        curr = self.root</span><br><span class="line">        <span class="keyword">for</span> letter <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> letter <span class="keyword">not</span> <span class="keyword">in</span> curr:</span><br><span class="line">                curr[letter] = &#123;&#125;</span><br><span class="line">            curr = curr[letter]</span><br><span class="line">        curr[<span class="string">&#x27;*&#x27;</span>] = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, word: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        curr = self.root</span><br><span class="line">        <span class="keyword">for</span> letter <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> letter <span class="keyword">not</span> <span class="keyword">in</span> curr:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            curr = curr[letter]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;*&#x27;</span> <span class="keyword">in</span> curr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">startsWith</span>(<span class="params">self, prefix: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        curr = self.root</span><br><span class="line">        <span class="keyword">for</span> letter <span class="keyword">in</span> prefix:</span><br><span class="line">            <span class="keyword">if</span> letter <span class="keyword">not</span> <span class="keyword">in</span> curr:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            curr = curr[letter]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-421-Maximum-XOR-of-Two-Numbers-in-an-Array"><a href="#Leetcode-421-Maximum-XOR-of-Two-Numbers-in-an-Array" class="headerlink" title="[Leetcode 421] Maximum XOR of Two Numbers in an Array"></a>[Leetcode 421] Maximum XOR of Two Numbers in an Array</h2><p>Given an integer array nums, return the maximum result of nums[i] XOR nums[j], where 0 &lt;&#x3D; i &lt;&#x3D; j &lt; n.</p>
<p>Example 1:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Input: nums = [<span class="number">3</span>,<span class="number">10</span>,<span class="number">5</span>,<span class="number">25</span>,<span class="number">2</span>,<span class="number">8</span>]</span><br><span class="line">Output: <span class="number">28</span></span><br><span class="line">Explanation: The maximum result <span class="keyword">is</span> <span class="number">5</span> XOR <span class="number">25</span> = <span class="number">28.</span></span><br><span class="line">Example <span class="number">2</span>:</span><br><span class="line"></span><br><span class="line">Input: nums = [<span class="number">14</span>,<span class="number">70</span>,<span class="number">53</span>,<span class="number">83</span>,<span class="number">49</span>,<span class="number">91</span>,<span class="number">36</span>,<span class="number">80</span>,<span class="number">92</span>,<span class="number">51</span>,<span class="number">66</span>,<span class="number">70</span>]</span><br><span class="line">Output: <span class="number">127</span></span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; nums.length &lt;&#x3D; 2 * 105</li>
<li>0 &lt;&#x3D; nums[i] &lt;&#x3D; 231 - 1</li>
</ul>
<p><strong>Solution:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trie</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.root = &#123;&#125;</span><br><span class="line">        self.maxLen = n  <span class="comment"># max length of all numbers                             </span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_num</span>(<span class="params">self, num</span>):</span><br><span class="line">        node = self.root </span><br><span class="line">        <span class="comment"># shift self.maxLen bits </span></span><br><span class="line">        <span class="keyword">for</span> shift <span class="keyword">in</span> <span class="built_in">range</span>(self.maxLen, -<span class="number">1</span>, -<span class="number">1</span>):      </span><br><span class="line">            <span class="comment"># get a bit from left to right</span></span><br><span class="line">            val = <span class="number">1</span> <span class="keyword">if</span> num &amp; (<span class="number">1</span> &lt;&lt; shift) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> val <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">                node[val] = &#123;&#125;</span><br><span class="line"></span><br><span class="line">            node = node[val]</span><br><span class="line">        <span class="comment"># mark the number</span></span><br><span class="line">        node[<span class="string">&#x27;*&#x27;</span>] = num</span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMaximumXOR</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># get max length of all numbers&#x27; binary. e.g. bin(6)=0b110</span></span><br><span class="line">        max_len = <span class="built_in">len</span>(<span class="built_in">bin</span>(<span class="built_in">max</span>(nums))) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># build trie with number&#x27;s binary</span></span><br><span class="line">        trie = Trie(max_len)</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums: trie.add_num(num)      </span><br><span class="line">        <span class="comment">#print(trie.root)</span></span><br><span class="line"></span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># for each num, find the number which can create max value with num using XOR</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:                              </span><br><span class="line">            node = trie.root </span><br><span class="line">            <span class="keyword">for</span> shift <span class="keyword">in</span> <span class="built_in">range</span>(max_len, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">                <span class="comment"># get a bit from left to right</span></span><br><span class="line">                val = <span class="number">1</span> <span class="keyword">if</span> num &amp; (<span class="number">1</span> &lt;&lt; shift) <span class="keyword">else</span> <span class="number">0</span>  </span><br><span class="line"></span><br><span class="line">                <span class="comment"># try opposite bit if it&#x27;s in trie to make it larger, otherwise use itself</span></span><br><span class="line">                node = node[<span class="number">1</span>-val] <span class="keyword">if</span> <span class="number">1</span>-val <span class="keyword">in</span> node <span class="keyword">else</span> node[val] </span><br><span class="line"></span><br><span class="line">            <span class="comment"># calculate xor and save to answer</span></span><br><span class="line">            ans = <span class="built_in">max</span>(ans, num ^ node[<span class="string">&#x27;*&#x27;</span>])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-1268-Search-Suggestions-System"><a href="#Leetcode-1268-Search-Suggestions-System" class="headerlink" title="[Leetcode 1268] Search Suggestions System"></a>[Leetcode 1268] Search Suggestions System</h2><p>You are given an array of strings products and a string searchWord.</p>
<p>Design a system that suggests at most three product names from products after each character of searchWord is typed. Suggested products should have common prefix with searchWord. If there are more than three products with a common prefix return the three lexicographically minimums products.</p>
<p>Return a list of lists of the suggested products after each character of searchWord is typed.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: products = [&quot;mobile&quot;,&quot;mouse&quot;,&quot;moneypot&quot;,&quot;monitor&quot;,&quot;mousepad&quot;], searchWord = &quot;mouse&quot;</span><br><span class="line">Output: [[&quot;mobile&quot;,&quot;moneypot&quot;,&quot;monitor&quot;],[&quot;mobile&quot;,&quot;moneypot&quot;,&quot;monitor&quot;],[&quot;mouse&quot;,&quot;mousepad&quot;],[&quot;mouse&quot;,&quot;mousepad&quot;],[&quot;mouse&quot;,&quot;mousepad&quot;]]</span><br><span class="line">Explanation: products sorted lexicographically = [&quot;mobile&quot;,&quot;moneypot&quot;,&quot;monitor&quot;,&quot;mouse&quot;,&quot;mousepad&quot;].</span><br><span class="line">After typing m and mo all products match and we show user [&quot;mobile&quot;,&quot;moneypot&quot;,&quot;monitor&quot;].</span><br><span class="line">After typing mou, mous and mouse the system suggests [&quot;mouse&quot;,&quot;mousepad&quot;].</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: products = [&quot;havana&quot;], searchWord = &quot;havana&quot;</span><br><span class="line">Output: [[&quot;havana&quot;],[&quot;havana&quot;],[&quot;havana&quot;],[&quot;havana&quot;],[&quot;havana&quot;],[&quot;havana&quot;]]</span><br><span class="line">Explanation: The only word &quot;havana&quot; will be always suggested while typing the search word.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; products.length &lt;&#x3D; 1000</li>
<li>1 &lt;&#x3D; products[i].length &lt;&#x3D; 3000</li>
<li>1 &lt;&#x3D; sum(products[i].length) &lt;&#x3D; 2 * 104</li>
<li>All the strings of products are unique.</li>
<li>products[i] consists of lowercase English letters.</li>
<li>1 &lt;&#x3D; searchWord.length &lt;&#x3D; 1000</li>
<li>searchWord consists of lowercase English letters.</li>
</ul>
<p>Solution:</p>
<p>Brute force:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">suggestedProducts</span>(<span class="params">self, products: <span class="type">List</span>[<span class="built_in">str</span>], searchWord: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]]:</span><br><span class="line">        res = []</span><br><span class="line">        products.sort() <span class="comment"># ensure the lexicographical order of products</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(searchWord):</span><br><span class="line">            curr = []</span><br><span class="line">            prefix = searchWord[:i+<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">for</span> prod <span class="keyword">in</span> products:</span><br><span class="line">                <span class="keyword">if</span> prefix == prod[:i+<span class="number">1</span>]:</span><br><span class="line">                    curr.append(prod)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(curr) == <span class="number">3</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            res.append(curr)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<p>Using Trie:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trie</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.root = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, word</span>):</span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">                node[c] = &#123;&#125; <span class="comment"># create a child node &#123;&#125;</span></span><br><span class="line">            </span><br><span class="line">            node = node[c] <span class="comment"># move down to the child node</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># add word list to current node</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;words&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">                node[<span class="string">&#x27;words&#x27;</span>] = []</span><br><span class="line"></span><br><span class="line">            <span class="comment"># add word to list since it goes through the current character</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(node[<span class="string">&#x27;words&#x27;</span>]) &lt; <span class="number">3</span>:</span><br><span class="line">                node[<span class="string">&#x27;words&#x27;</span>].append(word)          </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, prefix</span>):</span><br><span class="line">        node = self.root</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> prefix:</span><br><span class="line">            <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> node:</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">            node = node[c]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> node[<span class="string">&#x27;words&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">suggestedProducts</span>(<span class="params">self, products: <span class="type">List</span>[<span class="built_in">str</span>], searchWord: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">str</span>]]:</span><br><span class="line">        products.sort()</span><br><span class="line"></span><br><span class="line">        trie = Trie()</span><br><span class="line">        <span class="keyword">for</span> prod <span class="keyword">in</span> products:</span><br><span class="line">            trie.add(prod)</span><br><span class="line"></span><br><span class="line">        res = []</span><br><span class="line">        prefix = <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> searchWord:</span><br><span class="line">            prefix += c</span><br><span class="line">            res.append(trie.search(prefix))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">        </span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
        <tag>Data structure</tag>
        <tag>Trie</tag>
      </tags>
  </entry>
  <entry>
    <title>tuned - dynamic adaptive system tuning daemon</title>
    <url>/blog/tuned-dynamic-adaptive-system-tuning-daemon/</url>
    <content><![CDATA[<blockquote>
<p>Tuned</p>
<blockquote>
<p>Tuned is a daemon that uses udev to monitor connected devices and statically and dynamically tunes system settings according to a selected profile. Tuned is distributed with a number of predefined profiles for common use cases like high throughput, low latency, or powersave. It is possible to modify the rules defined for each profile and customize how to tune a particular device. To revert all changes made to the system settings by a certain profile, you can either switch to another profile or deactivate the tuned service.</p>
</blockquote>
</blockquote>
<h2 id="Check-the-tuned-service-and-active-profile"><a href="#Check-the-tuned-service-and-active-profile" class="headerlink" title="Check the tuned service and active profile"></a>Check the tuned service and active profile</h2><pre><code>[root@host1 ~]# systemctl status tuned
● tuned.service - Dynamic System Tuning Daemon
   Loaded: loaded (/usr/lib/systemd/system/tuned.service; enabled; vendor preset: enabled)
   Active: active (running) since Tue 2022-07-12 04:52:53 UTC; 13h ago
     Docs: man:tuned(8)
           man:tuned.conf(5)
           man:tuned-adm(8)
 Main PID: 2592 (tuned)
    Tasks: 5
   Memory: 33.9M
   CGroup: /system.slice/tuned.service
           └─2592 /usr/bin/python2 -Es /usr/sbin/tuned -l -P

Jul 12 04:52:52 host1 systemd[1]: Starting Dynamic System Tuning Daemon...
Jul 12 04:52:53 host1 systemd[1]: Started Dynamic System Tuning Daemon.

[root@host1 ~]# tuned-adm list
Available profiles:
- balanced                    - General non-specialized tuned profile
- desktop                     - Optimize for the desktop use-case
- hpc-compute                 - Optimize for HPC compute workloads
- latency-performance         - Optimize for deterministic performance at the cost of increased power consumption
- network-latency             - Optimize for deterministic performance at the cost of increased power consumption, focused on low latency network performance
- network-throughput          - Optimize for streaming network throughput, generally only necessary on older CPUs or 40G+ networks
- powersave                   - Optimize for low power consumption
- throughput-performance      - Broadly applicable tuning that provides excellent performance across a variety of common server workloads
- virtual-guest               - Optimize for running inside a virtual guest
- virtual-host                - Optimize for running KVM guests
Current active profile: balanced

[root@host1 ~]# tuned-adm active
Current active profile: balanced

[root@host1 ~]# cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor | head
powersave
powersave
&lt;omitted..&gt;
</code></pre>
<h2 id="Change-the-tuned-profile"><a href="#Change-the-tuned-profile" class="headerlink" title="Change the tuned profile"></a>Change the tuned profile</h2><pre><code>[root@host1 ~]# tuned-adm profile throughput-performance

[root@host1 ~]# tuned-adm active
Current active profile: throughput-performance

[root@host1 ~]# cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
performance
performance
&lt;omitted..&gt;
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://linux.die.net/man/8/tuned">https://linux.die.net/man/8/tuned</a></li>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/chap-red_hat_enterprise_linux-performance_tuning_guide-tuned">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux&#x2F;7&#x2F;html&#x2F;performance_tuning_guide&#x2F;chap-red_hat_enterprise_linux-performance_tuning_guide-tuned</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>Tuning kernel parameters in Docker</title>
    <url>/blog/tuning-kernel-parameters-in-docker/</url>
    <content><![CDATA[<p>Configure namespaced kernel parameters(sysctl) at runtime</p>
<p>The –sysctl sets namespaced kernel parameters (sysctls) in the container. For example, to turn on IP forwarding in the containers network namespace, run this command:</p>
<pre><code>$ docker run --sysctl net.ipv4.ip_forward=1 someimage
</code></pre>
<p><strong>Note</strong></p>
<p>Not all sysctls are namespaced. Docker does not support changing sysctls inside of a container that also modify the host system. As the kernel evolves we expect to see more sysctls become namespaced.</p>
<p><strong>CURRENTLY SUPPORTED SYSCTLS</strong></p>
<p>IPC Namespace:</p>
<ul>
<li>kernel.msgmax, kernel.msgmnb, kernel.msgmni, kernel.sem, kernel.shmall, kernel.shmmax, kernel.shmmni, kernel.shm_rmid_forced.</li>
<li>Sysctls beginning with fs.mqueue.*</li>
<li>If you use the –ipc&#x3D;host option these sysctls are not allowed.</li>
</ul>
<p>Network Namespace:</p>
<ul>
<li>Sysctls beginning with net.*</li>
<li>If you use the –network&#x3D;host option using these sysctls are not allowed.</li>
</ul>
<h1 id="Hard-and-soft-ulimit-settings"><a href="#Hard-and-soft-ulimit-settings" class="headerlink" title="Hard and soft ulimit settings"></a>Hard and soft ulimit settings</h1><p>There are two types of ulimit settings:</p>
<ul>
<li>The hard limit is the maximum value that is allowed for the soft limit. Any changes to the hard limit require root access.</li>
<li>The soft limit is the value that Linux uses to limit the system resources for running processes. The soft limit cannot be greater than the hard limit.</li>
</ul>
<h2 id="Updating-hard-and-soft-ulimit-settings-in-Linux"><a href="#Updating-hard-and-soft-ulimit-settings-in-Linux" class="headerlink" title="Updating hard and soft ulimit settings in Linux"></a>Updating hard and soft ulimit settings in Linux</h2><pre><code>To change the open files value on your operating system:
On RHEL and CentOS, edit the /etc/security/limits.d/91-nofile.conf file as shown in the following example:
@streamsadmin - nofile open-files-value
On SLES, edit the /etc/security/limits.conf file as shown in the following example:
@streamsadmin - nofile open-files-value

To change the max user processes value on your operating system:
On RHEL and CentOS, edit the /etc/security/limits.d/90-nproc.conf file as shown in the following example:
@streamsadmin hard nproc max-user-processes-value
@streamsadmin soft nproc max-user-processes-value
On SLES, edit the /etc/security/limits.conf file as shown in the following example:
@streamsadmin hard nproc max-user-processes-value
@streamsadmin soft nproc max-user-processes-value

To set the hard stack and soft stack values, add the following lines to the /etc/security/limits.conf file:
@streamsadmin hard stack unlimited 
@streamsadmin soft stack 20480

Use the following ulimit commands to verify the updated settings:
To verify the updated hard limit, enter the following command:
ulimit -aH
To verify the updated soft limit, enter the following command:
ulimit -aS
</code></pre>
<h2 id="Set-ulimits-in-container-–ulimit"><a href="#Set-ulimits-in-container-–ulimit" class="headerlink" title="Set ulimits in container (–ulimit)"></a>Set ulimits in container (–ulimit)</h2><p>Since setting ulimit settings in a container requires extra privileges not available in the default container, you can set these using the –ulimit flag. –ulimit is specified with a soft and hard limit as such: &#x3D;[:], for example:</p>
<pre><code>$ docker run --ulimit nofile=1024:1024 --rm debian sh -c &quot;ulimit -n&quot;
1024
</code></pre>
<p><strong>Note</strong></p>
<p>If you do not provide a hard limit, the soft limit is used for both values. If no ulimits are set, they are inherited from the default ulimits set on the daemon. The as option is disabled now. In other words, the following script is not supported:</p>
<pre><code>$ docker run -it --ulimit as=1024 fedora /bin/bash
</code></pre>
<p>The values are sent to the appropriate syscall as they are set. Docker doesn’t perform any byte conversion. Take this into account when setting the values.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>Understanding await in iostat</title>
    <url>/blog/understanding-await-in-iostat/</url>
    <content><![CDATA[<h2 id="What’s-meaning-of-await-in-iostat"><a href="#What’s-meaning-of-await-in-iostat" class="headerlink" title="What’s meaning of await in iostat?"></a>What’s meaning of await in iostat?</h2><p>The following is the description provided for await field in iostat man page.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ man iostat</span><br><span class="line">await</span><br><span class="line">    The average time (<span class="keyword">in</span> milliseconds) <span class="keyword">for</span> I/O requests issued to the device to be served. This includes the time spent by the requests <span class="keyword">in</span> queue and the time spent servicing them.</span><br></pre></td></tr></table></figure>

<p>It is a measure of disk I&#x2F;O latency in milliseconds. The latency is from the front of the I&#x2F;O scheduler to the I&#x2F;O completion time.</p>
<h2 id="I-O-path"><a href="#I-O-path" class="headerlink" title="I&#x2F;O path"></a>I&#x2F;O path</h2><p>The I&#x2F;O path mainly includes the following footprints from block layer to underneath storage device.</p>
<ul>
<li>Get the I&#x2F;O requests from application(filesystem)</li>
<li>Merge the I&#x2F;O requests to existing device queue</li>
<li>Dispatch the I&#x2F;O requests(by the I&#x2F;O scheduler) to the device driver</li>
<li>Hypervisor scheduler in virtualization if any</li>
<li>Multipathing if any</li>
<li>Hardware handling</li>
<li>HBA driver</li>
<li>Transportation(bus)</li>
<li>FC switch routing if any</li>
<li>Storage controller queuing, caching and processing</li>
<li>Actual disk latency</li>
</ul>
<h2 id="How-the-await-time-is-calculated"><a href="#How-the-await-time-is-calculated" class="headerlink" title="How the await time is calculated?"></a>How the await time is calculated?</h2><p>The await is the average time on a per I&#x2F;O basis, measured in milliseconds. It mainly includes the time spent in I&#x2F;O scheduler queue and time spent on storage servicing it if the HBA&#x2F;SAN latency is relatively marginal.</p>
<p>There are two queues involved in the I&#x2F;O processing path.</p>
<ul>
<li>The queue in I&#x2F;O scheduler</li>
<li>The queue in storage side(e.g. controller)</li>
</ul>
<p><em>nr_requests</em> limits the maximum number of I&#x2F;Os in the sorted request queue. The front thread will be blocked if the I&#x2F;O can’t be merged&#x2F;inserted into the scheduler queue due to the full occupancy of the queue . Note that the <em>nr_requests</em> is applied to read and write separately.</p>
<p>After the I&#x2F;O is passed to the driver, it is no longer in the scheduler queue and doesn’t cout to <em>nr_requests</em> limit. However, it will count to <em>avgqu-sz</em>. So, the <em>avgqu-sz</em> could reach the sum of <em>nr_requests</em> and LUN <em>queue_depth</em>.</p>
<h2 id="How-the-svctm-time-is-measured"><a href="#How-the-svctm-time-is-measured" class="headerlink" title="How the svctm time is measured?"></a>How the svctm time is measured?</h2><p>await measures the I&#x2F;O latency on a per I&#x2F;O basis while svctm take into account parallel I&#x2F;O. For example, if 100 I&#x2F;Os are submitted to the I&#x2F;O scheduler in parallel and queued onto storage(say queue_depth&#x3D;50), and the 100 I&#x2F;Os completes in 10ms, the await time would be 10ms, but the svctm time could be 2ms.</p>
<h2 id="Follow-up"><a href="#Follow-up" class="headerlink" title="Follow up"></a>Follow up</h2><p>Since await includes the time spent in I&#x2F;O scheduler and storage queue servicing. We may want to see a breakdown for the two phases by using blktrace. It would tell us the overheads on disk queue(I2D) and actual I&#x2F;O service latency(D2C). For furhter study of blktrace, you can read this <a href="https://www.flamingbytes.com/blog/blktrace-a-block-layer-io-tracing-utility">article</a>.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Observability</tag>
      </tags>
  </entry>
  <entry>
    <title>The Winning Shot</title>
    <url>/blog/the-winning-shot/</url>
    <content><![CDATA[<p>Anyone can try out for their dream sports team just by signing up. But the real question is, who will be qualified into the winning team?<span id="more"></span></p>
<p>It just so happened that one day, two best friends were strolling down the streets of New York City, looking for any possible sort of entertainment. Xavier and Viktor, who were just kicked out of the house five minutes ago for knocking over a vase, were suffering from the heat of the raging sun above, when they finally saw their lifesaver. An ice cream truck was coming in their direction and the kids waved like maniacs for the truck to stop. Luckily, the driver pulled over, and Viktor used his pocket money to buy two refreshing mint ice creams. Just as they were enjoying themselves, Xavier spotted a poster on the ice cream truck that read: The NJB tryouts are open! Please register before June 16th to play in the Summer League!</p>
<p>Xavier and Viktor exchanged glances. It would be a huge honor if their team won the season. The children dashed back home and yanked open the door. They looked at the calendar and it read June 16th. It was the last day they had to register for the tryouts.  They pleaded with the adults to let them participate, and after a lot of begging, they agreed to sign them up.</p>
<p>The next day, the two friends went to the gym where the tryouts were being held. However, when they saw the members of each team, their hearts sank. Viktor was put on the Panthers, while Xavier would be playing on the rival team, Jaguars. They shuffled to the benches to wait for their turn, and finally they were called up to play the last game of the tryouts.</p>
<p>“Three, two, one, let the game begin!” roared Coach Michael.</p>
<p>Xavier’s team quickly arranged themselves into a Triangle Offensive and surrounded the other team. They successfully dribbled and passed the ball to each other. Soon, the score was ten to nine. Viktor was the star of the Panthers, but the Jaguars’ strength in numbers allowed them to take the lead. Xavier told the team to block Viktor while he got to the hoop. This plan was immensely wrong. Viktor spiraled around the opponents and scored a three-pointer with a swish. The sudden turn of tide shocked Xavier so much that he was frozen when the ball was passed to him. His shock converted into panic, and the air was filled with electricity. His hair stood on end as if it had been zapped. Xavier uselessly threw the ball onto the ground, wishing it would be with anyone but him. Then, the match ended.</p>
<p>That day, Xavier received many resentful glares from his teammates for his cowardice. Even the enemy team chose to stay away from him. Worst of all, Viktor told him ‌he was ashamed of him. These words of pure disgust were said with a tone of such ice  that Xavier ran away without looking up. However, he was actually filled with steely determination once he returned home. He was going to show his worth and would not rest until he defeated the Panthers in the finals.</p>
<p>For the next month, he practiced day and night his techniques of both offense and defense. The matches flew past like a breeze, and victory was surely in their hands. They had fought through the teams like an unstoppable army, and earned their place in the finals. Xavier was the only one who remained cautious and worked into the night to create a battle strategy to counter Viktor’s Motion Offense. He woke the rest of the team up one hour early to discuss his plans. When Viktor retrieved the ball, Jake, the strongest and oldest teammate, would screen him and steal the ball from him. He would pass it to Xavier, who would score the winning shot. The team marched confidently into the stadium. Xavier himself put on a brave smile, but deep inside, he was trembling like a mouse being chased by a cat. Cold sweat trickled down his neck as he put on his uniform.</p>
<p>The final match began with both teams testing each other’s defense. Xavier was guarding Viktor, who looked a bit ill for some reason. He would not meet his eyes. Xavier ran up and down the court, playing both offense and defense. Finally, their team gained ball control and passed the ball to Xavier. Viktor shockingly refused to block the pass. Xavier, confident in his abilities, was about to shoot a half court when he felt himself swept off his feet. Viktor had slid and tackled him and the two players crashed onto the ground, with Xavier on the bottom. Viktor was disqualified for the foul, but the damage was done. Xavier, who was the star of the team, was left on the ground broken. He had been betrayed by his best friend, which proved that even the nicest people would take extreme measures to get what they wanted.</p>
<p>His teammates helped him to his feet and the referee asked him if he was to keep playing. Xavier was tempted to say no, but  when he saw the smug grin on Viktor’s face, his heart boiled with anger. He nodded to the referee, and the match continued. He could no longer run, for his ankle was surely twisted. But he had gained one advantage. The Panthers thought that he was now useless, so no one was guarding him. They were now piling up upon Jake, who suddenly spotted Xavier wide open at the three point line. He passed the ball to him, and ignoring the screams of agony inside him, Xavier shot the ball. Then, he felt himself faint and landed on the ground with a thud.</p>
<p>Later, he only remembered blurred memories when he woke up in the hospital. His teammates beamed at him and explained how they finally won. Xavier smiled weakly, but inside, he was glowing with pride at how he went on to the end and helped his team win the season.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Thread synchronization</title>
    <url>/blog/thread-synchronization/</url>
    <content><![CDATA[<h2 id="Mutexes"><a href="#Mutexes" class="headerlink" title="Mutexes"></a>Mutexes</h2><p>A mutex is basically a lock that we set (lock) before accessing a shared resource and release (unlock) when we’re done. While it is set, any other thread that tries to set it will block until we release it. If more than one thread is blocked when we unlock the mutex, then all threads blocked on the lock will be made runnable, and the first one to run will be able to set the lock. The others will see that the mutex is still locked and go back to waiting for it to become available again. In this way, only one thread will proceed at a time.</p>
<p>This mutual-exclusion mechanism works only if we design our threads to follow the same data-access rules. The operating system doesn’t serialize access to data for us. If we allow one thread to access a shared resource without first acquiring a lock, then inconsistencies can occur even though the rest of our threads do acquire the lock before attempting to access the shared resource.</p>
<h2 id="Reader-Wrtier-lock"><a href="#Reader-Wrtier-lock" class="headerlink" title="Reader&#x2F;Wrtier lock"></a>Reader&#x2F;Wrtier lock</h2><p>Reader–writer locks are similar to mutexes, except that they allow for higher degrees of parallelism. With a mutex, the state is either locked or unlocked, and only one thread can lock it at a time. Three states are possible with a reader–writer lock: locked in read mode, locked in write mode, and unlocked. Only one thread at a time can hold a reader–writer lock in write mode, but multiple threads can hold a reader–writer lock in read mode at the same time.</p>
<p>When a reader–writer lock is write locked, all threads attempting to lock it block until it is unlocked. When a reader–writer lock is read locked, all threads attempting to lock it in read mode are given access, but any threads attempting to lock it in write mode block until all the threads have released their read locks. Although implementations vary, reader–writer locks usually block additional readers if a lock is already held in read mode and a thread is blocked trying to acquire the lock in write mode. This prevents a constant stream of readers from starving waiting writers.</p>
<p>Reader–writer locks are well suited for situations in which data structures are read more often than they are modified. When a reader–writer lock is held in write mode, the data structure it protects can be modified safely, since only one thread at a time can hold the lock in write mode. When the reader–writer lock is held in read mode, the data structure it protects can be read by multiple threads, as long as the threads first acquire the lock in read mode.</p>
<p>Reader–writer locks are also called shared–exclusive locks. When a reader–writer lock is read locked, it is said to be locked in shared mode. When it is write locked, it is said to be locked in exclusive mode</p>
<h2 id="Spin-Locks"><a href="#Spin-Locks" class="headerlink" title="Spin Locks"></a>Spin Locks</h2><p>A spin lock is like a mutex, except that instead of blocking a process by sleeping, the process is blocked by busy-waiting (spinning) until the lock can be acquired. A spin lock could be used in situations where locks are held for short periods of times and threads don’t want to incur the cost of being descheduled.</p>
<p>Spin locks are often used as low-level primitives to implement other types of locks. Depending on the system architecture, they can be implemented efficiently using test- and-set instructions. Although efficient, they can lead to wasting CPU resources: while a thread is spinning and waiting for a lock to become available, the CPU can’t do anything else. This is why spin locks should be held only for short periods of time.</p>
<h2 id="Barriers"><a href="#Barriers" class="headerlink" title="Barriers"></a>Barriers</h2><p>Barriers are a synchronization mechanism that can be used to coordinate multiple threads working in parallel. A barrier allows each thread to wait until all cooperating threads have reached the same point, and then continue executing from there. We’ve already seen one form of barrier—the pthread_join function acts as a barrier to allow one thread to wait until another thread exits.</p>
<p>Barrier objects are more general than this, however. They allow an arbitrary number of threads to wait until all of the threads have completed processing, but the threads don’t have to exit. They can continue working after all threads have reached the barrier.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li>Advanced Programming in the UNIX Environment</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Understanding Azure disk bursting</title>
    <url>/blog/understanding-azure-disk-bursting/</url>
    <content><![CDATA[<p>Currently, there are two managed disk types that can burst, premium SSDs, and standard SSDs. Other disk types cannot currently burst. There are two models of bursting for disks:</p>
<p>An on-demand bursting model, where the disk bursts whenever its needs exceed its current capacity. This model incurs additional charges anytime the disk bursts. On-demand bursting is only available for premium SSDs larger than 512 GiB.</p>
<p>A credit-based model, where the disk will burst only if it has burst credits accumulated in its credit bucket. This model does not incur additional charges when the disk bursts. Credit-based bursting is only available for premium SSDs 512 GiB and smaller, and standard SSDs 1024 GiB and smaller.</p>
<p>Azure premium SSDs can use either bursting model, but standard SSDs currently only offer credit-based bursting.</p>
<p><strong>On-demand bursting</strong></p>
<p>Premium SSDs using the on-demand bursting model of disk bursting can burst beyond original provisioned targets, as often as needed by their workload, up to the max burst target. For example, on a 1-TiB P30 disk, the provisioned IOPS is 5000 IOPS. When disk bursting is enabled on this disk, your workloads can issue IOs to this disk up to the max burst performance of 30,000 IOPS and 1,000 MBps. For the max burst targets on each supported disk, see Scalability and performance targets for VM disks.</p>
<p>If you expect your workloads to frequently run beyond the provisioned perf target, disk bursting won’t be cost-effective. In this case, we recommend that you change your disk’s performance tier to a higher tier instead, for better baseline performance. Review your billing details and assess that against the traffic pattern of your workloads.</p>
<p>Before you enable on-demand bursting, understand the following:</p>
<ul>
<li>On-demand bursting cannot be enabled on a premium SSD that has less than or equal to 512 GiB. Premium SSDs less than or equal to 512 GiB will always use credit-based bursting.</li>
<li>On-demand bursting is only supported on premium SSDs. If a premium SSD with on-demand bursting enabled is switched to another disk type, then disk bursting is disabled.</li>
<li>On-demand bursting doesn’t automatically disable itself when the performance tier is changed. If you want to change your performance tier but do not want to keep disk bursting, you must disable it.</li>
<li>On-demand bursting can only be enabled when the disk is detached from a VM or when the VM is stopped. On-demand bursting can be disabled 12 hours after it has been enabled.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.microsoft.com/en-us/azure/virtual-machines/disks-types#premium-ssds">Azure managed disk types</a></li>
<li><a href="https://nicolgit.github.io/azure-disk-burst-in-a-nutshell/">Azure Disk Burst in a nutshell</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/virtual-machines/disk-bursting">Managed disk bursting</a></li>
<li><a href="https://azure.microsoft.com/en-us/pricing/details/managed-disks/">Managed disk pricing</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/virtual-machines/disks-enable-bursting?tabs=azure-powershell">Enable on-demand bursting</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>Azure</tag>
      </tags>
  </entry>
  <entry>
    <title>Understanding MapReduce with an example</title>
    <url>/blog/understanding-mapreduce-with-an-example/</url>
    <content><![CDATA[<h2 id="What-is-MapReduce"><a href="#What-is-MapReduce" class="headerlink" title="What is MapReduce"></a>What is MapReduce</h2><p><img src="/images/mapreduce.png" alt="Image"></p>
<p>MapReduce is a programming framework that allows us to perform distributed and parallel processing on large data sets in a distributed environment.</p>
<ul>
<li>MapReduce consists of two distinct tasks — Map and Reduce.</li>
<li>As the name MapReduce suggests, reducer phase takes place after the mapper phase has been completed.</li>
<li>So, the first is the map job, where a block of data is read and processed to produce key-value pairs as intermediate outputs.</li>
<li>The output of a Mapper or map job (key-value pairs) is input to the Reducer.</li>
<li>The reducer receives the key-value pair from multiple map jobs.</li>
<li>Then, the reducer aggregates those intermediate data tuples (intermediate key-value pair) into a smaller set of tuples or key-value pairs which is the final output.</li>
</ul>
<h2 id="A-Word-Count-Example-of-MapReduce"><a href="#A-Word-Count-Example-of-MapReduce" class="headerlink" title="A Word Count Example of MapReduce"></a>A Word Count Example of MapReduce</h2><p>Let us understand, how a MapReduce works by taking an example where I have a text file called example.txt whose contents are as follows:</p>
<p><em>Dear, Bear, River, Car, Car, River, Deer, Car and Bear</em></p>
<p>Now, suppose, we have to perform a word count on the sample.txt using MapReduce. So, we will be finding unique words and the number of occurrences of those unique words.</p>
<p><img src="/images/mapreduce-example.png" alt="Image"></p>
<ul>
<li>First, we divide the input into three splits as shown in the figure. This will distribute the work among all the map nodes.</li>
<li>Then, we tokenize the words in each of the mappers and give a hardcoded value (1) to each of the tokens or words. The rationale behind giving a hardcoded value equal to 1 is that every word, in itself, will occur once.</li>
<li>Now, a list of key-value pair will be created where the key is nothing but the individual words and value is one. So, for the first line (Dear Bear River) we have 3 key-value pairs — Dear, 1; Bear, 1; River, 1. The mapping process remains the same on all the nodes.</li>
<li>After the mapper phase, a partition process takes place where sorting and shuffling happen so that all the tuples with the same key are sent to the corresponding reducer.</li>
<li>So, after the sorting and shuffling phase, each reducer will have a unique key and a list of values corresponding to that very key. For example, Bear, [1,1]; Car, [1,1,1].., etc.</li>
<li>Now, each Reducer counts the values which are present in that list of values. As shown in the figure, reducer gets a list of values which is [1,1] for the key Bear. Then, it counts the number of ones in the very list and gives the final output as — Bear, 2.</li>
<li>Finally, all the output key&#x2F;value pairs are then collected and written in the output file.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://medium.com/edureka/mapreduce-tutorial-3d9535ddbe7c">Fundamentals of MapReduce with MapReduce Example</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title>Understanding RPO and RTO</title>
    <url>/blog/understanding-rpo-and-rto/</url>
    <content><![CDATA[<p>The disasters could occur unexpectedly and it could take down the whole data center in an application region. In the real world, a well designed disaster recovery solution would help restore the applications and related data at the time of disaster.</p>
<p>There are two critical metrics to measure how timely the applications and data can be recovered and how much data loss can be tolerated.</p>
<ul>
<li>Recovery Time Objective(RTO)</li>
<li>Recovery Point Objective(RPO)</li>
</ul>
<p><img src="/images/rpo-rto.png" alt="Image"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.enterprisestorageforum.com/management/rpo-and-rto-understanding-the-differences/">https://www.enterprisestorageforum.com/management/rpo-and-rto-understanding-the-differences/</a>)</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>Backup Recovery</tag>
      </tags>
  </entry>
  <entry>
    <title>Understanding the Linux inodes</title>
    <url>/blog/understanding-the-linux-inodes/</url>
    <content><![CDATA[<p>Linux makes a clear distinction between the contents of a file and the information about a file. All information needed by the filesystem to handle a file is included in a data structure called an <strong>inode</strong>. Each file has its own inode, which the filesystem uses to identify the file.</p>
<h2 id="File-types"><a href="#File-types" class="headerlink" title="File types"></a>File types</h2><p>Linux files may be one of the following types.</p>
<ul>
<li>Regular file</li>
<li>Directory</li>
<li>Symbolic link</li>
<li>Block device file</li>
<li>Character device file</li>
<li>Pipe(FIFO)</li>
<li>Socket</li>
</ul>
<p>The first three file types are constituents of any Linux filesystem. Device files are related both to I&#x2F;O devices, and to device drivers integrated into the kernel. When a program accesses a device file, it acts directly on the I&#x2F;O device associated with that file. Pipes and sockets are special files used for interprocess communication.</p>
<h2 id="File-inodes"><a href="#File-inodes" class="headerlink" title="File inodes"></a>File inodes</h2><p>Linux system must provide the following inode attributes, which are specified in the POSIX standard:</p>
<ul>
<li>File type</li>
<li>Link count: Number of hard links associated with the file</li>
<li>File size in bytes</li>
<li>Device ID(i.e., the identifier of the device containing the file)</li>
<li>Inode number that identifies the file in the filesystem</li>
<li>UID of the file owner</li>
<li>User group ID of the file</li>
<li>Timestamps which specify the inode change time, the last access time and the last modify time.</li>
<li>Access rights&#x2F;permissions(read, write, and execute) and file mode</li>
</ul>
<p>The following is inode struct definition in the Linux source code.</p>
<pre><code>/*
 * Keep mostly read-only and often accessed (especially for
 * the RCU path lookup and &#39;stat&#39; data) fields at the beginning
 * of the &#39;struct inode&#39;
 */
struct inode &#123;
    umode_t			i_mode;
    unsigned short		i_opflags;
    kuid_t			i_uid;
    kgid_t			i_gid;
    unsigned int		i_flags;

#ifdef CONFIG_FS_POSIX_ACL
    struct posix_acl	*i_acl;
    struct posix_acl	*i_default_acl;
#endif

    const struct inode_operations	*i_op;
    struct super_block	*i_sb;
    struct address_space	*i_mapping;

#ifdef CONFIG_SECURITY
    void			*i_security;
#endif

    /* Stat data, not accessed from path walking */
    unsigned long		i_ino;
    /*
     * Filesystems may only read i_nlink directly.  They shall use the
     * following functions for modification:
     *
     *    (set|clear|inc|drop)_nlink
     *    inode_(inc|dec)_link_count
     */
    union &#123;
        const unsigned int i_nlink;
        unsigned int __i_nlink;
    &#125;;
    dev_t			i_rdev;
    loff_t			i_size;
    struct timespec64	i_atime;
    struct timespec64	i_mtime;
    struct timespec64	i_ctime;
    spinlock_t		i_lock;	/* i_blocks, i_bytes, maybe i_size */
    unsigned short          i_bytes;
    u8			i_blkbits;
    u8			i_write_hint;
    blkcnt_t		i_blocks;

#ifdef __NEED_I_SIZE_ORDERED
    seqcount_t		i_size_seqcount;
#endif

    /* Misc */
    unsigned long		i_state;
    struct rw_semaphore	i_rwsem;

    unsigned long		dirtied_when;	/* jiffies of first dirtying */
    unsigned long		dirtied_time_when;

    struct hlist_node	i_hash;
    struct list_head	i_io_list;	/* backing dev IO list */
#ifdef CONFIG_CGROUP_WRITEBACK
    struct bdi_writeback	*i_wb;		/* the associated cgroup wb */

    /* foreign inode detection, see wbc_detach_inode() */
    int			i_wb_frn_winner;
    u16			i_wb_frn_avg_time;
    u16			i_wb_frn_history;
#endif
    struct list_head	i_lru;		/* inode LRU list */
    struct list_head	i_sb_list;
    struct list_head	i_wb_list;	/* backing dev writeback list */
    union &#123;
        struct hlist_head	i_dentry;
        struct rcu_head		i_rcu;
    &#125;;
    atomic64_t		i_version;
    atomic64_t		i_sequence; /* see futex */
    atomic_t		i_count;
    atomic_t		i_dio_count;
    atomic_t		i_writecount;
#if defined(CONFIG_IMA) || defined(CONFIG_FILE_LOCKING)
    atomic_t		i_readcount; /* struct files open RO */
#endif
    union &#123;
        const struct file_operations	*i_fop;	/* former -&gt;i_op-&gt;default_file_ops */
        void (*free_inode)(struct inode *);
    &#125;;
    struct file_lock_context	*i_flctx;
    struct address_space	i_data;
    struct list_head	i_devices;
    union &#123;
        struct pipe_inode_info	*i_pipe;
        struct cdev		*i_cdev;
        char			*i_link;
        unsigned		i_dir_seq;
    &#125;;

    __u32			i_generation;

#ifdef CONFIG_FSNOTIFY
    __u32			i_fsnotify_mask; /* all events this inode cares about */
    struct fsnotify_mark_connector __rcu	*i_fsnotify_marks;
#endif

#ifdef CONFIG_FS_ENCRYPTION
    struct fscrypt_info	*i_crypt_info;
#endif

#ifdef CONFIG_FS_VERITY
    struct fsverity_info	*i_verity_info;
#endif

    void			*i_private; /* fs or device private pointer */
&#125; __randomize_layout;
</code></pre>
<h2 id="Display-inode-information"><a href="#Display-inode-information" class="headerlink" title="Display inode information"></a>Display inode information</h2><p>The inode data can be displayed with the <strong>stat</strong> command.</p>
<pre><code>$ echo &quot;hello inode&quot; &gt; testfile

$ debugfs /dev/mapper/vgroot-lvroot
debugfs 1.42.9 (28-Dec-2013)
debugfs:  q
[root@init500-d1 ~]# stat testfile
  File: ‘testfile’
  Size: 12        	Blocks: 8          IO Block: 4096   regular file
Device: fd00h/64768d	Inode: 27787306    Links: 1
Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2022-04-30 20:40:38.496696650 +0000
Modify: 2022-04-30 20:40:38.496696650 +0000
Change: 2022-04-30 20:40:38.496696650 +0000
 Birth: -

$ stat --format=%i testfile
27787306
</code></pre>
<p>The inode number can be also displayed with <strong>ls -i</strong> command. With <strong>ls -l</strong> command, it displays file permissions for the owner, group and others. In the following example, the owner has read and write permission. The group and others only have read permission.</p>
<pre><code>$ ls -il testfile
27787306 -rw-r--r-- 1 root root 12 Apr 30 20:40 testfile
</code></pre>
<p>The inode space information can be displayed with <strong>df -i</strong> command.</p>
<pre><code>$ df -i
Filesystem                   Inodes  IUsed     IFree IUse% Mounted on
devtmpfs                  132058034    953 132057081    1% /dev
tmpfs                     132061520      4 132061516    1% /dev/shm
tmpfs                     132061520   1270 132060250    1% /run
tmpfs                     132061520     17 132061503    1% /sys/fs/cgroup
/dev/mapper/vgroot-lvroot  97320960 188567  97132393    1% /
/dev/nvme0n1p2               128016     31    127985    1% /boot
/dev/nvme0n1p1                    0      0         0     - /boot/efi
tmpfs                     132061520      1 132061519    1% /run/user/0
tmpfs                     132061520      1 132061519    1% /var/lib/osd/lttng
</code></pre>
<p>The content of filesystem superblock can be listed with <strong>tune2fs</strong> command. The inode related information can be grepped.</p>
<pre><code>$ tune2fs -l /dev/mapper/vgroot-lvroot | grep -i inode
Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery extent 64bit flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize
Inode count:              97320960
Free inodes:              97167388
Inodes per group:         8192
Inode blocks per group:   512
First inode:              11
Inode size:	          256
Journal inode:            8
First orphan inode:       22157990
Journal backup:           inode blocks
</code></pre>
<p>The debugfs program is an interactive file system debugger. It can be used to examine and change the state of an ext2, ext3, or ext4 file system.</p>
<pre><code>$ debugfs /dev/mapper/vgroot-lvroot
debugfs 1.42.9 (28-Dec-2013)

debugfs:  stat &lt;27787306&gt;
Inode: 27787306   Type: regular    Mode:  0644   Flags: 0x80000
Generation: 269876152    Version: 0x00000000:00000001
User:     0   Group:     0   Size: 12
File ACL: 0    Directory ACL: 0
Links: 1   Blockcount: 8
Fragment:  Address: 0    Number: 0    Size: 0
 ctime: 0x626d9ec6:766bf528 -- Sat Apr 30 20:40:38 2022
 atime: 0x626d9ec6:766bf528 -- Sat Apr 30 20:40:38 2022
 mtime: 0x626d9ec6:766bf528 -- Sat Apr 30 20:40:38 2022
crtime: 0x626d9ec6:766bf528 -- Sat Apr 30 20:40:38 2022
Size of extra inode fields: 32
EXTENTS:
(0):111182849
</code></pre>
<p>Inode structure for the directory can be displayed as below.</p>
<pre><code>$ stat /
  File: ‘/’
  Size: 4096      	Blocks: 8          IO Block: 4096   directory
Device: fd00h/64768d	Inode: 2           Links: 18
Access: (0555/dr-xr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2022-04-14 23:50:13.812053511 +0000
Modify: 2022-04-30 20:42:56.468650708 +0000
Change: 2022-04-30 20:42:56.468650708 +0000
 Birth: -
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://elixir.bootlin.com/linux/latest/source/include/linux/fs.h#L614">https://elixir.bootlin.com/linux/latest/source/include/linux/fs.h#L614</a></li>
<li><a href="https://www.cs.montana.edu/courses/309/topics/4-disks/debugfs_example.html">https://www.cs.montana.edu/courses/309/topics/4-disks/debugfs_example.html</a></li>
<li><a href="https://linoxide.com/linux-inode/">https://linoxide.com/linux-inode/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Administration</tag>
      </tags>
  </entry>
  <entry>
    <title>Understanding thin provisioning volume and snapshot</title>
    <url>/blog/understanding-thin-provisioning-volume-and-snapshot/</url>
    <content><![CDATA[<h2 id="Thin-provisioning-volume"><a href="#Thin-provisioning-volume" class="headerlink" title="Thin provisioning volume"></a>Thin provisioning volume</h2><p>Logical volume can be thinly provisioned. It allows storage administrator to overcommit the physical storage. In other words, it’s possible to create a logical volume which is larger than the available extents.</p>
<h2 id="Create-thin-provisioning-volume"><a href="#Create-thin-provisioning-volume" class="headerlink" title="Create thin provisioning volume"></a>Create thin provisioning volume</h2><p>In the following example, we create a 500GiB thin pool and 100GiB volume.</p>
<pre><code>$ vgcreate vg1 /dev/nvme0n1
  Physical volume &quot;/dev/nvme0n1&quot; successfully created.
  Volume group &quot;vg1&quot; successfully created

$ vgs
  VG     #PV #LV #SN Attr   VSize   VFree
  centos   1   3   0 wz--n- 893.05g      0
  vg1      1   0   0 wz--n- 931.51g 931.51g

$ lvcreate -L 500G --thinpool thinpool1 vg1
  Thin pool volume with chunk size 256.00 KiB can address at most 63.25 TiB of data.
  Logical volume &quot;thinpool1&quot; created.

$ lvs
  LV        VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  home      centos -wi-ao---- 839.05g
  root      centos -wi-ao----  50.00g
  swap      centos -wi-ao----   4.00g
  thinpool1 vg1    twi-a-tz-- 500.00g             0.00   10.41

$ lvs -ao name,size,stripesize,chunksize,metadata_percent
  LV                LSize   Stripe Chunk   Meta%
  home              839.05g     0       0
  root               50.00g     0       0
  swap                4.00g     0       0
  [lvol0_pmspare]   128.00m     0       0
  thinpool1         500.00g     0  256.00k 10.41
  [thinpool1_tdata] 500.00g     0       0
  [thinpool1_tmeta] 128.00m     0       
  
$ lvcreate -V 100G --thin -n thinvol1 vg1/thinpool1
  Logical volume &quot;thinvol1&quot; created.

$ lvs
  LV        VG     Attr       LSize   Pool      Origin Data%  Meta%  Move Log Cpy%Sync Convert
  home      centos -wi-ao---- 839.05g
  root      centos -wi-ao----  50.00g
  swap      centos -wi-ao----   4.00g
  thinpool1 vg1    twi-aotz-- 500.00g                  0.00   10.42
  thinvol1  vg1    Vwi-a-tz-- 100.00g thinpool1        0.00
</code></pre>
<h2 id="Thin-pool-volume-chunk-size"><a href="#Thin-pool-volume-chunk-size" class="headerlink" title="Thin pool volume chunk size"></a>Thin pool volume chunk size</h2><p>By default, lvm2 starts with 64KiB chunk size and increase its value when the resulting size of the thin pool metadata device grows above 128MiB.</p>
<p>In the previous example, the 500GiB thin pool results in 256KiB chunk size. In the following example, the 100MiB thin pool results in 64KiB chunk size.</p>
<pre><code>$ lvcreate  -L 100M --thinpool thinpool2 vg1
  Thin pool volume with chunk size 64.00 KiB can address at most 15.81 TiB of data.
  Logical volume &quot;thinpool2&quot; created.

$ lvs -ao name,size,stripesize,chunksize,metadata_percent
  LV                LSize   Stripe Chunk   Meta%
  home              839.05g     0       0
  root               50.00g     0       0
  swap                4.00g     0       0
  [lvol0_pmspare]   128.00m     0       0
  thinpool1         500.00g     0  256.00k 10.42
  [thinpool1_tdata] 500.00g     0       0
  [thinpool1_tmeta] 128.00m     0       0
  thinpool2         100.00m     0   64.00k 10.84
  [thinpool2_tdata] 100.00m     0       0
  [thinpool2_tmeta]   4.00m     0       0
  thinvol1          100.00g     0       0  
</code></pre>
<p>The “-c” option can be used to specify the desired chunk size if needed.</p>
<pre><code>$ lvcreate -c 128k -L 100M --thinpool thinpool3 vg1
  Thin pool volume with chunk size 128.00 KiB can address at most 31.62 TiB of data.
  Logical volume &quot;thinpool3&quot; created.

$ lvs -ao name,size,stripesize,chunksize,metadata_percent
  LV                LSize   Stripe Chunk   Meta%
  home              839.05g     0       0
  root               50.00g     0       0
  swap                4.00g     0       0
  [lvol0_pmspare]   128.00m     0       0
  thinpool1         500.00g     0  256.00k 10.42
  [thinpool1_tdata] 500.00g     0       0
  [thinpool1_tmeta] 128.00m     0       0
  thinpool2         100.00m     0   64.00k 10.84
  [thinpool2_tdata] 100.00m     0       0
  [thinpool2_tmeta]   4.00m     0       0
  thinpool3         100.00m     0  128.00k 10.84
  [thinpool3_tdata] 100.00m     0       0
  [thinpool3_tmeta]   4.00m     0       0
  thinvol1          100.00g     0       0
</code></pre>
<p>Use the following criteria for using the chunk size:</p>
<ul>
<li>A smaller chunk size requires more metadata and hinders performance, but provides better space utilization with snapshots.</li>
<li>A bigger chunk size requires less metadata manipulation, but makes the snapshot less space efficient.</li>
</ul>
<h2 id="Normal-snapshot-volume"><a href="#Normal-snapshot-volume" class="headerlink" title="Normal snapshot volume"></a>Normal snapshot volume</h2><p>The LVM snapshot provides the ability to create a virtual image of device at a point in time without a service interruption.</p>
<p>When the original data block is overwritten after snapshot is taken, the original data needs to be copied to the snapshot volume. This will introduce copy-on-write overhead whenever the original block is overwritten. The state of original data can be reconstructed with the snapshot.</p>
<h2 id="Thinly-provisioned-snapshot-volume"><a href="#Thinly-provisioned-snapshot-volume" class="headerlink" title="Thinly-provisioned snapshot volume"></a>Thinly-provisioned snapshot volume</h2><p>Unlike normal snapshot volume, the thin snapshot and volume are all about metadata. When the volume is snapshot, its metadata are copied for the thin snapshot volume use. As the metadata of the volume is changed, the snapshot volume still addresses the original data blocks. The new data will be written to new blocks. In other words, overwrites actually write the data to new blocks. Thus, the original data blocks can be still addressed by snapshot volume metadata after the data change.</p>
<h2 id="Create-the-snapshot-volume"><a href="#Create-the-snapshot-volume" class="headerlink" title="Create the snapshot volume"></a>Create the snapshot volume</h2><pre><code>$ lvcreate -s -L 100G -n thinvol1-snap /dev/vg1/thinvol1
  Logical volume &quot;thinvol1-snap&quot; created.

$ ls /dev/vg1
thinpool2  thinpool3   thinvol1  thinvol1-snap

$ lvs
  LV            VG     Attr       LSize   Pool      Origin   Data%  Meta%  Move Log Cpy%Sync Convert
  home          centos -wi-ao---- 839.05g
  root          centos -wi-ao----  50.00g
  swap          centos -wi-ao----   4.00g
  thinpool1     vg1    twi-aotz-- 500.00g                    0.00   10.42
  thinpool2     vg1    twi-a-tz-- 100.00m                    0.00   10.84
  thinpool3     vg1    twi-a-tz-- 100.00g                    0.00   10.43
  thinvol1      vg1    owi-a-tz-- 100.00g thinpool1          0.00
  thinvol1-snap vg1    swi-a-s--- 100.00g           thinvol1 0.00

$ lvs -ao name,size,stripesize,chunksize,metadata_percent
  LV                LSize   Stripe Chunk   Meta%
  home              839.05g     0       0
  root               50.00g     0       0
  swap                4.00g     0       0
  [lvol0_pmspare]   128.00m     0       0
  thinpool1         500.00g     0  256.00k 10.42
  [thinpool1_tdata] 500.00g     0       0
  [thinpool1_tmeta] 128.00m     0       0
  thinpool2         100.00m     0   64.00k 10.84
  [thinpool2_tdata] 100.00m     0       0
  [thinpool2_tmeta]   4.00m     0       0
  thinpool3         100.00m     0  128.00k 10.84
  [thinpool3_tdata] 100.00m     0       0
  [thinpool3_tmeta]   4.00m     0       0
  thinvol1          100.00g     0       0
  thinvol1-snap     100.00g     0    4.00k
</code></pre>
<p>The chunk size of snapshot volume can be specified with “-c” option.</p>
<pre><code>$ lvcreate -s -c 128k -L 100G -n thinvol1-snap2 /dev/vg1/thinvol1
  Logical volume &quot;thinvol1-snap2&quot; created.

$ lvs -ao name,size,stripesize,chunksize,metadata_percent
  LV                LSize   Stripe Chunk   Meta%
  home              839.05g     0       0
  root               50.00g     0       0
  swap                4.00g     0       0
  [lvol0_pmspare]   128.00m     0       0
  thinpool1         500.00g     0  256.00k 10.42
  [thinpool1_tdata] 500.00g     0       0
  [thinpool1_tmeta] 128.00m     0       0
  thinpool2         100.00m     0   64.00k 10.84
  [thinpool2_tdata] 100.00m     0       0
  [thinpool2_tmeta]   4.00m     0       0
  thinpool3         100.00m     0  128.00k 10.84
  [thinpool3_tdata] 100.00m     0       0
  [thinpool3_tmeta]   4.00m     0       0
  thinvol1          100.00g     0       0
  thinvol1-snap     100.00g     0    4.00k
  thinvol1-snap2    100.00g     0  128.00k
</code></pre>
<h2 id="Remove-the-snaphost-volume"><a href="#Remove-the-snaphost-volume" class="headerlink" title="Remove the snaphost volume"></a>Remove the snaphost volume</h2><pre><code>$ lvremove /dev/vg1/thinvol1-snap
Do you really want to remove active logical volume vg1/thinvol1-snap? [y/n]: y
  Logical volume &quot;thinvol1-snap&quot; successfully removed
$ lvremove /dev/vg1/thinvol1-snap2
Do you really want to remove active logical volume vg1/thinvol1-snap2? [y/n]: y
  Logical volume &quot;thinvol1-snap2&quot; successfully removed

$ lvs
  LV        VG     Attr       LSize   Pool      Origin Data%  Meta%  Move Log Cpy%Sync Convert
  home      centos -wi-ao---- 839.05g
  root      centos -wi-ao----  50.00g
  swap      centos -wi-ao----   4.00g
  thinpool1 vg1    twi-aotz-- 500.00g                  0.00   10.42
  thinpool2 vg1    twi-a-tz-- 100.00m                  0.00   10.84
  thinpool3 vg1    twi-a-tz-- 100.00g                  0.00   10.43
  thinvol1  vg1    Vwi-a-tz-- 100.00g thinpool1        0.00
</code></pre>
<h2 id="Remove-the-volume-and-pool"><a href="#Remove-the-volume-and-pool" class="headerlink" title="Remove the volume and pool"></a>Remove the volume and pool</h2><pre><code>$ lvremove /dev/vg1/thinvol1 -f
  Logical volume &quot;thinvol1&quot; successfully removed

$ lvremove /dev/vg1/thinpool1
Do you really want to remove active logical volume vg1/thinpool1? [y/n]: y
  Logical volume &quot;thinpool1&quot; successfully removed  
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_logical_volumes/assembly_thinly-provisioned-logical-volumes_configuring-and-managing-logical-volumes">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux&#x2F;8&#x2F;html&#x2F;configuring_and_managing_logical_volumes&#x2F;assembly_thinly-provisioned-logical-volumes_configuring-and-managing-logical-volumes</a></li>
<li><a href="https://www.ilsistemista.net/index.php/linux-a-unix/46-lvm-thin-volume-explained.html?start=3">https://www.ilsistemista.net/index.php/linux-a-unix/46-lvm-thin-volume-explained.html?start=3</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>LVM</tag>
      </tags>
  </entry>
  <entry>
    <title>Uninstall ceph storage cluster</title>
    <url>/blog/uninstall-ceph-storage-cluster/</url>
    <content><![CDATA[<h2 id="Check-the-pools-images-and-OSDs"><a href="#Check-the-pools-images-and-OSDs" class="headerlink" title="Check the pools, images and OSDs"></a>Check the pools, images and OSDs</h2><pre><code>[ceph: root@host1 /]$ ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME             STATUS  REWEIGHT  PRI-AFF
-1         83.83411  root default
-3         27.94470      host host1
 0    ssd   3.49309          osd.0             up   1.00000  1.00000
 1    ssd   3.49309          osd.1             up   1.00000  1.00000
 2    ssd   3.49309          osd.2             up   1.00000  1.00000
 3    ssd   3.49309          osd.3             up   1.00000  1.00000
 4    ssd   3.49309          osd.4             up   1.00000  1.00000
 5    ssd   3.49309          osd.5             up   1.00000  1.00000
 6    ssd   3.49309          osd.6             up   1.00000  1.00000
 7    ssd   3.49309          osd.7             up   1.00000  1.00000
-5         27.94470      host host2
 8    ssd   3.49309          osd.8             up   1.00000  1.00000
 9    ssd   3.49309          osd.9             up   1.00000  1.00000
10    ssd   3.49309          osd.10            up   1.00000  1.00000
11    ssd   3.49309          osd.11            up   1.00000  1.00000
12    ssd   3.49309          osd.12            up   1.00000  1.00000
13    ssd   3.49309          osd.13            up   1.00000  1.00000
14    ssd   3.49309          osd.14            up   1.00000  1.00000
15    ssd   3.49309          osd.15            up   1.00000  1.00000
-7         27.94470      host host3
16    ssd   3.49309          osd.16            up   1.00000  1.00000
17    ssd   3.49309          osd.17            up   1.00000  1.00000
18    ssd   3.49309          osd.18            up   1.00000  1.00000
19    ssd   3.49309          osd.19            up   1.00000  1.00000
20    ssd   3.49309          osd.20            up   1.00000  1.00000
21    ssd   3.49309          osd.21            up   1.00000  1.00000
22    ssd   3.49309          osd.22            up   1.00000  1.00000
23    ssd   3.49309          osd.23            up   1.00000  1.00000

[ceph: root@host1 /]$ ceph osd lspools
1 device_health_metrics
2 datapool

[ceph: root@host1 /]$ rbd showmapped
id  pool      namespace  image    snap  device
0   datapool             rbdvol1  -     /dev/rbd0
1   datapool             rbdvol2  -     /dev/rbd1
2   datapool             rbdvol3  -     /dev/rbd2
3   datapool             rbdvol4  -     /dev/rbd3
</code></pre>
<h2 id="Remove-the-images-and-pools"><a href="#Remove-the-images-and-pools" class="headerlink" title="Remove the images and pools"></a>Remove the images and pools</h2><pre><code>[ceph: root@host1 /]$ rbd unmap /dev/rbd0
[ceph: root@host1 /]$ rbd unmap /dev/rbd1
[ceph: root@host1 /]$ rbd unmap /dev/rbd2
[ceph: root@host1 /]$ rbd unmap /dev/rbd3

[ceph: root@host1 /]$ rbd showmapped

[ceph: root@host1 /]$ rbd rm datapool/rbdvol1
Removing image: 100% complete...done.
[ceph: root@host1 /]$ rbd rm datapool/rbdvol2
Removing image: 100% complete...done.
[ceph: root@host1 /]$ rbd rm datapool/rbdvol3
Removing image: 100% complete...done.
[ceph: root@host1 /]$ rbd rm datapool/rbdvol4
Removing image: 100% complete...done.

[ceph: root@host1 /]$ ceph osd pool rm datapool datapool --yes-i-really-really-mean-it
Error EPERM: pool deletion is disabled; you must first set the mon_allow_pool_delete config option to true before you can destroy a pool

[ceph: root@host1 /]$ ceph tell mon.\* injectargs &#39;--mon-allow-pool-delete=true&#39;
mon.host1: mon_allow_pool_delete = &#39;true&#39;
mon.host1: &#123;&#125;
mon.host3: mon_allow_pool_delete = &#39;true&#39;
mon.host3: &#123;&#125;
mon.host2: mon_allow_pool_delete = &#39;true&#39;
mon.host2: &#123;&#125;
[ceph: root@host1 /]$ ceph osd pool rm datapool datapool --yes-i-really-really-mean-it
pool &#39;datapool&#39; removed
</code></pre>
<h2 id="Remove-the-OSDs"><a href="#Remove-the-OSDs" class="headerlink" title="Remove the OSDs"></a>Remove the OSDs</h2><pre><code>[ceph: root@host1 /]$ for i in `seq 0 23`
&gt; do
&gt; ceph osd down $i &amp;&amp; ceph osd destroy $i --force
&gt; done
marked down osd.0.
destroyed osd.0
[omitted...]

[ceph: root@host1 /]$  ceph osd tree
ID  CLASS  WEIGHT    TYPE NAME             STATUS     REWEIGHT  PRI-AFF
-1         83.83411  root default
-3         27.94470      host host1
 0    ssd   3.49309          osd.0         destroyed   1.00000  1.00000
 1    ssd   3.49309          osd.1         destroyed   1.00000  1.00000
 2    ssd   3.49309          osd.2         destroyed   1.00000  1.00000
 3    ssd   3.49309          osd.3         destroyed   1.00000  1.00000
 4    ssd   3.49309          osd.4         destroyed   1.00000  1.00000
 5    ssd   3.49309          osd.5         destroyed   1.00000  1.00000
 6    ssd   3.49309          osd.6         destroyed   1.00000  1.00000
 7    ssd   3.49309          osd.7         destroyed   1.00000  1.00000
-5         27.94470      host host2
 8    ssd   3.49309          osd.8         destroyed   1.00000  1.00000
 9    ssd   3.49309          osd.9         destroyed   1.00000  1.00000
10    ssd   3.49309          osd.10        destroyed   1.00000  1.00000
11    ssd   3.49309          osd.11        destroyed   1.00000  1.00000
12    ssd   3.49309          osd.12        destroyed   1.00000  1.00000
13    ssd   3.49309          osd.13        destroyed   1.00000  1.00000
14    ssd   3.49309          osd.14        destroyed   1.00000  1.00000
15    ssd   3.49309          osd.15        destroyed   1.00000  1.00000
-7         27.94470      host host3
16    ssd   3.49309          osd.16        destroyed   1.00000  1.00000
17    ssd   3.49309          osd.17        destroyed   1.00000  1.00000
18    ssd   3.49309          osd.18        destroyed   1.00000  1.00000
19    ssd   3.49309          osd.19        destroyed   1.00000  1.00000
20    ssd   3.49309          osd.20        destroyed   1.00000  1.00000
21    ssd   3.49309          osd.21        destroyed   1.00000  1.00000
22    ssd   3.49309          osd.22        destroyed   1.00000  1.00000
23    ssd   3.49309          osd.23               up   1.00000  1.00000
</code></pre>
<h2 id="Remove-the-cluster-hosts"><a href="#Remove-the-cluster-hosts" class="headerlink" title="Remove the cluster hosts"></a>Remove the cluster hosts</h2><pre><code>[ceph: root@host1 /]$ ceph orch host rm host3
Removed host &#39;host3&#39;
[ceph: root@host1 /]$ ceph orch host rm host2
Removed host &#39;host2&#39;
[ceph: root@host1 /]$ ceph orch host rm host1
Removed host &#39;host1&#39;
</code></pre>
<h2 id="Check-if-there-is-ceph-daemon-running"><a href="#Check-if-there-is-ceph-daemon-running" class="headerlink" title="Check if there is ceph daemon running"></a>Check if there is ceph daemon running</h2><pre><code>[ceph: root@host1 /]$ ceph orch ps host3
No daemons reported
[ceph: root@host1 /]$ ceph orch ps host2
No daemons reported
[ceph: root@host1 /]$ ceph orch ps host1
No daemons reported
</code></pre>
<h2 id="Remove-the-ceph-storage-cluster"><a href="#Remove-the-ceph-storage-cluster" class="headerlink" title="Remove the ceph storage cluster"></a>Remove the ceph storage cluster</h2><pre><code>[root@host1 ~]$ cephadm rm-cluster --fsid fec2332e-1b0b-11ec-abbe-ac1f6bc8d268 --force
[root@host1 ~]$ cephadm ls
[]
</code></pre>
<h2 id="Cleanup-the-ceph-configuration-files"><a href="#Cleanup-the-ceph-configuration-files" class="headerlink" title="Cleanup the ceph configuration files"></a>Cleanup the ceph configuration files</h2><pre><code>[root@host1 ~]$ rm -rf /etc/ceph
[root@host1 ~]$ rm -rf /var/lib/ce
ceph/       cephadm/    certmonger/
[root@host1 ~]$ rm -rf /var/lib/ceph*
</code></pre>
<h2 id="Cleanup-the-ceph-block-devices"><a href="#Cleanup-the-ceph-block-devices" class="headerlink" title="Cleanup the ceph block devices"></a>Cleanup the ceph block devices</h2><p>Do the following on each cluster node.</p>
<pre><code>[root@host1 ~]$ lsblk
NAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
nvme0n1                                                                                               259:0    0  3.5T  0 disk
├─nvme0n1p3                                                                                           259:4    0  3.5T  0 part
│ ├─vgroot-lvswap01                                                                                   253:1    0    4G  0 lvm
│ └─vgroot-lvroot                                                                                     253:0    0  3.5T  0 lvm  /
├─nvme0n1p1                                                                                           259:2    0    1G  0 part /boot/efi
└─nvme0n1p2                                                                                           259:3    0  500M  0 part /boot
nvme3n1                                                                                               259:6    0  3.5T  0 disk
└─ceph--ab144c40--73d6--49bc--921b--65025c383bb1-osd--block--2b965e29--b194--4363--8c96--20ab5b97db33 253:3    0  3.5T  0 lvm
nvme2n1                                                                                               259:5    0  3.5T  0 disk
└─ceph--b1ffe76d--1043--43a2--848b--6ba117e71a75-osd--block--0d6ff85d--9c49--43a0--98a3--c519fbb20b9c 253:4    0  3.5T  0 lvm
nvme1n1                                                                                               259:1    0  3.5T  0 disk

[root@host1 ~]$ for i in `seq 2 9`; do dd if=/dev/zero of=/dev/nvme$&#123;i&#125;n1 bs=1M count=1000; done
[root@host1 ~]$ reboot
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title>How to upgrade kernel version in centos 7</title>
    <url>/blog/upgrade-kernel-version-in-centos-7/</url>
    <content><![CDATA[<h2 id="Before-upgrade"><a href="#Before-upgrade" class="headerlink" title="Before upgrade"></a>Before upgrade</h2><pre><code>$ cat /etc/centos-release
CentOS Linux release 7.4.1708 (Core)

$ uname -r
3.10.0-693.el7.x86_64

$ cat /boot/grub2/grubenv
saved_entry=CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core)
</code></pre>
<h2 id="Install-the-newer-kernel-package"><a href="#Install-the-newer-kernel-package" class="headerlink" title="Install the newer kernel package"></a>Install the newer kernel package</h2><pre><code>$ rpm -ivh kernel-ml-5.7.12-1.el7.elrepo.x86_64.rpm
warning: kernel-ml-5.7.12-1.el7.elrepo.x86_64.rpm: Header V4 DSA/SHA1 Signature, key ID baadae52: NOKEY
Preparing...                          ################################# [100%]
Updating / installing...
   1:kernel-ml-5.7.12-1.el7.elrepo    ################################# [100%]

$ rpm -ivh kernel-ml-devel-5.7.12-1.el7.elrepo.x86_64.rpm
warning: kernel-ml-devel-5.7.12-1.el7.elrepo.x86_64.rpm: Header V4 DSA/SHA1 Signature, key ID baadae52: NOKEY
Preparing...                          ################################# [100%]
Updating / installing...
   1:kernel-ml-devel-5.7.12-1.el7.elre################################# [100%]

$  rpm -qa | grep kernel
kernel-ml-devel-5.7.12-1.el7.elrepo.x86_64
kernel-3.10.0-693.el7.x86_64
kernel-tools-3.10.0-693.el7.x86_64
kernel-ml-5.7.12-1.el7.elrepo.x86_64
kernel-tools-libs-3.10.0-693.el7.x86_64
</code></pre>
<h2 id="Set-the-default-boot-kernel-entry"><a href="#Set-the-default-boot-kernel-entry" class="headerlink" title="Set the default boot kernel entry"></a>Set the default boot kernel entry</h2><p><strong>Notes:</strong> if &#x2F;etc&#x2F;grub2.cfg does not exist, do next step to rebuild the GRUB2 and come back later after that.</p>
<pre><code>$ awk -F\&#39; &#39;$1==&quot;menuentry &quot; &#123;print $2&#125;&#39; /etc/grub2.cfg
CentOS Linux (5.7.12-1.el7.elrepo.x86_64) 7 (Core)
CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core)
CentOS Linux (0-rescue-8ef36acf9f544b90bf0621450fe05f75) 7 (Core)

$ grub2-set-default 0 
$ grep saved /boot/grub2/grubenv
saved_entry=0
</code></pre>
<h2 id="Rebuild-the-GRUB2-configuration"><a href="#Rebuild-the-GRUB2-configuration" class="headerlink" title="Rebuild the GRUB2 configuration"></a>Rebuild the GRUB2 configuration</h2><pre><code>$ grub2-mkconfig -o /boot/grub2/grub.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-5.7.12-1.el7.elrepo.x86_64
Found initrd image: /boot/initramfs-5.7.12-1.el7.elrepo.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-693.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-693.el7.x86_64.img
Found linux image: /boot/vmlinuz-0-rescue-8ef36acf9f544b90bf0621450fe05f75
Found initrd image: /boot/initramfs-0-rescue-8ef36acf9f544b90bf0621450fe05f75.img
done
</code></pre>
<h2 id="Reboot-the-system"><a href="#Reboot-the-system" class="headerlink" title="Reboot the system"></a>Reboot the system</h2><pre><code>$ reboot
</code></pre>
<h2 id="Verify-the-new-kernel-version"><a href="#Verify-the-new-kernel-version" class="headerlink" title="Verify the new kernel version"></a>Verify the new kernel version</h2><pre><code>$ uname -r
5.7.12-1.el7.elrepo.x86_64
$ cat /etc/centos-release
CentOS Linux release 7.4.1708 (Core)
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.golinuxcloud.com/set-default-boot-kernel-version-old-previous-rhel-linux/">https://www.golinuxcloud.com/set-default-boot-kernel-version-old-previous-rhel-linux/</a></li>
<li><a href="https://www.tecmint.com/install-upgrade-kernel-version-in-centos-7/">https://www.tecmint.com/install-upgrade-kernel-version-in-centos-7/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Kernel</tag>
      </tags>
  </entry>
  <entry>
    <title>How to upgrade Linux kernel on CentOS 7</title>
    <url>/blog/upgrade-linux-kernel-on-centos-7/</url>
    <content><![CDATA[<h2 id="Before-upgrade"><a href="#Before-upgrade" class="headerlink" title="Before upgrade"></a>Before upgrade</h2><pre><code>$ cat /etc/centos-release
CentOS Linux release 7.4.1708 (Core)

$ uname -r
3.10.0-693.el7.x86_64

$ cat /boot/grub2/grubenv
saved_entry=CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core)
</code></pre>
<h2 id="Install-the-newer-kernel-package"><a href="#Install-the-newer-kernel-package" class="headerlink" title="Install the newer kernel package"></a>Install the newer kernel package</h2><pre><code>$ rpm -ivh kernel-ml-5.7.12-1.el7.elrepo.x86_64.rpm
warning: kernel-ml-5.7.12-1.el7.elrepo.x86_64.rpm: Header V4 DSA/SHA1 Signature, key ID baadae52: NOKEY
Preparing...                          ################################# [100%]
Updating / installing...
   1:kernel-ml-5.7.12-1.el7.elrepo    ################################# [100%]

$ rpm -ivh kernel-ml-devel-5.7.12-1.el7.elrepo.x86_64.rpm
warning: kernel-ml-devel-5.7.12-1.el7.elrepo.x86_64.rpm: Header V4 DSA/SHA1 Signature, key ID baadae52: NOKEY
Preparing...                          ################################# [100%]
Updating / installing...
   1:kernel-ml-devel-5.7.12-1.el7.elre################################# [100%]

$  rpm -qa | grep kernel
kernel-ml-devel-5.7.12-1.el7.elrepo.x86_64
kernel-3.10.0-693.el7.x86_64
kernel-tools-3.10.0-693.el7.x86_64
kernel-ml-5.7.12-1.el7.elrepo.x86_64
kernel-tools-libs-3.10.0-693.el7.x86_64
</code></pre>
<h2 id="Set-the-default-boot-kernel-entry"><a href="#Set-the-default-boot-kernel-entry" class="headerlink" title="Set the default boot kernel entry"></a>Set the default boot kernel entry</h2><p><strong>Notes:</strong> if &#x2F;etc&#x2F;grub2.cfg does not exist, do next step to rebuild the GRUB2 and come back later after that.</p>
<pre><code>$ awk -F\&#39; &#39;$1==&quot;menuentry &quot; &#123;print $2&#125;&#39; /etc/grub2.cfg
CentOS Linux (5.7.12-1.el7.elrepo.x86_64) 7 (Core)
CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core)
CentOS Linux (0-rescue-8ef36acf9f544b90bf0621450fe05f75) 7 (Core)

$ grub2-set-default 0 
$ grep saved /boot/grub2/grubenv
saved_entry=0
</code></pre>
<h2 id="Rebuild-the-GRUB2-configuration"><a href="#Rebuild-the-GRUB2-configuration" class="headerlink" title="Rebuild the GRUB2 configuration"></a>Rebuild the GRUB2 configuration</h2><pre><code>$ grub2-mkconfig -o /boot/grub2/grub.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-5.7.12-1.el7.elrepo.x86_64
Found initrd image: /boot/initramfs-5.7.12-1.el7.elrepo.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-693.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-693.el7.x86_64.img
Found linux image: /boot/vmlinuz-0-rescue-8ef36acf9f544b90bf0621450fe05f75
Found initrd image: /boot/initramfs-0-rescue-8ef36acf9f544b90bf0621450fe05f75.img
done
</code></pre>
<h2 id="Reboot-the-system"><a href="#Reboot-the-system" class="headerlink" title="Reboot the system"></a>Reboot the system</h2><pre><code>$ reboot
</code></pre>
<h2 id="Verify-the-new-kernel-version"><a href="#Verify-the-new-kernel-version" class="headerlink" title="Verify the new kernel version"></a>Verify the new kernel version</h2><pre><code>$ uname -r
5.7.12-1.el7.elrepo.x86_64
$ cat /etc/centos-release
CentOS Linux release 7.4.1708 (Core)
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.golinuxcloud.com/set-default-boot-kernel-version-old-previous-rhel-linux/">https://www.golinuxcloud.com/set-default-boot-kernel-version-old-previous-rhel-linux/</a></li>
<li><a href="https://www.tecmint.com/install-upgrade-kernel-version-in-centos-7/">https://www.tecmint.com/install-upgrade-kernel-version-in-centos-7/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Kernel</tag>
      </tags>
  </entry>
  <entry>
    <title>Useful Elastic cluster APIs</title>
    <url>/blog/useful-es-apis/</url>
    <content><![CDATA[<h2 id="Cluster-settings"><a href="#Cluster-settings" class="headerlink" title="Cluster settings"></a>Cluster settings</h2><pre><code>$ curl -q -s http://10.10.10.1:39200/_cluster/health
&#123;
  &quot;cluster_name&quot;: &quot;rally-benchmark&quot;,
  &quot;status&quot;: &quot;green&quot;,
  &quot;timed_out&quot;: false,
  &quot;number_of_nodes&quot;: 3,
  &quot;number_of_data_nodes&quot;: 3,
  &quot;active_primary_shards&quot;: 8,
  &quot;active_shards&quot;: 16,
  &quot;relocating_shards&quot;: 0,
  &quot;initializing_shards&quot;: 0,
  &quot;unassigned_shards&quot;: 0,
  &quot;delayed_unassigned_shards&quot;: 0,
  &quot;number_of_pending_tasks&quot;: 0,
  &quot;number_of_in_flight_fetch&quot;: 0,
  &quot;task_max_waiting_in_queue_millis&quot;: 0,
  &quot;active_shards_percent_as_number&quot;: 100
&#125;


$ curl -q -s http://10.10.10.1:39200/_cluster/settings| jq -r
&#123;
  &quot;persistent&quot;: &#123;&#125;,
  &quot;transient&quot;: &#123;&#125;
&#125;


$ curl -q -s http://10.10.10.1:39200/_cluster/stats| jq -r
&#123;
  &quot;_nodes&quot;: &#123;
    &quot;total&quot;: 3,
    &quot;successful&quot;: 3,
    &quot;failed&quot;: 0
  &#125;,
  &quot;cluster_name&quot;: &quot;rally-benchmark&quot;,
  &quot;cluster_uuid&quot;: &quot;_aPIWBOCS0Gtyo4fTp3Wrg&quot;,
  &quot;timestamp&quot;: 1667177008901,
  &quot;status&quot;: &quot;green&quot;,
  &quot;indices&quot;: &#123;
    &quot;count&quot;: 4,
    &quot;shards&quot;: &#123;
      &quot;total&quot;: 16,
      &quot;primaries&quot;: 8,
      &quot;replication&quot;: 1,
      &quot;index&quot;: &#123;
        &quot;shards&quot;: &#123;
          &quot;min&quot;: 2,
          &quot;max&quot;: 10,
          &quot;avg&quot;: 4
        &#125;,
        &quot;primaries&quot;: &#123;
          &quot;min&quot;: 1,
          &quot;max&quot;: 5,
          &quot;avg&quot;: 2
        &#125;,
        &quot;replication&quot;: &#123;
          &quot;min&quot;: 1,
          &quot;max&quot;: 1,
          &quot;avg&quot;: 1
        &#125;
      &#125;
    &#125;,
    &quot;docs&quot;: &#123;
      &quot;count&quot;: 11396556,
      &quot;deleted&quot;: 0
    &#125;,
    &quot;store&quot;: &#123;
      &quot;size_in_bytes&quot;: 6443644350,
      &quot;total_data_set_size_in_bytes&quot;: 6443644350,
      &quot;reserved_in_bytes&quot;: 0
    &#125;,
    &quot;fielddata&quot;: &#123;
      &quot;memory_size_in_bytes&quot;: 14072,
      &quot;evictions&quot;: 0
    &#125;,
    &quot;query_cache&quot;: &#123;
      &quot;memory_size_in_bytes&quot;: 0,
      &quot;total_count&quot;: 3883795,
      &quot;hit_count&quot;: 0,
      &quot;miss_count&quot;: 3883795,
      &quot;cache_size&quot;: 0,
      &quot;cache_count&quot;: 0,
      &quot;evictions&quot;: 0
    &#125;,
    &quot;completion&quot;: &#123;
      &quot;size_in_bytes&quot;: 0
    &#125;,
    &quot;segments&quot;: &#123;
      &quot;count&quot;: 219,
      &quot;memory_in_bytes&quot;: 1526988,
      &quot;terms_memory_in_bytes&quot;: 1189120,
      &quot;stored_fields_memory_in_bytes&quot;: 115320,
      &quot;term_vectors_memory_in_bytes&quot;: 0,
      &quot;norms_memory_in_bytes&quot;: 155520,
      &quot;points_memory_in_bytes&quot;: 0,
      &quot;doc_values_memory_in_bytes&quot;: 67028,
      &quot;index_writer_memory_in_bytes&quot;: 0,
      &quot;version_map_memory_in_bytes&quot;: 0,
      &quot;fixed_bit_set_memory_in_bytes&quot;: 0,
      &quot;max_unsafe_auto_id_timestamp&quot;: 1667016435644,
      &quot;file_sizes&quot;: &#123;&#125;
    &#125;,
    &quot;mappings&quot;: &#123;
      &quot;field_types&quot;: [
        &#123;
          &quot;name&quot;: &quot;boolean&quot;,
          &quot;count&quot;: 1,
          &quot;index_count&quot;: 1,
          &quot;script_count&quot;: 0
        &#125;,
        &#123;
          &quot;name&quot;: &quot;constant_keyword&quot;,
          &quot;count&quot;: 3,
          &quot;index_count&quot;: 1,
          &quot;script_count&quot;: 0
        &#125;,
        &#123;
          &quot;name&quot;: &quot;date&quot;,
          &quot;count&quot;: 6,
          &quot;index_count&quot;: 2,
          &quot;script_count&quot;: 0
        &#125;,
        &#123;
          &quot;name&quot;: &quot;geo_point&quot;,
          &quot;count&quot;: 1,
          &quot;index_count&quot;: 1,
          &quot;script_count&quot;: 0
        &#125;,
        &#123;
          &quot;name&quot;: &quot;integer&quot;,
          &quot;count&quot;: 1,
          &quot;index_count&quot;: 1,
          &quot;script_count&quot;: 0
        &#125;,
        &#123;
          &quot;name&quot;: &quot;ip&quot;,
          &quot;count&quot;: 1,
          &quot;index_count&quot;: 1,
          &quot;script_count&quot;: 0
        &#125;,
        &#123;
          &quot;name&quot;: &quot;keyword&quot;,
          &quot;count&quot;: 32,
          &quot;index_count&quot;: 3,
          &quot;script_count&quot;: 0
        &#125;,
        &#123;
          &quot;name&quot;: &quot;long&quot;,
          &quot;count&quot;: 3,
          &quot;index_count&quot;: 2,
          &quot;script_count&quot;: 0
        &#125;,
        &#123;
          &quot;name&quot;: &quot;object&quot;,
          &quot;count&quot;: 13,
          &quot;index_count&quot;: 2,
          &quot;script_count&quot;: 0
        &#125;,
        &#123;
          &quot;name&quot;: &quot;text&quot;,
          &quot;count&quot;: 17,
          &quot;index_count&quot;: 3,
          &quot;script_count&quot;: 0
        &#125;
      ],
      &quot;runtime_field_types&quot;: []
    &#125;,
    &quot;analysis&quot;: &#123;
      &quot;char_filter_types&quot;: [],
      &quot;tokenizer_types&quot;: [],
      &quot;filter_types&quot;: [],
      &quot;analyzer_types&quot;: [],
      &quot;built_in_char_filters&quot;: [],
      &quot;built_in_tokenizers&quot;: [],
      &quot;built_in_filters&quot;: [],
      &quot;built_in_analyzers&quot;: []
    &#125;,
    &quot;versions&quot;: [
      &#123;
        &quot;version&quot;: &quot;7.17.0&quot;,
        &quot;index_count&quot;: 4,
        &quot;primary_shard_count&quot;: 8,
        &quot;total_primary_bytes&quot;: 3221823908
      &#125;
    ]
  &#125;,
  &quot;nodes&quot;: &#123;
    &quot;count&quot;: &#123;
      &quot;total&quot;: 3,
      &quot;coordinating_only&quot;: 0,
      &quot;data&quot;: 3,
      &quot;data_cold&quot;: 3,
      &quot;data_content&quot;: 3,
      &quot;data_frozen&quot;: 3,
      &quot;data_hot&quot;: 3,
      &quot;data_warm&quot;: 3,
      &quot;ingest&quot;: 3,
      &quot;master&quot;: 3,
      &quot;ml&quot;: 0,
      &quot;remote_cluster_client&quot;: 3,
      &quot;transform&quot;: 3,
      &quot;voting_only&quot;: 0
    &#125;,
    &quot;versions&quot;: [
      &quot;7.17.0&quot;
    ],
    &quot;os&quot;: &#123;
      &quot;available_processors&quot;: 24,
      &quot;allocated_processors&quot;: 24,
      &quot;names&quot;: [
        &#123;
          &quot;name&quot;: &quot;Linux&quot;,
          &quot;count&quot;: 3
        &#125;
      ],
      &quot;pretty_names&quot;: [
        &#123;
          &quot;pretty_name&quot;: &quot;CentOS Linux 7 (Core)&quot;,
          &quot;count&quot;: 3
        &#125;
      ],
      &quot;architectures&quot;: [
        &#123;
          &quot;arch&quot;: &quot;amd64&quot;,
          &quot;count&quot;: 3
        &#125;
      ],
      &quot;mem&quot;: &#123;
        &quot;total_in_bytes&quot;: 100700000256,
        &quot;free_in_bytes&quot;: 68239511552,
        &quot;used_in_bytes&quot;: 32460488704,
        &quot;free_percent&quot;: 68,
        &quot;used_percent&quot;: 32
      &#125;
    &#125;,
    &quot;process&quot;: &#123;
      &quot;cpu&quot;: &#123;
        &quot;percent&quot;: 0
      &#125;,
      &quot;open_file_descriptors&quot;: &#123;
        &quot;min&quot;: 460,
        &quot;max&quot;: 493,
        &quot;avg&quot;: 474
      &#125;
    &#125;,
    &quot;jvm&quot;: &#123;
      &quot;max_uptime_in_millis&quot;: 169991344,
      &quot;versions&quot;: [
        &#123;
          &quot;version&quot;: &quot;1.8.0_352&quot;,
          &quot;vm_name&quot;: &quot;OpenJDK 64-Bit Server VM&quot;,
          &quot;vm_version&quot;: &quot;25.352-b08&quot;,
          &quot;vm_vendor&quot;: &quot;Red Hat, Inc.&quot;,
          &quot;bundled_jdk&quot;: true,
          &quot;using_bundled_jdk&quot;: false,
          &quot;count&quot;: 3
        &#125;
      ],
      &quot;mem&quot;: &#123;
        &quot;heap_used_in_bytes&quot;: 1671452984,
        &quot;heap_max_in_bytes&quot;: 3113877504
      &#125;,
      &quot;threads&quot;: 197
    &#125;,
    &quot;fs&quot;: &#123;
      &quot;total_in_bytes&quot;: 126144983040,
      &quot;free_in_bytes&quot;: 115344986112,
      &quot;available_in_bytes&quot;: 115344986112
    &#125;,
    &quot;plugins&quot;: [],
    &quot;network_types&quot;: &#123;
      &quot;transport_types&quot;: &#123;
        &quot;netty4&quot;: 3
      &#125;,
      &quot;http_types&quot;: &#123;
        &quot;netty4&quot;: 3
      &#125;
    &#125;,
    &quot;discovery_types&quot;: &#123;
      &quot;zen&quot;: 3
    &#125;,
    &quot;packaging_types&quot;: [
      &#123;
        &quot;flavor&quot;: &quot;default&quot;,
        &quot;type&quot;: &quot;tar&quot;,
        &quot;count&quot;: 3
      &#125;
    ],
    &quot;ingest&quot;: &#123;
      &quot;number_of_pipelines&quot;: 2,
      &quot;processor_stats&quot;: &#123;
        &quot;gsub&quot;: &#123;
          &quot;count&quot;: 0,
          &quot;failed&quot;: 0,
          &quot;current&quot;: 0,
          &quot;time_in_millis&quot;: 0
        &#125;,
        &quot;script&quot;: &#123;
          &quot;count&quot;: 0,
          &quot;failed&quot;: 0,
          &quot;current&quot;: 0,
          &quot;time_in_millis&quot;: 0
        &#125;
      &#125;
    &#125;
  &#125;
&#125;
</code></pre>
<h2 id="Node-stats"><a href="#Node-stats" class="headerlink" title="Node stats"></a>Node stats</h2><pre><code>$ curl -q -s http://10.10.10.1:39200/_nodes
$ curl -q -s http://10.10.10.1:39200/_nodes/usage
$ curl -q -s http://10.10.10.2:39200/_nodes/stats/fs?pretty
&#123;
  &quot;_nodes&quot; : &#123;
    &quot;total&quot; : 3,
    &quot;successful&quot; : 3,
    &quot;failed&quot; : 0
  &#125;,
  &quot;cluster_name&quot; : &quot;rally-benchmark&quot;,
  &quot;nodes&quot; : &#123;
    &quot;Zyv-MlSUQeOpU-__o2NblA&quot; : &#123;
      &quot;timestamp&quot; : 1667082588000,
      &quot;name&quot; : &quot;rally-node-1&quot;,
      &quot;transport_address&quot; : &quot;10.10.10.2:39300&quot;,
      &quot;host&quot; : &quot;10.10.10.2&quot;,
      &quot;ip&quot; : &quot;10.10.10.2:39300&quot;,
      &quot;roles&quot; : [
        &quot;data&quot;,
        &quot;data_cold&quot;,
        &quot;data_content&quot;,
        &quot;data_frozen&quot;,
        &quot;data_hot&quot;,
        &quot;data_warm&quot;,
        &quot;ingest&quot;,
        &quot;master&quot;,
        &quot;remote_cluster_client&quot;,
        &quot;transform&quot;
      ],
      &quot;attributes&quot; : &#123;
        &quot;xpack.installed&quot; : &quot;true&quot;,
        &quot;transform.node&quot; : &quot;true&quot;
      &#125;,
      &quot;fs&quot; : &#123;
        &quot;timestamp&quot; : 1667082588001,
        &quot;total&quot; : &#123;
          &quot;total_in_bytes&quot; : 42048327680,
          &quot;free_in_bytes&quot; : 39852097536,
          &quot;available_in_bytes&quot; : 39852097536
        &#125;,
        &quot;data&quot; : [
          &#123;
            &quot;path&quot; : &quot;/home/es/.rally/benchmarks/races/53e2ac0b-c648-4153-97d4-46749f90daed/rally-node-1/install/elasticsearch-7.17.0/data/nodes/0&quot;,
            &quot;mount&quot; : &quot;/home (/dev/mapper/centos-home)&quot;,
            &quot;type&quot; : &quot;xfs&quot;,
            &quot;total_in_bytes&quot; : 42048327680,
            &quot;free_in_bytes&quot; : 39852097536,
            &quot;available_in_bytes&quot; : 39852097536
          &#125;
        ],
        &quot;io_stats&quot; : &#123;
          &quot;devices&quot; : [
            &#123;
              &quot;device_name&quot; : &quot;dm-2&quot;,
              &quot;operations&quot; : 52115,
              &quot;read_operations&quot; : 0,
              &quot;write_operations&quot; : 52115,
              &quot;read_kilobytes&quot; : 0,
              &quot;write_kilobytes&quot; : 7926721,
              &quot;io_time_in_millis&quot; : 8549
            &#125;
          ],
          &quot;total&quot; : &#123;
            &quot;operations&quot; : 52115,
            &quot;read_operations&quot; : 0,
            &quot;write_operations&quot; : 52115,
            &quot;read_kilobytes&quot; : 0,
            &quot;write_kilobytes&quot; : 7926721,
            &quot;io_time_in_millis&quot; : 8549
          &#125;
        &#125;
      &#125;
    &#125;,
    [..]
  &#125;
&#125;
</code></pre>
<p>The data path can also be checked in node-config.json.</p>
<pre><code>$ cat races/aa826112-d371-4f09-9b68-f9084e7c9e0b/node-config.json
&#123;
  &quot;build-type&quot;: &quot;tar&quot;,
  &quot;car-runtime-jdks&quot;: &quot;17,16,15,14,13,12,11,8&quot;,
  &quot;car-provides-bundled-jdk&quot;: true,
  &quot;ip&quot;: &quot;10.10.10.1&quot;,
  &quot;node-name&quot;: &quot;rally-node-0&quot;,
  &quot;node-root-path&quot;: &quot;/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0&quot;,
  &quot;binary-path&quot;: &quot;/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0&quot;,
  &quot;data-paths&quot;: [
    &quot;/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/data&quot;
  ]
&#125;
</code></pre>
<h2 id="Shards-and-Replicas"><a href="#Shards-and-Replicas" class="headerlink" title="Shards and Replicas"></a>Shards and Replicas</h2><p>List all the available shards</p>
<pre><code>$ curl -q -s http://10.10.10.1:39200/_cat/shards
.geoip_databases                                              0 p STARTED      41  39.1mb 10.10.10.2 rally-node-1
.geoip_databases                                              0 r STARTED      41  39.1mb 10.10.10.3 rally-node-2
geonames                                                      4 p STARTED 2279907 610.1mb 10.10.10.1  rally-node-0
geonames                                                      3 p STARTED 2282121 605.1mb 10.10.10.2 rally-node-1
geonames                                                      2 p STARTED 2280777 603.3mb 10.10.10.3 rally-node-2
geonames                                                      1 p STARTED 2277042 609.3mb 10.10.10.1  rally-node-0
geonames                                                      0 p STARTED 2276656 605.3mb 10.10.10.2 rally-node-1
.ds-.logs-deprecation.elasticsearch-default-2022.10.29-000001 0 r STARTED                 10.10.10.1  rally-node-0
.ds-.logs-deprecation.elasticsearch-default-2022.10.29-000001 0 p STARTED                 10.10.10.3 rally-node-2
.ds-ilm-history-5-2022.10.29-000001                           0 r STARTED                 10.10.10.2 rally-node-1
.ds-ilm-history-5-2022.10.29-000001                           0 p STARTED                 10.10.10.1  rally-node-0                        0 p STARTED                 10.10.10.1  rally-node-0
</code></pre>
<p>List all the available shards for the target index</p>
<p>By default, there are 5 primary shards for the target index.</p>
<pre><code>$ curl -q -s http://10.10.10.1:39200/_cat/shards/geonames
geonames 4 p STARTED 2279907 610.1mb 10.10.10.1 rally-node-0
geonames 3 p STARTED 2282121 605.1mb 10.10.10.2 rally-node-1
geonames 2 p STARTED 2280777 603.3mb 10.10.10.3 rally-node-2
geonames 1 p STARTED 2277042 609.3mb 10.10.10.1 rally-node-0
geonames 0 p STARTED 2276656 605.3mb 10.10.10.2 rally-node-1
</code></pre>
<p>Check the settings of target index</p>
<p>By default, there are 5 primary shards and no replica shards for the target index.</p>
<pre><code>$ curl -q -s http://10.10.10.1:39200/geonames/_settings | jq -r
&#123;
  &quot;geonames&quot;: &#123;
    &quot;settings&quot;: &#123;
      &quot;index&quot;: &#123;
        &quot;routing&quot;: &#123;
          &quot;allocation&quot;: &#123;
            &quot;include&quot;: &#123;
              &quot;_tier_preference&quot;: &quot;data_content&quot;
            &#125;
          &#125;
        &#125;,
        &quot;number_of_shards&quot;: &quot;5&quot;,
        &quot;provided_name&quot;: &quot;geonames&quot;,
        &quot;creation_date&quot;: &quot;1667016351070&quot;,
        &quot;requests&quot;: &#123;
          &quot;cache&quot;: &#123;
            &quot;enable&quot;: &quot;false&quot;
          &#125;
        &#125;,
        &quot;store&quot;: &#123;
          &quot;type&quot;: &quot;fs&quot;
        &#125;,
        &quot;number_of_replicas&quot;: &quot;0&quot;,
        &quot;uuid&quot;: &quot;ecaI9RlcSvqu0Lwo4f3nOA&quot;,
        &quot;version&quot;: &#123;
          &quot;created&quot;: &quot;7170099&quot;
        &#125;
      &#125;
    &#125;
  &#125;
&#125;
</code></pre>
<p>Change number_of_replicas for the target index</p>
<pre><code>$ curl -X PUT -H &quot;Content-Type: application/json&quot; &quot;http://10.10.10.1:39200/geonames/_settings&quot; -d &#39;&#123;&quot;number_of_replicas&quot;:1&#125;&#39;
&#123;&quot;acknowledged&quot;:true&#125;

$ curl -q -s http://10.10.10.1:39200/geonames/_settings | jq -r
&#123;
  &quot;geonames&quot;: &#123;
    &quot;settings&quot;: &#123;
      &quot;index&quot;: &#123;
        &quot;routing&quot;: &#123;
          &quot;allocation&quot;: &#123;
            &quot;include&quot;: &#123;
              &quot;_tier_preference&quot;: &quot;data_content&quot;
            &#125;
          &#125;
        &#125;,
        &quot;number_of_shards&quot;: &quot;5&quot;,
        &quot;provided_name&quot;: &quot;geonames&quot;,
        &quot;creation_date&quot;: &quot;1667016351070&quot;,
        &quot;requests&quot;: &#123;
          &quot;cache&quot;: &#123;
            &quot;enable&quot;: &quot;false&quot;
          &#125;
        &#125;,
        &quot;store&quot;: &#123;
          &quot;type&quot;: &quot;fs&quot;
        &#125;,
        &quot;number_of_replicas&quot;: &quot;1&quot;,
        &quot;uuid&quot;: &quot;ecaI9RlcSvqu0Lwo4f3nOA&quot;,
        &quot;version&quot;: &#123;
          &quot;created&quot;: &quot;7170099&quot;
        &#125;
      &#125;
    &#125;
  &#125;
&#125;
</code></pre>
<p>Verify the index replicas</p>
<p>After changing number_of_replicas from 0 to 1, one replica shard is created for each primary shard. The primary shard and replica shard are located on different nodes. If a node goes down, the replica shards will automatically become primary shards and the cluster still works properly.</p>
<pre><code>$ curl -q -s http://10.10.10.1:39200/_cat/shards/geonames
geonames 3 p STARTED 2282121 605.1mb 10.10.10.2 rally-node-1
geonames 3 r STARTED 2282121 605.1mb 10.10.10.1 rally-node-0
geonames 4 r STARTED 2279907 610.1mb 10.10.10.2 rally-node-1
geonames 4 p STARTED 2279907 610.1mb 10.10.10.1 rally-node-0
geonames 2 r STARTED 2280777 603.3mb 10.10.10.2 rally-node-1
geonames 2 p STARTED 2280777 603.3mb 10.10.10.3 rally-node-2
geonames 1 p STARTED 2277042 609.3mb 10.10.10.1 rally-node-0
geonames 1 r STARTED 2277042 609.3mb 10.10.10.3 rally-node-2
geonames 0 p STARTED 2276656 605.3mb 10.10.10.2 rally-node-1
geonames 0 r STARTED 2276656 605.3mb 10.10.10.3 rally-node-2
</code></pre>
<h2 id="Task-management"><a href="#Task-management" class="headerlink" title="Task management"></a>Task management</h2><pre><code>$ curl -q -s http://10.10.10.1:39200/_tasks
&#123;
  &quot;nodes&quot;: &#123;
    &quot;Zyv-MlSUQeOpU-__o2NblA&quot;: &#123;
      &quot;name&quot;: &quot;rally-node-1&quot;,
      &quot;transport_address&quot;: &quot;10.10.10.2:39300&quot;,
      &quot;host&quot;: &quot;10.10.10.2&quot;,
      &quot;ip&quot;: &quot;10.10.10.2:39300&quot;,
      &quot;roles&quot;: [
        &quot;data&quot;,
        &quot;data_cold&quot;,
        &quot;data_content&quot;,
        &quot;data_frozen&quot;,
        &quot;data_hot&quot;,
        &quot;data_warm&quot;,
        &quot;ingest&quot;,
        &quot;master&quot;,
        &quot;remote_cluster_client&quot;,
        &quot;transform&quot;
      ],
      &quot;attributes&quot;: &#123;
        &quot;xpack.installed&quot;: &quot;true&quot;,
        &quot;transform.node&quot;: &quot;true&quot;
      &#125;,
      &quot;tasks&quot;: &#123;
        &quot;Zyv-MlSUQeOpU-__o2NblA:500850&quot;: &#123;
          &quot;node&quot;: &quot;Zyv-MlSUQeOpU-__o2NblA&quot;,
          &quot;id&quot;: 500850,
          &quot;type&quot;: &quot;transport&quot;,
          &quot;action&quot;: &quot;cluster:monitor/tasks/lists[n]&quot;,
          &quot;start_time_in_millis&quot;: 1667176733761,
          &quot;running_time_in_nanos&quot;: 125953,
          &quot;cancellable&quot;: false,
          &quot;parent_task_id&quot;: &quot;viZm_2hlQbib6qqccdXgNA:324323&quot;,
          &quot;headers&quot;: &#123;&#125;
        &#125;
      &#125;
    &#125;,
    [..]
  &#125;
&#125;
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html</a></li>
<li><a href="https://stackoverflow.com/questions/15694724/shards-and-replicas-in-elasticsearch">Shards and replics in Elasticsearch</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Elastic Search</tag>
      </tags>
  </entry>
  <entry>
    <title>Useful HBA commands</title>
    <url>/blog/useful-hba-commands/</url>
    <content><![CDATA[<h2 id="Check-HBA-card-type"><a href="#Check-HBA-card-type" class="headerlink" title="Check HBA card type"></a>Check HBA card type</h2><pre><code>$  lspci | grep -i fibre
18:00.0 Fibre Channel: QLogic Corp. ISP2722-based 16/32Gb Fibre Channel to PCIe Adapter (rev 01)
18:00.1 Fibre Channel: QLogic Corp. ISP2722-based 16/32Gb Fibre Channel to PCIe Adapter (rev 01)
</code></pre>
<h2 id="Physical-slot-driver-module-information"><a href="#Physical-slot-driver-module-information" class="headerlink" title="Physical slot, driver, module information"></a>Physical slot, driver, module information</h2><pre><code>$  lspci -v -s 18:00.0
18:00.0 Fibre Channel: QLogic Corp. ISP2722-based 16/32Gb Fibre Channel to PCIe Adapter (rev 01)
    Subsystem: QLogic Corp. QLE2692 Dual Port 16Gb Fibre Channel to PCIe Adapter
    Flags: bus master, fast devsel, latency 0, IRQ 231, NUMA node 0
    Memory at aab05000 (64-bit, prefetchable) [size=4K]
    Memory at aab02000 (64-bit, prefetchable) [size=8K]
    Memory at aaa00000 (64-bit, prefetchable) [size=1M]
    Expansion ROM at 9d900000 [disabled] [size=256K]
    Capabilities: [44] Power Management version 3
    Capabilities: [4c] Express Endpoint, MSI 00
    Capabilities: [88] Vital Product Data
    Capabilities: [90] MSI-X: Enable+ Count=16 Masked-
    Capabilities: [9c] Vendor Specific Information: Len=0c &lt;?&gt;
    Capabilities: [100] Advanced Error Reporting
    Capabilities: [154] Alternative Routing-ID Interpretation (ARI)
    Capabilities: [1c0] #19
    Capabilities: [1f4] Vendor Specific Information: ID=0001 Rev=1 Len=014 &lt;?&gt;
    Kernel driver in use: qla2xxx
    Kernel modules: qla2xxx
</code></pre>
<h2 id="Check-if-the-driver-module-loaded"><a href="#Check-if-the-driver-module-loaded" class="headerlink" title="Check if the driver&#x2F;module loaded"></a>Check if the driver&#x2F;module loaded</h2><pre><code>$  lsmod | grep qla2xxx
qla2xxx               792059  246
nvme_fc                33640  1 qla2xxx
scsi_transport_fc      64007  1 qla2xxx
</code></pre>
<h2 id="Check-module-file-name"><a href="#Check-module-file-name" class="headerlink" title="Check module file name"></a>Check module file name</h2><pre><code>$  modinfo -n qla2xxx
/lib/modules/3.10.0-1062.12.1.el7.x86_64/kernel/drivers/scsi/qla2xxx/qla2xxx.ko
</code></pre>
<h2 id="Check-HBA-driver-and-version"><a href="#Check-HBA-driver-and-version" class="headerlink" title="Check HBA driver and version"></a>Check HBA driver and version</h2><pre><code>$  modinfo -d qla2xxx
QLogic Fibre Channel HBA Driver
QLogic Fibre Channel HBA Driver (Target Mode Support, including 24xx+ ISP)

$  modinfo qla2xxx | grep version
version:        8.07.00.34.Trunk-SCST.19-k
rhelversion:    7.7
srcversion:     B6770D64FCF8A0A0273AD2C
vermagic:       3.10.0-1062.12.1.el7.x86_64 SMP mod_unload modversions
</code></pre>
<h2 id="Check-if-the-running-driver-is-same-as-kernel"><a href="#Check-if-the-running-driver-is-same-as-kernel" class="headerlink" title="Check if the running driver is same as kernel"></a>Check if the running driver is same as kernel</h2><pre><code>$  modinfo -k `uname -r` -n qla2xxx
/lib/modules/3.10.0-1062.12.1.el7.x86_64/kernel/drivers/scsi/qla2xxx/qla2xxx.ko
</code></pre>
<h2 id="Remove-current-module-driver"><a href="#Remove-current-module-driver" class="headerlink" title="Remove current module&#x2F;driver"></a>Remove current module&#x2F;driver</h2><pre><code>$  modprobe -r qla2xxx
</code></pre>
<h2 id="Show-dependent-devices-modules"><a href="#Show-dependent-devices-modules" class="headerlink" title="Show dependent devices&#x2F;modules"></a>Show dependent devices&#x2F;modules</h2><pre><code>$  modprobe --show-depends qla2xxx
insmod /lib/modules/3.10.0-1062.12.1.el7.x86_64/kernel/drivers/scsi/scsi_tgt.ko.xz
insmod /lib/modules/3.10.0-1062.12.1.el7.x86_64/kernel/drivers/scsi/scsi_transport_fc.ko.xz
insmod /lib/modules/3.10.0-1062.12.1.el7.x86_64/kernel/drivers/scsi/qla2xxx/qla2xxx.ko ql2xfwloadbin=2
</code></pre>
<h2 id="Check-the-available-HBA-ports"><a href="#Check-the-available-HBA-ports" class="headerlink" title="Check the available HBA ports"></a>Check the available HBA ports</h2><pre><code>$  ls -l /sys/class/fc_host
total 0
lrwxrwxrwx. 1 root root 0 Mar  9 13:01 host2 -&gt; ../../devices/pci0000:17/0000:17:00.0/0000:18:00.0/host2/fc_host/host2
lrwxrwxrwx. 1 root root 0 Mar  9 13:01 host3 -&gt; ../../devices/pci0000:17/0000:17:00.0/0000:18:00.1/host3/fc_host/host3
</code></pre>
<h2 id="View-used-HBA-ports-on-server"><a href="#View-used-HBA-ports-on-server" class="headerlink" title="View used HBA ports on server"></a>View used HBA ports on server</h2><pre><code>$  ls -ltr /sys/class/fc_transport/
total 0
lrwxrwxrwx. 1 root root 0 Mar  9 13:01 target6:0:0 -&gt; ../../devices/pci0000:85/0000:85:02.0/0000:87:00.0/host6/rport-6:0-0/target6:0:0/fc_transport/target6:0:0
lrwxrwxrwx. 1 root root 0 Mar  9 13:01 target4:0:0 -&gt; ../../devices/pci0000:85/0000:85:00.0/0000:86:00.0/host4/rport-4:0-0/target4:0:0/fc_transport/target4:0:0
[...]
</code></pre>
<h2 id="Find-the-WWN-numbers-for-your-fc-host"><a href="#Find-the-WWN-numbers-for-your-fc-host" class="headerlink" title="Find the WWN numbers for your fc host"></a>Find the WWN numbers for your fc host</h2><pre><code>$  cat /sys/class/fc_host/host?/port_name
0x210034800d6baa70
0x210034800d6baa71
</code></pre>
<h2 id="Port-state"><a href="#Port-state" class="headerlink" title="Port state"></a>Port state</h2><pre><code>$  cat /sys/class/fc_host/host?/port_state
Online
Online
</code></pre>
<h2 id="Queue-depth"><a href="#Queue-depth" class="headerlink" title="Queue depth"></a>Queue depth</h2><pre><code>$  cat /sys/module/qla2xxx/parameters/ql2xmaxqdepth
64
</code></pre>
<p>To set&#x2F;change the qdepth value on the fly:</p>
<pre><code>$ echo 16 &gt; /sys/module/qla2xxx/parameters/ql2xmaxqdepth
</code></pre>
<p>To set the qdepth value permanently:</p>
<pre><code>$ modinfo qla2xxx | grep ql2xmaxqdepth
parm: ql2xmaxqdepth:Maximum queue depth to set for each LUN. Default is 32. (int)

$ vi /etc/modprobe.conf
alias scsi_hostadapter1 qla2xxx
options qla2xxx ql2xmaxqdepth=16
</code></pre>
<h2 id="Scan-disks"><a href="#Scan-disks" class="headerlink" title="Scan disks"></a>Scan disks</h2><pre><code>$  ls /sys/class/scsi_host
host1  host2  host3  host4  host5  host6  host7

$  for x in `ls /sys/class/scsi_host`
&gt; do
&gt; echo &quot;- - -&quot; &gt; /sys/class/scsi_host/$x/scan
&gt; done
</code></pre>
<h2 id="Check-number-of-LUNs"><a href="#Check-number-of-LUNs" class="headerlink" title="Check number of LUNs"></a>Check number of LUNs</h2><pre><code>$  pwd
/opt/QLogic_Corporation/QConvergeConsoleCLI

$ ./scli -t 
HBA Instance 1: QLE2692 Port 2 WWPN 21-00-34-80-0D-6B-AA-71 PortID 02-1D-00
Link: Online

Path                           : 0
Target                         : 0
Device ID                      : 0xffff
Product Vendor                 : PURE
Product ID                     : FlashArray
Product Revision               : 8888
Serial Number                  : 7CA43045E3514BF700011025
Node Name                      : 52-4A-93-77-BB-B4-74-03
Port Name                      : 52-4A-93-77-BB-B4-74-03
Port ID                        : 02-2C-00
Product Type                   : FCP Disk
LUN Count(s)                   : 16
Status                         : Online
</code></pre>
<h2 id="Check-HBA-speed"><a href="#Check-HBA-speed" class="headerlink" title="Check HBA speed"></a>Check HBA speed</h2><pre><code>$  grep -Hv &quot;zz&quot; /sys/class/fc_host/host*/speed
/sys/class/fc_host/host2/speed:16 Gbit
/sys/class/fc_host/host3/speed:16 Gbit
</code></pre>
<h2 id="Collect-QLogic-HBA-stats"><a href="#Collect-QLogic-HBA-stats" class="headerlink" title="Collect QLogic HBA stats"></a>Collect QLogic HBA stats</h2><pre><code>$  pwd
/opt/QLogic_Corporation/QConvergeConsoleCLI

$  ./qaucli -gs 8 SetRate 10 LogToFile /tmp/fs.stat.csv
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://linoxide.com/storage/scandetect-luns-redhat-linux-outputs-remember/">https://linoxide.com/storage/scandetect-luns-redhat-linux-outputs-remember/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>HBA</tag>
      </tags>
  </entry>
  <entry>
    <title>Useful tools in Linux operating system</title>
    <url>/blog/useful-tools-in-linux-operating-system/</url>
    <content><![CDATA[<h2 id="Performance-tools"><a href="#Performance-tools" class="headerlink" title="Performance tools"></a>Performance tools</h2><ul>
<li>vmstat</li>
<li>iostat</li>
<li>ifstat</li>
<li>netstat</li>
<li>nfsstat</li>
<li>mpstat</li>
<li>nstat</li>
<li>dstat</li>
<li>sar</li>
<li>iftop</li>
<li>pidstat</li>
<li>xosview</li>
</ul>
<h2 id="Benchmark-tools"><a href="#Benchmark-tools" class="headerlink" title="Benchmark tools"></a>Benchmark tools</h2><ul>
<li>fio</li>
<li>iozone</li>
<li>iperf</li>
<li>netperf</li>
<li>vdbench</li>
<li>sysbench</li>
<li>pgbench</li>
<li>YCSB</li>
<li>SPEC SFS 2014&#x2F;SPECstorage Solution 2020</li>
<li>VMmark</li>
</ul>
<h2 id="Debugging-tools"><a href="#Debugging-tools" class="headerlink" title="Debugging tools"></a>Debugging tools</h2><ul>
<li>htop</li>
<li>lslk</li>
<li>lsof</li>
<li>top</li>
</ul>
<h2 id="Process-tracing"><a href="#Process-tracing" class="headerlink" title="Process tracing"></a>Process tracing</h2><ul>
<li>ltrace</li>
<li>strace</li>
<li>pstack&#x2F;gstack</li>
<li>ftrace</li>
<li>systemtap</li>
<li>ps</li>
<li>pmap</li>
<li>blktrace</li>
<li>ebpf</li>
</ul>
<h2 id="Binary-debugging"><a href="#Binary-debugging" class="headerlink" title="Binary debugging"></a>Binary debugging</h2><ul>
<li>ldd</li>
<li>file</li>
<li>nm</li>
<li>objdump</li>
<li>readelf</li>
</ul>
<h2 id="Memory-usage-tools"><a href="#Memory-usage-tools" class="headerlink" title="Memory usage tools"></a>Memory usage tools</h2><ul>
<li>free</li>
<li>memusage</li>
<li>memusagestat</li>
<li>slabtop</li>
</ul>
<h2 id="Accounting-tools"><a href="#Accounting-tools" class="headerlink" title="Accounting tools"></a>Accounting tools</h2><ul>
<li>dump-acct</li>
<li>dump-utmp</li>
<li>sa</li>
</ul>
<h2 id="Hardware-debugging-tools"><a href="#Hardware-debugging-tools" class="headerlink" title="Hardware debugging tools"></a>Hardware debugging tools</h2><ul>
<li>dmidecode</li>
<li>ifinfo</li>
<li>lsdev</li>
<li>lshal</li>
<li>lshw</li>
<li>lsmod</li>
<li>lspci</li>
<li>lsusb</li>
<li>smartctl</li>
<li>x86info</li>
<li>&#x2F;opt&#x2F;QLogic_Corporation&#x2F;QConvergeConsoleCLI&#x2F;qaucli</li>
</ul>
<h2 id="Application-debugging"><a href="#Application-debugging" class="headerlink" title="Application debugging"></a>Application debugging</h2><ul>
<li>mailstats</li>
<li>qshape</li>
</ul>
<h2 id="Xorg-related-tools"><a href="#Xorg-related-tools" class="headerlink" title="Xorg related tools"></a>Xorg related tools</h2><ul>
<li>xdpyinfo</li>
<li>xrestop</li>
</ul>
<h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><ul>
<li>collectl</li>
<li>proc</li>
<li>procinfo</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Observability</tag>
      </tags>
  </entry>
  <entry>
    <title>Five simple tools that helps analyze network latency in Linux</title>
    <url>/blog/useful-tools-to-analyze-network-latency/</url>
    <content><![CDATA[<h2 id="ping"><a href="#ping" class="headerlink" title="ping"></a>ping</h2><p>Ping is one of the most basic commands in network management, verifying network connectivity through the roundtrip times taken by the ICMP protocol packets sent to a target host.</p>
<p>ping - send ICMP ECHO_REQUEST to network hosts</p>
<ul>
<li>-c count</li>
</ul>
<p>Stop after sending <em>count</em> ECHO_REQUEST packets. With deadline option, ping waits for count ECHO_REPLY packets, until the timeout expires.</p>
<ul>
<li>-i interval</li>
</ul>
<p>Wait interval seconds between sending each packet. The default is to wait for one  second  between each packet normally, or not to wait in flood mode. Only super-user may set interval to values less 0.2 seconds.</p>
<pre><code>$ ping 10.10.1.17 -c 1000 -i 0.010
PING 10.10.1.17 (10.10.1.17) 56(84) bytes of data.
64 bytes from 10.10.1.17: icmp_seq=1 ttl=64 time=0.176 ms
64 bytes from 10.10.1.17: icmp_seq=2 ttl=64 time=0.173 ms
&lt;omitted...&gt;
64 bytes from 10.10.1.17: icmp_seq=999 ttl=64 time=0.197 ms
64 bytes from 10.10.1.17: icmp_seq=1000 ttl=64 time=0.195 ms

--- 10.10.1.17 ping statistics ---
1000 packets transmitted, 1000 received, 0% packet loss, time 10992ms
rtt min/avg/max/mdev = 0.096/0.173/0.210/0.025 ms
</code></pre>
<p>Round-trip time (RTT) is the duration, measured in milliseconds, from when the source server sends a request to when it receives a response from a target server. It’s a key performance metric to measure network latency.</p>
<p>Actual round trip time can be influenced by:</p>
<ul>
<li>Distance – The length a signal has to travel correlates with the time taken for a request to reach a server.</li>
<li>Transmission medium – The medium used to route a signal (e.g., copper wire, fiber optic cables) can impact how quickly a request is received by a server and routed back to a user.</li>
<li>Number of network hops – Intermediate routers or servers take time to process a signal, increasing RTT. The more hops a signal has to travel through, the higher the RTT.</li>
<li>Traffic levels – RTT typically increases when a network is congested with high levels of traffic. Conversely, low traffic times can result in decreased RTT.</li>
<li>Server response time – The time taken for a target server to respond to a request depends on its processing capacity, the number of requests being handled and the nature of the request (i.e., how much server-side work is required). A longer server response time increases RTT.</li>
</ul>
<h2 id="traceroute"><a href="#traceroute" class="headerlink" title="traceroute"></a>traceroute</h2><p>A traceroute displays the path that the signal took as it traveled around the Internet to the website. It also displays times which are the response times that occurred at each stop along the route. If there is a connection problem or latency connecting to a site, it will show up in these times. You will be able to identify which of the stops (also called ‘hops’) along the route is the culprit.</p>
<pre><code>$ for i in `seq 1 5`; do traceroute 10.10.1.17;sleep 3; done
traceroute to 10.10.1.17 (10.10.1.17), 30 hops max, 60 byte packets
 1  10.10.1.17 (10.10.1.17)  0.181 ms  0.086 ms  0.084 ms
traceroute to 10.10.1.17 (10.10.1.17), 30 hops max, 60 byte packets
 1  10.10.1.17 (10.10.1.17)  0.179 ms  0.087 ms  0.081 ms
traceroute to 10.10.1.17 (10.10.1.17), 30 hops max, 60 byte packets
 1  10.10.1.17 (10.10.1.17)  0.175 ms  0.087 ms  0.081 ms
traceroute to 10.10.1.17 (10.10.1.17), 30 hops max, 60 byte packets
 1  10.10.1.17 (10.10.1.17)  0.183 ms  0.073 ms  0.081 ms
traceroute to 10.10.1.17 (10.10.1.17), 30 hops max, 60 byte packets
 1  10.10.1.17 (10.10.1.17)  0.177 ms  0.080 ms  0.081 ms
</code></pre>
<p>-<br>Hop Number – the first column is simply the number of the hop along the route.</p>
<p>-<br>RTT Columns – The last three columns display the round trip time (RTT) for the packet to reach that point and return. It is listed in milliseconds. There are three columns because the traceroute sends three separate signal packets. This is to display consistency in the route.</p>
<h2 id="netperf"><a href="#netperf" class="headerlink" title="netperf"></a>netperf</h2><p>Netperf is a benchmark that can be used to measure the performance of many different types of networking. It provides tests for both unidirectional throughput, and end-to-end latency. The environments currently measureable by netperf include:</p>
<ul>
<li><p>TCP and UDP via BSD Sockets for both IPv4 and IPv6</p>
</li>
<li><p>DLPI</p>
</li>
<li><p>Unix Domain Sockets</p>
</li>
<li><p>SCTP for both IPv4 and IPv6</p>
<p>  netperf -h</p>
<p>  Usage: netperf [global options] – [test options]</p>
<p>  Global options:<br>  -a send,recv      Set the local send,recv buffer alignment<br>  -A send,recv      Set the remote send,recv buffer alignment<br>  -B brandstr       Specify a string to be emitted with brief output<br>  -c [cpu_rate]     Report local CPU usage<br>  -C [cpu_rate]     Report remote CPU usage<br>  -d                Increase debugging output<br>  -D [secs,units] * Display interim results at least every secs seconds<br>                    using units as the initial guess for units per second<br>  -f G|M|K|g|m|k    Set the output units<br>  -F fill_file      Pre-fill buffers with data from fill_file<br>  -h                Display this text<br>  -H name|ip,fam *  Specify the target machine and&#x2F;or local ip and family<br>  -i max,min        Specify the max and min number of iterations (15,1)<br>  -I lvl[,intvl]    Specify confidence level (95 or 99) (99)<br>                    and confidence interval in percentage (10)<br>  -j                Keep additional timing statistics<br>  -l testlen        Specify test duration (&gt;0 secs) (&lt;0 bytes|trans)<br>  -L name|ip,fam *  Specify the local ip|name and address family<br>  -o send,recv      Set the local send,recv buffer offsets<br>  -O send,recv      Set the remote send,recv buffer offset<br>  -n numcpu         Set the number of processors for CPU util<br>  -N                Establish no control connection, do ‘send’ side only<br>  -p port,lport*    Specify netserver port number and&#x2F;or local port<br>  -P 0|1            Donot&#x2F;Do display test headers<br>  -r                Allow confidence to be hit on result only<br>  -s seconds        Wait seconds between test setup and test start<br>  -S                Set SO_KEEPALIVE on the data connection<br>  -t testname       Specify test to perform<br>  -T lcpu,rcpu      Request netperf&#x2F;netserver be bound to local&#x2F;remote cpu<br>  -v verbosity      Specify the verbosity level<br>  -W send,recv      Set the number of send,recv buffers<br>  -v level          Set the verbosity level (default 1, min 0)<br>  -V                Display the netperf version and exit<br>  For those options taking two parms, at least one must be specified;<br>  specifying one value without a comma will set both parms to that<br>  value, specifying a value with a leading comma will set just the second<br>  parm, a value with a trailing comma will set just the first. To set<br>  each parm to unique values, specify both and separate them with a<br>  comma.</p>
<ul>
<li>For these options taking two parms, specifying one value with no comma<br>  will only set the first parms and will leave the second at the default<br>  value. To set the second value it must be preceded with a comma or be a<br>  comma-separated pair. This is to retain previous netperf behaviour.</li>
</ul>
<p>  $ wget -O netperf-2.5.0.tar.gz -c <a href="https://codeload.github.com/HewlettPackard/netperf/tar.gz/netperf-2.5.0">https://codeload.github.com/HewlettPackard/netperf/tar.gz/netperf-2.5.0</a><br>  $ tar xf netperf-2.5.0.tar.gz &amp;&amp; cd netperf-netperf-2.5.0<br>  $ .&#x2F;configure &amp;&amp; make &amp;&amp; make install</p>
<p>  [<a href="mailto:&#x72;&#x6f;&#x6f;&#x74;&#x40;&#49;&#48;&#46;&#48;&#46;&#48;&#x2e;&#49;&#x37;">&#x72;&#x6f;&#x6f;&#x74;&#x40;&#49;&#48;&#46;&#48;&#46;&#48;&#x2e;&#49;&#x37;</a>]$ netserver -D<br>  [<a href="mailto:&#114;&#x6f;&#x6f;&#116;&#x40;&#49;&#48;&#46;&#48;&#x2e;&#x30;&#x2e;&#49;&#x36;">&#114;&#x6f;&#x6f;&#116;&#x40;&#49;&#48;&#46;&#48;&#x2e;&#x30;&#x2e;&#49;&#x36;</a>]$ netperf -H 10.10.1.17 -l -1000000 -t TCP_RR -w 10ms -b 1 -v 2 – -O min_latency,mean_latency,max_latency,stddev_latency,transaction_rate<br>  Packet rate control is not compiled in.<br>  Packet burst size is not compiled in.<br>  MIGRATED TCP REQUEST&#x2F;RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 10.10.1.17 (10.10.1.17) port 0 AF_INET : first burst 0<br>  Minimum      Mean         Maximum      Stddev       Transaction<br>  Latency      Latency      Latency      Latency      Rate<br>  Microseconds Microseconds Microseconds Microseconds Tran&#x2F;s</p>
<p>  63           84.92        2980         7.86         11740.092</p>
</li>
</ul>
<h2 id="iperf"><a href="#iperf" class="headerlink" title="iperf"></a>iperf</h2><p>iPerf3 is a tool for active measurements of the maximum achievable bandwidth on IP networks. It supports tuning of various parameters related to timing, buffers and protocols (TCP, UDP, SCTP with IPv4 and IPv6). For each test it reports the bandwidth, loss, and other parameters.</p>
<h2 id="lldp"><a href="#lldp" class="headerlink" title="lldp"></a>lldp</h2><p>LLDP (Link Layer Discovery Protocol) can be essential in the situations of complex network-server infrastructure configurations and it’s extremely helpful in case there is no direct access to our setup but we need to determine what network ports on the switches are our servers NIC cards connected to.</p>
<p>Below example shows how to install and enable LLDP Daemon on CentOS and check what are the corresponding neighbor ports connected to the server network cards.</p>
<pre><code>$ yum install lldpd
$ systemctl --now enable lldpd
  
$ lldpcli show neighbors
-------------------------------------------------------------------------------
LLDP neighbors:
-------------------------------------------------------------------------------
Interface:    enp6s0f1, via: LLDP, RID: 1, Time: 0 day, 00:01:29
  Chassis:
    ChassisID:    mac 00:1c:73:82:07:ee
    SysName:      xx-ay-01.06.09
    SysDescr:     Arista Networks EOS version 4.16.6M running on an Arista Networks Lab-71x-28
    MgmtIP:       10.0.254.9
    Capability:   Bridge, on
    Capability:   Router, on
  Port:
    PortID:       ifname Ethernet17
    TTL:          120
-------------------------------------------------------------------------------
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://hewlettpackard.github.io/netperf/">https://hewlettpackard.github.io/netperf/</a></li>
<li><a href="https://iperf.fr/">https://iperf.fr/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Observability</tag>
      </tags>
  </entry>
  <entry>
    <title>Using ANOVA in R to analyze US COVID data to understand age impact to death rate</title>
    <url>/blog/using-anova-in-r-to-analyze-us-covid-data/</url>
    <content><![CDATA[<h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>We want to analysis the relationship between age and covid death rate in US.</p>
<h2 id="Download-the-data"><a href="#Download-the-data" class="headerlink" title="Download the data"></a>Download the data</h2><p>We will use NCHS(National Center for Health Statistics) as our data source.</p>
<p>Visit <a href="https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified">https://data.cdc.gov/browse?category=NCHS&sortBy=last_modified</a>, and search <code>VSRR Quarterly</code>, we will find the data we are intrest.</p>
<p><a href="https://data.cdc.gov/NCHS/NCHS-VSRR-Quarterly-provisional-estimates-for-sele/489q-934x">https://data.cdc.gov/NCHS/NCHS-VSRR-Quarterly-provisional-estimates-for-sele/489q-934x</a>, in this page, we can export data into csv file.</p>
<p>With that, we may can download data source csv, <code>NCHS_-_VSRR_Quarterly_provisional_estimates_for_selected_indicators_of_mortality.csv</code></p>
<h2 id="Load-the-data"><a href="#Load-the-data" class="headerlink" title="Load the data"></a>Load the data</h2><pre><code>library(&quot;dplyr&quot;)
library(&quot;ggplot2&quot;)
library(&quot;janitor&quot;)
library(&quot;tidyr&quot;)
library(&quot;readr&quot;)
library(&quot;sjPlot&quot;)
df &lt;- readr::read_csv(file.path(getwd(), &quot;NCHS_-_VSRR_Quarterly_provisional_estimates_for_selected_indicators_of_mortality.csv&quot;), col_names = TRUE)
df &lt;- clean_names(df)
# To fix the colname format from origin csv file
df &lt;- df %&gt;% rename(&quot;rate_age_65_74&quot; = &quot;rate_65_74&quot;)
</code></pre>
<h2 id="Filter-and-Select"><a href="#Filter-and-Select" class="headerlink" title="Filter and Select"></a>Filter and Select</h2><pre><code>df1 &lt;- df %&gt;%
    filter(time_period == &quot;3-month period&quot; &amp; rate_type == &quot;Crude&quot; &amp; cause_of_death %in% c(&quot;COVID-19&quot;)) %&gt;%
    select(year_and_quarter, rate_age_1_4, rate_age_5_14,rate_age_15_24, rate_age_25_34,rate_age_35_44, rate_age_45_54,rate_age_55_64,rate_age_65_74,rate_age_75_84,rate_age_85_plus)
  # take a quick look from column&#39;s point of view
!&gt; glimpse(df1)
 Rows: 14
 Columns: 11
 $ year_and_quarter &lt;chr&gt; &quot;2019 Q1&quot;, &quot;2019 Q2&quot;, &quot;2019 Q3&quot;, &quot;2019 Q4&quot;, &quot;2020 Q1&quot;…
 $ rate_age_1_4     &lt;dbl&gt; NA, NA, NA, NA, NA, 0.2, NA, 0.2, 0.2, 0.2, 0.6, 0.4,…
 $ rate_age_5_14    &lt;dbl&gt; NA, NA, NA, NA, NA, 0.1, 0.2, 0.2, 0.2, 0.1, 0.6, 0.5…
 $ rate_age_15_24   &lt;dbl&gt; NA, NA, NA, NA, 0.1, 1.4, 1.5, 1.6, 1.9, 1.0, 5.9, 4.…
 $ rate_age_25_34   &lt;dbl&gt; NA, NA, NA, NA, 0.8, 6.6, 5.4, 6.7, 8.9, 4.6, 23.8, 1…
 $ rate_age_35_44   &lt;dbl&gt; NA, NA, NA, NA, 2.1, 18.8, 16.1, 20.6, 25.1, 11.9, 65…
 $ rate_age_45_54   &lt;dbl&gt; NA, NA, NA, NA, 4.9, 56.1, 42.8, 64.4, 79.9, 33.9, 14…
 $ rate_age_55_64   &lt;dbl&gt; NA, NA, NA, NA, 9.3, 129.8, 96.0, 162.0, 201.9, 67.4,…
 $ rate_age_65_74   &lt;dbl&gt; NA, NA, NA, NA, 19.0, 293.6, 206.7, 413.6, 464.8, 109…
 $ rate_age_75_84   &lt;dbl&gt; NA, NA, NA, NA, 43.9, 725.6, 465.8, 1112.4, 1110.7, 1…
 $ rate_age_85_plus &lt;dbl&gt; NA, NA, NA, NA, 97.7, 2210.8, 1127.2, 3101.9, 2783.8,…
</code></pre>
<h2 id="pivot-longer-to-reorg-the-data-for-grouping"><a href="#pivot-longer-to-reorg-the-data-for-grouping" class="headerlink" title="pivot_longer to reorg the data for grouping"></a><code>pivot_longer</code> to reorg the data for grouping</h2><pre><code>df2 = df1 %&gt;% pivot_longer(names_to = &quot;rate_type&quot;, values_to = &quot;rate_of_10k&quot;,cols = -c(year_and_quarter))
# convert NA to 0
df2 &lt;- df2 %&gt;%
    mutate_at(c(&quot;rate_of_10k&quot;), ~coalesce(.,0))
!&gt; df2
 # A tibble: 140 × 3
    year_and_quarter rate_type        rate_of_10k
    &lt;chr&gt;            &lt;chr&gt;                  &lt;dbl&gt;
  1 2019 Q1          rate_age_1_4               0
  2 2019 Q1          rate_age_5_14              0
  3 2019 Q1          rate_age_15_24             0
  4 2019 Q1          rate_age_25_34             0
  5 2019 Q1          rate_age_35_44             0
  6 2019 Q1          rate_age_45_54             0
  7 2019 Q1          rate_age_55_64             0
  8 2019 Q1          rate_age_65_74             0
  9 2019 Q1          rate_age_75_84             0
 10 2019 Q1          rate_age_85_plus           0	
</code></pre>
<h2 id="Draw-diagram-for-each-group"><a href="#Draw-diagram-for-each-group" class="headerlink" title="Draw diagram for each group"></a>Draw diagram for each group</h2><pre><code>plotdata &lt;- df2 %&gt;%
    group_by(rate_type) %&gt;%
    summarize(n = n(),
            mean = mean(rate_of_10k),
            sd = sd(rate_of_10k),
            ci = qt(0.975, df = n - 1) * sd / sqrt(n))
p = ggplot(plotdata,
       aes(x = factor(rate_type, level = c(&#39;rate_age_1_4&#39;,&#39;rate_age_5_14&#39;,&#39;rate_age_15_24&#39;,&#39;rate_age_25_34&#39;,&#39;rate_age_35_44&#39;,&#39;rate_age_45_54&#39;,&#39;rate_age_55_64&#39;,&#39;rate_age_65_74&#39;,&#39;rate_age_75_84&#39;,&#39;rate_age_85_plus&#39;))
         , y = mean, group = 1)) +
    geom_line(linetype=&quot;dashed&quot;, color=&quot;darkgrey&quot;) +
    geom_errorbar(aes(ymin = mean - ci,
                      ymax = mean + ci),
                  width = .2) +
    geom_point(size = 3, color=&quot;red&quot;) +
    scale_y_continuous(breaks=seq(0,1800,100)) +
    theme_bw() +
    labs(x=&quot;rate_type&quot;,
         y=&quot;rate_of_10k&quot;,
         title=&quot;Mean Plot with 95% Confidence Interval&quot;)
save_plot(&quot;covid_plot_age_impact1.svg&quot;, fig = p, width=30, height=20)
</code></pre>
<p><img src="/images/covid_plot_age_impact1.svg" alt="Image"></p>
<h2 id="ANOVA-analysis"><a href="#ANOVA-analysis" class="headerlink" title="ANOVA analysis"></a>ANOVA analysis</h2><p>Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures (such as the “variation” among and between groups) used to analyze the differences among means.</p>
<p>It’s very useful in our case, <code>aov</code> from R make it very easy to use, see <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/aov">https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/aov</a> for more detail.</p>
<pre><code>fit &lt;- aov(rate_of_10k ~ rate_type, data=df2)
summary(fit)
!&gt; summary(fit)
              Df   Sum Sq Mean Sq F value   Pr(&gt;F)
 rate_type     9 12988087 1443121   10.21 8.49e-12 ***
 Residuals   130 18370513  141312
 ---
 Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
</code></pre>
<p>we saw <code>8.49e-12 ***</code>, which means age has big impact on covid death rate.</p>
<h2 id="Compare-diffrent-age-ranges"><a href="#Compare-diffrent-age-ranges" class="headerlink" title="Compare diffrent age ranges"></a>Compare diffrent age ranges</h2><pre><code>pairwise &lt;- TukeyHSD(fit)
pairwise

 + pairwise
 &gt;   Tukey multiple comparisons of means
     95% family-wise confidence level

 Fit: aov(formula = rate_of_10k ~ rate_type, data = df2)

 $rate_type
                                          diff        lwr       upr     p adj
 rate_age_15_24-rate_age_1_4        1.25000000 -456.16974  458.6697 1.0000000
 rate_age_25_34-rate_age_1_4        5.87857143 -451.54117  463.2983 1.0000000
 rate_age_35_44-rate_age_1_4       16.50714286 -440.91259  473.9269 1.0000000
 rate_age_45_54-rate_age_1_4       43.45714286 -413.96259  500.8769 0.9999996
 rate_age_5_14-rate_age_1_4        -0.02857143 -457.44831  457.3912 1.0000000
 ....
</code></pre>
<p>if <code>p adj</code> is close to 1, it means <code>no big difference</code> for this pair</p>
<p>if <code>p adj</code> is close to 0, it means <code>big difference</code> for this pair</p>
<h2 id="Draw-diagram-for-pairs-comparisons"><a href="#Draw-diagram-for-pairs-comparisons" class="headerlink" title="Draw diagram for pairs comparisons"></a>Draw diagram for pairs comparisons</h2><pre><code>plotdata &lt;- as.data.frame(pairwise[[1]])
plotdata$conditions &lt;- row.names(plotdata)
p = ggplot(data=plotdata, aes(x=conditions, y=diff)) +
  geom_errorbar(aes(ymin=lwr, ymax=upr, width=.2)) +
  geom_hline(yintercept=0, color=&quot;red&quot;, linetype=&quot;dashed&quot;) +
  geom_point(size=3, color=&quot;red&quot;) +
  theme_bw() +
  labs(y=&quot;Difference in mean levels&quot;, x=&quot;&quot;,
       title=&quot;95% family-wise confidence level&quot;) +
   coord_flip()
save_plot(&quot;covid_plot_age_impact2.svg&quot;, fig = p, width=30, height=20)
</code></pre>
<p><img src="/images/covid_plot_age_impact2.svg" alt="Image"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>Age does have big impact for covid death rate.</li>
<li>85+ was impacted the most.</li>
<li>For 35-,  the death rate is very low, and no big difference from age 0-35.</li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use AWS CLI to manage EBS volumes</title>
    <url>/blog/using-aws-cli-to-manage-ebs-volumes/</url>
    <content><![CDATA[<h2 id="Intro-to-AWS-CLI"><a href="#Intro-to-AWS-CLI" class="headerlink" title="Intro to AWS CLI"></a>Intro to AWS CLI</h2><p>The AWS Command Line Interface (AWS CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.</p>
<h2 id="AWS-CLI-install"><a href="#AWS-CLI-install" class="headerlink" title="AWS CLI install"></a>AWS CLI install</h2><p>To install AWS CLI on MacOS:</p>
<pre><code>$ curl &quot;https://awscli.amazonaws.com/AWSCLIV2.pkg&quot; -o &quot;AWSCLIV2.pkg&quot;
$ sudo installer -pkg AWSCLIV2.pkg -target /
$ which aws
/usr/local/bin/aws
$ aws --version
aws-cli/2.10.3 Python/3.9.11 Darwin/20.4.0 exe/x86_64 prompt/off
</code></pre>
<h2 id="Configure-AWS-credentials"><a href="#Configure-AWS-credentials" class="headerlink" title="Configure AWS credentials"></a>Configure AWS credentials</h2><p>The AWS CLI stores sensitive credential information that you specify with <em>aws configure</em> in a local file ~&#x2F;.aws&#x2F;credentials.</p>
<p>$ aws configure</p>
<p>AWS Access Key ID [****************6N7Q]: [Your AWS access Key ID]</p>
<p>AWS Secret Access Key [****************+Ic+]: [Your AWS Secret Access Key]</p>
<p>Default region name [None]: [Your Region Name]</p>
<p>Default output format [None]:</p>
<p>$ cat ~&#x2F;.aws&#x2F;credentials</p>
<p>[default]</p>
<p>aws_access_key_id &#x3D;  [Your AWS access Key ID]</p>
<p>aws_secret_access_key &#x3D; [Your AWS Secret Access Key]</p>
<p>aws_region &#x3D; [Your Region Name]</p>
<h2 id="AWS-CLI-Commands"><a href="#AWS-CLI-Commands" class="headerlink" title="AWS CLI Commands"></a>AWS CLI Commands</h2><p>To get commands help:</p>
<pre><code>$ aws eks help
$ aws ec2 help
</code></pre>
<p>To list and describe clusters:</p>
<pre><code>$ aws eks list-clusters
$ aws eks describe-cluster --name [your-cluster-name]
</code></pre>
<p>To list and describe nodegroups:</p>
<pre><code>$ aws eks list-nodegroups --cluster-name [your-cluster-name]
$ aws eks describe-nodegroup --cluster-name [your-cluster-name] --nodegroup-name [you-nodegroup-name]
</code></pre>
<p>To delete multiple EBS volumes(example):</p>
<pre><code>$ aws ec2 describe-volumes --filters &quot;Name=tag:Name,Values=gp3-volume-*&quot;  | egrep &quot;VolumeId&quot; | awk &#39;&#123;print $NF&#125;&#39; | sed &#39;s/\&quot;//g;s/\,//&#39; &gt; vol_ids

$ for id in `cat vol_ids`
do
    aws ec2 delete-volume --volume-id $id
done

$ aws ec2 describe-volumes --filters &quot;Name=tag:Name,Values=gp3-volume-*&quot;  | egrep &quot;VolumeId&quot; | awk &#39;&#123;print $NF&#125;&#39;  | wc -l
0
</code></pre>
<p>To create multiple EBS volumes(example):</p>
<pre><code>$ for i in `seq 1 8`
do
aws ec2 create-volume --volume-type gp3 --size 256 --iops 16000 --throughput 1000 --availability-zone us-east-1a --tag-specifications &quot;ResourceType=volume,Tags=[&#123;Key=Name,Value=gp3-volume-$i&#125;]&quot;
done
</code></pre>
<p>To attach EBS volume to EC2 instance:</p>
<pre><code>$ aws ec2 describe-volumes --filters &quot;Name=tag:Name,Values=gp3-volume-1&quot; --query &quot;Volumes[*].&#123;ID:VolumeId&#125;&quot;
[
    &#123;
        &quot;ID&quot;: &quot;vol-0101002b66d4fc211&quot;
    &#125;
]

$ aws ec2 attach-volume --volume-id vol-0101002b66d4fc211 --instance-id i-0c2b7553a99a7277b --device /dev/sdf
&#123;
    &quot;AttachTime&quot;: &quot;2023-03-04T01:58:37.155000+00:00&quot;,
    &quot;Device&quot;: &quot;/dev/sdf&quot;,
    &quot;InstanceId&quot;: &quot;i-0c2b7553a99a7277b&quot;,
    &quot;State&quot;: &quot;attaching&quot;,
    &quot;VolumeId&quot;: &quot;vol-0101002b66d4fc211&quot;
&#125;
</code></pre>
<p>To describe instance:</p>
<pre><code>$ aws ec2 describe-instances --instance-ids i-0c2b7553a99a7277b | egrep &quot;DeviceName|Status|VolumeId&quot;
&quot;DeviceName&quot;: &quot;/dev/sdf&quot;,
&quot;Status&quot;: &quot;attached&quot;,
&quot;VolumeId&quot;: &quot;vol-0101002b66d4fc211&quot;
&quot;Status&quot;: &quot;attached&quot;
</code></pre>
<p>To verify the attached volumes in EC2 instance:</p>
<pre><code>[ec2-user@ip-192-168-25-183 ~]$ ls -la /dev/sdf
lrwxrwxrwx 1 root root 7 Mar  4 01:58 /dev/sdf -&gt; nvme1n1

[ec2-user@ip-192-168-25-183 ~]$ lsblk
NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
nvme0n1       259:0    0  256G  0 disk
├─nvme0n1p1   259:1    0  256G  0 part /
└─nvme0n1p128 259:2    0    1M  0 part
nvme1n1       259:3    0  256G  0 disk
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html">https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>AWS</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use bcc-tools for dynamic kernel tracing</title>
    <url>/blog/using-bcc-tools-for-dynamic-kernel-tracing/</url>
    <content><![CDATA[<p>In Red Hat Enterprise Linux 8.1, Red Hat ships a set of fully supported on x86_64 dynamic kernel tracing tools, called bcc-tools, that make use of a kernel technology called extended Berkeley Packet Filter (eBPF). With these tools, you can quickly gain insight into certain aspects of system performance that would have previously required more time and effort from the system and operator.</p>
<p>The eBPF technology allows dynamic kernel tracing without requiring kernel modules (like systemtap) or rebooting of the kernel (as with debug kernels). eBPF accomplishes this while maintaining minimal overhead for each trace point, making these tools an ideal way to instrument running kernels in production.</p>
<p>To ensure that an eBPF program will not harm the running kernel, tools built on eBPF go through the following process when instantiated by root on the command line:</p>
<ul>
<li>The program is compiled into eBPF bytecode.</li>
<li>Loaded into the kernel.</li>
<li>Run through a technology called the eBPF verifier to ensure that the program will not harm the running kernel.</li>
<li>Upon passing the verifier, it begins execution. If it does not pass the verifier, the code is unloaded and does not execute.</li>
</ul>
<p>That said, bear in mind that you are still inserting tracing and some system calls are called significantly more than others, so depending on what you are tracing, there may be increased overhead.</p>
<p>If you are interested in more information on eBPF in general, please see Stanislav Kozina’s blog: <a href="https://www.redhat.com/en/blog/introduction-ebpf-red-hat-enterprise-linux-7">Introduction to eBPF in Red Hat Enterprise Linux 7</a>.</p>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>With RHEL 8.1, bcc-tools has gone fully supported on x86. To install bcc-tools on RHEL 7 (7.6+) and RHEL 8, run yum install as root:</p>
<pre><code>$ uname -r
3.10.0-1062.40.1.el7.x86_64
$  cat /etc/redhat-release
Red Hat Enterprise Linux Server release 7.7 (Maipo)
$ yum install bcc-tools
Installed:
  bcc-tools.x86_64 0:0.10.0-1.el7
Dependency Installed:
  bcc.x86_64 0:0.10.0-1.el7                kernel-devel.x86_64 0:3.10.0-1160.21.1.el7
  llvm-private.x86_64 0:7.0.1-1.el7        python-bcc.x86_64 0:0.10.0-1.el7

$ pwd
/usr/share/bcc/tools
$  ls
argdist       drsnoop         memleak         pythonstat   tclobjnew
bashreadline  execsnoop       mountsnoop      reset-trace  tclstat
biolatency    ext4dist        mysqld_qslower  rubycalls    tcpaccept
biosnoop      ext4slower      nfsdist         rubyflow     tcpconnect
biotop        filelife        nfsslower       rubygc       tcpconnlat
bitesize      fileslower      nodegc          rubyobjnew   tcpdrop
bpflist       filetop         nodestat        rubystat     tcplife
btrfsdist     funccount       offcputime      runqlat      tcpretrans
btrfsslower   funclatency     offwaketime     runqlen      tcpsubnet
cachestat     funcslower      oomkill         runqslower   tcptop
cachetop      gethostlatency  opensnoop       shmsnoop     tcptracer
capable       hardirqs        perlcalls       slabratetop  tplist
cobjnew       javacalls       perlflow        sofdsnoop    trace
cpudist       javaflow        perlstat        softirqs     ttysnoop
cpuunclaimed  javagc          phpcalls        solisten     vfscount
dbslower      javaobjnew      phpflow         sslsniff     vfsstat
dbstat        javastat        phpstat         stackcount   wakeuptime
dcsnoop       javathreads     pidpersec       statsnoop    xfsdist
dcstat        killsnoop       profile         syncsnoop    xfsslower
deadlock      lib             pythoncalls     syscount
deadlock.c    llcstat         pythonflow      tclcalls
doc           mdflush         pythongc        tclflow  
</code></pre>
<h2 id="bcc-tools-Framework"><a href="#bcc-tools-Framework" class="headerlink" title="bcc-tools Framework"></a>bcc-tools Framework</h2><p>Before we dive into the different types of tools that are included in bcc-tools, it’s important to note a few things:</p>
<ul>
<li>All of these tools live in &#x2F;usr&#x2F;share&#x2F;bcc&#x2F;tools.</li>
<li>These tools must run as the root user as any eBPF program can read kernel data. As such, injecting eBPF bytecode as a regular user is not allowed in RHEL 8.1.</li>
<li>Each tool has a man page. To view the man page, run man . These man pages include descriptions of the tools, provide the options that can be called, and have information on the expected overhead of the specific tool.</li>
</ul>
<p>Since there are a lot of tools in bcc-tools, I’m going to divide the tools into the following classes and then we’ll dive into each class:</p>
<ul>
<li>The Snoops</li>
<li>Latency Detectors</li>
<li>Slower</li>
<li>Top Up with bcc-tools</li>
<li>Java&#x2F;Perl&#x2F;Python&#x2F;Ruby</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://www.redhat.com/en/blog/introduction-ebpf-red-hat-enterprise-linux-7">https://www.redhat.com/en/blog/introduction-ebpf-red-hat-enterprise-linux-7</a></li>
<li><a href="https://www.redhat.com/en/blog/bcc-tools-brings-dynamic-kernel-tracing-red-hat-enterprise-linux-81">https://www.redhat.com/en/blog/bcc-tools-brings-dynamic-kernel-tracing-red-hat-enterprise-linux-81</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>BCC</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use cgroup to limit resource for ssh session</title>
    <url>/blog/using-cgroup-to-limit-resource-for-ssh-session/</url>
    <content><![CDATA[<h2 id="etc-profile-and-etc-profile-d"><a href="#etc-profile-and-etc-profile-d" class="headerlink" title="&#x2F;etc&#x2F;profile and &#x2F;etc&#x2F;profile.d"></a>&#x2F;etc&#x2F;profile and &#x2F;etc&#x2F;profile.d</h2><p>Before we discuss how to limit process resource usage with cgroup for ssh session, let’s take a moment to know about &#x2F;etc&#x2F;profile and &#x2F;etc&#x2F;profile.d in Linux.</p>
<span id="more"></span>

<h3 id="etc-profile"><a href="#etc-profile" class="headerlink" title="&#x2F;etc&#x2F;profile"></a>&#x2F;etc&#x2F;profile</h3><p>This is the system-wide profile file, which will be executed, when a user logs in to the shell environment.</p>
<pre><code>$ cat /etc/profile
# /etc/profile: system-wide .profile file for the Bourne shell (sh(1))
# and Bourne compatible shells (bash(1), ksh(1), ash(1), ...).

if [ &quot;$&#123;PS1-&#125;&quot; ]; then
  if [ &quot;$&#123;BASH-&#125;&quot; ] &amp;&amp; [ &quot;$BASH&quot; != &quot;/bin/sh&quot; ]; then
    # The file bash.bashrc already sets the default PS1.
    # PS1=&#39;\h:\w\$ &#39;
    if [ -f /etc/bash.bashrc ]; then
      . /etc/bash.bashrc
    fi
  else
    if [ &quot;$(id -u)&quot; -eq 0 ]; then
      PS1=&#39;# &#39;
    else
      PS1=&#39;$ &#39;
    fi
  fi
fi

if [ -d /etc/profile.d ]; then
  for i in /etc/profile.d/*.sh; do
    if [ -r $i ]; then
      . $i
    fi
  done
  unset i
fi
</code></pre>
<h3 id="etc-profile-d"><a href="#etc-profile-d" class="headerlink" title="&#x2F;etc&#x2F;profile.d"></a>&#x2F;etc&#x2F;profile.d</h3><p>Files in &#x2F;etc&#x2F;profile.d&#x2F; are run when a user logs in (unless you’ve modified &#x2F;etc&#x2F;profile above to not do this) and are generally used to set environment variables.</p>
<p>Depending on which distribution of Linux you are using, you will find a variety of files in this directory.</p>
<pre><code>$ ls -ltr /etc/profile.d/
total 92
-rw-r--r--  1 root root  379 Jun 13  2012 qt-graphicssystem.sh
-rw-r--r--  1 root root  313 Jun 29  2012 qt-graphicssystem.csh
-rw-r--r--. 1 root root  169 Jan 27  2014 which2.sh
-rw-r--r--. 1 root root  164 Jan 27  2014 which2.csh
-rw-r--r--. 1 root root  121 Jul 30  2015 less.sh
-rw-r--r--. 1 root root  123 Jul 30  2015 less.csh
-rw-r--r--. 1 root root  201 Mar 24  2017 colorgrep.sh
-rw-r--r--. 1 root root  196 Mar 24  2017 colorgrep.csh
-rw-r--r--. 1 root root 1606 Aug  6  2019 colorls.sh
-rw-r--r--. 1 root root 1741 Aug  6  2019 colorls.csh
-rw-r--r--. 1 root root  660 Apr  1  2020 bash_completion.sh
-rw-r--r--. 1 root root   81 Apr  1  2020 sh.local
-rw-r--r--. 1 root root   80 Apr  1  2020 csh.local
-rw-r--r--. 1 root root 1348 Oct  1  2020 abrt-console-notification.sh
-rw-r--r--. 1 root root 2703 Oct 13  2020 lang.sh
-rw-r--r--. 1 root root 1706 Oct 13  2020 lang.csh
-rw-r--r--. 1 root root  841 Oct 13  2020 256term.sh
-rw-r--r--. 1 root root  771 Oct 13  2020 256term.csh
-rw-r--r--  1 root root  269 Dec 15  2020 vim.sh
-rw-r--r--  1 root root  105 Dec 15  2020 vim.csh
-r--r--r--  1 root root   31 Apr  4  2022 99bash_history.sh
-r-xr-xr-x  1 root root  948 Apr  4  2022 99remote_user_info.sh
-r-xr-xr-x  1 root root  551 Apr  4  2022 99bash_logging.sh
lrwxrwxrwx  1 root root   39 Apr  4  2022 90-updates-available.sh -&gt; /etc/update-motd.d/90-updates-available
</code></pre>
<h2 id="Create-ssh-profile-under-etc-profile-d-for-cgroup-resource-limit"><a href="#Create-ssh-profile-under-etc-profile-d-for-cgroup-resource-limit" class="headerlink" title="Create ssh profile under &#x2F;etc&#x2F;profile.d for cgroup resource limit"></a>Create ssh profile under &#x2F;etc&#x2F;profile.d for cgroup resource limit</h2><p>In this example, we want to limit the IO bandwidth for the ssd sessions logs in the system.</p>
<p>To achieve this, we create a ssh.sh profile as below.</p>
<pre><code>$ cat ssh.sh
# log a ssh session date
date &gt; /root/ssh.log

# set resource limit
/root/config_cgroup_io.sh &gt;&gt; /root/ssh.log  

# limit reource for all the processes of current ssh session
echo $$ &gt; /sys/fs/cgroup/blkio/cgroup.procs 
</code></pre>
<p>Move the ssh profile to &#x2F;etc&#x2F;profile.d so that it will be executed automatically for future ssh login.</p>
<pre><code>$ mv ssh.sh /etc/profile.d/
</code></pre>
<p>The following is an example script to limit r&#x2F;s and w&#x2F;s for the device “259:2” to 8000 with cgroup.</p>
<pre><code>$ cat config_cgroup_io.sh
iops=8000
bps=`expr 50 \* 1024 \* 1024` # bytes per second
major=259
minor=2

echo &quot;$major:$minor  $iops&quot; &gt; /sys/fs/cgroup/blkio/blkio.throttle.write_iops_device
echo &quot;$major:$minor  $iops&quot; &gt; /sys/fs/cgroup/blkio/blkio.throttle.read_iops_device
</code></pre>
<p>At this point, we have setup a way to limit ssh session resource usage with cgroup. Hope this is helpful.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Cgroups</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use dm-delay to emulate slower SSD on NVME device</title>
    <url>/blog/using-dm-delay-to-emulate-slower-ssd-on-nvme-device/</url>
    <content><![CDATA[<h2 id="Intro-to-dm-delay"><a href="#Intro-to-dm-delay" class="headerlink" title="Intro to dm-delay"></a>Intro to dm-delay</h2><p><a href="https://www.kernel.org/doc/html/latest/admin-guide/device-mapper/delay.html">Device-Mapper’s “delay” target </a> delays reads and&#x2F;or writes and maps them to different devices.</p>
<span id="more"></span>

<h2 id="Setup-dm-delay"><a href="#Setup-dm-delay" class="headerlink" title="Setup dm-delay"></a>Setup dm-delay</h2><p>To setup a dm-delay for extra 1ms latency:</p>
<pre><code>$ cat setup_dm_delay.sh
disk=nvme2n1
size=$(blockdev --getsize /dev/$disk)
echo &quot;0 $size delay /dev/$disk 0 1&quot; | dmsetup create delayed2
dmsetup table delayed2
</code></pre>
<h2 id="Verify-the-dm-delay"><a href="#Verify-the-dm-delay" class="headerlink" title="Verify the dm-delay"></a>Verify the dm-delay</h2><p>To ensure the dm-delay is configured successfully,</p>
<pre><code>$ ls -la /dev/nvme2n1
brw-rw---- 1 root disk 259, 1 Jun 3 17:03 /dev/nvme2n1

$ dmsetup table delayed2
0 3125627568 delay 259:1 0 1

$ls -la /dev/mapper/ | grep delayed2
lrwxrwxrwx  1 root root       7 Jun  3 17:24 delayed2 -&gt; ../dm-2
</code></pre>
<p>From iostat, you can verify the delayed latency on dm-2 device when you have some workload running.</p>
<pre><code>06/03/2023 05:24:56 PM
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util

nvme2n1           0.00     0.00    0.00 63667.20     0.00 254668.80     8.00     1.03    0.02    0.00    0.02   0.01  49.74
dm-2              0.00     0.00    0.00 63667.40     0.00 254669.60     8.00   126.13    1.98    0.00    1.98   0.02 100.02
</code></pre>
<h2 id="Remove-dm-delay"><a href="#Remove-dm-delay" class="headerlink" title="Remove dm-delay"></a>Remove dm-delay</h2><pre><code>$ dmsetup remove delayed2
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Dm-delay</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use fio to generate millions of files</title>
    <url>/blog/using-fio-to-generate-millions-of-files/</url>
    <content><![CDATA[<p><a href="https://fio.readthedocs.io/en/latest/fio_doc.html#">fio</a> is the acronym for Flexible IO Tester and is a tool for I&#x2F;O performance measurement.</p>
<p>From time to time, I use fio to run performance benchmark test, either to investigate I&#x2F;O performance for certain storage or reproduce performance issues.</p>
<p>In this article, we are not target to discuss how to use fio for general I&#x2F;O performance test. Instead, we will explore how to use it to create millions of files in the file system and simulate the real-world data content.</p>
<p>fio provides two options, dedupe_percentage and buffer_compress_percentage, from which the percentage of identical buffers and percentage of compressible buffer content can be specified. To meet specific file content requirement, a buffer_pattern option can be used to reflect the real-world data content.</p>
<pre><code>buffer_compress_percentage=int

If this is set, then fio will attempt to provide I/O buffer content (on WRITEs) that compresses to the specified level. Fio does this by providing a mix of random data followed by fixed pattern data. The fixed pattern is either zeros, or the pattern specified by buffer_pattern. If the buffer_pattern option is used, it might skew the compression ratio slightly. Setting buffer_compress_percentage to a value other than 100 will also enable refill_buffers in order to reduce the likelihood that adjacent blocks are so similar that they over compress when seen together. See buffer_compress_chunk for how to set a finer or coarser granularity for the random/fixed data region. Defaults to unset i.e., buffer data will not adhere to any compression level.

buffer_pattern=str

If set, fio will fill the I/O buffers with this pattern or with the contents of a file. If not set, the contents of I/O buffers are defined by the other options related to buffer contents. The setting can be any pattern of bytes, and can be prefixed with 0x for hex values. It may also be a string, where the string must then be wrapped with &quot;&quot;. Or it may also be a filename, where the filename must be wrapped with &#39;&#39; in which case the file is opened and read. Note that not all the file contents will be read if that would cause the buffers to overflow. 

So, for example: buffer_pattern=&quot;0123456789&quot;

dedupe_percentage=int

If set, fio will generate this percentage of identical buffers when writing. These buffers will be naturally dedupable. The contents of the buffers depend on what other buffer compression settings have been set. It’s possible to have the individual buffers either fully compressible, or not at all – this option only controls the distribution of unique buffers. Setting this option will also enable refill_buffers to prevent every buffer being identical.
</code></pre>
<p>The following example shows how we use fio to simulate real-world data.</p>
<pre><code>$ fio -v
fio-3.7

$ cat fio_job_file.ini
[job1]
ioengine=libaio
iodepth=8
rw=write
direct=1
nrfiles=1000
filesize=102400
blocksize=4096
dedupe_percentage=30
buffer_compress_percentage=50
buffer_pattern=&quot;0123456789&quot;
numjobs=8
directory=/testdir
</code></pre>
<p>The following example shows how the generated file content looks like. It contains integer numbers which is defined by buffer_pattern. With the real-world like data generated, we could simulate the workloads and deliver more realistic performance result.</p>
<pre><code>$ ls -la /testdir/job1.0.0
-rw-r--r--. 1 root root 102400 Feb 20 19:10 /testdir/job1.0.0

$  strings /testdir/job1.0.0 | head
-(=,(
0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345
2&gt;&#125;o
l&amp;&#39;TI
0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345
    |6=
&amp;&lt;y|
FSlK
t+pf|
0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Fio</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use ftrace to analyze latency of the kernel module function</title>
    <url>/blog/using-ftrace-to-analyze-latency-of-the-kernel-module-function/</url>
    <content><![CDATA[<p>In this example, we study the latency of the function “nfsd_vfs_write” from kernel module “nfsd”.</p>
<h2 id="ftrace-configuration"><a href="#ftrace-configuration" class="headerlink" title="ftrace configuration"></a>ftrace configuration</h2><p>The following ftrace options are used in this example. There are 8 nfsd processes to be traced.</p>
<ul>
<li>current_tracer:  function_graph</li>
<li>pids: 3591 3592 3593 3594 3595 3596 3597 3598</li>
<li>filters: nfsd_vfs_write [nfsd]</li>
</ul>
<h2 id="ftrace-result"><a href="#ftrace-result" class="headerlink" title="ftrace result"></a>ftrace result</h2><p>From the following trace result, it helps understand the call time of the function “nfsd_vfs_write”. This can be very helpful if we need to analyze the function latency in the kernel module.</p>
<pre><code>$ cat ftrace.out | grep nfsd_vfs_write | grep &quot;nfsd_vfs_write \[nfsd\]();&quot; | head -5
  1)  ! 185.011 us  |  nfsd_vfs_write [nfsd]();
  2)  ! 161.237 us  |  nfsd_vfs_write [nfsd]();
  3)  ! 200.954 us  |  nfsd_vfs_write [nfsd]();
  4)  ! 255.285 us  |  nfsd_vfs_write [nfsd]();
  5)  ! 171.537 us  |  nfsd_vfs_write [nfsd]();
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.kernel.org/doc/Documentation/trace/ftrace.txt">https://www.kernel.org/doc/Documentation/trace/ftrace.txt</a></li>
<li><a href="https://lwn.net/Articles/365835/">https://lwn.net/Articles/365835/</a></li>
<li><a href="https://lwn.net/Articles/366796/">https://lwn.net/Articles/366796/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Profiling Tracing</tag>
        <tag>Ftrace</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use gstack for docker container process</title>
    <url>/blog/using-gstack-for-docker-container-process/</url>
    <content><![CDATA[<p>In a Docker container environment, we won’t get a valid stack trace directly on the container host as below.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$  ps -ef |grep smbd</span><br><span class="line">root     171118 167977  0 Apr22 ?        00:00:02 /usr/sbin/smbd --foreground --no-process-group</span><br><span class="line">root     171166 171118  0 Apr22 ?        00:00:00 /usr/sbin/smbd --foreground --no-process-group</span><br><span class="line">root     171168 171118  0 Apr22 ?        00:00:00 /usr/sbin/smbd --foreground --no-process-group</span><br><span class="line">root     171208 171118  0 Apr22 ?        00:00:00 /usr/sbin/smbd --foreground --no-process-group</span><br><span class="line">root     190574 186140  0 15:01 pts/3    00:00:00 grep --color=auto smbd</span><br><span class="line">    </span><br><span class="line">$  gstack 171118</span><br><span class="line">#0  0x00007fb66768afb3 in ?? () from /lib64/libc.so.6</span><br><span class="line">#1  0x00007fb667b70f1b in ?? ()</span><br><span class="line">#2  0x0000000000000060 in ?? ()</span><br><span class="line">#3  0x00007fb668a92f5c in ?? ()</span><br><span class="line">#4  0x0000000000000000 in ?? () </span><br></pre></td></tr></table></figure>


<p>To print the stack trace for a process running inside container, we can do the following.</p>
<ol>
<li><p>Get the process id inside container</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ docker exec -it iNfcP_9-0-1 bash</span><br><span class="line">bash-4.2# ps -ef | grep smbd</span><br><span class="line">root       1864      1  0 Apr22 ?        00:00:02 /usr/sbin/smbd --foreground --no-process-group</span><br><span class="line">root       1908   1864  0 Apr22 ?        00:00:00 /usr/sbin/smbd --foreground --no-process-group</span><br><span class="line">root       1910   1864  0 Apr22 ?        00:00:00 /usr/sbin/smbd --foreground --no-process-group</span><br><span class="line">root       1950   1864  0 Apr22 ?        00:00:00 /usr/sbin/smbd --foreground --no-process-group</span><br><span class="line">root     177346 177338  0 15:02 ?        00:00:00 grep smbd </span><br></pre></td></tr></table></figure>

</li>
<li><p>Get the container instance pid</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ /bin/docker inspect --format &#x27;\&#123;\&#123; .State.Pid &#125;&#125;&#x27; e2590333640e</span><br><span class="line">167977</span><br></pre></td></tr></table></figure>
</li>
<li><p>Print stack trace with gstack as below</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$  /bin/nsenter -Z -m -n -p -t 167977 /bin/gstack 1864</span><br><span class="line">#0  0x00007fb66768afb3 in __epoll_wait_nocancel () from /lib64/libc.so.6</span><br><span class="line">#1  0x00007fb667b70f1b in epoll_event_loop_once () from /lib64/libtevent.so.0</span><br><span class="line">#2  0x00007fb667b6f057 in std_event_loop_once () from /lib64/libtevent.so.0</span><br><span class="line">#3  0x00007fb667b6a25d in _tevent_loop_once () from /lib64/libtevent.so.0</span><br><span class="line">#4  0x00007fb667b6a4bb in tevent_common_loop_wait () from /lib64/libtevent.so.0</span><br><span class="line">#5  0x00007fb667b6eff7 in std_event_loop_wait () from /lib64/libtevent.so.0</span><br><span class="line">#6  0x00005588a9175a98 in main ()</span><br></pre></td></tr></table></figure></li>
</ol>
<p>We use nsenter to get the stack trace of the target process in contaienr namespace.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ man nsenter</span><br><span class="line">   </span><br><span class="line">NAME</span><br><span class="line">       nsenter - run program with namespaces of other processes</span><br><span class="line">    </span><br><span class="line">SYNOPSIS</span><br><span class="line">       nsenter [options] [program [arguments]]</span><br><span class="line">    </span><br><span class="line">DESCRIPTION</span><br><span class="line">       Enters the namespaces of one or more other processes and then executes the specified program.  Enterable namespaces are:</span><br><span class="line">    </span><br><span class="line">       mount namespace</span><br><span class="line">              Mounting  and  unmounting  filesystems  will  not affect the rest of the system (CLONE_NEWNS flag), except for filesystems which are explicitly marked as</span><br><span class="line">              shared (with mount --make-shared; see /proc/self/mountinfo for the shared flag).</span><br><span class="line">    </span><br><span class="line">       UTS namespace</span><br><span class="line">              Setting hostname or domainname will not affect the rest of the system.  (CLONE_NEWUTS flag)</span><br><span class="line">    </span><br><span class="line">       IPC namespace</span><br><span class="line">              The process will have an independent namespace for System V message queues, semaphore sets and shared memory segments.  (CLONE_NEWIPC flag)</span><br><span class="line">    </span><br><span class="line">       network namespace</span><br><span class="line">              The process will have independent IPv4 and IPv6 stacks, IP routing tables, firewall rules, the /proc/net and  /sys/class/net  directory  trees,  sockets,</span><br><span class="line">              etc.  (CLONE_NEWNET flag)</span><br><span class="line">    </span><br><span class="line">       PID namespace</span><br><span class="line">              Children  will have a set of PID to process mappings separate from the nsenter process (CLONE_NEWPID flag).  nsenter will fork by default if changing the</span><br><span class="line">              PID namespace, so that the new program and its children share the same PID namespace and are visible to each other.  If --no-fork is used, the  new  pro‐</span><br><span class="line">              gram will be exec&#x27;ed without forking.</span><br><span class="line">    </span><br><span class="line">       user namespace</span><br><span class="line">              The process will have a distinct set of UIDs, GIDs and capabilities.  (CLONE_NEWUSER flag)</span><br><span class="line">    </span><br><span class="line">       See clone(2) for the exact semantics of the flags.</span><br><span class="line">    </span><br><span class="line">       If program is not given, then ``$&#123;SHELL&#125;&#x27;&#x27; is run (default: /bin/sh).</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Observability</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use iperf to measure the network bandwidth</title>
    <url>/blog/using-iperf-to-measure-the-network-bandwidth/</url>
    <content><![CDATA[<p>iPerf is a tool for measurements of the maximum achievable bandwidth on IP network. The following iperf option can be used on the client side to saturate the network bandwidth if single client thread is not sufficient.</p>
<ul>
<li>-P, –parallel  # number of parallel client threads to run</li>
</ul>
<h2 id="Benchmark-a-100GbE-network"><a href="#Benchmark-a-100GbE-network" class="headerlink" title="Benchmark a 100GbE network"></a>Benchmark a 100GbE network</h2><ol>
<li>Single thread test</li>
</ol>
<p>Start iperf on the server side:</p>
<pre><code>$ iperf -v
iperf version 2.0.13 (21 Jan 2019) pthreads

$ iperf -s
------------------------------------------------------------
Server listening on TCP port 5001
TCP window size:  128 KByte (default)
------------------------------------------------------------
[  4] local 192.168.1.240 port 5001 connected with 192.168.1.245 port 44292
[ ID] Interval       Transfer     Bandwidth
[  4]  0.0-10.0 sec  37.7 GBytes  32.4 Gbits/sec
</code></pre>
<p>Run iperf benchmark on the client side:</p>
<pre><code>$ iperf -c 192.168.1.240
------------------------------------------------------------
Client connecting to 192.168.1.240, TCP port 5001
TCP window size: 2.44 MByte (default)
------------------------------------------------------------
[  3] local 192.168.1.245 port 44292 connected with 192.168.1.240 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  37.7 GBytes  32.4 Gbits/sec
</code></pre>
<p>With the default single thread, the achievable bandwidth is 32.4Gb&#x2F;s which is far less than the available 100Gb&#x2F;s bandwidth.</p>
<ol start="2">
<li>Multi-threads test</li>
</ol>
<p>We can increase the number of iperf client threads until the maximum bandwidth is hit.</p>
<pre><code>$ iperf -c 192.168.1.240 -P 1
------------------------------------------------------------
Client connecting to 192.168.1.240, TCP port 5001
TCP window size: 4.00 MByte (default)
------------------------------------------------------------
[  3] local 192.168.1.245 port 44436 connected with 192.168.1.240 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  38.8 GBytes  33.4 Gbits/sec

$ iperf -c 192.168.1.240 -P 2
------------------------------------------------------------
Client connecting to 192.168.1.240, TCP port 5001
TCP window size: 1.13 MByte (default)
------------------------------------------------------------
[  4] local 192.168.1.245 port 44440 connected with 192.168.1.240 port 5001
[  3] local 192.168.1.245 port 44438 connected with 192.168.1.240 port 5001
[ ID] Interval       Transfer     Bandwidth
[  4]  0.0-10.0 sec  35.3 GBytes  30.4 Gbits/sec
[  3]  0.0-10.0 sec  37.6 GBytes  32.3 Gbits/sec
[SUM]  0.0-10.0 sec  72.9 GBytes  62.6 Gbits/sec

$ iperf -c 192.168.1.240 -P 4
------------------------------------------------------------
Client connecting to 192.168.1.240, TCP port 5001
TCP window size:  366 KByte (default)
------------------------------------------------------------
[  6] local 192.168.1.245 port 44448 connected with 192.168.1.240 port 5001
[  5] local 192.168.1.245 port 44446 connected with 192.168.1.240 port 5001
[  4] local 192.168.1.245 port 44444 connected with 192.168.1.240 port 5001
[  3] local 192.168.1.245 port 44442 connected with 192.168.1.240 port 5001
[ ID] Interval       Transfer     Bandwidth
[  6]  0.0-10.0 sec  26.0 GBytes  22.3 Gbits/sec
[  5]  0.0-10.0 sec  26.3 GBytes  22.6 Gbits/sec
[  4]  0.0-10.0 sec  26.3 GBytes  22.6 Gbits/sec
[  3]  0.0-10.0 sec  26.4 GBytes  22.6 Gbits/sec
[SUM]  0.0-10.0 sec   105 GBytes  90.1 Gbits/sec

$ iperf -c 192.168.1.240 -P 8
------------------------------------------------------------
Client connecting to 192.168.1.240, TCP port 5001
TCP window size:  518 KByte (default)
------------------------------------------------------------
[ 16] local 192.168.1.245 port 44464 connected with 192.168.1.240 port 5001
[  3] local 192.168.1.245 port 44452 connected with 192.168.1.240 port 5001
[  7] local 192.168.1.245 port 44456 connected with 192.168.1.240 port 5001
[  4] local 192.168.1.245 port 44450 connected with 192.168.1.240 port 5001
[  6] local 192.168.1.245 port 44454 connected with 192.168.1.240 port 5001
[  9] local 192.168.1.245 port 44462 connected with 192.168.1.240 port 5001
[  8] local 192.168.1.245 port 44460 connected with 192.168.1.240 port 5001
[  5] local 192.168.1.245 port 44458 connected with 192.168.1.240 port 5001
[ ID] Interval       Transfer     Bandwidth
[ 16]  0.0-10.0 sec  15.0 GBytes  12.9 Gbits/sec
[  3]  0.0-10.0 sec  9.06 GBytes  7.78 Gbits/sec
[  7]  0.0-10.0 sec  15.1 GBytes  12.9 Gbits/sec
[  4]  0.0-10.0 sec  15.1 GBytes  13.0 Gbits/sec
[  6]  0.0-10.0 sec  15.1 GBytes  12.9 Gbits/sec
[  9]  0.0-10.0 sec  15.1 GBytes  12.9 Gbits/sec
[  8]  0.0-10.0 sec  8.97 GBytes  7.70 Gbits/sec
[  5]  0.0-10.0 sec  15.1 GBytes  12.9 Gbits/sec
[SUM]  0.0-10.0 sec   108 GBytes  93.1 Gbits/sec
</code></pre>
<p>We can verify the network throughput with the command <em>sar -n DEV 2</em>.</p>
<pre><code>03:26:05 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
03:26:07 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00
03:26:07 PM      eth0     15.00      0.50      0.91      0.33      0.00      0.00      0.00
03:26:07 PM      eth3      0.00      0.00      0.00      0.00      0.00      0.00      0.00
03:26:07 PM      eth1 8045242.00 252491.50 11895015.32  16273.87      0.00      0.00      0.00
03:26:07 PM      eth2      0.00      0.00      0.00      0.00      0.00      0.00      0.00

03:26:07 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
03:26:10 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00
03:26:10 PM      eth0     10.50      1.00      0.65      0.41      0.00      0.00      0.00
03:26:10 PM      eth3      0.00      0.00      0.00      0.00      0.00      0.00      0.00
03:26:10 PM      eth1 8046641.50 251968.50 11897084.51  16240.16      0.00      0.00      0.00
03:26:10 PM      eth2      0.00      0.00      0.00      0.00      0.00      0.00      0.00
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Iperf</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use kubeconfig to access to remote Kubernetes cluster</title>
    <url>/blog/using-kubeconfig-to-configure-access-to-remote-kubernetes-cluster/</url>
    <content><![CDATA[<h2 id="Install-kubectl-binary"><a href="#Install-kubectl-binary" class="headerlink" title="Install kubectl binary"></a>Install kubectl binary</h2><p>The Kubernetes command-line tool, <a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a>, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># curl -LO &quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl&quot;</span></span><br><span class="line">[root@localhost ~]<span class="comment"># sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl</span></span><br><span class="line">[root@localhost ~]<span class="comment"># ls -la /usr/local/bin/kubectl</span></span><br><span class="line">[root@localhost ~]<span class="comment"># kubectl version --output=yaml</span></span><br><span class="line">clientVersion:</span><br><span class="line">  buildDate: <span class="string">&quot;2022-09-21T14:33:49Z&quot;</span></span><br><span class="line">  compiler: gc</span><br><span class="line">  gitCommit: 5835544ca568b757a8ecae5c153f317e5736700e</span><br><span class="line">  gitTreeState: clean</span><br><span class="line">  gitVersion: v1.25.2</span><br><span class="line">  goVersion: go1.19.1</span><br><span class="line">  major: <span class="string">&quot;1&quot;</span></span><br><span class="line">  minor: <span class="string">&quot;25&quot;</span></span><br><span class="line">  platform: linux/amd64</span><br><span class="line">kustomizeVersion: v4.5.7</span><br></pre></td></tr></table></figure>
<h2 id="Use-kubeconfig-file-to-access-remote-Kubernetes-cluster"><a href="#Use-kubeconfig-file-to-access-remote-Kubernetes-cluster" class="headerlink" title="Use kubeconfig file to access remote Kubernetes cluster"></a>Use kubeconfig file to access remote Kubernetes cluster</h2><p>A <em>kubeconfig</em> is a YAML file with all the Kubernetes cluster details, certificate, and secret token to authenticate the cluster. You might get this config file directly from the cluster administrator or from a cloud platform if you are using managed Kubernetes cluster. A <em>kubeconfig</em> file is a file to configure access to Kubernetes when to use with <em>kubectl</em> cli tool.</p>
<p>When to deploy Kubernetes cluster, a <em>kubeconfig</em> is automatically generated. It’s saved in ~&#x2F;.kube&#x2F;config drectory.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ~]<span class="comment">#  ls -la ~/.kube/config</span></span><br><span class="line">-rw------- 1 root root 5577 May  9 02:25 /root/.kube/config</span><br></pre></td></tr></table></figure>

<p>You can access the Kubernetes cluster by providing the kubeconfig file remotely.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># scp host1:/root/.kube/config ./kubeconfig-remote</span></span><br><span class="line">[root@localhost ~]<span class="comment"># kubectl --kubeconfig=./kubeconfig-remote get nodes</span></span><br><span class="line">NAME            STATUS   ROLES    AGE    VERSION</span><br><span class="line">host0   Ready    &lt;none&gt;   135d   v1.19.2</span><br><span class="line">host1   Ready    &lt;none&gt;   135d   v1.19.2</span><br><span class="line">host2   Ready    &lt;none&gt;   135d   v1.19.2</span><br><span class="line">host3   Ready    master   135d   v1.19.2</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>How to tune nconnect to improve NFS performance</title>
    <url>/blog/using-nconnect-to-improve-nfs-performance/</url>
    <content><![CDATA[<p>The nfs mount option “nconnect&#x3D;n” exists in all Linux distributions with kernel 5.3 or higher. nconnect enables multiple TCP connections for a single NFS mount.</p>
<p>From the nfs <a href="https://man7.org/linux/man-pages/man5/nfs.5.html">manual page</a>:</p>
<blockquote>
<p>nconnect&#x3D;n</p>
<blockquote>
<p>When using a connection oriented protocol such as TCP, it may sometimes be advantageous to set up multiple connections between the client and server. For instance, if your clients and&#x2F;or servers are equipped with multiple network interface cards (NICs), using multiple connections to spread the load may improve overall performance.  In such cases, the nconnect option allows the user to specify the number of connections that should be established between the client and server up to a limit of 16.</p>
</blockquote>
</blockquote>
<p>The nconnect option can be easily added during nfs mount as the following command.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mount -t nfs -o nconnect=16 nfs-server-hostname:/mnt/nfsshare /mnt/nfsmnt1 		</span><br></pre></td></tr></table></figure>

<p>The following is the 4k sequential write performance comparison by increasing nconnect to 16 over a single 10GbE link.</p>
<p><img src="/images/nconnect.png" alt="Image"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.suse.com/support/kb/doc/?id=000019933">https://www.suse.com/support/kb/doc/?id=000019933</a></li>
<li><a href="https://medium.com/@emilypotyraj/use-nconnect-to-effortlessly-increase-nfs-performance-4ceb46c64089">https:&#x2F;&#x2F;medium.com&#x2F;@emilypotyraj&#x2F;use-nconnect-to-effortlessly-increase-nfs-performance-4ceb46c64089</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>NFS</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use netstat to check network package retransmission</title>
    <url>/blog/using-netstat-to-check-network-package-retransmission/</url>
    <content><![CDATA[<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">while</span> [ 1 ]; <span class="keyword">do</span> netstat -s | grep <span class="string">&#x27;segments retransmited&#x27;</span> ; <span class="built_in">sleep</span> 1; <span class="keyword">done</span></span><br><span class="line">    2500906 segments retransmited</span><br><span class="line">    2500912 segments retransmited</span><br><span class="line">    2501072 segments retransmited</span><br><span class="line">    2501102 segments retransmited</span><br><span class="line">    2501191 segments retransmited</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use node selector to assign virtual machines to a node</title>
    <url>/blog/using-node-selector-to-assign-virtual-mahcines-to-a-node/</url>
    <content><![CDATA[<p>In some circumstances, we want to control which node the virtual machine or pod deploys to. The <strong>node selector</strong> can be used to assign virtual machine or pod to a node.</p>
<h2 id="Add-label-to-a-node"><a href="#Add-label-to-a-node" class="headerlink" title="Add label to a node"></a>Add label to a node</h2><p>The label can be added to a node from either command line or openshift web console.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ oc get nodes</span><br><span class="line">NAME                   STATUS                     ROLES    AGE   VERSION</span><br><span class="line">master1   Ready                      master   46h   v1.20.11+63f841a</span><br><span class="line">master2   Ready                      master   46h   v1.20.11+63f841a</span><br><span class="line">master3   Ready                      master   46h   v1.20.11+63f841a</span><br><span class="line">worker1    Ready                      worker   45h   v1.20.11+63f841a</span><br><span class="line">worker2    Ready                      worker   45h   v1.20.11+63f841a</span><br><span class="line">worker3    Ready,SchedulingDisabled   worker   45h   v1.20.11+63f841a</span><br><span class="line"></span><br><span class="line">$ oc label nodes worker1 worker-node-name=w1</span><br><span class="line"></span><br><span class="line">$ oc describe node worker1 | grep worker-node-name</span><br><span class="line">                    worker-node-name=w1</span><br><span class="line"></span><br><span class="line">$ oc get nodes --show-labels</span><br><span class="line">NAME       STATUS                     ROLES    AGE   VERSION            LABELS</span><br><span class="line">worker1    Ready                      worker   45h   v1.20.11+63f841a   &lt;omitted..&gt;worker-node-name=w1</span><br><span class="line">&lt;omitted..&gt;</span><br></pre></td></tr></table></figure>

<p>This can also be done through openshift web console by clicking the “edit labels” option on a node.</p>
<p><img src="/images/edit-labels.png" alt="Image"></p>
<h2 id="Add-a-nodeSelector-field-to-the-virtual-machine"><a href="#Add-a-nodeSelector-field-to-the-virtual-machine" class="headerlink" title="Add a nodeSelector field to the virtual machine"></a>Add a nodeSelector field to the virtual machine</h2><p>A node selector can be added through openshift web console.</p>
<p><img src="/images/node-selector-1.png" alt="Image"></p>
<p><img src="/images/nodeselector.png" alt="Image"></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ oc describe vm vm1</span><br><span class="line">    Spec:</span><br><span class="line">      Node Selector:</span><br><span class="line">        Worker - Node - Name:            w1</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Openshift</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use Priority Queue in Python</title>
    <url>/blog/using-priority-queue-in-python/</url>
    <content><![CDATA[<p>The Priority Queue is implemented with binary heap. To understand how it’s implemented, refer to this <a href="https://www.flamingbytes.com/blog/priority-queue-implementation-in-python/">post</a>. Python provides a built-in implementation of the Priority Queue data structure.</p>
<p>For insertion, it uses the <em>put</em> function. The <em>get</em> function dequeues the highest priority item from the queue.</p>
<p>To use the Priority Queue class object:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> queue <span class="keyword">import</span> PriorityQueue</span><br><span class="line">    </span><br><span class="line">q = PriorityQueue()</span><br></pre></td></tr></table></figure>

<p>To insert a tuple pair which has a priority associated with it:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">q.put((<span class="number">2</span>, <span class="string">&quot;Joe&quot;</span>))</span><br><span class="line">q.put((<span class="number">1</span>, <span class="string">&quot;Tim&quot;</span>))</span><br><span class="line">q.put((<span class="number">4</span>, <span class="string">&quot;Watson&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(q.get())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(q.empty())</span><br><span class="line"><span class="built_in">print</span>(q.full())</span><br><span class="line"></span><br><span class="line">q.put((<span class="number">3</span>, <span class="string">&quot;Michael&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(q.get())</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> q.empty():</span><br><span class="line">    <span class="built_in">print</span>(q.get())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(q.empty())</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"></span><br><span class="line">(<span class="number">1</span>,<span class="string">&#x27;Tim&#x27;</span>)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">False</span></span><br><span class="line">(<span class="number">2</span>,<span class="string">&#x27;Joe&#x27;</span>)</span><br><span class="line">(<span class="number">3</span>,<span class="string">&#x27;Michael&#x27;</span>)</span><br><span class="line">(<span class="number">4</span>,<span class="string">&quot;watson)</span></span><br><span class="line"><span class="string">True</span></span><br></pre></td></tr></table></figure>

<p>To simply insert values as priority:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">q.put(<span class="number">12</span>)</span><br><span class="line">q.put(<span class="number">20</span>)</span><br><span class="line"><span class="built_in">print</span>(q.qsize())</span><br><span class="line">q.put(<span class="number">15</span>)</span><br><span class="line"><span class="built_in">print</span>(q.qsize())</span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> q.empty():</span><br><span class="line">    <span class="built_in">print</span>(q.get())</span><br><span class="line"><span class="built_in">print</span>(q.empty())</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">12</span></span><br><span class="line"><span class="number">15</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data structure</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use R to analyze US COVID pandemic waves and peaks</title>
    <url>/blog/using-r-to-analyze-us-covid-waves-and-peaks/</url>
    <content><![CDATA[<h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>According to weekly data between 2020 to 2022, we want to get to know the waves and peaks of COVID pandemic in these years.</p>
<h2 id="Download-the-data"><a href="#Download-the-data" class="headerlink" title="Download the data"></a>Download the data</h2><p>We will continue to use NCHS(National Center for Health Statistics) as our data source.</p>
<p>Visit <a href="https://data.cdc.gov/browse?category=NCHS&amp;sortBy=last_modified">https://data.cdc.gov/browse?category=NCHS&sortBy=last_modified</a>, and search <code>Provisional COVID-19 Death Counts by Week</code>, we will find the data we are intrest.</p>
<p><a href="https://data.cdc.gov/NCHS/Provisional-COVID-19-Death-Counts-by-Week-Ending-D/r8kw-7aab">https://data.cdc.gov/NCHS/Provisional-COVID-19-Death-Counts-by-Week-Ending-D/r8kw-7aab</a>, in this page, we can export data into csv file.</p>
<p>With that, we may get the data source csv, <code>Provisional_COVID-19_Death_Counts_by_Week_Ending_Date_and_State.csv</code>.</p>
<h2 id="Load-the-data"><a href="#Load-the-data" class="headerlink" title="Load the data"></a>Load the data</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ibrary(&quot;dplyr&quot;)</span><br><span class="line">ibrary(&quot;janitor&quot;)</span><br><span class="line">ibrary(&quot;tidyr&quot;)</span><br><span class="line">ibrary(&quot;readr&quot;)</span><br><span class="line">f &lt;- readr::read_csv(file.path(getwd(), &quot;Provisional_COVID-19_Death_Counts_by_Week_Ending_Date_and_State.csv&quot;), col_names = TRUE)</span><br><span class="line">f &lt;- clean_names(df)</span><br><span class="line">mp_start_date &lt;- strptime(df$start_date, &quot;%m/%d/%Y&quot;)</span><br><span class="line">f$start_date &lt;- format(tmp_start_date, &quot;%Y-%m-%d&quot;)</span><br><span class="line">&gt; glimpse(df)</span><br><span class="line">Rows: 10,800</span><br><span class="line">Columns: 17</span><br><span class="line">$ data_as_of                             &lt;chr&gt; &quot;01/09/2023&quot;, &quot;01/09/2023&quot;, &quot;01…</span><br><span class="line">$ start_date                             &lt;chr&gt; &quot;2019-12-29&quot;, &quot;2020-01-05&quot;, &quot;20…</span><br><span class="line">$ end_date                               &lt;chr&gt; &quot;01/04/2020&quot;, &quot;01/11/2020&quot;, &quot;01…</span><br><span class="line">$ group                                  &lt;chr&gt; &quot;By Week&quot;, &quot;By Week&quot;, &quot;By Week&quot;…</span><br><span class="line">$ year                                   &lt;chr&gt; &quot;2019/2020&quot;, &quot;2020&quot;, &quot;2020&quot;, &quot;2…</span><br><span class="line">$ month                                  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA,…</span><br><span class="line">$ mmwr_week                              &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, …</span><br><span class="line">$ week_ending_date                       &lt;chr&gt; &quot;01/04/2020&quot;, &quot;01/11/2020&quot;, &quot;01…</span><br><span class="line">$ state                                  &lt;chr&gt; &quot;United States&quot;, &quot;United States…</span><br><span class="line">$ covid_19_deaths                        &lt;dbl&gt; 0, 1, 2, 3, 0, 4, 6, 6, 9, 38, …</span><br><span class="line">$ total_deaths                           &lt;dbl&gt; 60176, 60734, 59362, 59162, 588…</span><br><span class="line">$ percent_of_expected_deaths             &lt;dbl&gt; 98, 97, 98, 99, 99, 100, 100, 1…</span><br><span class="line">$ pneumonia_deaths                       &lt;dbl&gt; 4111, 4153, 4066, 3915, 3818, 3…</span><br><span class="line">$ pneumonia_and_covid_19_deaths          &lt;dbl&gt; 0, 1, 2, 0, 0, 1, 1, 3, 5, 19, …</span><br><span class="line">$ influenza_deaths                       &lt;dbl&gt; 434, 475, 468, 500, 481, 520, 5…</span><br><span class="line">$ pneumonia_influenza_or_covid_19_deaths &lt;dbl&gt; 4545, 4628, 4534, 4418, 4299, 4…</span><br><span class="line">$ footnote                               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA,…</span><br><span class="line">!&gt; df</span><br><span class="line"># A tibble: 10,800 × 17</span><br><span class="line">   data_as_of start_date end_d…¹ group year  month mmwr_…² week_…³ state covid…⁴</span><br><span class="line">   &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;</span><br><span class="line"> 1 01/09/2023 2019-12-29 01/04/… By W… 2019…    NA       1 01/04/… Unit…       0</span><br><span class="line"> 2 01/09/2023 2020-01-05 01/11/… By W… 2020     NA       2 01/11/… Unit…       1</span><br><span class="line"> 3 01/09/2023 2020-01-12 01/18/… By W… 2020     NA       3 01/18/… Unit…       2</span><br><span class="line"> 4 01/09/2023 2020-01-19 01/25/… By W… 2020     NA       4 01/25/… Unit…       3</span><br><span class="line"> 5 01/09/2023 2020-01-26 02/01/… By W… 2020     NA       5 02/01/… Unit…       0</span><br><span class="line"> 6 01/09/2023 2020-02-02 02/08/… By W… 2020     NA       6 02/08/… Unit…       4</span><br><span class="line"> 7 01/09/2023 2020-02-09 02/15/… By W… 2020     NA       7 02/15/… Unit…       6</span><br><span class="line"> 8 01/09/2023 2020-02-16 02/22/… By W… 2020     NA       8 02/22/… Unit…       6</span><br><span class="line"> 9 01/09/2023 2020-02-23 02/29/… By W… 2020     NA       9 02/29/… Unit…       9</span><br><span class="line">10 01/09/2023 2020-03-01 03/07/… By W… 2020     NA      10 03/07/… Unit…      38</span><br><span class="line"># … with 10,790 more rows, 7 more variables: total_deaths &lt;dbl&gt;,</span><br><span class="line">#   percent_of_expected_deaths &lt;dbl&gt;, pneumonia_deaths &lt;dbl&gt;,</span><br><span class="line">#   pneumonia_and_covid_19_deaths &lt;dbl&gt;, influenza_deaths &lt;dbl&gt;,</span><br><span class="line">#   pneumonia_influenza_or_covid_19_deaths &lt;dbl&gt;, footnote &lt;chr&gt;, and</span><br><span class="line">#   abbreviated variable names ¹ end_date, ² mmwr_week, ³ week_ending_date,</span><br><span class="line">#   ⁴ covid_19_deaths</span><br><span class="line"># ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names</span><br></pre></td></tr></table></figure>

<h2 id="Identify-the-data-we-want-to-focus"><a href="#Identify-the-data-we-want-to-focus" class="headerlink" title="Identify the data we want to focus"></a>Identify the data we want to focus</h2><p>As we can see, there are 4 diffrent groups, and it has the <code>whole United states</code> and each state’s data</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; unique(df$group)</span><br><span class="line">[1] &quot;By Week&quot;  &quot;By Month&quot; &quot;By Year&quot;  &quot;By Total&quot;</span><br><span class="line">&gt;</span><br><span class="line">```   </span><br><span class="line"></span><br><span class="line">We only want to get the weekly data, so we may want to `filter` with &quot;group=By Week&quot;, and &quot;state=United states&quot;</span><br><span class="line"></span><br><span class="line">in the mean time, we may only want to `select` only 2 columns.</span><br><span class="line"></span><br><span class="line">- start_date</span><br><span class="line">- covid_19_deaths</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>df1 &lt;- df %&gt;%<br>    filter(state &#x3D;&#x3D; “United States” &amp; group &#x3D;&#x3D; “By Week”) %&gt;%<br>    select(start_date, covid_19_deaths)<br>print(df1,n&#x3D;20)</p>
</blockquote>
<ul>
<li><blockquote>
<h1 id="A-tibble-158-×-2"><a href="#A-tibble-158-×-2" class="headerlink" title="A tibble: 158 × 2"></a>A tibble: 158 × 2</h1><p> start_date covid_19_deaths<br> <chr>                <dbl><br> 1 2019-12-29               0<br> 2 2020-01-05               1<br> 3 2020-01-12               2<br> 4 2020-01-19               3<br> 5 2020-01-26               0<br> 6 2020-02-02               4<br> 7 2020-02-09               6<br> 8 2020-02-16               6<br> 9 2020-02-23               9<br>10 2020-03-01              38<br>11 2020-03-08              60<br>12 2020-03-15             588<br>13 2020-03-22            3226<br>14 2020-03-29           10141<br>15 2020-04-05           16347<br>16 2020-04-12           17221<br>17 2020-04-19           15557<br>18 2020-04-26           13223<br>19 2020-05-03           11243<br>20 2020-05-10            9239<br>…</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## Draw the graph to see the wave</span><br><span class="line"></span><br></pre></td></tr></table></figure>
library(“ggplot2”)<br>library(“sjPlot”)<br>p &#x3D; ggplot(df1, aes( x&#x3D;start_date, y&#x3D;covid_19_deaths, group&#x3D;1)) +<br>  geom_line(color&#x3D;”blue”) +<br>  theme(axis.text.x&#x3D;element_text(angle&#x3D;45,hjust&#x3D;1,size&#x3D;5))<br>save_plot(“covid_plot_weekly_wave.svg”, fig &#x3D; p, width&#x3D;60, height&#x3D;20)<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">![Image](/images/covid_plot_weekly_wave.svg)</span><br><span class="line"></span><br><span class="line">## Find the peak by R mark it in the graph</span><br><span class="line"></span><br><span class="line">From above graph, we can easily to figure out the waves and peaks, but we also can let R help us to do it, it&#x27;s pretty useful if we have to deal with many data and many graphs.</span><br><span class="line"></span><br><span class="line">To achive it, firstly we can call `findpeaks` from `pracma` library to find the peaks</span><br><span class="line"></span><br></pre></td></tr></table></figure>
library(“pracma”)</li>
<li>peaks &#x3D; findpeaks(df1$covid_19_deaths, npeaks&#x3D;5,  sortstr&#x3D;TRUE)<blockquote>
<blockquote>
<p>peaks<br>    [,1] [,2] [,3] [,4]</p>
</blockquote>
</blockquote>
</li>
</ul>
<p>[1,] 26027   54   40   66<br>[2,] 21364  108   98  121<br>[3,] 17221   16    8   26<br>[4,] 15536   88   79   98<br>[5,]  8308   31   26   38</p>
<blockquote>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">The 2nd column means the the row index of the peak. in this case, we can tell, the 54th row has the top peak covid death number `26027`.</span><br><span class="line"></span><br><span class="line">It&#x27;s not very obvious which week(start_date) is hitting the peak, so we can do something like this.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>is_peak &lt;- vector( “logical” , length(df1$covid_19_deaths ))<br>df1$is_peak &#x3D; is_peak</p>
<p>for (x in peaks[,2]) {<br>  df1$is_peak[x] &#x3D; TRUE<br>}</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">As you can see, we added a new column `is_peak`, so we can use it to filter out those none peak data, sort the peak data points.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>!&gt; df2 &#x3D; df1 %&gt;% filter(is_peak &#x3D;&#x3D; TRUE)</p>
<ul>
<li>df2[order(-df2$covid_19_deaths),]<blockquote>
<h1 id="A-tibble-5-×-3"><a href="#A-tibble-5-×-3" class="headerlink" title="A tibble: 5 × 3"></a>A tibble: 5 × 3</h1><p>start_date covid_19_deaths is_peak<br><chr>                <dbl> <lgl><br> 1 2021-01-03           26027 TRUE<br> 2 2022-01-16           21364 TRUE<br> 3 2020-04-12           17221 TRUE<br> 4 2021-08-29           15536 TRUE<br> 5 2020-07-26            8308 TRUE</p>
<blockquote>
</blockquote>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">## Hightlight the peak points</span><br><span class="line"></span><br></pre></td></tr></table></figure>
p &#x3D; ggplot(df1, aes(x&#x3D;start_date, y&#x3D;covid_19_deaths, group&#x3D;1)) +<br> geom_line(color&#x3D;”blue”) +<br> geom_point(data &#x3D; . %&gt;% filter(is_peak &#x3D;&#x3D; TRUE), stat&#x3D;”identity”, size &#x3D; 4, color &#x3D; “red”) +<br> scale_y_continuous(breaks&#x3D;seq(0,30000,4000)) +<br> theme(axis.text.x&#x3D;element_text(angle&#x3D;45,hjust&#x3D;1,size&#x3D;5))</li>
</ul>
<p>save_plot(“covid_plot_weekly_peak.svg”, fig &#x3D; p, width&#x3D;60, height&#x3D;20)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">![Image](/images/covid_plot_weekly_peak.svg)</span><br><span class="line"></span><br><span class="line">## Other finding</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>!&gt; &gt; sum(df1$covid_19_deaths)<br> [1] 1089714   &#x3D;&#x3D;&#x3D;&gt; the total covid_19_deaths death number from 2020 to 2022</p>
<p>!&gt; summary(df1$covid_19_deaths)<br>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.<br>       0    2223    4428    6897    9862   26027</p>
<blockquote>
</blockquote>
<p>!&gt; df3 &lt;- df %&gt;%</p>
<ul>
<li><pre><code>filter(state == &quot;United States&quot; &amp; group == &quot;By Week&quot;) %&gt;%
</code></pre>
</li>
<li><pre><code>select(start_date, total_deaths)
</code></pre>
</li>
<li>sum(df3$total_deaths)</li>
<li><blockquote>
<p>[1] 10077273  &#x3D;&#x3D;&#x3D;&gt; the total death number from 2020 to 2022 </p>
</blockquote>
</li>
<li>summary(df3$total_deaths)<blockquote>
<p>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.<br> 7100   58522   60451   63780   68610   87415</p>
</blockquote>
</li>
</ul>
<pre><code>
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use sar to understand Linux system activity</title>
    <url>/blog/using-sar-to-understand-system-activity/</url>
    <content><![CDATA[<h2 id="About-sadc-and-sar"><a href="#About-sadc-and-sar" class="headerlink" title="About sadc and sar"></a>About sadc and sar</h2><p>sadc is known as system activity data collector. It samples system data a specified number of times (count) at a specified interval measured in seconds (interval). It writes in binary format to the specified outfile or to standard output. The sadc command is intended to be used as a backend to the sar command. The sar command can be used to collect, report or save system activity information.</p>
<p>The following are examples for some of the often used options to understand system activity including CPU, Memory, Disk IO and network.</p>
<h2 id="Report-CPU-utilization"><a href="#Report-CPU-utilization" class="headerlink" title="Report CPU utilization"></a>Report CPU utilization</h2><p>The ALL keyword with “-u” indicates that all the CPU fields should be displayed.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ~]<span class="comment"># sar -u 1 2</span></span><br><span class="line">Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)</span><br><span class="line"></span><br><span class="line">06:47:11 PM     CPU     %user     %<span class="built_in">nice</span>   %system   %iowait    %steal     %idle</span><br><span class="line">06:47:12 PM     all      0.00      0.00      0.01      0.00      0.00     99.99</span><br><span class="line">06:47:13 PM     all      0.01      0.00      0.00      0.00      0.00     99.99</span><br><span class="line">Average:        all      0.01      0.00      0.01      0.00      0.00     99.99</span><br><span class="line"></span><br><span class="line">[root@host1 ~]<span class="comment"># sar -u ALL 1 2</span></span><br><span class="line">Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)</span><br><span class="line"></span><br><span class="line">06:47:16 PM     CPU      %usr     %<span class="built_in">nice</span>      %sys   %iowait    %steal      %irq     %soft    %guest    %gnice     %idle</span><br><span class="line">06:47:17 PM     all      0.02      0.00      0.02      0.00      0.00      0.00      0.00      0.00      0.00     99.96</span><br><span class="line">06:47:18 PM     all      0.00      0.00      0.01      0.00      0.00      0.00      0.00      0.00      0.00     99.99</span><br><span class="line">Average:        all      0.01      0.00      0.02      0.00      0.00      0.00      0.00      0.00      0.00     99.97</span><br></pre></td></tr></table></figure>

<h2 id="Report-memory-statistics"><a href="#Report-memory-statistics" class="headerlink" title="Report memory statistics"></a>Report memory statistics</h2><pre><code>[root@host1 ~]# sar -R 1 2
Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)

07:00:19 PM   frmpg/s   bufpg/s   campg/s
07:00:20 PM    -10.00      0.00      0.00
07:00:21 PM      8.00      0.00      0.00
Average:        -1.00      0.00      0.00
</code></pre>
<h2 id="Report-memory-utilization-statistics"><a href="#Report-memory-utilization-statistics" class="headerlink" title="Report memory utilization statistics"></a>Report memory utilization statistics</h2><pre><code>[root@host1 ~]# sar -r 1 2
Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)

06:47:59 PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty
06:48:00 PM 1052993760   3499308      0.33    155084    226796   1805324      0.17    343248    178520       184
06:48:01 PM 1052993776   3499292      0.33    155084    226796   1805324      0.17    343464    178520       184
Average:    1052993768   3499300      0.33    155084    226796   1805324      0.17    343356    178520       184
</code></pre>
<h2 id="Report-swap-space-utilization-statistics"><a href="#Report-swap-space-utilization-statistics" class="headerlink" title="Report swap space utilization statistics"></a>Report swap space utilization statistics</h2><pre><code>[root@host1 ~]# sar -S 1 2
Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)

06:47:25 PM kbswpfree kbswpused  %swpused  kbswpcad   %swpcad
06:47:26 PM   4194300         0      0.00         0      0.00
06:47:27 PM   4194300         0      0.00         0      0.00
Average:      4194300         0      0.00         0      0.00
</code></pre>
<h2 id="Report-swapping-statistics"><a href="#Report-swapping-statistics" class="headerlink" title="Report swapping statistics"></a>Report swapping statistics</h2><pre><code>[root@host1 ~]# sar -W 1 2
Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)

06:50:10 PM  pswpin/s pswpout/s
06:50:11 PM      0.00      0.00
06:50:12 PM      0.00      0.00
Average:         0.00      0.00
</code></pre>
<h2 id="Report-paging-statistics"><a href="#Report-paging-statistics" class="headerlink" title="Report paging statistics"></a>Report paging statistics</h2><pre><code>[root@host1 ~]# sar -B 1 2
Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)

06:51:29 PM  pgpgin/s pgpgout/s   fault/s  majflt/s  pgfree/s pgscank/s pgscand/s pgsteal/s    %vmeff
06:51:30 PM      0.00      0.00     22.00      0.00     48.00      0.00      0.00      0.00      0.00
06:51:31 PM      0.00      0.00     19.00      0.00     47.00      0.00      0.00      0.00      0.00
Average:         0.00      0.00     20.50      0.00     47.50      0.00      0.00      0.00      0.00
</code></pre>
<h2 id="Report-hugepages-utilization-statistics"><a href="#Report-hugepages-utilization-statistics" class="headerlink" title="Report hugepages utilization statistics"></a>Report hugepages utilization statistics</h2><pre><code>[root@host1 ~]# sar -H 1 2
Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)

06:54:34 PM kbhugfree kbhugused  %hugused
06:54:35 PM         0         0      0.00
06:54:36 PM         0         0      0.00
Average:            0         0      0.00
</code></pre>
<h2 id="Report-task-creation-and-system-switching-activity"><a href="#Report-task-creation-and-system-switching-activity" class="headerlink" title="Report task creation and system switching activity"></a>Report task creation and system switching activity</h2><pre><code>[root@host1 ~]# sar -w 1 2
Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)

06:50:34 PM    proc/s   cswch/s
06:50:35 PM      0.00    524.00
06:50:36 PM      0.00    721.00
Average:         0.00    622.50
</code></pre>
<h2 id="Report-I-O-and-transfer-rate-statistics"><a href="#Report-I-O-and-transfer-rate-statistics" class="headerlink" title="Report I&#x2F;O and transfer rate statistics"></a>Report I&#x2F;O and transfer rate statistics</h2><pre><code>[root@host1 ~]# sar -b 1 2
Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)

06:52:14 PM       tps      rtps      wtps   bread/s   bwrtn/s
06:52:15 PM      0.00      0.00      0.00      0.00      0.00
06:52:16 PM      0.00      0.00      0.00      0.00      0.00
Average:         0.00      0.00      0.00      0.00      0.00
</code></pre>
<h2 id="Report-activity-for-each-block-device"><a href="#Report-activity-for-each-block-device" class="headerlink" title="Report  activity for each block device"></a>Report  activity for each block device</h2><pre><code>[root@host1 ~]# sar -d 1 2
Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)

06:53:04 PM       DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util
06:53:05 PM  dev259-0      1.00      0.00      8.00      8.00      0.00      0.00      1.00      0.10
06:53:05 PM  dev259-4      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:05 PM  dev259-5      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:05 PM  dev259-6      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:05 PM  dev259-7      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:05 PM  dev259-8      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:05 PM  dev259-9      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:05 PM dev259-10      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:05 PM dev259-11      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:05 PM dev259-12      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:05 PM  dev253-0      1.00      0.00      8.00      8.00      0.00      0.00      1.00      0.10
06:53:05 PM  dev253-1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

06:53:05 PM       DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util
06:53:06 PM  dev259-0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:06 PM  dev259-4      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:06 PM  dev259-5      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:06 PM  dev259-6      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:06 PM  dev259-7      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:06 PM  dev259-8      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:06 PM  dev259-9      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:06 PM dev259-10      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:06 PM dev259-11      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:06 PM dev259-12      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:06 PM  dev253-0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:53:06 PM  dev253-1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

Average:          DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util
Average:     dev259-0      0.50      0.00      4.00      8.00      0.00      0.00      1.00      0.05
Average:     dev259-4      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:     dev259-5      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:     dev259-6      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:     dev259-7      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:     dev259-8      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:     dev259-9      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:    dev259-10      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:    dev259-11      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:    dev259-12      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:     dev253-0      0.50      0.00      4.00      8.00      0.00      0.00      1.00      0.05
Average:     dev253-1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
</code></pre>
<h2 id="Report-network-statistics"><a href="#Report-network-statistics" class="headerlink" title="Report network statistics"></a>Report network statistics</h2><pre><code>[root@host1 ~]# sar -n DEV 1 2
Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)

06:56:34 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
06:56:35 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:56:35 PM      eth0     15.00      1.00      0.89      0.14      0.00      0.00      0.00
06:56:35 PM      eth3      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:56:35 PM      eth1      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:56:35 PM      eth2      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:56:35 PM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00

06:56:35 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
06:56:36 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:56:36 PM      eth0     16.00      1.00      0.94      0.74      0.00      0.00      0.00
06:56:36 PM      eth3      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:56:36 PM      eth1      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:56:36 PM      eth2      0.00      0.00      0.00      0.00      0.00      0.00      0.00
06:56:36 PM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00

Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s
Average:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:         eth0     15.50      1.00      0.92      0.44      0.00      0.00      0.00
Average:         eth3      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:         eth1      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:         eth2      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:      docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00
</code></pre>
<h2 id="Save-and-extract-records-from-file"><a href="#Save-and-extract-records-from-file" class="headerlink" title="Save and extract records from file"></a>Save and extract records from file</h2><p>Save the readings in the file in binary form. Each reading is in a separate record. The default value of the filename parameter is the current daily data file,  the &#x2F;var&#x2F;log&#x2F;sa&#x2F;sadd file. The -o option is exclusive of the -f option.  All the data available from the kernel are saved in the file (in fact, sar calls its data collector sadc with the option “-S ALL”).</p>
<pre><code>[root@host1 ~]# sar -o sar.out.1.2

[root@host1 ~]# sar -f sar.out.1.2 -u
Linux 5.7.12-1.el7.elrepo.x86_64 (host1) 	07/06/2022 	_x86_64_	(96 CPU)

07:04:45 PM     CPU     %user     %nice   %system   %iowait    %steal     %idle
07:04:46 PM     all      0.02      0.00      0.05      0.00      0.00     99.93
07:04:47 PM     all      0.06      0.00      0.11      0.00      0.00     99.82
Average:        all      0.04      0.00      0.08      0.00      0.00     99.87
</code></pre>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Observability</tag>
        <tag>Sar</tag>
      </tags>
  </entry>
  <entry>
    <title>Using signals with kill command in Linux</title>
    <url>/blog/using-signals-with-kill-in-linux/</url>
    <content><![CDATA[<h2 id="Usage-of-signals-with-kill"><a href="#Usage-of-signals-with-kill" class="headerlink" title="Usage of signals with kill"></a>Usage of signals with kill</h2><pre><code>$ kill -l
 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP
 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1
11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM
16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP
21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ
26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR
31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3
38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8
43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13
48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12
53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7
58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2
63) SIGRTMAX-1 64) SIGRTMAX
</code></pre>
<h2 id="An-example-to-send-SIGINT-signal-in-shell"><a href="#An-example-to-send-SIGINT-signal-in-shell" class="headerlink" title="An example to send SIGINT signal in shell"></a>An example to send SIGINT signal in shell</h2><p>In the following example, we start strace for a given process for given amounts of seconds. Once the time is up, we send SIGINT(Ctrl+C) signal to stop the tracing process. And a trace report will be generated to the output file strace.c.out.</p>
<pre><code>$ cat strace_summary.sh
processname=$1
duration=$2
while true
do
    strace -p `pidof $processname` -c &gt; strace.c.out 2&gt;&amp;1 &amp;
    sleep $duration
    break
done

kill -2 `pidof strace`
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://linux.die.net/Bash-Beginners-Guide/sect_12_01.html#:~:text=The%20interrupt%20signal%2C%20sends%20SIGINT,job%20running%20in%20the%20foreground.&amp;text=The%20delayed%20suspend%20character.,background%20or%20kill%20the%20process.">Signals in Linux</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title>Using sysbench for OLTP workload performance benchmark</title>
    <url>/blog/using-sysbench-for-oltp-workload-performance-benchmark/</url>
    <content><![CDATA[<h2 id="Intro-to-Sysbench"><a href="#Intro-to-Sysbench" class="headerlink" title="Intro to Sysbench"></a>Intro to Sysbench</h2><p>sysbench is a scriptable multi-threaded benchmark tool based on LuaJIT. It is most frequently used for database benchmarks, but can also be used to create arbitrarily complex workloads that do not involve a database server.</p>
<p>sysbench comes with the following bundled benchmarks:</p>
<ul>
<li>oltp_*.lua: a collection of OLTP-like database benchmarks</li>
<li>fileio: a filesystem-level benchmark</li>
<li>cpu: a simple CPU benchmark</li>
<li>memory: a memory access benchmark</li>
<li>threads: a thread-based scheduler benchmark</li>
<li>mutex: a POSIX mutex benchmark</li>
</ul>
<p>Below is a description of typical test commands and their purpose:</p>
<ul>
<li>prepare: performs preparative actions for those tests which need them, e.g. creating the necessary files on disk for the fileio test, or filling the test database for database benchmarks.</li>
<li>run: runs the actual test specified with the testname argument. This command is provided by all tests.</li>
<li>cleanup: removes temporary data after the test run in those tests which create one.</li>
<li>help: displays usage information for the test specified with the testname argument. This includes the full list of commands provided by the test, so it should be used to get the available commands.</li>
</ul>
<h2 id="Install-sysbench-on-CentOS-7-5"><a href="#Install-sysbench-on-CentOS-7-5" class="headerlink" title="Install sysbench on CentOS 7.5"></a>Install sysbench on CentOS 7.5</h2><pre><code>$ cat /etc/centos-release
CentOS Linux release 7.5.1804 (Core)
$ uname -r
5.7.12-1.el7.elrepo.x86_64

$ curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash
$ sudo yum -y install sysbench

$ sysbench --version
sysbench 1.0.20

$ sysbench --help
Usage:
  sysbench [options]... [testname] [command]

Commands implemented by most tests: prepare run cleanup help

General options:
  --threads=N                     number of threads to use [1]
  --events=N                      limit for total number of events [0]
  --time=N                        limit for total execution time in seconds [10]
  --forced-shutdown=STRING        number of seconds to wait after the --time limit before forcing shutdown, or &#39;off&#39; to disable [off]
  --thread-stack-size=SIZE        size of stack per thread [64K]
  --rate=N                        average transactions rate. 0 for unlimited rate [0]
  --report-interval=N             periodically report intermediate statistics with a specified interval in seconds. 0 disables intermediate reports [0]
  --report-checkpoints=[LIST,...] dump full statistics and reset all counters at specified points in time. The argument is a list of comma-separated values representing the amount of time in seconds elapsed from start of test when report checkpoint(s) must be performed. Report checkpoints are off by default. []
  --debug[=on|off]                print more debugging info [off]
  --validate[=on|off]             perform validation checks where possible [off]
  --help[=on|off]                 print help and exit [off]
  --version[=on|off]              print version and exit [off]
  --config-file=FILENAME          File containing command line options
  --tx-rate=N                     deprecated alias for --rate [0]
  --max-requests=N                deprecated alias for --events [0]
  --max-time=N                    deprecated alias for --time [0]
  --num-threads=N                 deprecated alias for --threads [1]

Pseudo-Random Numbers Generator options:
  --rand-type=STRING random numbers distribution &#123;uniform,gaussian,special,pareto&#125; [special]
  --rand-spec-iter=N number of iterations used for numbers generation [12]
  --rand-spec-pct=N  percentage of values to be treated as &#39;special&#39; (for special distribution) [1]
  --rand-spec-res=N  percentage of &#39;special&#39; values to use (for special distribution) [75]
  --rand-seed=N      seed for random number generator. When 0, the current time is used as a RNG seed. [0]
  --rand-pareto-h=N  parameter h for pareto distribution [0.2]

Log options:
  --verbosity=N verbosity level &#123;5 - debug, 0 - only critical messages&#125; [3]

  --percentile=N       percentile to calculate in latency statistics (1-100). Use the special value of 0 to disable percentile calculations [95]
  --histogram[=on|off] print latency histogram in report [off]

General database options:

  --db-driver=STRING  specifies database driver to use (&#39;help&#39; to get list of available drivers) [mysql]
  --db-ps-mode=STRING prepared statements usage mode &#123;auto, disable&#125; [auto]
  --db-debug[=on|off] print database-specific debug information [off]


Compiled-in database drivers:
  mysql - MySQL driver
  pgsql - PostgreSQL driver

mysql options:
  --mysql-host=[LIST,...]          MySQL server host [localhost]
  --mysql-port=[LIST,...]          MySQL server port [3306]
  --mysql-socket=[LIST,...]        MySQL socket
  --mysql-user=STRING              MySQL user [sbtest]
  --mysql-password=STRING          MySQL password []
  --mysql-db=STRING                MySQL database name [sbtest]
  --mysql-ssl[=on|off]             use SSL connections, if available in the client library [off]
  --mysql-ssl-cipher=STRING        use specific cipher for SSL connections []
  --mysql-compression[=on|off]     use compression, if available in the client library [off]
  --mysql-debug[=on|off]           trace all client library calls [off]
  --mysql-ignore-errors=[LIST,...] list of errors to ignore, or &quot;all&quot; [1213,1020,1205]
  --mysql-dry-run[=on|off]         Dry run, pretend that all MySQL client API calls are successful without executing them [off]

pgsql options:
  --pgsql-host=STRING     PostgreSQL server host [localhost]
  --pgsql-port=N          PostgreSQL server port [5432]
  --pgsql-user=STRING     PostgreSQL user [sbtest]
  --pgsql-password=STRING PostgreSQL password []
  --pgsql-db=STRING       PostgreSQL database name [sbtest]

Compiled-in tests:
  fileio - File I/O test
  cpu - CPU performance test
  memory - Memory functions speed test
  threads - Threads subsystem performance test
  mutex - Mutex performance test

See &#39;sysbench &lt;testname&gt; help&#39; for a list of options for each test.

$ ls -la /usr/share/sysbench/tests/include/oltp_legacy/
total 56
drwxr-xr-x 2 root root  284 Sep  7 20:53 .
drwxr-xr-x 3 root root 4096 Sep  7 20:53 ..
-rw-r--r-- 1 root root 1195 Apr 24  2020 bulk_insert.lua
-rw-r--r-- 1 root root 4696 Apr 24  2020 common.lua
-rw-r--r-- 1 root root  366 Apr 24  2020 delete.lua
-rw-r--r-- 1 root root 1171 Apr 24  2020 insert.lua
-rw-r--r-- 1 root root 3004 Apr 24  2020 oltp.lua
-rw-r--r-- 1 root root  368 Apr 24  2020 oltp_simple.lua
-rw-r--r-- 1 root root  527 Apr 24  2020 parallel_prepare.lua
-rw-r--r-- 1 root root  369 Apr 24  2020 select.lua
-rw-r--r-- 1 root root 1448 Apr 24  2020 select_random_points.lua
-rw-r--r-- 1 root root 1556 Apr 24  2020 select_random_ranges.lua
-rw-r--r-- 1 root root  369 Apr 24  2020 update_index.lua
-rw-r--r-- 1 root root  578 Apr 24  2020 update_non_index.lua
</code></pre>
<h2 id="MariaDB-vs-MySQL"><a href="#MariaDB-vs-MySQL" class="headerlink" title="MariaDB vs. MySQL"></a>MariaDB vs. MySQL</h2><p>MariaDB is a community-developed, commercially supported fork of the MySQL relational database management system (RDBMS), intended to remain free and open-source software under the GNU General Public License. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation in 2009. Refer to <a href="https://en.wikipedia.org/wiki/MariaDB">wiki</a> for more information.</p>
<h2 id="Create-the-MariaDB-database"><a href="#Create-the-MariaDB-database" class="headerlink" title="Create the MariaDB database"></a>Create the MariaDB database</h2><h3 id="Provision-the-MariaDB-docker-instance"><a href="#Provision-the-MariaDB-docker-instance" class="headerlink" title="Provision the MariaDB docker instance"></a>Provision the MariaDB docker instance</h3><p>In this example, we use Portworx to manage the disk storage. A volume <em>testVol</em> is created to store MariaDB data.</p>
<pre><code>$ pxctl v create testVol --size 1024 --repl 1

$ docker run --name mariadbtest -v testVol:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=password -p 3306:3306 -d docker.io/library/mariadb:latest

$ docker ps | egrep &quot;CONTAINER|mariadbtest&quot;
CONTAINER ID   IMAGE                        COMMAND                  CREATED          STATUS          PORTS                                       NAMES
2e5fe8ca177d   mariadb:latest               &quot;docker-entrypoint.s…&quot;   39 seconds ago   Up 37 seconds   0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp   mariadbtest
</code></pre>
<h3 id="Create-a-database"><a href="#Create-a-database" class="headerlink" title="Create a database"></a>Create a database</h3><pre><code>$ docker exec -it mariadbtest bash

root@2e5fe8ca177d:/# ip a | grep eth
139: eth0@if140: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.5/16 brd 172.17.255.255 scope global eth0

root@2e5fe8ca177d:/# df -h 
Filesystem                       Size  Used Avail Use% Mounted on
overlay                           50G   23G   28G  45% /
tmpfs                             64M     0   64M   0% /dev
tmpfs                            126G     0  126G   0% /sys/fs/cgroup
shm                               64M     0   64M   0% /dev/shm
/dev/mapper/centos-root           50G   23G   28G  45% /etc/hosts
/dev/pxd/pxd1020609855122786711 1007G  209M  956G   1% /var/lib/mysql
tmpfs                            126G     0  126G   0% /proc/acpi
tmpfs                            126G     0  126G   0% /proc/scsi
tmpfs                            126G     0  126G   0% /sys/firmware

root@2e5fe8ca177d:/# ls -la /var/lib/mysql
total 123332
drwxr-xr-x. 5 mysql mysql      4096 Sep  7 21:02 .
drwxr-xr-x  1 root  root         68 Aug 31 03:44 ..
-rw-rw----  1 mysql mysql    417792 Sep  7 21:02 aria_log.00000001
-rw-rw----  1 mysql mysql        52 Sep  7 21:02 aria_log_control
-rw-rw----  1 mysql mysql         9 Sep  7 21:02 ddl_recovery.log
-rw-rw----  1 mysql mysql       946 Sep  7 21:02 ib_buffer_pool
-rw-rw----  1 mysql mysql 100663296 Sep  7 21:02 ib_logfile0
-rw-rw----  1 mysql mysql  12582912 Sep  7 21:02 ibdata1
-rw-rw----  1 mysql mysql  12582912 Sep  7 21:02 ibtmp1
-rw-rw----  1 mysql mysql         0 Sep  7 21:00 multi-master.info
drwx------  2 mysql mysql      4096 Sep  7 21:00 mysql
drwx------  2 mysql mysql      4096 Sep  7 21:00 performance_schema
drwx------  2 mysql mysql     12288 Sep  7 21:00 sys

root@2e5fe8ca177d:/#  mysql -u root -p
Enter password:
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 3
Server version: 10.6.4-MariaDB-1:10.6.4+maria~focal mariadb.org binary distribution

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.

MariaDB [(none)]&gt; SHOW VARIABLES LIKE &quot;%version%&quot;;
+-----------------------------------+------------------------------------------+
| Variable_name                     | Value                                    |
+-----------------------------------+------------------------------------------+
| in_predicate_conversion_threshold | 1000                                     |
| innodb_version                    | 10.6.4                                   |
| protocol_version                  | 10                                       |
| slave_type_conversions            |                                          |
| system_versioning_alter_history   | ERROR                                    |
| system_versioning_asof            | DEFAULT                                  |
| tls_version                       | TLSv1.1,TLSv1.2,TLSv1.3                  |
| version                           | 10.6.4-MariaDB-1:10.6.4+maria~focal      |
| version_comment                   | mariadb.org binary distribution          |
| version_compile_machine           | x86_64                                   |
| version_compile_os                | debian-linux-gnu                         |
| version_malloc_library            | system                                   |
| version_source_revision           | 2db692f5b4d6bb31a331dab44544171c455f6aca |
| version_ssl_library               | OpenSSL 1.1.1f  31 Mar 2020              |
| wsrep_patch_version               | wsrep_26.22                              |
+-----------------------------------+------------------------------------------+
15 rows in set (0.002 sec)

MariaDB [(none)]&gt; SHOW VARIABLES WHERE Variable_Name LIKE &quot;%dir&quot;;
+---------------------------+----------------------------+
| Variable_name             | Value                      |
+---------------------------+----------------------------+
| aria_sync_log_dir         | NEWFILE                    |
| basedir                   | /usr                       |
| character_sets_dir        | /usr/share/mysql/charsets/ |
| datadir                   | /var/lib/mysql/            |
| innodb_data_home_dir      |                            |
| innodb_log_group_home_dir | ./                         |
| innodb_tmpdir             |                            |
| lc_messages_dir           | /usr/share/mysql           |
| plugin_dir                | /usr/lib/mysql/plugin/     |
| slave_load_tmpdir         | /tmp                       |
| tmpdir                    | /tmp                       |
| wsrep_data_home_dir       | /var/lib/mysql/            |
+---------------------------+----------------------------+
12 rows in set (0.002 sec)

MariaDB [(none)]&gt; CREATE DATABASE sbtest;
Query OK, 1 row affected (0.001 sec)

MariaDB [(none)]&gt; CREATE USER sbtest@localhost;
Query OK, 0 rows affected (0.004 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON sbtest.* TO sbtest@localhost;
Query OK, 0 rows affected (0.002 sec)

MariaDB [(none)]&gt; use sbtest;
Database changed

MariaDB [sbtest]&gt; select database();
+------------+
| database() |
+------------+
| sbtest     |
+------------+
1 row in set (0.000 sec)

MariaDB [sbtest]&gt; show tables;
Empty set (0.000 sec)

MariaDB [(none)]&gt;  exit
Bye
root@2e5fe8ca177d:/# exit
exit

root@2e5fe8ca177d:/# ls -la /var/lib/mysql
total 123336
drwxr-xr-x. 6 mysql mysql      4096 Sep  7 21:07 .
drwxr-xr-x  1 root  root         68 Aug 31 03:44 ..
-rw-rw----  1 mysql mysql    417792 Sep  7 21:07 aria_log.00000001
-rw-rw----  1 mysql mysql        52 Sep  7 21:02 aria_log_control
-rw-rw----  1 mysql mysql         9 Sep  7 21:02 ddl_recovery.log
-rw-rw----  1 mysql mysql       946 Sep  7 21:02 ib_buffer_pool
-rw-rw----  1 mysql mysql 100663296 Sep  7 21:02 ib_logfile0
-rw-rw----  1 mysql mysql  12582912 Sep  7 21:02 ibdata1
-rw-rw----  1 mysql mysql  12582912 Sep  7 21:02 ibtmp1
-rw-rw----  1 mysql mysql         0 Sep  7 21:00 multi-master.info
drwx------  2 mysql mysql      4096 Sep  7 21:00 mysql
drwx------  2 mysql mysql      4096 Sep  7 21:00 performance_schema
drwx------  2 mysql mysql      4096 Sep  7 21:07 sbtest
drwx------  2 mysql mysql     12288 Sep  7 21:00 sys
root@2e5fe8ca177d:/# ls -la /var/lib/mysql/sbtest/
total 12
drwx------  2 mysql mysql 4096 Sep  7 21:07 .
drwxr-xr-x. 6 mysql mysql 4096 Sep  7 21:07 ..
-rw-rw----  1 mysql mysql   67 Sep  7 21:07 db.opt
</code></pre>
<h3 id="Build-the-database"><a href="#Build-the-database" class="headerlink" title="Build the database"></a>Build the database</h3><p>On the host, using sysbench to create tables and insert data rows in the database. We need know how much data should be created in the database. 1 million rows will result in ~240MB of data. So, 32 tables, 2 millions rows each create 15GB data.</p>
<pre><code>$ sysbench /usr/share/sysbench/tests/include/oltp_legacy/oltp.lua --threads=1 --mysql-host=172.17.0.5 --mysql-password=password  --mysql-user=root --mysql-db=sbtest --oltp-tables-count=32 --oltp-table-size=2000000 prepare
sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)

Creating table &#39;sbtest1&#39;...
Inserting 2000000 records into &#39;sbtest1&#39;
Creating secondary indexes on &#39;sbtest1&#39;...
[omitted...]
</code></pre>
<p>In the MariaDB container, we can check the created data and table size.</p>
<pre><code>root@2e5fe8ca177d:/#  mysql -u root -p
MariaDB [sbtest]&gt; select * from sbtest1 limit 6;
+----+---------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+
| id | k       | c                                                                                                                       | pad                                                         |
+----+---------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+
|  1 |  998567 | 83868641912-28773972837-60736120486-75162659906-27563526494-20381887404-41576422241-93426793964-56405065102-33518432330 | 67847967377-48000963322-62604785301-91415491898-96926520291 |
|  2 | 1003937 | 38014276128-25250245652-62722561801-27818678124-24890218270-18312424692-92565570600-36243745486-21199862476-38576014630 | 23183251411-36241541236-31706421314-92007079971-60663066966 |
|  3 | 1008521 | 33973744704-80540844748-72700647445-87330233173-87249600839-07301471459-22846777364-58808996678-64607045326-48799346817 | 38615512647-91458489257-90681424432-95014675832-60408598704 |
|  4 | 1004027 | 37002370280-58842166667-00026392672-77506866252-09658311935-56926959306-83464667271-94685475868-28264244556-14550208498 | 63947013338-98809887124-59806726763-79831528812-45582457048 |
|  5 |  999625 | 44257470806-17967007152-32809666989-26174672567-29883439075-95767161284-94957565003-35708767253-53935174705-16168070783 | 34551750492-67990399350-81179284955-79299808058-21257255869 |
|  6 | 1001169 | 37216201353-39109531021-11197415756-87798784755-02463049870-83329763120-57551308766-61100580113-80090253566-30971527105 | 05161542529-00085727016-35134775864-52531204064-98744439797 |
+----+---------+-------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+
6 rows in set (0.004 sec)

MariaDB [sbtest]&gt; SELECT   TABLE_NAME AS `Table`,   ROUND((DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024) AS `Size (MB)` FROM   information_schema.TABLES WHERE   TABLE_SCHEMA = &quot;sbtest&quot; ORDER BY   (DATA_LENGTH + INDEX_LENGTH) DESC;
+---------+-----------+
| Table   | Size (MB) |
+---------+-----------+
| sbtest3 |       459 |
| sbtest1 |       459 |
| sbtest4 |       459 |
| sbtest2 |       459 |
| sbtest5 |       459 |
| sbtest6 |       146 |
+---------+-----------+
6 rows in set (0.002 sec)

MariaDB [sbtest]&gt; SELECT table_schema &quot;DB Name&quot;, ROUND(SUM(data_length + index_length) / 1024 / 1024, 1) &quot;DB Size in MB&quot;  FROM information_schema.tables  GROUP BY table_schema;
+--------------------+---------------+
| DB Name            | DB Size in MB |
+--------------------+---------------+
| information_schema |           0.2 |
| mysql              |          10.5 |
| performance_schema |           0.0 |
| sbtest             |       14702.0 |
| sys                |           0.0 |
+--------------------+---------------+
5 rows in set (0.033 sec)
</code></pre>
<p>We also can check the running process in the MariaDB.</p>
<pre><code>MariaDB [sbtest]&gt; show processlist;
+----+------+------------------+--------+---------+------+----------
| Id | User | Host             | db     | Command | Time | State    | Info                                                                                                 | Progress |
+----+------+------------------+--------+---------+------+----------
|  7 | root | 172.17.0.1:55000 | sbtest | Query   |    0 | Update   | INSERT INTO sbtest13(k, c, pad) VALUES(1185731, &#39;26498931212-26730519067-66264645428-09623019003-787&#39; |    0.000 |
| 11 | root | localhost        | sbtest | Query   |    0 | starting | show processlist                                                                                     |    0.000 |
+----+------+------------------+--------+---------+------+----------
2 rows in set (0.000 sec)
</code></pre>
<h2 id="Run-sysbench-benchmark"><a href="#Run-sysbench-benchmark" class="headerlink" title="Run sysbench benchmark"></a>Run sysbench benchmark</h2><pre><code>$ threads=1; seconds=1800; interval=60
$ sysbench /usr/share/sysbench/tests/include/oltp_legacy/oltp.lua --threads=$threads --mysql-host=172.17.0.5 --mysql-password=password  --mysql-user=root  --oltp-tables-count=32 --oltp-table-size=2000000 --events=0 --time=$seconds --report-interval=$interval --delete_inserts=10 --index_updates=10 --non_index_updates=10 --db-ps-mode=disable run

SQL statistics:
    queries performed:
        read:                            1315888
        write:                           375968
        other:                           187984
        total:                           1879840
    transactions:                        93992  (52.22 per sec.)
    queries:                             1879840 (1044.35 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          1800.0066s
    total number of events:              93992

Latency (ms):
         min:                                    6.52
         avg:                                   19.14
         max:                                 1018.82
         95th percentile:                       25.28
         sum:                              1799473.52

Threads fairness:
    events (avg/stddev):           93992.0000/0.00
    execution time (avg/stddev):   1799.4735/0.00
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/MariaDB">https://en.wikipedia.org/wiki/MariaDB</a></li>
<li><a href="https://github.com/akopytov/sysbench">https://github.com/akopytov/sysbench</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Sysbench</tag>
        <tag>OLTP</tag>
      </tags>
  </entry>
  <entry>
    <title>Using systemtap to analyze latency of the kernel module function</title>
    <url>/blog/using-systemtap-to-analyze-latency-of-the-kernel-module-function/</url>
    <content><![CDATA[<p>In this post, we continue to explore how to use SystemTap to analyze the latency of the kernel module function. In the following example, we want to analyze the latency of the function “nfsd_vfs_write” from the kernel module “nfsd”.</p>
<h2 id="Deploy-the-SystemTap-packages"><a href="#Deploy-the-SystemTap-packages" class="headerlink" title="Deploy the SystemTap packages"></a>Deploy the SystemTap packages</h2><p>Refer to <a href="/blog/getting-started-with-systemtap-for-linux-system-profiling/">this</a> post to deploy SystemTap and its required packages.</p>
<h2 id="Check-the-nfsd-kernel-module-info"><a href="#Check-the-nfsd-kernel-module-info" class="headerlink" title="Check the nfsd kernel module info"></a>Check the nfsd kernel module info</h2><pre><code>[root@host1 ~]# uname -r
3.10.0-1160.el7.x86_64

[root@host1 ~]# ls /lib/modules/`uname -r`/kernel/
arch  crypto  drivers  fs  kernel  lib  mm  net  sound  virt
[root@host1 ~]# ls /lib/modules/`uname -r`/kernel/fs
binfmt_misc.ko.xz  btrfs  cachefiles  ceph  cifs  cramfs  dlm  exofs  ext4  fat  fscache  fuse  gfs2  isofs  jbd2  lockd  mbcache.ko.xz  nfs  nfs_common  nfsd  nls  overlayfs  pstore  squashfs  udf  xfs

[root@host1 ~]# ls /lib/modules/`uname -r`/kernel/fs/ext4
ext4.ko.xz
[root@host1 ~]# ls /lib/modules/`uname -r`/kernel/fs/xfs
xfs.ko.xz
[root@host1 ~]# ls /lib/modules/`uname -r`/kernel/fs/btrfs
btrfs.ko.xz
[root@host1 ~]# ls /lib/modules/`uname -r`/kernel/fs/nfs
blocklayout  filelayout  flexfilelayout  nfs.ko.xz  nfsv3.ko.xz  nfsv4.ko.xz  objlayout


[root@host1 ~]# lsmod | grep &quot;nfsd &quot;
nfsd                  351321  13
[root@host1 ~]# modinfo nfsd
filename:       /lib/modules/3.10.0-1160.el7.x86_64/kernel/fs/nfsd/nfsd.ko.xz
license:        GPL
author:         Olaf Kirch &lt;okir@monad.swb.de&gt;
alias:          fs-nfsd
retpoline:      Y
rhelversion:    7.9
srcversion:     61A6390CD82AA4A7492CB06
depends:        auth_rpcgss,sunrpc,grace,lockd,nfs_acl
intree:         Y
vermagic:       3.10.0-1160.el7.x86_64 SMP mod_unload modversions
signer:         CentOS Linux kernel signing key
sig_key:        E1:FD:B0:E2:A7:E8:61:A1:D1:CA:80:A2:3D:CF:0D:BA:3A:A4:AD:F5
sig_hashalgo:   sha256
parm:           cltrack_prog:Path to the nfsdcltrack upcall program (string)
parm:           cltrack_legacy_disable:Disable legacy recoverydir conversion. Default: false (bool)
parm:           nfs4_disable_idmapping:Turn off server&#39;s NFSv4 idmapping when using &#39;sec=sys&#39; (bool)

[root@host1 ~]# cat /proc/kallsyms |  egrep -i -w &quot;nfsd_vfs_write&quot;
ffffffffc094fdd0 t nfsd_vfs_write	[nfsd] 
</code></pre>
<h2 id="SystemTap-script"><a href="#SystemTap-script" class="headerlink" title="SystemTap script"></a>SystemTap script</h2><p>In the following SystemTap script, we have implemented</p>
<ul>
<li><p>a probe to detect the kernel module “nfsd” and its function “nfsd_vfs_write”</p>
</li>
<li><p>a probe to detect the return of the function “nfsd_vfs_write”</p>
</li>
<li><p>handlers to get the execname, pid, tid and timestamp in each probe function</p>
</li>
<li><p>a probe as timer(5 seconds tracing)</p>
</li>
<li><p>a probe “end” to analyze the collected runtime of the function “nfsd_vfs_write” in the end of tracing</p>
<p>  [root@host1 ~]# cat nfs_trace.stp<br>  global count<br>  global start_t,diff_t[1000000]</p>
<p>  probe module(“nfsd”).function(“nfsd_vfs_write”){<br>  count++<br>  e &#x3D; execname()<br>  p &#x3D; pid()<br>  t &#x3D; tid()<br>  start_t[e,p,t] &#x3D; gettimeofday_us()<br>  }</p>
<p>  probe module(“nfsd”).function(“nfsd_vfs_write”).return {<br>  e &#x3D; execname()<br>  p &#x3D; pid()<br>  t &#x3D; tid()<br>  start_ts &#x3D; start_t[e,p,t]<br>  end_ts &#x3D; gettimeofday_us()<br>  if(start_ts&gt;0)<br>      diff_t[e,p,t,start_ts] &#x3D; end_ts - start_ts<br>  }</p>
<p>  probe timer.s(5){exit()}</p>
<p>  probe end{<br>  count&#x3D;1<br>  total_time&#x3D;0<br>  foreach([e,p,t,ts] in diff_t){<br>      printf(“nfsd_vfs_write(%d %s %d %d %d) call time: %d\n”,count,e,p,t,ts,diff_t[e,p,t,ts])<br>      count++<br>      total_time+&#x3D;diff_t[e,p,t,ts]<br>  }<br>  count–<br>  printf(“nfsd_vfs_write total call time(us): %d\n”,total_time)<br>  printf(“nfsd_vfs_write calls: %d\n”,count)<br>  printf(“nfsd_vfs_write average call time(us): %d\n”,total_time&#x2F;count)<br>  }</p>
</li>
</ul>
<h2 id="Run-the-SystemTap-script"><a href="#Run-the-SystemTap-script" class="headerlink" title="Run the SystemTap script"></a>Run the SystemTap script</h2><pre><code>[root@host1 ~]# cat stp.sh
runid=$1
sleep 10
stap -D MAXACTION=1000000 nfs_trace.stp &gt; nfs_trace.$&#123;runid&#125;.out

[root@host1 ~]# ./stp.sh kernel-3.10-run1
</code></pre>
<p>Note that, the script will wait for 10 seconds before tracing. This is to make sure the network workload to be monitored will be running stable.</p>
<h2 id="Get-the-tracing-result"><a href="#Get-the-tracing-result" class="headerlink" title="Get the tracing result"></a>Get the tracing result</h2><pre><code>[root@host1 ~]# cat stp_3.10/nfs_trace.1.out | head -3
nfsd_vfs_write(1 nfsd 3597 3597 1658189341055849) call time: 139
nfsd_vfs_write(2 nfsd 3598 3598 1658189341055864) call time: 136
nfsd_vfs_write(3 nfsd 3596 3596 1658189341055977) call time: 149
[root@host1 ~]# cat stp_3.10/nfs_trace.1.out | tail -5
nfsd_vfs_write(116340 nfsd 3595 3595 1658189346055536) call time: 182
nfsd_vfs_write(116341 nfsd 3598 3598 1658189346055621) call time: 237
nfsd_vfs_write total call time(us): 23050967
nfsd_vfs_write calls: 116341
nfsd_vfs_write average call time(us): 198
</code></pre>
<p>As we can see, there are totally 116341 calls for the function “nfsd_vfs_write” and the average call time is 198 us. This gives us a clear sense of the function call latency so that we can compare the same for different system configurations(e.g. different kernel versions).</p>
<h2 id="SystemTap-runtime-errors"><a href="#SystemTap-runtime-errors" class="headerlink" title="SystemTap runtime errors"></a>SystemTap runtime errors</h2><p>When we run the SystemTap script, we added a option “-D MAXACTION&#x3D;1000000” to fix the following runtime error.</p>
<pre><code>[root@host1 ~]# stap nfs_trace.stp &gt; nfs_trace.out
ERROR: MAXACTION exceeded near identifier &#39;printf&#39; at nfs_trace.stp:27:3
WARNING: Number of errors: 1, skipped probes: 0
WARNING: /usr/bin/staprun exited with status: 1
Pass 5: run failed.  [man error::pass5]
</code></pre>
<p>What does “⁠MAXACTION exceeded” mean?</p>
<blockquote>
<p>The probe handler attempted to execute too many statements in the probe handler. The default number of actions allowed in a probe handler is 1000.</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://sourceware.org/systemtap/SystemTap_Beginners_Guide/">https://sourceware.org/systemtap/SystemTap_Beginners_Guide&#x2F;</a></li>
<li><a href="https://sourceware.org/systemtap/examples/">https://sourceware.org/systemtap/examples/</a></li>
<li><a href="https://sourceware.org/systemtap/SystemTap_Beginners_Guide/errors.html">https://sourceware.org/systemtap/SystemTap_Beginners_Guide&#x2F;errors.html</a></li>
<li><a href="https://sourceware.org/systemtap/SystemTap_Beginners_Guide/runtimeerror.html">https://sourceware.org/systemtap/SystemTap_Beginners_Guide&#x2F;runtimeerror.html</a></li>
<li><a href="https://mediatemple.net/blog/legacy/wrangling-nsf-load-with-systemtap/">https://mediatemple.net/blog/legacy/wrangling-nsf-load-with-systemtap/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>SystemTap</tag>
        <tag>Profiling Tracing</tag>
      </tags>
  </entry>
  <entry>
    <title>Using virtctl to access virtual machine in Kubernetes</title>
    <url>/blog/using-virtctl-to-access-virtual-machine-in-kubernetes/</url>
    <content><![CDATA[<h2 id="Install-the-virtctl-client-tool"><a href="#Install-the-virtctl-client-tool" class="headerlink" title="Install the virtctl client tool"></a>Install the virtctl client tool</h2><p>Basic VirtualMachineInstance operations can be performed with the stock kubectl utility. However, the virtctl binary utility is required to use advanced features such as:</p>
<ul>
<li>Serial and graphical console access</li>
</ul>
<p>It also provides convenience commands for:</p>
<ul>
<li>Starting and stopping VirtualMachineInstances</li>
<li>Live migrating VirtualMachineInstances</li>
<li>Uploading virtual machine disk images</li>
</ul>
<p>There are two ways to get it:</p>
<ul>
<li>the most recent version of the tool can be retrieved from the official release page</li>
<li>it can be installed as a kubectl plugin using krew</li>
</ul>
<p>Example:</p>
<pre><code>$ export VERSION=v0.48.1
$ wget https://github.com/kubevirt/kubevirt/releases/download/$&#123;VERSION&#125;/virtctl-$&#123;VERSION&#125;-linux-amd64
$ ln -s virtctl-v0.48.1-linux-amd64 virtctl
$ chmod +x virtctl-v0.48.1-linux-amd64
$ ./virtctl version
Client Version: version.Info&#123;GitVersion:&quot;v0.48.1&quot;, ...&#125;
</code></pre>
<h2 id="Access-the-virtual-machine-console"><a href="#Access-the-virtual-machine-console" class="headerlink" title="Access the virtual machine console"></a>Access the virtual machine console</h2><pre><code>$ ./virtctl -h
virtctl controls virtual machine related operations on your kubernetes cluster.

Available Commands:
  addvolume         add a volume to a running VM
  console           Connect to a console of a virtual machine instance.
  expose            Expose a virtual machine instance, virtual machine, or virtual machine instance replica set as a new service.
  fslist            Return full list of filesystems available on the guest machine.
  guestfs           Start a shell into the libguestfs pod
  guestosinfo       Return guest agent info about operating system.
  help              Help about any command
  image-upload      Upload a VM image to a DataVolume/PersistentVolumeClaim.
  migrate           Migrate a virtual machine.
  pause             Pause a virtual machine
  permitted-devices List the permitted devices for vmis.
  port-forward      Forward local ports to a virtualmachine or virtualmachineinstance.
  removevolume      remove a volume from a running VM
  restart           Restart a virtual machine.
  soft-reboot       Soft reboot a virtual machine instance
  ssh               Open a SSH connection to a virtual machine instance.
  start             Start a virtual machine.
  stop              Stop a virtual machine.
  unpause           Unpause a virtual machine
  usbredir          Redirect a usb device to a virtual machine instance.
  userlist          Return full list of logged in users on the guest machine.
  version           Print the client and server version information.
  vnc               Open a vnc connection to a virtual machine instance.

Use &quot;virtctl &lt;command&gt; --help&quot; for more information about a given command.
Use &quot;virtctl options&quot; for a list of global command-line options (applies to all commands).

$ ./virtctl console vm1
[root@vm1 output]# hostname
vm1
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/kubevirt/kubevirt/releases">https://github.com/kubevirt/kubevirt/releases</a></li>
<li><a href="https://kubevirt.io/user-guide/operations/virtctl_client_tool/">https://kubevirt.io/user-guide/operations/virtctl_client_tool&#x2F;</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Vdbench file system operations</title>
    <url>/blog/vdbench-file-system-operations/</url>
    <content><![CDATA[<h2 id="Specify-file-system-operations-in-FWD-and-RD"><a href="#Specify-file-system-operations-in-FWD-and-RD" class="headerlink" title="Specify file system operations in FWD and RD"></a>Specify file system operations in FWD and RD</h2><p>In the File system Workload Definition(FWD), we can specify a single file sysetm operation that must be executed for the workload, with the parameter ‘operation&#x3D;’. The valid operation can be mkdir,rmdir,create,delete,open,close,read,write,getattr and setattr. We can specify ‘fwd&#x3D;xxx,rdpct&#x3D;nn’ to allow for mixed read and write operations against the same file.</p>
<p>If one or more operations need to be specified, the parameter ‘fwd&#x3D;xxx,operations&#x3D;’ in Run Definition(RD) can be leveraged to override the single operation in FWD. For example, the parameter ‘operations&#x3D;mkdir’ or ‘operations&#x3D;(read,getattr)’ can be specified in RD. Vdbench will fail the operation if the file structure is not created before it is accessed.</p>
<p>In the following example, we will run vdbench to make directories, create files, getattr, delete files and remove directories.</p>
<h2 id="Make-directories"><a href="#Make-directories" class="headerlink" title="Make directories"></a>Make directories</h2><p>Prepare the job file to make the directories:</p>
<pre><code>$ cat jobfile/vdb_mkdir.job
hd=default,vdbench=/home/tester/vdbench_test,shell=ssh,user=root
hd=host1,jvms=1,system=192.168.1.2

fsd=fsd1,anchor=/mnt/bench1,depth=5,width=5,files=2,size=4k
fwd=fwd1,fsd=fsd1,host=host1,operation=mkdir
rd=rd1,fwd=fwd1,format=no,fwdrate=max
</code></pre>
<p>Run vdbench job to make the directories:</p>
<pre><code>$ ./vdbench -f jobfile/vdb_mkdir.job
Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.
Vdbench distribution: vdbench50407 Tue June 05  9:49:29 MDT 2018
For documentation, see &#39;vdbench.pdf&#39;.

02:06:05.755 input argument scanned: &#39;-fjobfile/vdb_mkdir.job&#39;
02:06:05.833 Anchor size: anchor=/mnt/bench1: dirs:        3,905; files:        6,250; bytes:    24.414m (25,600,000)
02:06:06.097 Starting slave: ssh 192.168.1.2 -l root /home/tester/vdbench_test/vdbench SlaveJvm -m 192.168.1.3 -n 192.168.1.2-10-220316-02.06.05.716 -l host1-0 -p 5570
02:06:06.527 All slaves are now connected
02:06:08.002 Starting RD=rd1; elapsed=30; fwdrate=max. For loops: None

Mar 16, 2022 ..Interval.. .ReqstdOps... ...cpu%...  read ....read..... ....write.... ..mb/sec... mb/sec .xfer.. ...mkdir.... ....open.... ...close....
                            rate   resp total  sys   pct   rate   resp   rate   resp  read write  total    size  rate   resp  rate   resp  rate   resp
02:06:09.065            1 3905.0  0.018   3.7 0.41   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0  3905  0.018   0.0  0.000   0.0  0.000
02:06:09.102      avg_2-1    NaN  0.000   NaN  NaN   0.0    NaN  0.000    NaN  0.000   NaN   NaN    NaN       0   NaN  0.000   NaN  0.000   NaN  0.000
02:06:09.102      std_2-1
02:06:09.103      max_2-1
02:06:09.216
02:06:09.216 Miscellaneous statistics:
02:06:09.216 (These statistics do not include activity between the last reported interval and shutdown.)
02:06:09.216 DIRECTORY_CREATES   Directories created:                          3,905      3,905/sec
02:06:09.216
02:06:09.932 Vdbench execution completed successfully. Output directory: /home/tester/vdbench_test/output

$ ls -la /mnt/bench1/vdb.1_1.dir/vdb.2_1.dir/vdb.3_1.dir/vdb.4_1.dir/vdb.5_1.dir/
total 8
drwxr-xr-x 2 root root 4096 Mar 16 02:06 .
drwxr-xr-x 7 root root 4096 Mar 16 02:06 ..
</code></pre>
<h2 id="Create-files-in-the-existing-directories"><a href="#Create-files-in-the-existing-directories" class="headerlink" title="Create files in the existing directories"></a>Create files in the existing directories</h2><p>Prepare the job file to create files in the existing directories:</p>
<pre><code>$ cat jobfile/vdb_create.job
hd=default,vdbench=/home/tester/vdbench_test,shell=ssh,user=root
hd=host1,jvms=1,system=192.168.1.2

fsd=fsd1,anchor=/mnt/bench1,depth=5,width=5,files=2,size=4k
fwd=fwd1,fsd=fsd1,host=host1,operation=create
rd=rd1,fwd=fwd1,format=restart,fwdrate=max
</code></pre>
<p>Note that we specify the parameter <strong>‘format&#x3D;restart’</strong>. It will create files that have not been created and will also expand files that have not reached the proper file size.</p>
<p>Run vdbench job to create files:</p>
<pre><code>$ ./vdbench -f jobfile/vdb_create.job
Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.
Vdbench distribution: vdbench50407 Tue June 05  9:49:29 MDT 2018
For documentation, see &#39;vdbench.pdf&#39;.

02:06:11.387 input argument scanned: &#39;-fjobfile/vdb_create.job&#39;
02:06:11.468 Anchor size: anchor=/mnt/bench1: dirs:        3,905; files:        6,250; bytes:    24.414m (25,600,000)
02:06:11.719 Starting slave: ssh 192.168.1.2 -l root /home/tester/vdbench_test/vdbench SlaveJvm -m 192.168.1.3 -n 192.168.1.2-10-220316-02.06.11.352 -l host1-0 -p 5570
02:06:12.148 All slaves are now connected
02:06:13.002 Starting RD=format_for_rd1
02:06:13.238 host1-0: anchor=/mnt/bench1 mkdir complete.
02:06:13.418 host1-0: anchor=/mnt/bench1 create complete.

Mar 16, 2022 ..Interval.. .ReqstdOps... ...cpu%...  read ....read..... ....write.... ..mb/sec... mb/sec .xfer.. ...mkdir.... ...rmdir.... ...create... ....open.... ...close.... ...delete...
                            rate   resp total  sys   pct   rate   resp   rate   resp  read write  total    size  rate   resp  rate   resp  rate   resp  rate   resp  rate   resp  rate   resp
02:06:14.063            1 6250.0  0.019  12.7 0.99   0.0    0.0  0.000 6250.0  0.019  0.00 24.41  24.41    4096   0.0  0.000   0.0  0.000  6250  0.219  6250  0.071  6250  0.006   0.0  0.000
02:06:14.103      avg_2-1    NaN  0.000   NaN  NaN   0.0    NaN  0.000    NaN  0.000   NaN   NaN    NaN       0   NaN  0.000   NaN  0.000   NaN  0.000   NaN  0.000   NaN  0.000   NaN  0.000
02:06:14.103      std_2-1
02:06:14.103      max_2-1
02:06:14.217
02:06:14.217 Miscellaneous statistics:
02:06:14.217 (These statistics do not include activity between the last reported interval and shutdown.)
02:06:14.218 FILE_CREATES        Files created:                                6,250      6,250/sec
02:06:14.218 WRITE_OPENS         Files opened for write activity:              6,250      6,250/sec
02:06:14.218 DIR_EXISTS          Directory may not exist (yet):                    7          7/sec
02:06:14.218 FILE_CLOSES         Close requests:                               6,250      6,250/sec
02:06:14.218
02:06:15.001 Starting RD=rd1; elapsed=30; fwdrate=max. For loops: None
02:06:15.344
02:06:15.344 Message from slave host1-0:
02:06:15.345 Anchor: /mnt/bench1
02:06:15.345 Vdbench is trying to create a new file, but all files already exist,
02:06:15.345 and no threads are currently active deleting files
02:06:15.345 FwgThread.canWeGetMoreFiles(): Shutting down threads for operation=create
02:06:15.345

Mar 16, 2022 ..Interval.. .ReqstdOps... ...cpu%...  read ....read..... ....write.... ..mb/sec... mb/sec .xfer.. ...mkdir.... ...rmdir.... ...create... ....open.... ...close.... ...delete...
                            rate   resp total  sys   pct   rate   resp   rate   resp  read write  total    size  rate   resp  rate   resp  rate   resp  rate   resp  rate   resp  rate   resp
02:06:16.021            1    0.0  0.000   1.8 0.09   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000   0.0  0.000   0.0  0.000   0.0  0.000   0.0  0.000
02:06:16.030      avg_2-1    NaN  0.000   NaN  NaN   0.0    NaN  0.000    NaN  0.000   NaN   NaN    NaN       0   NaN  0.000   NaN  0.000   NaN  0.000   NaN  0.000   NaN  0.000   NaN  0.000
02:06:16.030      std_2-1
02:06:16.030      max_2-1
02:06:16.135 Miscellaneous statistics: All counters are zero
02:06:16.602 Vdbench execution completed successfully. Output directory: /home/tester/vdbench_test/output

$ ls -la /mnt/bench1/vdb.1_1.dir/vdb.2_1.dir/vdb.3_1.dir/vdb.4_1.dir/vdb.5_1.dir/
total 16
drwxr-xr-x 2 root root 4096 Mar 16 02:06 .
drwxr-xr-x 7 root root 4096 Mar 16 02:06 ..
-rw-r--r-- 1 root root 4096 Mar 16 02:06 vdb_f0000.file
-rw-r--r-- 1 root root 4096 Mar 16 02:06 vdb_f0001.file
</code></pre>
<h2 id="Getattr"><a href="#Getattr" class="headerlink" title="Getattr"></a>Getattr</h2><p>Prepare the job file to getattr:</p>
<pre><code>$ cat jobfile/vdb_getattr.job
hd=default,vdbench=/home/tester/vdbench_test,shell=ssh,user=root
hd=host1,jvms=1,system=192.168.1.2

fsd=fsd1,anchor=/mnt/bench1,depth=5,width=5,files=2,size=4k
fwd=fwd1,fsd=fsd1,host=host1,operation=getattr
rd=rd1,fwd=fwd1,format=no,fwdrate=max
</code></pre>
<p>Run vdbench job to getattr:</p>
<pre><code>$ ./vdbench -f jobfile/vdb_getattr.job
Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.
Vdbench distribution: vdbench50407 Tue June 05  9:49:29 MDT 2018
For documentation, see &#39;vdbench.pdf&#39;.

02:06:18.059 input argument scanned: &#39;-fjobfile/vdb_getattr.job&#39;
02:06:18.145 Anchor size: anchor=/mnt/bench1: dirs:        3,905; files:        6,250; bytes:    24.414m (25,600,000)
02:06:18.408 Starting slave: ssh 192.168.1.2 -l root /home/tester/vdbench_test/vdbench SlaveJvm -m 192.168.1.3 -n 192.168.1.2-10-220316-02.06.18.020 -l host1-0 -p 5570
02:06:18.778 All slaves are now connected
02:06:20.002 Starting RD=rd1; elapsed=30; fwdrate=max. For loops: None

Mar 16, 2022 ..Interval.. .ReqstdOps... ...cpu%...  read ....read..... ....write.... ..mb/sec... mb/sec .xfer.. ....open.... ...close.... ..getattr...
                            rate   resp total  sys   pct   rate   resp   rate   resp  read write  total    size  rate   resp  rate   resp  rate   resp
02:06:21.062            1  39541  0.015   8.1 0.84   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 39541  0.015
02:06:22.032            2  77608  0.010   5.3 1.47   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 77608  0.010
02:06:23.016            3  83661  0.009   4.5 1.26   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 83661  0.009
02:06:24.014            4  83054  0.009   4.4 1.35   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 83054  0.009
02:06:25.014            5  83753  0.009   4.3 1.26   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 83753  0.009
02:06:26.012            6  83907  0.009   4.3 1.23   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 83907  0.009
02:06:27.012            7  83520  0.009   4.3 1.26   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 83520  0.009
02:06:28.011            8  82677  0.009   4.4 1.19   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82677  0.009
02:06:29.016            9  83093  0.009   4.6 1.19   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 83093  0.009
02:06:30.012           10  84090  0.009   4.5 1.23   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 84090  0.009
02:06:31.011           11  83013  0.009   4.2 1.16   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 83013  0.009
02:06:32.013           12  83158  0.010   4.3 1.38   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 83158  0.010
02:06:33.010           13  74121  0.011   4.3 1.19   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 74121  0.011
02:06:34.009           14  77405  0.010   4.4 1.35   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 77405  0.010
02:06:35.011           15  82874  0.010   4.6 1.28   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82874  0.010
02:06:36.011           16  82855  0.010   4.6 1.34   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82855  0.010
02:06:37.010           17  82508  0.010   4.6 1.38   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82508  0.010
02:06:38.007           18  82766  0.010   4.6 1.38   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82766  0.010
02:06:39.009           19  82048  0.010   4.3 1.22   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82048  0.010
02:06:40.010           20  81357  0.010   4.4 1.25   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 81357  0.010
02:06:41.008           21  82723  0.010   4.4 1.28   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82723  0.010
02:06:42.007           22  82702  0.010   4.5 1.50   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82702  0.010
02:06:43.008           23  82793  0.010   4.3 1.41   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82793  0.010
02:06:44.009           24  82465  0.010   4.4 1.28   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82465  0.010
02:06:45.009           25  82695  0.010   4.4 1.32   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82695  0.010
02:06:46.008           26  82119  0.010   4.6 1.47   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82119  0.010
02:06:47.008           27  82297  0.010   4.8 1.72   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82297  0.010
02:06:48.007           28  82508  0.010   4.4 1.35   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82508  0.010
02:06:49.008           29  82545  0.010   4.4 1.41   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82545  0.010
02:06:50.007           30  82641  0.010   4.7 1.44   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82641  0.010
02:06:50.015     avg_2-30  82170  0.010   4.5 1.33   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000 82170  0.010
02:06:50.015     std_2-30 2147.1  0.018                                                                                                    2147  0.018
02:06:50.016     max_2-30  84090  5.854                                                                                                   84090  5.854
02:06:50.234
02:06:50.234 Miscellaneous statistics:
02:06:50.234 (These statistics do not include activity between the last reported interval and shutdown.)
02:06:50.234 GET_ATTR            Getattr requests:                         2,422,766     80,758/sec
02:06:50.234
02:06:51.201 Vdbench execution completed successfully. Output directory: /home/tester/vdbench_test/output

$ ls -la /mnt/bench1/vdb.1_1.dir/vdb.2_1.dir/vdb.3_1.dir/vdb.4_1.dir/vdb.5_1.dir/
total 16
drwxr-xr-x 2 root root 4096 Mar 16 02:06 .
drwxr-xr-x 7 root root 4096 Mar 16 02:06 ..
-rw-r--r-- 1 root root 4096 Mar 16 02:06 vdb_f0000.file
-rw-r--r-- 1 root root 4096 Mar 16 02:06 vdb_f0001.file
</code></pre>
<h2 id="Delete-files"><a href="#Delete-files" class="headerlink" title="Delete files"></a>Delete files</h2><p>Prepare the job file to delete files:</p>
<pre><code>$ cat jobfile/vdb_delete.job
hd=default,vdbench=/home/tester/vdbench_test,shell=ssh,user=root
hd=host1,jvms=1,system=192.168.1.2

fsd=fsd1,anchor=/mnt/bench1,depth=5,width=5,files=2,size=4k
fwd=fwd1,fsd=fsd1,host=host1,operation=delete
rd=rd1,fwd=fwd1,format=no,fwdrate=max
</code></pre>
<p>Run vdbench job to delete files:</p>
<pre><code>$ ./vdbench -f jobfile/vdb_delete.job
Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.
Vdbench distribution: vdbench50407 Tue June 05  9:49:29 MDT 2018
For documentation, see &#39;vdbench.pdf&#39;.

02:06:52.656 input argument scanned: &#39;-fjobfile/vdb_delete.job&#39;
02:06:52.757 Anchor size: anchor=/mnt/bench1: dirs:        3,905; files:        6,250; bytes:    24.414m (25,600,000)
02:06:53.060 Starting slave: ssh 192.168.1.2 -l root /home/tester/vdbench_test/vdbench SlaveJvm -m 192.168.1.3 -n 192.168.1.2-10-220316-02.06.52.608 -l host1-0 -p 5570
02:06:53.542 All slaves are now connected
02:06:55.002 Starting RD=rd1; elapsed=30; fwdrate=max. For loops: None
02:06:55.403
02:06:55.403 Message from slave host1-0:
02:06:55.403 Anchor: /mnt/bench1
02:06:55.403 Vdbench is trying to delete a file, but no files are available, and no
02:06:55.404 threads are currently active creating new files.
02:06:55.404 FwgThread.canWeGetMoreFiles(): Shutting down threads for operation=delete
02:06:55.404

Mar 16, 2022 ..Interval.. .ReqstdOps... ...cpu%...  read ....read..... ....write.... ..mb/sec... mb/sec .xfer.. ....open.... ...close.... ...delete...
                            rate   resp total  sys   pct   rate   resp   rate   resp  read write  total    size  rate   resp  rate   resp  rate   resp
02:06:56.061            1 6250.0  0.044   7.3 0.61   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0   0.0  0.000   0.0  0.000  6250  0.044
02:06:56.096      avg_2-1    NaN  0.000   NaN  NaN   0.0    NaN  0.000    NaN  0.000   NaN   NaN    NaN       0   NaN  0.000   NaN  0.000   NaN  0.000
02:06:56.096      std_2-1
02:06:56.097      max_2-1
02:06:56.210
02:06:56.210 Miscellaneous statistics:
02:06:56.210 (These statistics do not include activity between the last reported interval and shutdown.)
02:06:56.210 FILE_DELETES        Files deleted:                                6,250      6,250/sec
02:06:56.211 FILE_MUST_EXIST     File does not exist (yet):                        1          1/sec
02:06:56.211
02:06:56.949 Vdbench execution completed successfully. Output directory: /home/tester/vdbench_test/output

$ ls -la /mnt/bench1/vdb.1_1.dir/vdb.2_1.dir/vdb.3_1.dir/vdb.4_1.dir/vdb.5_1.dir/
total 8
drwxr-xr-x 2 root root 4096 Mar 16 02:06 .
drwxr-xr-x 7 root root 4096 Mar 16 02:06 ..
</code></pre>
<h2 id="Remove-directories"><a href="#Remove-directories" class="headerlink" title="Remove directories"></a>Remove directories</h2><p>Prepare the job file to remove directories:</p>
<pre><code>$ cat jobfile/vdb_rmdir.job
hd=default,vdbench=/home/tester/vdbench_test,shell=ssh,user=root
hd=host1,jvms=1,system=192.168.1.2

fsd=fsd1,anchor=/mnt/bench1,depth=5,width=5,files=2,size=4k
fwd=fwd1,fsd=fsd1,host=host1,operation=rmdir
rd=rd1,fwd=fwd1,format=no,fwdrate=max
</code></pre>
<p>Run vdbench job to remove directories:</p>
<pre><code>$ ./vdbench -f jobfile/vdb_rmdir.job
Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.
Vdbench distribution: vdbench50407 Tue June 05  9:49:29 MDT 2018
For documentation, see &#39;vdbench.pdf&#39;.

02:06:58.406 input argument scanned: &#39;-fjobfile/vdb_rmdir.job&#39;
02:06:58.484 Anchor size: anchor=/mnt/bench1: dirs:        3,905; files:        6,250; bytes:    24.414m (25,600,000)
02:06:58.736 Starting slave: ssh 192.168.1.2 -l root /home/tester/vdbench_test/vdbench SlaveJvm -m 192.168.1.3 -n 192.168.1.2-10-220316-02.06.58.369 -l host1-0 -p 5570
02:06:59.158 All slaves are now connected
02:07:00.002 Starting RD=rd1; elapsed=30; fwdrate=max. For loops: None

Mar 16, 2022 ..Interval.. .ReqstdOps... ...cpu%...  read ....read..... ....write.... ..mb/sec... mb/sec .xfer.. ...rmdir.... ....open.... ...close....
                            rate   resp total  sys   pct   rate   resp   rate   resp  read write  total    size  rate   resp  rate   resp  rate   resp
02:07:01.051            1 3905.0  0.039   5.4 0.65   0.0    0.0  0.000    0.0  0.000  0.00  0.00   0.00       0  3905  0.039   0.0  0.000   0.0  0.000
02:07:01.089      avg_2-1    NaN  0.000   NaN  NaN   0.0    NaN  0.000    NaN  0.000   NaN   NaN    NaN       0   NaN  0.000   NaN  0.000   NaN  0.000
02:07:01.089      std_2-1
02:07:01.090      max_2-1
02:07:01.204
02:07:01.205 Miscellaneous statistics:
02:07:01.205 (These statistics do not include activity between the last reported interval and shutdown.)
02:07:01.205 DIRECTORY_DELETES   Directories deleted:                          3,905      3,905/sec
02:07:01.205
02:07:01.550 Vdbench execution completed successfully. Output directory: /home/tester/vdbench_test/output

$ ls -la /mnt/bench1/vdb.1_1.dir/vdb.2_1.dir/vdb.3_1.dir/vdb.4_1.dir/vdb.5_1.dir/
ls: cannot access /mnt/bench1/vdb.1_1.dir/vdb.2_1.dir/vdb.3_1.dir/vdb.4_1.dir/vdb.5_1.dir/: No such file or directory
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.oracle.com/technetwork/server-storage/vdbench-1901683.pdf">https://www.oracle.com/technetwork/server-storage/vdbench-1901683.pdf</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Vdbench</tag>
      </tags>
  </entry>
  <entry>
    <title>Vdbench performance test on filesystem</title>
    <url>/blog/vdbench-performance-test-on-filesystem/</url>
    <content><![CDATA[<h2 id="File-system-testing-Method-1"><a href="#File-system-testing-Method-1" class="headerlink" title="File system testing - Method 1"></a>File system testing - Method 1</h2><p><strong>Vdbench filesystem testing terminologies:</strong></p>
<ul>
<li>Anchor - A directory or a filesystme mount point. A file system structure will be created by specifying the structure information including directory depth, width, number of files and file size. Multiple anchor can be defined and used by filesystem workloads.</li>
<li>Operation - File system operations. For example, directory create&#x2F;delete, file create&#x2F;delete, file read&#x2F;write, file open&#x2F;close, setattr and getattr.</li>
</ul>
<p><strong>Vdbench parameters for filesystem benchmark:</strong></p>
<ul>
<li>File system definition(FSD) - Describe the directory structure.</li>
<li>File system workload definition(FWD) - Describe the workload parameters.</li>
<li>Run definition(RD) - Describe how the workload will be run.</li>
</ul>
<p><strong>The following is an example of the vdbench job file.</strong></p>
<pre><code>hd=default,vdbench=/home/tester/vdbench_test,shell=ssh,user=root
hd=host1,jvms=1,system=192.168.1.50
fsd=fsd1,anchor=/mnt/testdir1,depth=1,width=1,files=4,size=50g,openflag=o_direct
fsd=fsd2,anchor=/mnt/testdir2,depth=1,width=1,files=4,size=50g,openflag=o_direct
fwd=fwd1,fsd=fsd1,host=host1,fileio=random,operation=write,xfersize=4k,fileselect=random,threads=$th
fwd=fwd2,fsd=fsd2,host=host1,fileio=random,operation=write,xfersize=4k,fileselect=random,threads=$th
rd=rd1,fwd=fwd*,fwdrate=max,format=yes,elapsed=180,interval=30
</code></pre>
<p>Explanation:</p>
<ol>
<li><p>hd - it specifies which host to run the filesystem workload. The number of jvms is default to 1. It can be increased if the number of jvms can not handle very high iops with a fast system.</p>
</li>
<li><p>fsd - it specifies under which directory to create the filesystem structures. “depth” defines how many level of directories will be created. “width” defines how many sub-directories will be created under each parent directory. “files” defines how many files will be created under each directory. “size” defines the file size. “openflag” controls how the file will be opened.</p>
</li>
<li><p>fwd - it specifies what workload will be run on the target filesystems. In this example, it will run random write with 4k blocksize. The specified number of threads will be used to write corresponding number of files. The files will be randomly selected for the workload to run. Note that, the number of threads should less than or equal to the number of files. Note that, the writes to the file is single threaded unless “filiio&#x3D;(random,shared) is specified.</p>
</li>
<li><p>rd - it controls how the workload will be run. In this example, the workload will be run for 3 minutes. The “format” option is very useful. It will recreate the filesystem structure before the workload run. It will give us more repeatable results. “fwdrate” indicates the iorate will be unlimited in order to stress the system as much as possible.</p>
</li>
<li><p>In this example, we run the 4k random write workload concurrently on two filesystems. The number of threads to write each filesystem can be controlled by passing from shell variable during run time as below.</p>
<p> .&#x2F;vdbench jobfile&#x2F;vdb.job th&#x3D;2</p>
</li>
</ol>
<h2 id="File-system-testing-Method-2"><a href="#File-system-testing-Method-2" class="headerlink" title="File system testing - Method 2"></a>File system testing - Method 2</h2><p>The similar workload in the method one can also be run as below.</p>
<pre><code>hd=default,vdbench=/home/tester/vdbench_test,shell=ssh,user=root
hd=host1,system=192.168.1.50
sd=sd1,host=host1,lun=/mnt/testdir1/testfile,hitarea=1m,openflag=o_direct,size=50g
sd=sd2,host=host1,lun=/mnt/testdir2/testfile,hitarea=1m,openflag=o_direct,size=50g
wd=wd1,sd=sd*,seekpct=100
rd=rd1,wd=wd1,iorate=max,rdpct=0,xfersize=4K,elapsed=180,interval=30,th=$th
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.oracle.com/technetwork/server-storage/vdbench-1901683.pdf">https://www.oracle.com/technetwork/server-storage/vdbench-1901683.pdf</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Vdbench</tag>
      </tags>
  </entry>
  <entry>
    <title>Vdbench performance test on raw device</title>
    <url>/blog/vdbench-performance-test-on-raw-device/</url>
    <content><![CDATA[<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><ul>
<li><p><strong>Master and Slave</strong>: Vdbench runs as two or more Java Virtual Machines (JVMs). The JVM that you start is the master. The master takes care of the parsing of all the parameters, it determines which workloads should run, and then will also do all the reporting. The actual workload is executed by one or more Slaves. A Slave can run on the host where the Master was started, or it can run on any remote host as defined in the parameter file.</p>
</li>
<li><p><strong>Raw I&#x2F;O workload</strong> parameters describe the storage configuration to be used and the workload to be generated. The parameters include <strong>General</strong>, <strong>Host Definition (HD)</strong>, <strong>Replay Group (RG)</strong>, <strong>Storage Definition (SD)</strong>, <strong>Workload Definition (WD)</strong> and <strong>Run Definition (RD)</strong> and must always be entered in the order in which they are listed here. A <strong>Run</strong> is the execution of one workload requested by a <strong>Run Definition</strong>. Multiple Runs can be requested within one Run Definition.</p>
</li>
<li><p><strong>File system Workload</strong> parameters describe the file system configuration to be used and the workload to be generated. The parameters include <strong>General</strong>, <strong>Host Definition (HD)</strong>, <strong>File System Definition (FSD)</strong>, <strong>File system Workload Definition (FWD)</strong> and <strong>Run Definition(RD)</strong> and must always be entered in the order in which they are listed here. A <strong>Run</strong> is the execution of one workload requested by a <strong>Run Definition</strong>. Multiple Runs can be requested within one Run Definition.</p>
</li>
</ul>
<h2 id="Install-java"><a href="#Install-java" class="headerlink" title="Install java"></a>Install java</h2><p>java is required by vdbench on both master and slave hosts.</p>
<pre><code>$ apt install default-jre

$ java -version
openjdk version &quot;11.0.11&quot; 2021-04-20
OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04)
OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)
</code></pre>
<h2 id="Install-vdbench"><a href="#Install-vdbench" class="headerlink" title="Install vdbench"></a>Install vdbench</h2><p>Vdbench is packaged as a zip file. Unzip the file and you’re ready to go.</p>
<pre><code>$ ls -la vdbench50407.zip
-rw-r--r--. 1 root root 3073219 Aug 26 21:23 vdbench50407.zip

$ unzip vdbench50407.zip

$ file vdbench
vdbench: Bourne-Again shell script, ASCII text executable
</code></pre>
<h2 id="Vdbench-job-file"><a href="#Vdbench-job-file" class="headerlink" title="Vdbench job file"></a>Vdbench job file</h2><p>The following is an example job file to run random read I&#x2F;O for a minute on the raw block device of the remote host.</p>
<pre><code>$ cat jobfile/dryrun.job
hd=default,vdbench=/home/tester/vdbench,shell=ssh,user=root
hd=host1,system=&lt;slave-host-ip&gt;
sd=sd1,host=host1,lun=/dev/sdd,hitarea=10m,openflag=o_direct,size=20000m
wd=wd_random_rd1,sd=sd1,seekpct=100

# 4KB random read
rd=rd_4KB_randread,wd=wd_random_rd1,iorate=max,rdpct=100,xfersize=4K,elapsed=60,interval=10,th=1
</code></pre>
<h2 id="Benchmark-run-and-result"><a href="#Benchmark-run-and-result" class="headerlink" title="Benchmark run and result"></a>Benchmark run and result</h2><p>You can run the benchmark job with the following command.</p>
<pre><code>$ ./vdbench -f jobfile/dryrun.job
Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.
Vdbench distribution: vdbench50407 Tue June 05  9:49:29 MDT 2018
For documentation, see &#39;vdbench.pdf&#39;.

22:06:52.177 input argument scanned: &#39;-fjobfile/dryrun.job&#39;
22:06:52.353 Starting slave: ssh &lt;slave-host-ip&gt; -l root /home/tester/vdbench/vdbench SlaveJvm -m &lt;master-host-ip&gt; -n &lt;slave-host-ip&gt;-10-210826-22.06.52.140 -l host1-0 -p 5570
22:06:53.123 Clock synchronization warning: slave host1-0 is 41 seconds out of sync. This can lead to heartbeat issues.
22:06:53.140 All slaves are now connected
22:06:54.002 Starting RD=rd_4KB_randread; I/O rate: Uncontrolled MAX; elapsed=60; For loops: rdpct=100 xfersize=4k threads=1

Aug 26, 2021    interval        i/o   MB/sec   bytes   read     resp     read    write     read    write     resp  queue  cpu%  cpu%
                               rate  1024**2     i/o    pct     time     resp     resp      max      max   stddev  depth sys+u   sys
22:07:04.055           1     5563.1    21.73    4096 100.00    0.157    0.157    0.000    10.89     0.00    0.342    0.9  16.5   7.6
22:07:14.011           2     7022.2    27.43    4096 100.00    0.127    0.127    0.000     7.54     0.00    0.108    0.9  10.7   7.5
22:07:24.009           3     7018.5    27.42    4096 100.00    0.128    0.128    0.000     9.67     0.00    0.087    0.9  10.9   8.2
22:07:34.009           4     7026.8    27.45    4096 100.00    0.127    0.127    0.000     6.99     0.00    0.105    0.9  10.7   7.9
22:07:44.008           5     7264.4    28.38    4096 100.00    0.123    0.123    0.000     7.75     0.00    0.082    0.9  10.7   7.7
22:07:54.014           6     7311.4    28.56    4096 100.00    0.122    0.122    0.000     7.21     0.00    0.076    0.9  10.5   7.4
22:07:54.024     avg_2-6     7128.7    27.85    4096 100.00    0.126    0.126    0.000     9.67     0.00    0.092    0.9  10.7   7.7
22:07:54.445 Vdbench execution completed successfully. Output directory: /data/vdbench_test/output
</code></pre>
<p>The result is saved in output directory by default. You can also check the result summary by opening “summary.html” in a browser.</p>
<pre><code>$ ls output/
config.html    flatfile.html   host1-0.html         host1.html               logfile.html   parmscan.html       sd1.html   status.html   swat_mon_total.txt  totals.html errorlog.html  histogram.html  host1-0.stdout.html  host1.var_adm_msgs.html  parmfile.html  sd1.histogram.html  skew.html  summary.html  swat_mon.txt

$ cat output/summary.html
&lt;title&gt;Vdbench output/summary.html&lt;/title&gt;&lt;pre&gt;
Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.
Vdbench summary report, created 22:06:52 Aug 26 2021 UTC (22:06:52 Aug 26 2021 UTC)

Link to logfile:                 &lt;A HREF=&quot;logfile.html&quot;&gt;logfile&lt;/A&gt;
Run totals:                      &lt;A HREF=&quot;totals.html&quot;&gt;totals&lt;/A&gt;
Vdbench status:                  &lt;A HREF=&quot;status.html&quot;&gt;status&lt;/A&gt;
Copy of input parameter files:   &lt;A HREF=&quot;parmfile.html&quot;&gt;parmfile&lt;/A&gt;
Copy of parameter scan detail:   &lt;A HREF=&quot;parmscan.html&quot;&gt;parmscan&lt;/A&gt;
Link to errorlog:                &lt;A HREF=&quot;errorlog.html&quot;&gt;errorlog&lt;/A&gt;
Link to flatfile:                &lt;A HREF=&quot;flatfile.html&quot;&gt;flatfile&lt;/A&gt;

Link to HOST reports:            &lt;A HREF=&quot;host1.html&quot;&gt;host1&lt;/A&gt;
Link to response time histogram: &lt;A HREF=&quot;histogram.html&quot;&gt;histogram&lt;/A&gt;
Link to workload skew report:    &lt;A HREF=&quot;skew.html&quot;&gt;skew&lt;/A&gt;
Link to SD reports:              &lt;A HREF=&quot;sd1.html&quot;&gt;sd1&lt;/A&gt;

Link to Run Definitions:         &lt;A HREF=&quot;#_463345942&quot;&gt;rd_4KB_randread For loops: rdpct=100 xfersize=4k threads=1&lt;/A&gt;

Link to config output:           &lt;A HREF=&quot;config.html&quot;&gt;config&lt;/A&gt;

&lt;a name=&quot;_463345942&quot;&gt;&lt;/a&gt;&lt;i&gt;&lt;b&gt;22:06:54.002 Starting RD=rd_4KB_randread; I/O rate: Uncontrolled MAX; elapsed=60; For loops: rdpct=100 xfersize=4k threads=1&lt;/b&gt;&lt;/i&gt;


Aug 26, 2021    interval        i/o   MB/sec   bytes   read     resp     read    write     read    write     resp  queue  cpu%  cpu%
                               rate  1024**2     i/o    pct     time     resp     resp      max      max   stddev  depth sys+u   sys
22:07:04.052           1     5563.1    21.73    4096 100.00    0.157    0.157    0.000    10.89     0.00    0.342    0.9  16.5   7.6
22:07:14.010           2     7022.2    27.43    4096 100.00    0.127    0.127    0.000     7.54     0.00    0.108    0.9  10.7   7.5
22:07:24.008           3     7018.5    27.42    4096 100.00    0.128    0.128    0.000     9.67     0.00    0.087    0.9  10.9   8.2
22:07:34.008           4     7026.8    27.45    4096 100.00    0.127    0.127    0.000     6.99     0.00    0.105    0.9  10.7   7.9
22:07:44.008           5     7264.4    28.38    4096 100.00    0.123    0.123    0.000     7.75     0.00    0.082    0.9  10.7   7.7
22:07:54.014           6     7311.4    28.56    4096 100.00    0.122    0.122    0.000     7.21     0.00    0.076    0.9  10.5   7.4
22:07:54.023     avg_2-6     7128.7    27.85    4096 100.00    0.126    0.126    0.000     9.67     0.00    0.092    0.9  10.7   7.7
22:07:54.445 Vdbench execution completed successfully
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://ubuntu.com/tutorials/install-jre#2-installing-openjdk-jre">https://ubuntu.com/tutorials/install-jre#2-installing-openjdk-jre</a></li>
<li><a href="https://www.oracle.com/technetwork/server-storage/vdbench-1901683.pdf">https://www.oracle.com/technetwork/server-storage/vdbench-1901683.pdf</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Vdbench</tag>
      </tags>
  </entry>
  <entry>
    <title>We Shall Fight on the Beaches</title>
    <url>/blog/we-shall-fight-on-the-beaches/</url>
    <content><![CDATA[<p>Good afternoon, my name is [] and I will be presenting the speech titled “We shall fight on the beaches”. During the June of 1940 in an attempt to boost his citizen’s morale and confidence, Winston Churchill, the Prime Minister of the United Kingdom, gave his speech “We shall fight on the beaches” at the British House of Commons. I was inspired by his motivational speech, which gave hope and confidence to the people of German-occupied countries. In his speech, Winston Churchill stated that Britain would only end the war when they stand victorious, even if it means using total war.</p>
<p>We Shall Fight on the Beaches</p>
<p>Winston Churchill, June 4,1940, House of Commons</p>
<p>I have, myself, full confidence that if all do their duty, if nothing is neglected, and if the best arrangements are made, as they are being made, we shall prove ourselves once again able to defend our Island home, to ride out the storm of war, and to outlive the menace of tyranny, if necessary for years, if necessary alone.</p>
<p>At any rate, that is what we are going to try to do. That is the resolve of His Majesty’s Government-every man of them. That is the will of Parliament and the nation.</p>
<p>The British Empire and the French Republic, linked together in their cause and in their need, will defend to the death their native soil, aiding each other like good comrades to the utmost of their strength.</p>
<p>Even though large tracts of Europe and many old and famous States have fallen or may fall into the grip of the Gestapo and all the odious apparatus of Nazi rule, we shall not flag or fail.</p>
<p>We shall go on to the end, we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing confidence and growing strength in the air, we shall defend our Island, whatever the cost may be, we shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and in the streets, we shall fight in the hills; we shall never surrender, and even if, which I do not for a moment believe, this Island or a large part of it were subjugated and starving, then our Empire beyond the seas, armed and guarded by the British Fleet, would carry on the struggle, until, in God’s good time, the New World, with all its power and might, steps forth to the rescue and the liberation of the old.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>What is high availability</title>
    <url>/blog/what-is-high-availability/</url>
    <content><![CDATA[<h2 id="What-is-high-availability"><a href="#What-is-high-availability" class="headerlink" title="What is high availability?"></a>What is high availability?</h2><p>High availability refers to a system or component that is operational without interruption for long periods of time.</p>
<p>High availability is measured as a percentage, with a 100% percent system indicating a service that experiences zero downtime. This would be a system that never fails. It’s pretty rare with complex systems. Most services fall somewhere between 99% and 100% uptime. Most cloud vendors offer some type of Service Level Agreement around availability. Amazon, Google, and Microsoft’s set their cloud SLAs at 99.9%. The industry generally recognizes this as very reliable uptime. A step above, 99.99%, or “four nines,” as is considered excellent uptime.</p>
<p>But four nines uptime is still 52 minutes of downtime per year. Consider how many people rely on web tools to run their lives and businesses. A lot can go wrong in 52 minutes.</p>
<p>So what is it that makes four nines so hard? What are the best practices for high availability engineering? And why is 100% uptime so difficult?</p>
<h2 id="Availability-and-downtime"><a href="#Availability-and-downtime" class="headerlink" title="Availability and downtime"></a>Availability and downtime</h2><p>As shown in the table below, the number of nines(availability %) correlates to the system downtime.</p>
<p><img src="/images/availability-down-time.png" alt="Image"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/High_availability">https://en.wikipedia.org/wiki/High_availability</a></li>
<li><a href="https://www.atlassian.com/blog/statuspage/high-availability#:~:text=What%20is%20high%20availability%3F,a%20system%20that%20never%20fails.">https://www.atlassian.com/blog/statuspage/high-availability#:~:text&#x3D;What is high availability%3F,a system that never fails.</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>High Availability</tag>
      </tags>
  </entry>
  <entry>
    <title>What is patrol read</title>
    <url>/blog/what-is-patrol-read/</url>
    <content><![CDATA[<h2 id="What-is-patrolread"><a href="#What-is-patrolread" class="headerlink" title="What is patrolread"></a>What is patrolread</h2><p>The Patrol Read feature is designed as a preventative measure to ensure hard drive health and data integrity. Patrol Read scans for and resolves potential problems on configured hard drives.</p>
<h2 id="Patrol-Read-Behavior"><a href="#Patrol-Read-Behavior" class="headerlink" title="Patrol Read Behavior"></a>Patrol Read Behavior</h2><p>The following is an overview of Patrol Read behavior:</p>
<ul>
<li><p>Patrol Read runs on all disks on the controller that are configured as part of a virtual disk, including hot spares.</p>
</li>
<li><p>Patrol Read does not run on hard drives that are not part of a virtual disk or are in Ready state.</p>
</li>
<li><p>Patrol Read adjusts the amount of controller resources that are dedicated to Patrol Read operations based on outstanding disk I&#x2F;O. For example, if the system is busy processing I&#x2F;O operation, then Patrol Read uses fewer resources to allow the I&#x2F;O to take a higher priority.</p>
</li>
<li><p>Patrol Read does not run on any disks that are involved in any of the following operations:</p>
</li>
<li><p>Rebuild</p>
</li>
<li><p>Replace Member</p>
</li>
<li><p>Full or Background Initialization</p>
</li>
<li><p>Consistency Check</p>
</li>
</ul>
<h2 id="Patrol-Read-Modes"><a href="#Patrol-Read-Modes" class="headerlink" title="Patrol Read Modes"></a>Patrol Read Modes</h2><p>Patrol Read Mode can be set in BIOS configuration utility and UEFI RAID Configuration Utility. The default Patrol Read mode is automatic. When Patrol Read is in automatic mode, Patrol Read runs automatically at its scheduled interval.</p>
<p>Below is a summary of the different Patrol Read modes:</p>
<ul>
<li>In Auto Mode, Patrol Read will run every 168 hours (7 days). Whenever started, the duration depends on the size of storage. For NBFS, it would last 24 hours</li>
<li>In Auto Mode, Firmware allows Patrol Read to be started manually</li>
<li>In Auto Mode, Patrol Read can be stopped manually.</li>
<li>In Manual Mode, Patrol Read does not start automatically as per the scheduled time.</li>
<li>In Manual Mode, Patrol Read can be started manually.</li>
<li>In Manual Mode, Patrol Read can be stopped manually.</li>
<li>In Disabled Mode, Patrol Read will not run automatically.</li>
<li>In Disabled Mode, Patrol Read cannot be started manually.</li>
</ul>
<h2 id="Performance-issues"><a href="#Performance-issues" class="headerlink" title="Performance issues"></a>Performance issues</h2><p>We have observed a few performance issues that were caused by Patrol read. When Patrol read is running, much higher IO service time (svctm) can be seen.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
  </entry>
  <entry>
    <title>What is Performance Engineering</title>
    <url>/blog/what-is-performance-engineering/</url>
    <content><![CDATA[<p><img src="/images/perfeng.png" alt="Image"></p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
  </entry>
  <entry>
    <title>Why performance tools need BPF</title>
    <url>/blog/why-performance-tools-need-bpf/</url>
    <content><![CDATA[<p>BPF is efficient, production safe and built into the Linux kernel. With BPF, you can use the performance tools in production environment without needing to add any new kernel components.</p>
<p>The following example summarizes the block I&#x2F;O latency as a histogram.</p>
<pre><code>$ biolatency
Tracing block device I/O... Hit Ctrl-C to end.
^C
usecs           : count     distribution
0 -&gt; 1          : 0        |                                        |
2 -&gt; 3          : 0        |                                        |
4 -&gt; 7          : 1        |                                        |
8 -&gt; 15         : 1        |                                        |
16 -&gt; 31        : 66       |****************************************|
32 -&gt; 63        : 11       |******                                  |
64 -&gt; 127       : 21       |************                            |
128 -&gt; 255      : 28       |****************                        |
</code></pre>
<p>The following figure explains how BPF improves the efficiency of this tool.</p>
<p><img src="/images/BPF.png" alt="Image"></p>
<p>The BPF program is run in the kernel for each event. It fetches the block I&#x2F;O latency which is saved into a BPF map histogram. In the user space, the BPF map histogram can be read and printed out.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>BPF</tag>
      </tags>
  </entry>
  <entry>
    <title>YCSB performance benchmark on CockroachDB</title>
    <url>/blog/ycsb-performance-benchmark-on-cockroachdb/</url>
    <content><![CDATA[<h2 id="Intro-to-go-ycsb"><a href="#Intro-to-go-ycsb" class="headerlink" title="Intro to go-ycsb"></a>Intro to go-ycsb</h2><p>go-ycsb is a Go port of YCSB. It fully supports all YCSB generators and the Core workload so we can do the basic CRUD benchmarks with Go.</p>
<h2 id="Install-golang-package"><a href="#Install-golang-package" class="headerlink" title="Install golang package"></a>Install golang package</h2><pre><code>$ wget https://dl.google.com/go/go1.19.2.linux-amd64.tar.gz
$ tar -C /usr/local -xvzf go1.19.2.linux-amd64.tar.gz
$ cat /usr/local/go/VERSION
go1.19.2

$ vim /root/.bash_profile
PATH=$PATH:$HOME/bin:/usr/local/go/bin
export PATH

$ source /root/.bash_profile
$ go version
go version go1.19.2 linux/amd64
</code></pre>
<h2 id="Install-go-ycsb"><a href="#Install-go-ycsb" class="headerlink" title="Install go-ycsb"></a>Install go-ycsb</h2><pre><code>$ git clone https://github.com/pingcap/go-ycsb.git
$ cd go-ycsb

$ make
go build -tags &quot; libsqlite3&quot; -o bin/go-ycsb cmd/go-ycsb/*
# github.com/mattn/go-sqlite3
/root/go/pkg/mod/github.com/mattn/go-sqlite3@v2.0.1+incompatible/backup.go:12:21: fatal error: sqlite3.h: No such file or directory
 #include &lt;sqlite3.h&gt;
                     ^
compilation terminated.
make: *** [build] Error 2

$ cat /etc/centos-release
CentOS Linux release 7.9.2009 (Core)

$ yum install sqlite-devel.x86_64
$ rpm -qa | grep sqlite
sqlite-3.7.17-8.el7_7.1.x86_64
sqlite-devel-3.7.17-8.el7_7.1.x86_64

$ make
go build -tags &quot; libsqlite3&quot; -o bin/go-ycsb cmd/go-ycsb/*

$ ./bin/go-ycsb --help
Go YCSB

Usage:
  go-ycsb [command]

Available Commands:
  help        Help about any command
  load        YCSB load benchmark
  run         YCSB run benchmark
  shell       YCSB Command Line Client

Flags:
  -h, --help   help for go-ycsb

Use &quot;go-ycsb [command] --help&quot; for more information about a command.
</code></pre>
<h2 id="YCSB-workloads"><a href="#YCSB-workloads" class="headerlink" title="YCSB workloads"></a>YCSB workloads</h2><p>YCSB includes a set of core workloads that define a basic benchmark for cloud systems. Of course, you can define your own workloads, as described in Implementing New Workloads. However, the core workloads are a useful first step, and obtaining these benchmark numbers for a variety of different systems would allow you to understand the performance tradeoffs of different systems.</p>
<p>The core workloads consist of six different workloads:</p>
<pre><code>[root@cockroach-db-host go-ycsb]# ls db
aerospike  basic   cassandra      etcd          minio    mysql    pg     rocksdb  sqlite
badger     boltdb  elasticsearch  foundationdb  mongodb  pegasus  redis  spanner  tikv

[root@cockroach-db-host go-ycsb]# ls workloads/
minio  workloada  workloadb  workloadc  workloadd  workloadd.orig  workloade  workloadf  workload_template
</code></pre>
<h3 id="Workload-A-Update-heavy-workload"><a href="#Workload-A-Update-heavy-workload" class="headerlink" title="Workload A: Update heavy workload"></a>Workload A: Update heavy workload</h3><pre><code>$ cat go-ycsb/workloads/workloada
# Workload A: Update heavy workload
#   Application example: Session store recording recent actions
#
#   Read/update ratio: 50/50
#   Default data size: 1 KB records (10 fields, 100 bytes each, plus key)
#   Request distribution: zipfian

recordcount=1000
operationcount=1000
workload=core

readallfields=true

readproportion=0.5
updateproportion=0.5
scanproportion=0
insertproportion=0

requestdistribution=uniform
</code></pre>
<h3 id="Workload-B-Read-mostly-workload"><a href="#Workload-B-Read-mostly-workload" class="headerlink" title="Workload B: Read mostly workload"></a>Workload B: Read mostly workload</h3><pre><code>$ cat go-ycsb/workloads/workloadb
# Yahoo! Cloud System Benchmark
# Workload B: Read mostly workload
#   Application example: photo tagging; add a tag is an update, but most operations are to read tags
#
#   Read/update ratio: 95/5
#   Default data size: 1 KB records (10 fields, 100 bytes each, plus key)
#   Request distribution: zipfian

recordcount=1000
operationcount=1000
workload=core

readallfields=true

readproportion=0.95
updateproportion=0.05
scanproportion=0
insertproportion=0

requestdistribution=uniform
</code></pre>
<h3 id="Workload-C-Read-only"><a href="#Workload-C-Read-only" class="headerlink" title="Workload C: Read only"></a>Workload C: Read only</h3><pre><code>$ cat go-ycsb/workloads/workloadc
# Yahoo! Cloud System Benchmark
# Workload C: Read only
#   Application example: user profile cache, where profiles are constructed elsewhere (e.g., Hadoop)
#
#   Read/update ratio: 100/0
#   Default data size: 1 KB records (10 fields, 100 bytes each, plus key)
#   Request distribution: zipfian

recordcount=1000
operationcount=1000
workload=core

readallfields=true

readproportion=1
updateproportion=0
scanproportion=0
insertproportion=0

requestdistribution=uniform
</code></pre>
<h3 id="Workload-D-Read-latest-workload"><a href="#Workload-D-Read-latest-workload" class="headerlink" title="Workload D: Read latest workload"></a>Workload D: Read latest workload</h3><pre><code>$ cat go-ycsb/workloads/workloadd
# Yahoo! Cloud System Benchmark
# Workload D: Read latest workload
#   Application example: user status updates; people want to read the latest
#
#   Read/update/insert ratio: 95/0/5
#   Default data size: 1 KB records (10 fields, 100 bytes each, plus key)
#   Request distribution: latest

# The insert order for this is hashed, not ordered. The &quot;latest&quot; items may be
# scattered around the keyspace if they are keyed by userid.timestamp. A workload
# which orders items purely by time, and demands the latest, is very different than
# workload here (which we believe is more typical of how people build systems.)

recordcount=1000
operationcount=1000
workload=core

readallfields=true

readproportion=0.95
updateproportion=0
scanproportion=0
insertproportion=0.05

requestdistribution=latest
</code></pre>
<h3 id="Workload-E-Short-ranges"><a href="#Workload-E-Short-ranges" class="headerlink" title="Workload E: Short ranges"></a>Workload E: Short ranges</h3><pre><code>$ cat go-ycsb/workloads/workloade
# Yahoo! Cloud System Benchmark
# Workload E: Short ranges
#   Application example: threaded conversations, where each scan is for the posts in a given thread (assumed to be clustered by thread id)
#
#   Scan/insert ratio: 95/5
#   Default data size: 1 KB records (10 fields, 100 bytes each, plus key)
#   Request distribution: zipfian

# The insert order is hashed, not ordered. Although the scans are ordered, it does not necessarily
# follow that the data is inserted in order. For example, posts for thread 342 may not be inserted contiguously, but
# instead interspersed with posts from lots of other threads. The way the YCSB client works is that it will pick a start
# key, and then request a number of records; this works fine even for hashed insertion.

recordcount=1000
operationcount=1000
workload=core

readallfields=true

readproportion=0
updateproportion=0
scanproportion=0.95
insertproportion=0.05

requestdistribution=uniform

maxscanlength=1

scanlengthdistribution=uniform
</code></pre>
<h3 id="Workload-F-Read-modify-write"><a href="#Workload-F-Read-modify-write" class="headerlink" title="Workload F: Read-modify-write"></a>Workload F: Read-modify-write</h3><pre><code>$ cat go-ycsb/workloads/workloadf
# Yahoo! Cloud System Benchmark
# Workload F: Read-modify-write workload
#   Application example: user database, where user records are read and modified by the user or to record user activity.
#
#   Read/read-modify-write ratio: 50/50
#   Default data size: 1 KB records (10 fields, 100 bytes each, plus key)
#   Request distribution: zipfian

recordcount=1000
operationcount=1000
workload=core

readallfields=true

readproportion=0.5
updateproportion=0
scanproportion=0
insertproportion=0
readmodifywriteproportion=0.5

requestdistribution=uniform
</code></pre>
<h2 id="Load-the-data-to-database"><a href="#Load-the-data-to-database" class="headerlink" title="Load the data to database"></a>Load the data to database</h2><p>The database needs to be created before data load.</p>
<pre><code>root@cockroach-db-host-ip:26257/defaultdb&gt; create database test;
CREATE DATABASE

Time: 16ms total (execution 16ms / network 0ms)

root@cockroach-db-host-ip:26257/defaultdb&gt; show databases;
  database_name | owner | primary_region | regions | survival_goal
----------------+-------+----------------+---------+----------------
  defaultdb     | root  | NULL           | &#123;&#125;      | NULL
  postgres      | root  | NULL           | &#123;&#125;      | NULL
  system        | node  | NULL           | &#123;&#125;      | NULL
  test          | root  | NULL           | &#123;&#125;      | NULL
(4 rows)

Time: 6ms total (execution 5ms / network 1ms)
</code></pre>
<p>Load the database as below:</p>
<pre><code>[root@cockroach-db-host go-ycsb]# ./bin/go-ycsb load --help
YCSB load benchmark

Usage:
  go-ycsb load db [flags]

Flags:
  -h, --help                    help for load
      --interval int            Interval of outputting measurements in seconds (default 10)
  -p, --prop stringArray        Specify a property value with name=value
  -P, --property_file strings   Spefify a property file
      --table string            Use the table name instead of the default &quot;usertable&quot;
      --target int              Attempt to do n operations per second (default: unlimited) - can also be specified as the &quot;target&quot; property
      --threads int             Execute using n threads - can also be specified as the &quot;threadcount&quot; property (default 1)

$ ./bin/go-ycsb load cockroach -P workloads/workloadd -p pg.host=cockroach-db-host-ip -p pg.port=26257 -p pg.user=root -p pg.db=test -p pg.sslmode=disable -p dropdata=true
***************** properties *****************
&quot;insertproportion&quot;=&quot;0.05&quot;
&quot;dropdata&quot;=&quot;true&quot;
&quot;scanproportion&quot;=&quot;0&quot;
&quot;workload&quot;=&quot;core&quot;
&quot;recordcount&quot;=&quot;1000&quot;
&quot;pg.port&quot;=&quot;26257&quot;
&quot;dotransactions&quot;=&quot;false&quot;
&quot;updateproportion&quot;=&quot;0&quot;
&quot;pg.user&quot;=&quot;root&quot;
&quot;requestdistribution&quot;=&quot;latest&quot;
&quot;readallfields&quot;=&quot;true&quot;
&quot;pg.sslmode&quot;=&quot;disable&quot;
&quot;command&quot;=&quot;load&quot;
&quot;pg.host&quot;=&quot;cockroach-db-host-ip&quot;
&quot;operationcount&quot;=&quot;1000&quot;
&quot;pg.db&quot;=&quot;test&quot;
&quot;readproportion&quot;=&quot;0.95&quot;
**********************************************
Run finished, takes 2.794042708s
INSERT - Takes(s): 2.8, Count: 1000, OPS: 361.7, Avg(us): 2734, Min(us): 1794, Max(us): 29903, 99th(us): 3557, 99.9th(us): 10999, 99.99th(us): 29903
</code></pre>
<p>The “–threads” option can be specified to increase the number of threads for data load.</p>
<pre><code>$ ./bin/go-ycsb load cockroach -P workloads/workloadd --threads 96 -p pg.host=cockroach-db-host-ip -p pg.port=26257 -p pg.user=root -p pg.db=test -p pg.sslmode=disable -p dropdata=true
</code></pre>
<h2 id="Verify-the-loaded-data"><a href="#Verify-the-loaded-data" class="headerlink" title="Verify the loaded data"></a>Verify the loaded data</h2><pre><code>root@cockroach-db-host-ip:26257/defaultdb&gt; use test;
SET

Time: 1ms total (execution 1ms / network 0ms)

root@cockroach-db-host-ip:26257/test&gt; show tables;
  schema_name | table_name | type  | owner | estimated_row_count | locality
--------------+------------+-------+-------+---------------------+-----------
  public      | usertable  | table | root  |                1000 | NULL
(1 row)

Time: 39ms total (execution 38ms / network 0ms)

root@cockroach-db-host-ip:26257/test&gt; show columns from usertable;
  column_name |  data_type   | is_nullable | column_default | generation_expression |     indices      | is_hidden
--------------+--------------+-------------+----------------+-----------------------+------------------+------------
  ycsb_key    | VARCHAR(64)  |    false    | NULL           |                       | &#123;usertable_pkey&#125; |   false
  field0      | VARCHAR(100) |    true     | NULL           |                       | &#123;usertable_pkey&#125; |   false
  field1      | VARCHAR(100) |    true     | NULL           |                       | &#123;usertable_pkey&#125; |   false
  field2      | VARCHAR(100) |    true     | NULL           |                       | &#123;usertable_pkey&#125; |   false
  field3      | VARCHAR(100) |    true     | NULL           |                       | &#123;usertable_pkey&#125; |   false
  field4      | VARCHAR(100) |    true     | NULL           |                       | &#123;usertable_pkey&#125; |   false
  field5      | VARCHAR(100) |    true     | NULL           |                       | &#123;usertable_pkey&#125; |   false
  field6      | VARCHAR(100) |    true     | NULL           |                       | &#123;usertable_pkey&#125; |   false
  field7      | VARCHAR(100) |    true     | NULL           |                       | &#123;usertable_pkey&#125; |   false
  field8      | VARCHAR(100) |    true     | NULL           |                       | &#123;usertable_pkey&#125; |   false
  field9      | VARCHAR(100) |    true     | NULL           |                       | &#123;usertable_pkey&#125; |   false
(11 rows)

Time: 56ms total (execution 55ms / network 1ms)

root@cockroach-db-host-ip:26257/test&gt; show ranges from table usertable;
  start_key | end_key | range_id |     range_size_mb     | lease_holder | lease_holder_locality | replicas | replica_localities
------------+---------+----------+-----------------------+--------------+-----------------------+----------+---------------------
  NULL      | NULL    |       45 | 1.0670000000000000000 |            1 |                       | &#123;1,2,3&#125;  | &#123;,,&#125;
(1 row)

Time: 26ms total (execution 25ms / network 1ms)

root@cockroach-db-host-ip:26257/test&gt; select count(*) from usertable;
  count
---------
   1000
(1 row)

Time: 3ms total (execution 2ms / network 0ms)

root@cockroach-db-host-ip:26257/test&gt; select * from usertable limit 2;
         ycsb_key         |                                                field0                                                |                                                field1                                                |                                                field2                                                |                                                field3                                                |                                                field4                                                |                                                field5                                                |                                                field6                                                |                                                field7                                                |                                                field8                                                |                                                
         field9

  user6282602628620641459 | CRvLzkeCGxHtROFZjmDBFtRGwQigtZAPNCOSMdNXgDNsmpONJsVKRyJpleAmmigBbLoCUvtqDwPNYNoipLCXFqrEvBLnIoDrHSVq | dmhlgvUykChQAJzTfgSwBwEuWwBhLgcYUkBFAObtredxAUHWkCVaWfgMQCKIYUUrnHbguWZGrZXmYVRKAoZHVOnKdBsMYYaUqCWC | UtVaaGeRrAoupCumOJNesfunCVXnJhFoWBlKqebBYFZyooDIaYHPeQVgvCqEfCdmGpVtcLZJYhjvMSyFBZixzyMiykEoTZXMbFic | VmmyYkAgPawZuSqZmSiIEoEnssZHuLfbBhcpFixthpYsZUHgbxfpiffrgOFSlIYgejLLQuXMaGDxBbKHegWaHXpDBdxUUHZgJSLx | cCRKTqdUukzOdPwxqTBKyBEQPkIyIgAkxdaqXAiYbNwhuJVCoEqpBPSUApjLyvSXdyAqovEwSnsiIVvuviUCTiqcelbJuwvnCGyx | JuggBRLaLLqWhcCNoxEpbJZsXhcxSGyaPNRUrLoqrnKvpJfLvABueeZQOsmCERRMXasJIIJazBQoqLKVHfELiOwzolwgcXfOtLco | DJrJlWIgICwWgOuEQjnzUiZoCnGnKYXcJtrrgEYznFnKsOEmapuWzuwFVekgElVlYuwOoruMKqHoIOjHqLYZCUzoiSsIbPHZscaq | MOZiEsuFeHnouWlGmBcvNNZvXuOWqHlHKBDEFfYXcsaHNUGJPLUWpEGeHOKOilztleethxYoHLdAFBZdClYRRApEDoezGpuEvpwa | YvdqeMSnQPLoXYfExfduQrNSFRiTVKFPZdGATDpQMvtCHAdqTqMYjMpElQgcRjUuJUBMqwXJDzcVWlGiQSAirgVVDBGoAHqyzgdA | mkKBHtYOEsdXlpndKPOYHTQiqkFbTAsDbESpWeYXVSiBximrDHhjBARBZHkjWQjzZGITkqjecazQeCHvmkePqefSNieJtFTdbvRI

  user6282603728132269670 | DSNUAvAFJxstogmWRPJmbNikPspeilFAZQJKzLRdxJpyOQJeQbltpxqnYJQtWwihrPbvZoOzbaMrMlLKBiAWjttcMskyfguSFidf | pSYiwZdqValtHyYGoYwkVYGawNErokKdMTiNCJUrcKTQWSQWKGixRVDxJTcaCfUXHMLxSSvDdFTVjTuxqWGWkWlxKocypFUGupqH | YBoLQdTXAQpjUgZReUGUfAOzsrBMKZyoZCdwxWJrOBkRlbbpXUmZLQxHHnDgnLNCNEnxozGPFCGEOVilyqLCixqDIsPNhccTdhbq | UQNzeZJXzbvwwupufEHeacjxxrTGCYfppLsbOCxLyfvuhZYPmoSyhKupkGRkHsYSnQVCFHCokWmHFWnLbYctVGdAunsPnkDjQAPk | HkqgViwhjGaBGUXJikhpwPMFCxqtOxoIunZQjmeKcytoItvGrfvcztncPzUukauoovIoQRmBCcQhJpoZxDLmXcpQJgFAbDYsbBGc | VDazXEFSiKZSKoSPMpAnFNAzqbrGcZwwrSMjFIJoEJPfvDzdwWwiVmHRkRGuFzwrXTOAKOiazelzsQFMAARAZdreFMWzKIwXieCP | lwbhKgtyTCZYJZmeRbVmWzkmPsWvdURifyZgGuSwVRrcCDCmrwvEdnFTWgCsYmcqzFgziAjgakhSuJjyixzoNThbtIgTkPuzAMVX | cKRURxsRlJvcKgrRQwMFeRLKCWfiavzTKqTJbYbWKZgOkZuutCIltPbwufcIMPDRWdgAZVaPqGCussiIxcFjMHMioVNLLQklptbG | OzGsnjbnnUtzsxjSVLpukwOcdegoTYrBLhOISwiNdNMGzKXBbDIRpBQqvNFEUEVfMPPJIWGSVzvSGTyAazIOvSMnWeEdKnEuggge | aHOwSIrvztvWYhrsdYaYGjtdjLTiVgfZluTgohVwNABYlZyiYPdrZgmMCFTQLOXqcbBbtahLNzPxdtoeqxrCBAussTPjmJrIzxns
(2 rows)

Time: 2ms total (execution 1ms / network 1ms)
</code></pre>
<h2 id="Run-the-YCSB-benchmark"><a href="#Run-the-YCSB-benchmark" class="headerlink" title="Run the YCSB benchmark"></a>Run the YCSB benchmark</h2><pre><code>[root@cockroach-db-host go-ycsb]# ./bin/go-ycsb run --help
YCSB run benchmark

Usage:
  go-ycsb run db [flags]

Flags:
  -h, --help                    help for run
      --interval int            Interval of outputting measurements in seconds (default 10)
  -p, --prop stringArray        Specify a property value with name=value
  -P, --property_file strings   Spefify a property file
      --table string            Use the table name instead of the default &quot;usertable&quot;
      --target int              Attempt to do n operations per second (default: unlimited) - can also be specified as the &quot;target&quot; property
      --threads int             Execute using n threads - can also be specified as the &quot;threadcount&quot; property (default 1)

[root@cockroach-db-host go-ycsb]# ./bin/go-ycsb run cockroach -P workloads/workloadd --threads 96 -p pg.host=cockroach-db-host-ip -p pg.port=26257 -p pg.user=root -p pg.db=test -p pg.sslmode=disable
Run finished, takes 41m57.357806424s
INSERT - Takes(s): 2517.3, Count: 4992285, OPS: 1983.2, Avg(us): 5053, Min(us): 1214, Max(us): 32863, 99th(us): 26719, 99.9th(us): 31471, 99.99th(us): 32671
READ   - Takes(s): 2517.3, Count: 94554325, OPS: 37561.2, Avg(us): 2131, Min(us): 389, Max(us): 20687, 99th(us): 11543, 99.9th(us): 19695, 99.99th(us): 20591
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/pingcap/go-ycsb">https://github.com/pingcap/go-ycsb</a></li>
<li><a href="https://github.com/brianfrankcooper/YCSB/wiki/Core-Workloads">https://github.com/brianfrankcooper/YCSB/wiki/Core-Workloads</a></li>
<li><a href="https://github.com/brianfrankcooper/YCSB/wiki/Running-a-Workload">https://github.com/brianfrankcooper/YCSB/wiki/Running-a-Workload</a></li>
<li><a href="https://www.scylladb.com/product/benchmarks/scylla-vs-cockroachdb/">https://www.scylladb.com/product/benchmarks/scylla-vs-cockroachdb/</a></li>
<li><a href="https://medium.com/@siddontang/use-go-ycsb-to-benchmark-different-databases-8850f6edb3a7">https:&#x2F;&#x2F;medium.com&#x2F;@siddontang&#x2F;use-go-ycsb-to-benchmark-different-databases-8850f6edb3a7</a></li>
<li><a href="https://github.com/cockroachdb/docs/blob/master/v2.1/cockroach-workload.md">https://github.com/cockroachdb/docs/blob/master/v2.1/cockroach-workload.md</a></li>
<li><a href="https://research.yahoo.com/news/yahoo-cloud-serving-benchmark/">Yahoo announcement</a></li>
<li><a href="https://courses.cs.duke.edu/fall13/compsci590.4/838-CloudPapers/ycsb.pdf">Paper - Benchmarking Cloud Serving Systems with YCSB</a></li>
<li><a href="https://s.yimg.com/ge/labs/v1/files/ycsb-v4.pdf">Yahoo lab experiments</a></li>
<li><a href="https://helenchw.github.io/files/ycsb-overview.pdf">A Quick Introduction to YCSB</a></li>
<li><a href="https://benchant.com/blog/ycsb">The Ultimate YCSB Benchmark Guide 2021</a></li>
<li><a href="http://highscalability.com/blog/2021/2/17/benchmark-ycsb-numbers-for-redis-mongodb-couchbase2-yugabyte.html">http://highscalability.com/blog/2021/2/17/benchmark-ycsb-numbers-for-redis-mongodb-couchbase2-yugabyte.html</a></li>
<li><a href="https://en.wikipedia.org/wiki/YCSB">https://en.wikipedia.org/wiki/YCSB</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>CockroachDB</tag>
        <tag>YCSB</tag>
      </tags>
  </entry>
  <entry>
    <title>YCSB benchmark on PostgreSQL</title>
    <url>/blog/ycsb-benchmark-on-postgresql/</url>
    <content><![CDATA[<h2 id="Create-a-PostgreSQL-database"><a href="#Create-a-PostgreSQL-database" class="headerlink" title="Create a PostgreSQL database"></a>Create a PostgreSQL database</h2><p>You can follow this <a href="https://www.flamingbytes.com/blog/getting-started-with-postgresql/">post</a> to install PostgreSQL and create the database cluster.</p>
<p><strong>To create a database:</strong></p>
<pre><code>$ db_host=10.10.10.243; db_port=5432; db_user=postgres; db_name=testdb

$ psql --host=$db_host --port=$db_port --username=$db_user -w -c &quot;create database $db_name&quot;
$ psql --host=$db_host --port=$db_port --username=$db_user -w -c &quot;\l&quot;
$ psql --host=$db_host --port=$db_port --username=$db_user -w -d $db_name -c &quot;\dt+&quot;
</code></pre>
<h2 id="Load-data-to-database"><a href="#Load-data-to-database" class="headerlink" title="Load data to database"></a>Load data to database</h2><p><a href="https://github.com/pingcap/go-ycsb">go-ycsb</a> is a Go port of YCSB. It fully supports all YCSB generators and the Core workload so we can do the basic CRUD benchmarks with Go.</p>
<pre><code>$ ./bin/go-ycsb load postgresql -P workloads/workloadd --threads 768 -p pg.host=$db_host -p pg.port=$db_port -p pg.user=$db_user -p pg.db=$db_name -p pg.sslmode=disable -p dropdata=true
&lt;snippet&gt;
Run finished, takes 29m39.847077518s
INSERT - Takes(s): 1779.8, Count: 97404992, OPS: 54727.5, Avg(us): 11176, Min(us): 179, Max(us): 80767, 99th(us): 72127, 99.9th(us): 79807, 99.99th(us): 80703
</code></pre>
<h2 id="Run-YCSB-benchmark"><a href="#Run-YCSB-benchmark" class="headerlink" title="Run YCSB benchmark"></a>Run YCSB benchmark</h2><pre><code>$ ./bin/go-ycsb run postgresql -P workloads/workloadd --threads 768 -p pg.host=$db_host -p pg.port=$db_port -p pg.user=$db_user -p pg.db=$db_name -p pg.sslmode=disable
&lt;snippet&gt;
Run finished, takes 14m8.828228178s
INSERT - Takes(s): 848.8, Count: 4981537, OPS: 5868.8, Avg(us): 1968, Min(us): 226, Max(us): 19327, 99th(us): 11623, 99.9th(us): 17567, 99.99th(us): 19119
READ   - Takes(s): 848.8, Count: 93786269, OPS: 110492.3, Avg(us): 5899, Min(us): 78, Max(us): 41247, 99th(us): 32831, 99.9th(us): 40063, 99.99th(us): 41119
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/pingcap/go-ycsb">https://github.com/pingcap/go-ycsb</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>PostgreSQL</tag>
        <tag>YCSB</tag>
      </tags>
  </entry>
  <entry>
    <title>Art - Sketch 2023</title>
    <url>/blog/art-sketch-2023/</url>
    <content><![CDATA[<p><img src="/images/drawing/ear_dragon.png"></p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Drawing</tag>
      </tags>
  </entry>
  <entry>
    <title>Heap and priority queue in Python</title>
    <url>/blog/heap-and-priority-queue-in-python/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>In Python, “heapq” module is available to use. Whenever elements are pushed or popped, heap structure is maintained. The heap[0] element also returns the smallest element each time.<span id="more"></span></p>
<h2 id="Leetcode-857-Minimum-Cost-to-Hire-K-Workers"><a href="#Leetcode-857-Minimum-Cost-to-Hire-K-Workers" class="headerlink" title="[Leetcode 857] Minimum Cost to Hire K Workers"></a>[Leetcode 857] Minimum Cost to Hire K Workers</h2><p>There are n workers. You are given two integer arrays quality and wage where quality[i] is the quality of the ith worker and wage[i] is the minimum wage expectation for the ith worker.</p>
<p>We want to hire exactly k workers to form a paid group. To hire a group of k workers, we must pay them according to the following rules:</p>
<ul>
<li>Every worker in the paid group should be paid in the ratio of their quality compared to other workers in the paid group.</li>
<li>Every worker in the paid group must be paid at least their minimum wage expectation.</li>
</ul>
<p>Given the integer k, return the least amount of money needed to form a paid group satisfying the above conditions. Answers within 10-5 of the actual answer will be accepted. </p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: quality = [10,20,5], wage = [70,50,30], k = 2</span><br><span class="line">Output: 105.00000</span><br><span class="line">Explanation: We pay 70 to 0th worker and 35 to 2nd worker.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: quality = [3,1,10,10,1], wage = [4,8,2,2,7], k = 3</span><br><span class="line">Output: 30.66667</span><br><span class="line">Explanation: We pay 4 to 0th worker, 13.33333 to 2nd and 3rd workers separately.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>n &#x3D;&#x3D; quality.length &#x3D;&#x3D; wage.length</li>
<li>1 &lt;&#x3D; k &lt;&#x3D; n &lt;&#x3D; 104</li>
<li>1 &lt;&#x3D; quality[i], wage[i] &lt;&#x3D; 104</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mincostToHireWorkers</span>(<span class="params">self, quality: <span class="type">List</span>[<span class="built_in">int</span>], wage: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        res, qualitySum, maxHeap = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>), <span class="number">0</span>, []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ration, q <span class="keyword">in</span> <span class="built_in">sorted</span>([(w / q, q) <span class="keyword">for</span> w, q <span class="keyword">in</span> <span class="built_in">zip</span>(wage, quality)]):</span><br><span class="line">            qualitySum += q</span><br><span class="line">            heappush(maxHeap, -q)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(maxHeap) == k:</span><br><span class="line">                res = <span class="built_in">min</span>(res, qualitySum * ration)</span><br><span class="line">                qualitySum -= -heappop(maxHeap)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res </span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-1383-Maximum-Performance-of-a-Team"><a href="#Leetcode-1383-Maximum-Performance-of-a-Team" class="headerlink" title="[Leetcode 1383] Maximum Performance of a Team"></a>[Leetcode 1383] Maximum Performance of a Team</h2><p>You are given two integers n and k and two integer arrays speed and efficiency both of length n. There are n engineers numbered from 1 to n. speed[i] and efficiency[i] represent the speed and efficiency of the ith engineer respectively.</p>
<p>Choose at most k different engineers out of the n engineers to form a team with the maximum performance.</p>
<p>The performance of a team is the sum of its engineers’ speeds multiplied by the minimum efficiency among its engineers.</p>
<p>Return the maximum performance of this team. Since the answer can be a huge number, return it modulo 109 + 7.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 6, speed = [2,10,3,1,5,8], efficiency = [5,4,3,9,7,2], k = 2</span><br><span class="line">Output: 60</span><br><span class="line">Explanation: </span><br><span class="line">We have the maximum performance of the team by selecting engineer 2 (with speed=10 and efficiency=4) and engineer 5 (with speed=5 and efficiency=7). That is, performance = (10 + 5) * min(4, 7) = 60.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 6, speed = [2,10,3,1,5,8], efficiency = [5,4,3,9,7,2], k = 3</span><br><span class="line">Output: 68</span><br><span class="line">Explanation:</span><br><span class="line">This is the same example as the first but k = 3. We can select engineer 1, engineer 2 and engineer 5 to get the maximum performance of the team. That is, performance = (2 + 10 + 5) * min(5, 4, 7) = 68.</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 6, speed = [2,10,3,1,5,8], efficiency = [5,4,3,9,7,2], k = 4</span><br><span class="line">Output: 72</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; k &lt;&#x3D; n &lt;&#x3D; 105</li>
<li>speed.length &#x3D;&#x3D; n</li>
<li>efficiency.length &#x3D;&#x3D; n</li>
<li>1 &lt;&#x3D; speed[i] &lt;&#x3D; 105</li>
<li>1 &lt;&#x3D; efficiency[i] &lt;&#x3D; 108</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxPerformance</span>(<span class="params">self, n: <span class="built_in">int</span>, speed: <span class="type">List</span>[<span class="built_in">int</span>], efficiency: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        res, speedSum, minHeap = <span class="number">0</span>, <span class="number">0</span>, []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> ef, sp <span class="keyword">in</span> <span class="built_in">sorted</span>(<span class="built_in">zip</span>(efficiency, speed), reverse=<span class="literal">True</span>):</span><br><span class="line">            speedSum += sp</span><br><span class="line">            heappush(minHeap, sp)</span><br><span class="line">            res = <span class="built_in">max</span>(res, speedSum * ef)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(minHeap) == k:</span><br><span class="line">                speedSum -= heappop(minHeap)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res % (<span class="number">10</span>**<span class="number">9</span> + <span class="number">7</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-2462-Total-Cost-to-Hire-K-Workers"><a href="#Leetcode-2462-Total-Cost-to-Hire-K-Workers" class="headerlink" title="[Leetcode 2462] Total Cost to Hire K Workers"></a>[Leetcode 2462] Total Cost to Hire K Workers</h2><p>You are given a 0-indexed integer array costs where costs[i] is the cost of hiring the ith worker.</p>
<p>You are also given two integers k and candidates. We want to hire exactly k workers according to the following rules:</p>
<ul>
<li>You will run k sessions and hire exactly one worker in each session.</li>
<li>In each hiring session, choose the worker with the lowest cost from either the first candidates(instead, read candidates as n to make it easier understand) workers or the last candidates workers. Break the tie by the smallest index.<ul>
<li>For example, if costs &#x3D; [3,2,7,7,1,2] and candidates &#x3D; 2, then in the first hiring session, we will choose the 4th worker because they have the lowest cost [3,2,7,7,1,2].</li>
<li>In the second hiring session, we will choose 1st worker because they have the same lowest cost as 4th worker but they have the smallest index [3,2,7,7,2]. Please note that the indexing may be changed in the process.</li>
</ul>
</li>
<li>If there are fewer than candidates workers remaining, choose the worker with the lowest cost among them. Break the tie by the smallest index.</li>
<li>A worker can only be chosen once.</li>
</ul>
<p>Return the total cost to hire exactly k workers.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: costs = [17,12,10,2,7,2,11,20,8], k = 3, candidates = 4</span><br><span class="line">Output: 11</span><br><span class="line">Explanation: We hire 3 workers in total. The total cost is initially 0.</span><br><span class="line">- In the first hiring round we choose the worker from [17,12,10,2,7,2,11,20,8]. The lowest cost is 2, and we break the tie by the smallest index, which is 3. The total cost = 0 + 2 = 2.</span><br><span class="line">- In the second hiring round we choose the worker from [17,12,10,7,2,11,20,8]. The lowest cost is 2 (index 4). The total cost = 2 + 2 = 4.</span><br><span class="line">- In the third hiring round we choose the worker from [17,12,10,7,11,20,8]. The lowest cost is 7 (index 3). The total cost = 4 + 7 = 11. Notice that the worker with index 3 was common in the first and last four workers.</span><br><span class="line">The total hiring cost is 11.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: costs = [1,2,4,1], k = 3, candidates = 3</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: We hire 3 workers in total. The total cost is initially 0.</span><br><span class="line">- In the first hiring round we choose the worker from [1,2,4,1]. The lowest cost is 1, and we break the tie by the smallest index, which is 0. The total cost = 0 + 1 = 1. Notice that workers with index 1 and 2 are common in the first and last 3 workers.</span><br><span class="line">- In the second hiring round we choose the worker from [2,4,1]. The lowest cost is 1 (index 2). The total cost = 1 + 1 = 2.</span><br><span class="line">- In the third hiring round there are less than three candidates. We choose the worker from the remaining workers [2,4]. The lowest cost is 2 (index 0). The total cost = 2 + 2 = 4.</span><br><span class="line">The total hiring cost is 4.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; costs.length &lt;&#x3D; 105 </li>
<li>1 &lt;&#x3D; costs[i] &lt;&#x3D; 105</li>
<li>1 &lt;&#x3D; k, candidates &lt;&#x3D; costs.length</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">totalCost</span>(<span class="params">self, costs: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span>, candidates: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        q1 = []</span><br><span class="line">        q2 = []</span><br><span class="line">        i, j = <span class="number">0</span>, <span class="built_in">len</span>(costs) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> k &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># construct two queues w/o common elements</span></span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">len</span>(q1) &lt; candidates <span class="keyword">and</span> i &lt;= j:</span><br><span class="line">                heapq.heappush(q1, costs[i])</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">len</span>(q2) &lt; candidates <span class="keyword">and</span> i &lt;= j:</span><br><span class="line">                heapq.heappush(q2, costs[j])</span><br><span class="line">                j -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># peek the smallest element from each qeue</span></span><br><span class="line">            cost1 = q1[<span class="number">0</span>] <span class="keyword">if</span> q1 <span class="keyword">else</span> <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line">            cost2 = q2[<span class="number">0</span>] <span class="keyword">if</span> q2 <span class="keyword">else</span> <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># select smaller cost from two queues</span></span><br><span class="line">            <span class="keyword">if</span> cost1 &lt;= cost2:</span><br><span class="line">                ans += cost1</span><br><span class="line">                heapq.heappop(q1)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ans += cost2</span><br><span class="line">                heapq.heappop(q2)</span><br><span class="line"></span><br><span class="line">            k -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-2542-Maximum-Subsequence-Score"><a href="#Leetcode-2542-Maximum-Subsequence-Score" class="headerlink" title="[Leetcode 2542] Maximum Subsequence Score"></a>[Leetcode 2542] Maximum Subsequence Score</h2><p>You are given two 0-indexed integer arrays nums1 and nums2 of equal length n and a positive integer k. You must choose a subsequence of indices from nums1 of length k.</p>
<p>For chosen indices i0, i1, …, ik - 1, your score is defined as:</p>
<ul>
<li>The sum of the selected elements from nums1 multiplied with the minimum of the selected elements from nums2.</li>
<li>It can defined simply as: (nums1[i0] + nums1[i1] +…+ nums1[ik - 1]) * min(nums2[i0] , nums2[i1], … ,nums2[ik - 1]).<br>Return the maximum possible score.</li>
</ul>
<p>A subsequence of indices of an array is a set that can be derived from the set {0, 1, …, n-1} by deleting some or no elements.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums1 = [1,3,3,2], nums2 = [2,1,3,4], k = 3</span><br><span class="line">Output: 12</span><br><span class="line">Explanation: </span><br><span class="line">The four possible subsequence scores are:</span><br><span class="line">- We choose the indices 0, 1, and 2 with score = (1+3+3) * min(2,1,3) = 7.</span><br><span class="line">- We choose the indices 0, 1, and 3 with score = (1+3+2) * min(2,1,4) = 6. </span><br><span class="line">- We choose the indices 0, 2, and 3 with score = (1+3+2) * min(2,3,4) = 12. </span><br><span class="line">- We choose the indices 1, 2, and 3 with score = (3+3+2) * min(1,3,4) = 8.</span><br><span class="line">Therefore, we return the max score, which is 12.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>n &#x3D;&#x3D; nums1.length &#x3D;&#x3D; nums2.length</li>
<li>1 &lt;&#x3D; n &lt;&#x3D; 105</li>
<li>0 &lt;&#x3D; nums1[i], nums2[j] &lt;&#x3D; 105</li>
<li>1 &lt;&#x3D; k &lt;&#x3D; n</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxScore</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], nums2: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        preSum = <span class="number">0</span></span><br><span class="line">        minHeap = []</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. construct pair list by zipping nums1 and nums2, </span></span><br><span class="line">        <span class="comment"># 2. sort the list by 2nd element in descending order</span></span><br><span class="line">        <span class="comment"># 3. calculate presum with the first elements in zipped list</span></span><br><span class="line">        <span class="comment"># 4. if there is k elements in minheap, multiply presum with the second element in each pair.</span></span><br><span class="line">        <span class="comment"># 5. the second elelment in each pair is the minimum element as we iterate so far</span></span><br><span class="line">        <span class="comment"># 6. pop the first element from minheap </span></span><br><span class="line">        <span class="keyword">for</span> n1, n2 <span class="keyword">in</span> <span class="built_in">sorted</span>(<span class="built_in">list</span>(<span class="built_in">zip</span>(nums1, nums2)), key = <span class="keyword">lambda</span> n : n[<span class="number">1</span>], reverse = <span class="literal">True</span>):</span><br><span class="line">            preSum += n1</span><br><span class="line">            heapq.heappush(minHeap, n1)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(minHeap) == k:</span><br><span class="line">                res = <span class="built_in">max</span>(res, preSum * n2)</span><br><span class="line">                preSum -= heapq.heappop(minHeap)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
        <tag>Data structure</tag>
        <tag>Heap</tag>
        <tag>Priority Queue</tag>
      </tags>
  </entry>
  <entry>
    <title>Common used functions in Python</title>
    <url>/blog/common-used-functions-in-python/</url>
    <content><![CDATA[<h1 id="index"><a href="#index" class="headerlink" title="index()"></a>index()</h1><p>The index() method returns the index of the specified element in the list.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">animals = [<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;rabbit&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the index of &#x27;dog&#x27;</span></span><br><span class="line">index = animals.index(<span class="string">&#x27;dog&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: 1</span></span><br></pre></td></tr></table></figure>

<h2 id="set"><a href="#set" class="headerlink" title="set()"></a>set()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">triplets = <span class="built_in">set</span>() <span class="comment"># automatically remove duplicates</span></span><br><span class="line">triplets.add((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">triplets.add((<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">triplets.add((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(triplets)</span><br><span class="line"><span class="comment"># output: &#123;(1, 2, 3), (1, 2, 4)&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="isalnum"><a href="#isalnum" class="headerlink" title="isalnum()"></a>isalnum()</h2><p>The isalnum() method returns True if all the characters are alphanumeric, meaning alphabet letter (a-z) and numbers (0-9).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(s) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> s[left].isalnum():</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> s[right].isalnum():</span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> s[left].lower() != s[right].lower():</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            </span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<h2 id="ord"><a href="#ord" class="headerlink" title="ord()"></a>ord()</h2><p>ord() function returns the Unicode code from a given character. <span id="more"></span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>)) <span class="comment"># 97</span></span><br></pre></td></tr></table></figure>

<h2 id="bin"><a href="#bin" class="headerlink" title="bin()"></a>bin()</h2><p>bin() function returns the binary string of a given integer.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n=<span class="number">12</span> <span class="comment"># 1100</span></span><br><span class="line">countOnesInStr=<span class="built_in">bin</span>(n).count(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(countOnesInStr)</span><br></pre></td></tr></table></figure>

<h2 id="zip"><a href="#zip" class="headerlink" title="zip()"></a>zip()</h2><p>zip() method takes iterable containers and returns a single iterator object, having mapped values from all the containers.</p>
<p>It is used to map the similar index of multiple containers so that they can be used just using a single entity.</p>
<p>Syntax :  zip(*iterators)</p>
<p>Parameters : Python iterables or containers ( list, string etc )<br>Return Value : Returns a single iterator object.</p>
<p>Example:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fruit = [ <span class="string">&quot;apple&quot;</span>, <span class="string">&quot;orange&quot;</span>, <span class="string">&quot;banana&quot;</span>, <span class="string">&quot;pear&quot;</span> ]</span><br><span class="line">amount = [ <span class="number">4</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span> ]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># using zip() to map values</span></span><br><span class="line">mapped = <span class="built_in">zip</span>(fruit, amount)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">set</span>(mapped))</span><br><span class="line"><span class="comment"># output: &#123;(&#x27;banana&#x27;, 3), (&#x27;pear&#x27;, 2), (&#x27;apple&#x27;, 4), (&#x27;orange&#x27;, 1)&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mapped)</span><br><span class="line"><span class="keyword">for</span> f, a <span class="keyword">in</span> mapped:</span><br><span class="line">    <span class="built_in">print</span>(f,a)</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># &lt;zip object at 0x7fa50d60d480&gt;</span></span><br><span class="line"><span class="comment"># apple 4</span></span><br><span class="line"><span class="comment"># orange 1</span></span><br><span class="line"><span class="comment"># banana 3</span></span><br><span class="line"><span class="comment"># pear 2</span></span><br></pre></td></tr></table></figure>

<h2 id="random-randint"><a href="#random-randint" class="headerlink" title="random.randint()"></a>random.randint()</h2><p>random.randint() return random integer in range [a,b] inclusively.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># return random integer in [0,10]</span></span><br><span class="line">random.randint(<span class="number">0</span>, <span class="built_in">len</span>(self.nums) - <span class="number">1</span>) </span><br></pre></td></tr></table></figure>

<h2 id="random-choice"><a href="#random-choice" class="headerlink" title="random.choice()"></a>random.choice()</h2><p>random.choice() chooses a random element from non-empty sequence.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">charset = <span class="string">&quot;abcdefghijklmnopqrstuvwxyxABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>.join([random.choice(charset) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)])) <span class="comment"># 1H0N</span></span><br></pre></td></tr></table></figure>

<h2 id="strip"><a href="#strip" class="headerlink" title="strip()"></a>strip()</h2><p>The strip() method removes any leading, and trailing whitespaces.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">txt = <span class="string">&quot;     banana     &quot;</span></span><br><span class="line"></span><br><span class="line">x = txt.strip()</span><br></pre></td></tr></table></figure>

<h2 id="bisect-bisect-left"><a href="#bisect-bisect-left" class="headerlink" title="bisect.bisect_left()"></a>bisect.bisect_left()</h2><p>Return the index where to insert item x in list a, assuming a is sorted.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> bisect</span><br><span class="line">arr=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">12</span>]</span><br><span class="line">insertIdx = bisect.bisect_left(arr, <span class="number">8</span>) <span class="comment"># 5</span></span><br></pre></td></tr></table></figure>

<h2 id="slice-operations"><a href="#slice-operations" class="headerlink" title="slice operations"></a>slice operations</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s=<span class="string">&quot;abcdefcd&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># reverse string</span></span><br><span class="line"><span class="built_in">print</span>(s[::-<span class="number">1</span>]) <span class="comment"># dcfedcba</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># count substring</span></span><br><span class="line"><span class="built_in">print</span>(s.count(s[<span class="number">2</span>:<span class="number">4</span>])) <span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># last k elements</span></span><br><span class="line"><span class="built_in">print</span>(s[-<span class="number">2</span>:]) <span class="comment"># cd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># repeat string</span></span><br><span class="line"><span class="built_in">print</span>(s * <span class="number">2</span>) <span class="comment"># abcdefcdabcdefcd</span></span><br></pre></td></tr></table></figure>

<h2 id="Dictionary-pop"><a href="#Dictionary-pop" class="headerlink" title="Dictionary pop()"></a>Dictionary pop()</h2><p>pop() method removes and returns an element from a dictionary having the given key.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">marks = &#123; <span class="string">&#x27;Physics&#x27;</span>: <span class="number">85</span>, <span class="string">&#x27;Chemistry&#x27;</span>: <span class="number">90</span>, <span class="string">&#x27;Math&#x27;</span>: <span class="number">100</span> &#125;</span><br><span class="line"></span><br><span class="line">element = marks.pop(<span class="string">&#x27;Math&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Popped Marks:&#x27;</span>, element)<span class="comment"># Popped Marks: 100</span></span><br></pre></td></tr></table></figure>

<h2 id="heapq"><a href="#heapq" class="headerlink" title="heapq"></a>heapq</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line">minHeap = [(<span class="number">2</span>, (<span class="number">0</span>, <span class="number">0</span>)),(<span class="number">4</span>, (<span class="number">1</span>, <span class="number">0</span>)),(<span class="number">1</span>, (<span class="number">2</span>, <span class="number">4</span>))]</span><br><span class="line">heapq.heapify(minHeap)</span><br><span class="line"><span class="built_in">print</span>(minHeap[<span class="number">0</span>]) <span class="comment"># (1, (2, 4))</span></span><br></pre></td></tr></table></figure>

<p>To remove particular item in heap:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(minHeap) <span class="comment"># [(1, (2, 4)), (4, (1, 0)), (2, (0, 0))]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># replace the target item with the last item</span></span><br><span class="line">minHeap[<span class="number">0</span>] = minHeap[-<span class="number">1</span>]</span><br><span class="line"><span class="comment"># remove the target item from list</span></span><br><span class="line">minHeap.pop()</span><br><span class="line"><span class="comment"># re-heapify the list</span></span><br><span class="line">heapq.heapify(minHeap)</span><br><span class="line"><span class="built_in">print</span>(minHeap) <span class="comment"># [(2, (0, 0)), (4, (1, 0))] </span></span><br></pre></td></tr></table></figure>

<h2 id="sort-with-lambda"><a href="#sort-with-lambda" class="headerlink" title="sort with lambda"></a>sort with lambda</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr = [(<span class="number">1</span>,<span class="number">3</span>),(<span class="number">4</span>,<span class="number">2</span>),(<span class="number">6</span>,<span class="number">4</span>),(<span class="number">3</span>,<span class="number">5</span>)]</span><br><span class="line">arr.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(arr) <span class="comment"># [(3, 5), (6, 4), (1, 3), (4, 2)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Or:</span></span><br><span class="line"><span class="comment"># arr.sort(key=lambda x: -x[1])</span></span><br><span class="line"><span class="comment"># print(arr) # [(3, 5), (6, 4), (1, 3), (4, 2)]</span></span><br></pre></td></tr></table></figure>

<h2 id="list-pop-and-insert"><a href="#list-pop-and-insert" class="headerlink" title="list pop() and insert()"></a>list pop() and insert()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">arr.pop(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(arr) <span class="comment"># [1, 2, 3, 5]</span></span><br><span class="line">arr.insert(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(arr) <span class="comment"># [1, 2, 3, 4, 5]</span></span><br></pre></td></tr></table></figure>

<h2 id="check-if-integer"><a href="#check-if-integer" class="headerlink" title="check if integer"></a>check if integer</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(<span class="number">10</span>) <span class="keyword">is</span> <span class="built_in">int</span>) <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(<span class="number">10.0</span>) <span class="keyword">is</span> <span class="built_in">int</span>) <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(<span class="number">10</span>, <span class="built_in">int</span>)) <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line">n=<span class="number">10.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">isinstance</span>(n, <span class="built_in">float</span>) <span class="keyword">and</span> n.is_integer()) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>

<h2 id="check-if-string-is-a-decimal"><a href="#check-if-string-is-a-decimal" class="headerlink" title="check if string is a decimal"></a>check if string is a decimal</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s=<span class="string">&quot;123&quot;</span></span><br><span class="line">num = <span class="number">0</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt; <span class="built_in">len</span>(s) <span class="keyword">and</span> s[i].isdecimal():</span><br><span class="line">    num = <span class="number">10</span> * num + <span class="built_in">int</span>(s[i])</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(num)</span><br></pre></td></tr></table></figure>

<h2 id="reduce"><a href="#reduce" class="headerlink" title="reduce()"></a>reduce()</h2><p><a href="https://thepythonguru.com/python-builtin-functions/reduce/">https://thepythonguru.com/python-builtin-functions/reduce/</a></p>
<p>The reduce() function accepts a function and a sequence and returns a single value calculated as follows:</p>
<p>Initially, the function is called with the first two items from the sequence and the result is returned.<br>The function is then called again with the result obtained in step 1 and the next value in the sequence. This process keeps repeating until there are items in the sequence.<br>The syntax of the reduce() function is as follows:</p>
<p>Syntax: reduce(function, sequence[, initial]) -&gt; value</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce <span class="comment"># only in Python 3</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">singleNumber</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># hint: xor the same number results in 0</span></span><br><span class="line">        <span class="comment">#res = nums[0]</span></span><br><span class="line">        <span class="comment">#for i in range(1,len(nums)):</span></span><br><span class="line">        <span class="comment">#    res ^= nums[i]</span></span><br><span class="line">        <span class="comment">#return res</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> reduce(<span class="keyword">lambda</span> res, num: res ^ num, nums)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Monotonic stack</title>
    <url>/blog/monotonic-stack/</url>
    <content><![CDATA[<h2 id="What-is-a-Monotonic-Stack"><a href="#What-is-a-Monotonic-Stack" class="headerlink" title="What is a Monotonic Stack"></a>What is a Monotonic Stack</h2><p>A monotonic stack is a stack whose elements are monotonically increasing or decreasing. It contains all properties that a typical stack has and its elements are  monotonic decreasing or increasing.</p>
<p>Monotonic increasing stack: [2  3  5  6  9]<br>Monotonic decreasing stack: [9  6  5  3  2]</p>
<span id="more"></span>

<h2 id="Leetcode-739-Daily-Temperatures"><a href="#Leetcode-739-Daily-Temperatures" class="headerlink" title="[Leetcode 739] Daily Temperatures"></a>[Leetcode 739] Daily Temperatures</h2><p>Given an array of integers temperatures represents the daily temperatures, return an array answer such that answer[i] is the number of days you have to wait after the ith day to get a warmer temperature. If there is no future day for which this is possible, keep answer[i] &#x3D;&#x3D; 0 instead.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: temperatures = [73,74,75,71,69,72,76,73]</span><br><span class="line">Output: [1,1,4,2,1,1,0,0]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; temperatures.length &lt;&#x3D; 105</li>
<li>30 &lt;&#x3D; temperatures[i] &lt;&#x3D; 100</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># e.g. temperatures = [73,74,75,71,69,72,76,73]</span></span><br><span class="line">    <span class="comment"># i  stack     result</span></span><br><span class="line">    <span class="comment"># 0  [0]       [0, 0, 0, 0, 0, 0, 0, 0]</span></span><br><span class="line">    <span class="comment"># 1  [1]       [1, 0, 0, 0, 0, 0, 0, 0]</span></span><br><span class="line">    <span class="comment"># 2  [2]       [1, 1, 0, 0, 0, 0, 0, 0]</span></span><br><span class="line">    <span class="comment"># 3  [2, 3]    [1, 1, 0, 0, 0, 0, 0, 0]</span></span><br><span class="line">    <span class="comment"># 4  [2, 3, 4] [1, 1, 0, 0, 0, 0, 0, 0]</span></span><br><span class="line">    <span class="comment"># 5  [2, 5]    [1, 1, 0, 2, 1, 0, 0, 0]</span></span><br><span class="line">    <span class="comment"># 6  [6]       [1, 1, 4, 2, 1, 1, 0, 0]</span></span><br><span class="line">    <span class="comment"># 7  [6, 7]    [1, 1, 4, 2, 1, 1, 0, 0]</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dailyTemperatures</span>(<span class="params">self, temperatures: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        st = []</span><br><span class="line">        res = [<span class="number">0</span>] * <span class="built_in">len</span>(temperatures)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(temperatures)):</span><br><span class="line">            <span class="keyword">while</span> st <span class="keyword">and</span> temperatures[i] &gt; temperatures[st[-<span class="number">1</span>]]:</span><br><span class="line">                idx = st.pop()</span><br><span class="line">                res[idx] = i - idx</span><br><span class="line"></span><br><span class="line">            st.append(i)</span><br><span class="line">            <span class="comment">#print(st, res)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-901-Online-Stock-Span"><a href="#Leetcode-901-Online-Stock-Span" class="headerlink" title="[Leetcode 901] Online Stock Span"></a>[Leetcode 901] Online Stock Span</h2><p>Design an algorithm that collects daily price quotes for some stock and returns the span of that stock’s price for the current day.</p>
<p>The span of the stock’s price in one day is the maximum number of consecutive days (starting from that day and going backward) for which the stock price was less than or equal to the price of that day.</p>
<ul>
<li>For example, if the prices of the stock in the last four days is [7,2,1,2] and the price of the stock today is 2, then the span of today is 4 because starting from today, the price of the stock was less than or equal 2 for 4 consecutive days.</li>
<li>Also, if the prices of the stock in the last four days is [7,34,1,2] and the price of the stock today is 8, then the span of today is 3 because starting from today, the price of the stock was less than or equal 8 for 3 consecutive days.</li>
</ul>
<p>Implement the StockSpanner class:</p>
<ul>
<li>StockSpanner() Initializes the object of the class.</li>
<li>int next(int price) Returns the span of the stock’s price given that today’s price is price.</li>
</ul>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input</span><br><span class="line">[&quot;StockSpanner&quot;, &quot;next&quot;, &quot;next&quot;, &quot;next&quot;, &quot;next&quot;, &quot;next&quot;, &quot;next&quot;, &quot;next&quot;]</span><br><span class="line">[[], [100], [80], [60], [70], [60], [75], [85]]</span><br><span class="line">Output</span><br><span class="line">[null, 1, 1, 1, 2, 1, 4, 6]</span><br><span class="line"></span><br><span class="line">Explanation</span><br><span class="line">StockSpanner stockSpanner = new StockSpanner();</span><br><span class="line">stockSpanner.next(100); // return 1</span><br><span class="line">stockSpanner.next(80);  // return 1</span><br><span class="line">stockSpanner.next(60);  // return 1</span><br><span class="line">stockSpanner.next(70);  // return 2</span><br><span class="line">stockSpanner.next(60);  // return 1</span><br><span class="line">stockSpanner.next(75);  // return 4, because the last 4 prices (including today&#x27;s price of 75) were less than or equal to today&#x27;s price.</span><br><span class="line">stockSpanner.next(85);  // return 6</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; price &lt;&#x3D; 105</li>
<li>At most 104 calls will be made to next.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">StockSpanner</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.st = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># e.g.</span></span><br><span class="line">    <span class="comment">#                                          (60,1)</span></span><br><span class="line">    <span class="comment">#                         (60,1)  (70,2)   (70,2)   (75,4)</span></span><br><span class="line">    <span class="comment">#                 (80,1)  (80,1)  (80,1)   (80,1)   (80,1)  (85,6)</span></span><br><span class="line">    <span class="comment">#         (100,1) (100,1) (100,1) (100,1)  (100,1)  (100,1) (100,1)</span></span><br><span class="line">    <span class="comment"># input:   100     80      60      70       60       75      85  </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">next</span>(<span class="params">self, price: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        span = <span class="number">1</span> <span class="comment"># span for itself is 1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># accumulate the span with lower(or equal) prices and only save higher price in stack</span></span><br><span class="line">        <span class="keyword">while</span> self.st <span class="keyword">and</span> self.st[-<span class="number">1</span>][<span class="number">0</span>] &lt;= price:</span><br><span class="line">            prevPrice, prevSpan = self.st.pop()</span><br><span class="line">            span += prevSpan</span><br><span class="line"></span><br><span class="line">        self.st.append((price, span))</span><br><span class="line">        <span class="keyword">return</span> span</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
        <tag>Data structure</tag>
        <tag>Stack</tag>
      </tags>
  </entry>
  <entry>
    <title>Bit manipulation</title>
    <url>/blog/bit-manipulation/</url>
    <content><![CDATA[<h2 id="Leetcode-136-Single-Number"><a href="#Leetcode-136-Single-Number" class="headerlink" title="[Leetcode 136] Single Number"></a>[Leetcode 136] Single Number</h2><p>Given a non-empty array of integers nums, every element appears twice except for one. Find that single one.</p>
<p>You must implement a solution with a linear runtime complexity and use only constant extra space.</p>
<span id="more"></span>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [2,2,1]</span><br><span class="line">Output: 1</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [4,1,2,1,2]</span><br><span class="line">Output: 4</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1]</span><br><span class="line">Output: 1</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; nums.length &lt;&#x3D; 3 * 104</li>
<li>-3 * 104 &lt;&#x3D; nums[i] &lt;&#x3D; 3 * 104</li>
<li>Each element in the array appears twice except for one element which appears only once.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">singleNumber</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment"># hint: xor the same number results in 0</span></span><br><span class="line">        <span class="comment">#res = nums[0]</span></span><br><span class="line">        <span class="comment">#for i in range(1,len(nums)):</span></span><br><span class="line">        <span class="comment">#    res ^= nums[i]</span></span><br><span class="line">        <span class="comment">#return res</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> reduce(<span class="keyword">lambda</span> res, num: res ^ num, nums)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">singleNumber1</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        set1 = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> num <span class="keyword">in</span> set1:</span><br><span class="line">                set1.remove(num)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                set1.add(num)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(set1)[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">singleNumber2</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        nums.sort()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums),<span class="number">2</span>):</span><br><span class="line">            <span class="keyword">if</span> nums[i] != nums[i-<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">return</span> nums[i-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> nums[<span class="built_in">len</span>(nums) - <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-190-Reverse-Bits"><a href="#Leetcode-190-Reverse-Bits" class="headerlink" title="[Leetcode 190] Reverse Bits"></a>[Leetcode 190] Reverse Bits</h2><p>Reverse bits of a given 32 bits unsigned integer.</p>
<p>Note:</p>
<ul>
<li>Note that in some languages, such as Java, there is no unsigned integer type. In this case, both input and output will be given as a signed integer type. They should not affect your implementation, as the integer’s internal binary representation is the same, whether it is signed or unsigned.</li>
<li>In Java, the compiler represents the signed integers using 2’s complement notation. Therefore, in Example 2 above, the input represents the signed integer -3 and the output represents the signed integer -1073741825.</li>
</ul>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 00000010100101000001111010011100</span><br><span class="line">Output:    964176192 (00111001011110000010100101000000)</span><br><span class="line">Explanation: The input binary string 00000010100101000001111010011100 represents the unsigned integer 43261596, so return 964176192 which its binary representation is 00111001011110000010100101000000.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 11111111111111111111111111111101</span><br><span class="line">Output:   3221225471 (10111111111111111111111111111111)</span><br><span class="line">Explanation: The input binary string 11111111111111111111111111111101 represents the unsigned integer 4294967293, so return 3221225471 which its binary representation is 10111111111111111111111111111111.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>The input must be a binary string of length 32</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseBits</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">            lastBit = n &amp; <span class="number">1</span></span><br><span class="line">            n &gt;&gt;= <span class="number">1</span></span><br><span class="line">            res &lt;&lt;= <span class="number">1</span></span><br><span class="line">            res |= lastBit</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-338-Counting-Bits"><a href="#Leetcode-338-Counting-Bits" class="headerlink" title="[Leetcode 338] Counting Bits"></a>[Leetcode 338] Counting Bits</h2><p>Given an integer n, return an array ans of length n + 1 such that for each i (0 &lt;&#x3D; i &lt;&#x3D; n), ans[i] is the number of 1’s in the binary representation of i.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 2</span><br><span class="line">Output: [0,1,1]</span><br><span class="line">Explanation:</span><br><span class="line">0 --&gt; 0</span><br><span class="line">1 --&gt; 1</span><br><span class="line">2 --&gt; 10</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 5</span><br><span class="line">Output: [0,1,1,2,1,2]</span><br><span class="line">Explanation:</span><br><span class="line">0 --&gt; 0</span><br><span class="line">1 --&gt; 1</span><br><span class="line">2 --&gt; 10</span><br><span class="line">3 --&gt; 11</span><br><span class="line">4 --&gt; 100</span><br><span class="line">5 --&gt; 101</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>0 &lt;&#x3D; n &lt;&#x3D; 105</li>
</ul>
<p>Follow up:</p>
<ul>
<li>It is very easy to come up with a solution with a runtime of O(n log n). Can you do it in linear time O(n) and possibly in a single pass?<br>Can you do it without using any built-in function (i.e., like __builtin_popcount in C++)?</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countBits</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n+<span class="number">1</span>):</span><br><span class="line">            num = i</span><br><span class="line">            <span class="keyword">if</span> num == <span class="number">0</span>:</span><br><span class="line">                res.append(<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> num:</span><br><span class="line">              count += num &amp; <span class="number">1</span></span><br><span class="line">              num &gt;&gt;= <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            res.append(count)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-1318-Minimum-Flips-to-Make-a-OR-b-Equal-to-c"><a href="#Leetcode-1318-Minimum-Flips-to-Make-a-OR-b-Equal-to-c" class="headerlink" title="[Leetcode 1318] Minimum Flips to Make a OR b Equal to c"></a>[Leetcode 1318] Minimum Flips to Make a OR b Equal to c</h2><p>Given 3 positives numbers a, b and c. Return the minimum flips required in some bits of a and b to make ( a OR b &#x3D;&#x3D; c ). (bitwise OR operation).<br>Flip operation consists of change any single bit 1 to 0 or change the bit 0 to 1 in their binary representation.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: a = 2, b = 6, c = 5</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: After flips a = 1 , b = 4 , c = 5 such that (a OR b == c)</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: a = 4, b = 2, c = 7</span><br><span class="line">Output: 1</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: a = 1, b = 2, c = 3</span><br><span class="line">Output: 0</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; a &lt;&#x3D; 10^9</li>
<li>1 &lt;&#x3D; b &lt;&#x3D; 10^9</li>
<li>1 &lt;&#x3D; c &lt;&#x3D; 10^9</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># e.g.</span></span><br><span class="line">    <span class="comment"># a = 8 (1000), c = 3 (0011), c = 5 (0101)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minFlips</span>(<span class="params">self, a: <span class="built_in">int</span>, b: <span class="built_in">int</span>, c: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> a <span class="keyword">or</span> b <span class="keyword">or</span> c:</span><br><span class="line">            lastBit = c &amp; <span class="number">1</span> </span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> lastBit:</span><br><span class="line">                <span class="comment"># flip a or b to have bit 1</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> (a &amp; <span class="number">1</span> <span class="keyword">or</span> b &amp; <span class="number">1</span>):</span><br><span class="line">                    res += <span class="number">1</span></span><br><span class="line">                   </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># flip a to have bit 0</span></span><br><span class="line">                <span class="keyword">if</span> a &amp; <span class="number">1</span>:</span><br><span class="line">                    res += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># flip b to have bit 0</span></span><br><span class="line">                <span class="keyword">if</span> b &amp; <span class="number">1</span>:</span><br><span class="line">                    res += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            a &gt;&gt;= <span class="number">1</span></span><br><span class="line">            b &gt;&gt;= <span class="number">1</span></span><br><span class="line">            c &gt;&gt;= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
        <tag>Bit manipulation</tag>
      </tags>
  </entry>
  <entry>
    <title>Best time to buy and sell stock</title>
    <url>/blog/best-time-to-buy-and-sell-stock/</url>
    <content><![CDATA[<h2 id="Leetcode-121-Best-Time-to-Buy-and-Sell-Stock"><a href="#Leetcode-121-Best-Time-to-Buy-and-Sell-Stock" class="headerlink" title="[Leetcode 121] Best Time to Buy and Sell Stock"></a>[Leetcode 121] Best Time to Buy and Sell Stock</h2><p>You are given an array prices where prices[i] is the price of a given stock on the ith day.</p>
<p>You want to maximize your profit by choosing a single day to buy one stock and choosing a different day in the future to sell that stock.</p>
<p>Return the maximum profit you can achieve from this transaction. If you cannot achieve any profit, return 0.</p>
<span id="more"></span>

<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: prices = [7,1,5,3,6,4]</span><br><span class="line">Output: 5</span><br><span class="line">Explanation: Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5.</span><br><span class="line">Note that buying on day 2 and selling on day 1 is not allowed because you must buy before you sell.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: prices = [7,6,4,3,1]</span><br><span class="line">Output: 0</span><br><span class="line">Explanation: In this case, no transactions are done and the max profit = 0.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; prices.length &lt;&#x3D; 105</li>
<li>0 &lt;&#x3D; prices[i] &lt;&#x3D; 104</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        profit = <span class="number">0</span></span><br><span class="line">        minPrice = prices[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(prices)):</span><br><span class="line">            <span class="keyword">if</span> prices[i] &lt; minPrice:</span><br><span class="line">                minPrice = prices[i]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                profit = <span class="built_in">max</span>(profit, prices[i] - minPrice)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> profit</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-122-Best-Time-to-Buy-and-Sell-Stock-II"><a href="#Leetcode-122-Best-Time-to-Buy-and-Sell-Stock-II" class="headerlink" title="[Leetcode 122] Best Time to Buy and Sell Stock II"></a>[Leetcode 122] Best Time to Buy and Sell Stock II</h2><p>You are given an integer array prices where prices[i] is the price of a given stock on the ith day.</p>
<p>On each day, you may decide to buy and&#x2F;or sell the stock. You can only hold at most one share of the stock at any time. However, you can buy it then immediately sell it on the same day.</p>
<p>Find and return the maximum profit you can achieve.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: prices = [7,1,5,3,6,4]</span><br><span class="line">Output: 7</span><br><span class="line">Explanation: Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4.</span><br><span class="line">Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3.</span><br><span class="line">Total profit is 4 + 3 = 7.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: prices = [1,2,3,4,5]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: Buy on day 1 (price = 1) and sell on day 5 (price = 5), profit = 5-1 = 4.</span><br><span class="line">Total profit is 4.</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: prices = [7,6,4,3,1]</span><br><span class="line">Output: 0</span><br><span class="line">Explanation: There is no way to make a positive profit, so we never buy the stock to achieve the maximum profit of 0.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; prices.length &lt;&#x3D; 3 * 104</li>
<li>0 &lt;&#x3D; prices[i] &lt;&#x3D; 104</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(prices)</span><br><span class="line">        <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        profit = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> prices[i] &gt; prices[i - <span class="number">1</span>]:</span><br><span class="line">                profit += prices[i] - prices[i - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> profit</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>Dynamic programming</title>
    <url>/blog/dynamic-programming/</url>
    <content><![CDATA[<h2 id="What-is-Dynamic-Programming"><a href="#What-is-Dynamic-Programming" class="headerlink" title="What is Dynamic Programming?"></a>What is Dynamic Programming?</h2><p>Dynamic Programming is mainly an optimization over plain recursion. Wherever we see a recursive solution that has repeated calls for same inputs, we can optimize it using Dynamic Programming. The idea is to simply store the results of subproblems, so that we do not have to re-compute them when needed later. This simple optimization reduces time complexities from exponential to polynomial.<span id="more"></span></p>
<h2 id="Leetcode-198-House-Robber"><a href="#Leetcode-198-House-Robber" class="headerlink" title="[Leetcode 198] House Robber"></a>[Leetcode 198] House Robber</h2><p>You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security systems connected and it will automatically contact the police if two adjacent houses were broken into on the same night.</p>
<p>Given an integer array nums representing the amount of money of each house, return the maximum amount of money you can rob tonight without alerting the police.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [1,2,3,1]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: Rob house 1 (money = 1) and then rob house 3 (money = 3).</span><br><span class="line">Total amount you can rob = 1 + 3 = 4.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [2,7,9,3,1]</span><br><span class="line">Output: 12</span><br><span class="line">Explanation: Rob house 1 (money = 2), rob house 3 (money = 9) and rob house 5 (money = 1).</span><br><span class="line">Total amount you can rob = 2 + 9 + 1 = 12.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; nums.length &lt;&#x3D; 100</li>
<li>0 &lt;&#x3D; nums[i] &lt;&#x3D; 400</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rob</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums) &lt;=<span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">max</span>(nums)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dp[i] is the maximum amount of money to rob till house[i]</span></span><br><span class="line">        dp = [<span class="literal">None</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">        dp[<span class="number">0</span>] = nums[<span class="number">0</span>]</span><br><span class="line">        dp[<span class="number">1</span>] = <span class="built_in">max</span>(nums[<span class="number">0</span>], nums[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">            dp[i] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>], dp[i-<span class="number">2</span>] + nums[i])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-746-Min-Cost-Climbing-Stairs"><a href="#Leetcode-746-Min-Cost-Climbing-Stairs" class="headerlink" title="[Leetcode 746] Min Cost Climbing Stairs"></a>[Leetcode 746] Min Cost Climbing Stairs</h2><p>You are given an integer array cost where cost[i] is the cost of ith step on a staircase. Once you pay the cost, you can either climb one or two steps.</p>
<p>You can either start from the step with index 0, or the step with index 1.</p>
<p>Return the minimum cost to reach the top of the floor.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: cost = [10,15,20]</span><br><span class="line">Output: 15</span><br><span class="line">Explanation: You will start at index 1.</span><br><span class="line">- Pay 15 and climb two steps to reach the top.</span><br><span class="line">The total cost is 15.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: cost = [1,100,1,1,1,100,1,1,100,1]</span><br><span class="line">Output: 6</span><br><span class="line">Explanation: You will start at index 0.</span><br><span class="line">- Pay 1 and climb two steps to reach index 2.</span><br><span class="line">- Pay 1 and climb two steps to reach index 4.</span><br><span class="line">- Pay 1 and climb two steps to reach index 6.</span><br><span class="line">- Pay 1 and climb one step to reach index 7.</span><br><span class="line">- Pay 1 and climb two steps to reach index 9.</span><br><span class="line">- Pay 1 and climb one step to reach the top.</span><br><span class="line">The total cost is 6.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>2 &lt;&#x3D; cost.length &lt;&#x3D; 1000</li>
<li>0 &lt;&#x3D; cost[i] &lt;&#x3D; 999</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># dp[i] = min(dp[i-1] + cost[i-1], dp[i-2] + cost[i-2])</span></span><br><span class="line">    <span class="comment"># e.g. [10, 15, 20]</span></span><br><span class="line">    <span class="comment"># n = 0, dp[0] = 0 (start)</span></span><br><span class="line">    <span class="comment"># n = 1, dp[1] = 0 (start)</span></span><br><span class="line">    <span class="comment"># n = 2, dp[2] = min(0 + 15, 0 + 10) = 10</span></span><br><span class="line">    <span class="comment"># n = 3, dp[3] = min(10 + 20, 15 + 0) = 15</span></span><br><span class="line">    <span class="comment"># Note: the top is with index 3</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minCostClimbingStairs</span>(<span class="params">self, cost: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(cost)</span><br><span class="line">        <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        dp = [<span class="literal">None</span>] * (n + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># no cost needed to reach the 0th or 1th staircaes</span></span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        dp[<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># choose the smaller cost from previous two staircases</span></span><br><span class="line">            dp[i] = <span class="built_in">min</span>(dp[i - <span class="number">1</span>] + cost[i - <span class="number">1</span>], dp[i - <span class="number">2</span>] + cost[i - <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-790-Domino-and-Tromino-Tiling"><a href="#Leetcode-790-Domino-and-Tromino-Tiling" class="headerlink" title="[Leetcode 790] Domino and Tromino Tiling"></a>[Leetcode 790] Domino and Tromino Tiling</h2><p>You have two types of tiles: a 2 x 1 domino shape and a tromino shape. You may rotate these shapes.</p>
<p>Given an integer n, return the number of ways to tile an 2 x n board. Since the answer may be very large, return it modulo 109 + 7.</p>
<p>In a tiling, every square must be covered by a tile. Two tilings are different if and only if there are two 4-directionally adjacent cells on the board such that exactly one of the tilings has both squares occupied by a tile.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 3</span><br><span class="line">Output: 5</span><br><span class="line">Explanation: The five different ways are show above.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 1</span><br><span class="line">Output: 1</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; n &lt;&#x3D; 1000</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numTilings</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        m = <span class="number">1000000007</span></span><br><span class="line">        dp = [<span class="literal">None</span>] * (n + <span class="number">1</span>) <span class="comment"># dp[i] denotes the number of ways to tile an 2*i board</span></span><br><span class="line">        dp[<span class="number">0</span>], dp[<span class="number">1</span>], dp[<span class="number">2</span>] = <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>, n + <span class="number">1</span>):</span><br><span class="line">            dp[i] = (<span class="number">2</span> * dp[i-<span class="number">1</span>] + dp[i - <span class="number">3</span>]) % m</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-1137-N-th-Tribonacci-Number"><a href="#Leetcode-1137-N-th-Tribonacci-Number" class="headerlink" title="[Leetcode 1137] N-th Tribonacci Number"></a>[Leetcode 1137] N-th Tribonacci Number</h2><p>The Tribonacci sequence Tn is defined as follows: </p>
<p>T0 &#x3D; 0, T1 &#x3D; 1, T2 &#x3D; 1, and Tn+3 &#x3D; Tn + Tn+1 + Tn+2 for n &gt;&#x3D; 0.</p>
<p>Given n, return the value of Tn.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 4</span><br><span class="line">Output: 4</span><br><span class="line">Explanation:</span><br><span class="line">T_3 = 0 + 1 + 1 = 2</span><br><span class="line">T_4 = 1 + 1 + 2 = 4</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: n = 25</span><br><span class="line">Output: 1389537</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>0 &lt;&#x3D; n &lt;&#x3D; 37</li>
<li>The answer is guaranteed to fit within a 32-bit integer, ie. answer &lt;&#x3D; 2^31 - 1.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tribonacci</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        a, b, c, = <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span></span><br><span class="line">        d = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> a</span><br><span class="line">        <span class="keyword">elif</span> n == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> b</span><br><span class="line">        <span class="keyword">elif</span> n == <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> c</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">2</span>):</span><br><span class="line">                d = a + b + c</span><br><span class="line">                a = b</span><br><span class="line">                b = c</span><br><span class="line">                c = d</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Dynamic programming</tag>
        <tag>Recursive</tag>
      </tags>
  </entry>
  <entry>
    <title>The importance of hope</title>
    <url>/blog/the-importance-of-hope/</url>
    <content><![CDATA[<p>People who only hope for a better future get nowhere in life. Nothing will  change unless someone acts. Those who act in the hour of darkness are the ones who  are true heroes. Those who hope are the ones who benefit from these actions.<span id="more"></span>  </p>
<p>There are many ways in which to look at some situations. Hope is one of these  situations. Some could say that hope is an essential part of human nature. It is what  motivates us and keeps us going through the most difficult times. When your will is  broken, you will give up and think all is lost. While this is true, it doesn’t mean that  you can’t change your fate. Action is what describes a person. If they act to improve  their lives, then they are rational. If they are too scared to act, well, that’s their own  choice. Hope and action are very important to live a happy and successful lifestyle,  and with hope, you can achieve even your wildest dreams.  </p>
<p>Hope and action combined make people unstoppable. This was shown again  and again during the COVID-19 pandemic. Millions of people have been infected,  and hundreds of thousands have lost their lives. But hope and action kept those alive  motivated. People came together to support each other. Doctors and nurses risked  their lives to treat those infected, and some died trying to save others’ lives. Others  volunteered to develop food and necessary supplies for those in need. Either way,  people have been constantly stepping up and helping those in need. </p>
<p>It is inspiring to see how all this kindness, compassion, generosity, and  bravery all concludes in one statement: There is hope for a better future. But hope is  not the only thing required for a better future. We need action. Science, technology,  and policy are all required to defeat the virus. Once again, humans have shown  success in these areas as well. </p>
<p>Medical researchers, who started from nothing, have come to understand this  virus in great depths. Multiple vaccines have been developed to save people from  this disease. Millions of people have already received them, which is why many  more people survived this disease now than back in 2020.  </p>
<p>Technology has also helped us adapt to the impact of the pandemic. People  had to do social distancing, and therefore schools were shut down. Online learning,  remote work, and virtual communication have become the norm, allowing us to stay  productive and keep in touch with friends and family. </p>
<p>At the same time, government and organizations have passed laws and made  policies to make sure that people are doing social distancing and staying safe. They  also helped people in poverty and bad financial or health situations by making  donations. They made business loans and strived to help those in unemployment,<br>trying to find a job for everyone. These acts of kindness greatly benefited the people  and gave them relief during times of stress.  </p>
<p>Even though there are still challenges, and new challenges will arise in the  future, people will always find a solution to it as long as the spirit of hope channels  through them. They need to be vigilant and productive to maintain a comfortable  lifestyle and be safe. In the face of a global crisis, people shall unite and fight it.  Sometime in the future, they will find a cure to COVID-19, just as they will find a  solution to any problem. We can find hope in our great advances of science,  technology, and policy, which help us battle the impact of the pandemic. Lastly, we  can find hope in common knowledge that the fate of the world is in our hands, and  even the smallest changes can result in a big impact.  </p>
<p>Hope is the key to inner peace. It is a powerful and motivating force that  channels through everyone. We just must bring out that hope, and through our hard  work and actions, we can end the pandemic. Hope is what united us. Hope is what  has ignited the blazing fire in every soul. Hope is what motivated us to make new  technology to help us progress. Hope gives us comfortable warmth. It gives us  confidence that there will be a better future where everyone is happy and successful.  As we fight our way through this global pandemic, everyone shall never lose hope  and work hard and rationally towards a brighter future.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode algorithm questions</title>
    <url>/blog/algorithm-questions/</url>
    <content><![CDATA[<h2 id="Moore-Voting-Algorithm"><a href="#Moore-Voting-Algorithm" class="headerlink" title="Moore Voting Algorithm"></a>Moore Voting Algorithm</h2><h3 id="Leetcode-169-Majority-Element"><a href="#Leetcode-169-Majority-Element" class="headerlink" title="[Leetcode 169] Majority Element"></a>[Leetcode 169] Majority Element</h3><p>Given an array nums of size n, return the majority element.</p>
<p>The majority element is the element that appears more than ⌊n &#x2F; 2⌋ times. You may assume that the majority element always exists in the array.</p>
<span id="more"></span>

<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [3,2,3]</span><br><span class="line">Output: 3</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [2,2,1,1,1,2,2]</span><br><span class="line">Output: 2</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>n &#x3D;&#x3D; nums.length</li>
<li>1 &lt;&#x3D; n &lt;&#x3D; 5 * 104</li>
<li>-109 &lt;&#x3D; nums[i] &lt;&#x3D; 109</li>
</ul>
<p>Follow-up: Could you solve the problem in linear time and in O(1) space?</p>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># Moore Voting Algorithm</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">majorityElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        candidate = <span class="number">0</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> count == <span class="number">0</span>:</span><br><span class="line">                candidate = num</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> num == candidate:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                count -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> candidate</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">majorityElement_sort</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        nums.sort()</span><br><span class="line">        <span class="keyword">return</span> nums[<span class="built_in">len</span>(nums)//<span class="number">2</span>] </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">majorityElement_hashmap</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        m = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            m[num] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> m:</span><br><span class="line">            <span class="keyword">if</span> m[k] &gt; <span class="built_in">len</span>(nums) // <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">return</span> k</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Getting started with vcluster</title>
    <url>/blog/getting-started-with-vcluster/</url>
    <content><![CDATA[<h2 id="What-are-Virtual-Kubernetes-Clusters-VCluster"><a href="#What-are-Virtual-Kubernetes-Clusters-VCluster" class="headerlink" title="What are Virtual Kubernetes Clusters(VCluster)?"></a>What are Virtual Kubernetes Clusters(VCluster)?</h2><p>Virtual clusters are fully working Kubernetes clusters that run on top of other Kubernetes clusters. Compared to fully separate “real” clusters, virtual clusters reuse worker nodes and networking of the host cluster. They have their own control plane and schedule all workloads into a single namespace of the host cluster. Like virtual machines, virtual clusters partition a single physical cluster into multiple separate ones.</p>
<p><img src="/images/vcluster-architecture.svg"></p>
<span id="more"></span>

<p>For more about the vcluster, please refer to its official website. The goal for this post is to learn how to deploy vcluster in an existing kubernetes cluster.</p>
<h2 id="Install-vCluster-CLI"><a href="#Install-vCluster-CLI" class="headerlink" title="Install vCluster CLI"></a>Install vCluster CLI</h2><p>Requirements:</p>
<ul>
<li>kubectl (check via kubectl version)</li>
<li>helm v3 (check with helm version)</li>
<li>a working kube-context with access to a Kubernetes cluster (check with kubectl get namespaces)</li>
</ul>
<p>To install kubectl, refer to this <a href="https://www.flamingbytes.com/blog/using-kubeconfig-to-configure-access-to-remote-kubernetes-cluster/#Install-kubectl-binary">post</a>.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl version</span><br><span class="line">Kustomize Version: v4.5.7</span><br></pre></td></tr></table></figure>

<p>To install Helm(Helm is the package manager for Kubernetes):</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">chmod</span> 700 get_helm.sh</span><br><span class="line"></span><br><span class="line">$ ./get_helm.sh</span><br><span class="line">Downloading https://get.helm.sh/helm-v3.13.1-linux-amd64.tar.gz</span><br><span class="line">Verifying checksum... Done.</span><br><span class="line">Preparing to install helm into /usr/local/bin</span><br><span class="line">helm installed into /usr/local/bin/helm</span><br><span class="line"></span><br><span class="line">$  helm version</span><br><span class="line">version.BuildInfo&#123;Version:<span class="string">&quot;v3.13.1&quot;</span> ...</span><br></pre></td></tr></table></figure>

<p>To install vcluster CLI:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ curl -L -o vcluster <span class="string">&quot;https://github.com/loft-sh/vcluster/releases/latest/download/vcluster-linux-amd64&quot;</span> &amp;&amp; sudo install -c -m 0755 vcluster /usr/local/bin &amp;&amp; <span class="built_in">rm</span> -f vcluster</span><br><span class="line"></span><br><span class="line">$ vcluster --version</span><br><span class="line">vcluster version 0.16.4</span><br></pre></td></tr></table></figure>

<h2 id="Method-1-Deploy-vcluster-with-kubectl"><a href="#Method-1-Deploy-vcluster-with-kubectl" class="headerlink" title="Method 1 - Deploy vcluster with kubectl"></a>Method 1 - Deploy vcluster with kubectl</h2><p>Check the kubernetes cluster nodes:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">export</span> KUBECONFIG=./kubeconfig</span><br><span class="line">$ kubectl get nodes -o wide</span><br><span class="line">NAME          STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">node11   Ready    &lt;none&gt;          39h   v1.27.2   10.10.0.11    &lt;none&gt;        CentOS Linux 7 (Core)   5.7.12-1.el7.elrepo.x86_64   docker://23.0.6</span><br><span class="line">node12   Ready    control-plane   39h   v1.27.2   10.10.0.12    &lt;none&gt;        CentOS Linux 7 (Core)   5.7.12-1.el7.elrepo.x86_64   docker://23.0.6</span><br><span class="line">node13   Ready    &lt;none&gt;          39h   v1.27.2   10.10.0.13    &lt;none&gt;        CentOS Linux 7 (Core)   5.7.12-1.el7.elrepo.x86_64   docker://24.0.6</span><br><span class="line">node14   Ready    &lt;none&gt;          39h   v1.27.2   10.10.0.14    &lt;none&gt;        CentOS Linux 7 (Core)   5.7.12-1.el7.elrepo.x86_64   docker://24.0.6</span><br></pre></td></tr></table></figure>

<p>Create namespace for vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl create namespace vcluster-my-vcluster</span><br><span class="line">$ kubectl get ns</span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">vcluster-my-vcluster   Active   7s</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>

<p>Make a storageclass default for vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl patch storageclass px-db -p <span class="string">&#x27;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&#x27;</span></span><br><span class="line">$ kubectl get sc</span><br><span class="line">NAME                                 PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">px-db (default)                      kubernetes.io/portworx-volume   Delete          Immediate           <span class="literal">true</span>                   29h</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>

<p>Deploy vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ helm template my-vcluster vcluster --repo https://charts.loft.sh -n vcluster-my-vcluster | kubectl apply -f -</span><br><span class="line">serviceaccount/vc-my-vcluster created</span><br><span class="line">serviceaccount/vc-workload-my-vcluster created</span><br><span class="line">configmap/my-vcluster-coredns created</span><br><span class="line">configmap/my-vcluster-init-manifests created</span><br><span class="line">role.rbac.authorization.k8s.io/my-vcluster created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/my-vcluster created</span><br><span class="line">service/my-vcluster created</span><br><span class="line">service/my-vcluster-headless created</span><br><span class="line">statefulset.apps/my-vcluster created</span><br></pre></td></tr></table></figure>

<p>Verify vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                             CLUSTER         AUTHINFO           NAMESPACE</span><br><span class="line">*         kubernetes-admin@cluster.local   cluster.local   kubernetes-admin</span><br><span class="line"></span><br><span class="line">$ vcluster list</span><br><span class="line"></span><br><span class="line">       NAME     |      NAMESPACE       | STATUS  | VERSION | CONNECTED |            CREATED            |  AGE   | PRO</span><br><span class="line">  --------------+----------------------+---------+---------+-----------+-------------------------------+--------+------</span><br><span class="line">    my-vcluster | vcluster-my-vcluster | Running | 0.16.4  |           | 2023-10-31 21:40:04 +0000 UTC | 52m30s |</span><br><span class="line"></span><br><span class="line">$ kubectl get statefulset -n vcluster-my-vcluster</span><br><span class="line">NAME          READY   AGE</span><br><span class="line">my-vcluster   1/1     52m</span><br><span class="line"></span><br><span class="line">$ kubectl get pod -n vcluster-my-vcluster</span><br><span class="line">NAME                                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-68bdd584b4-79rpj-x-kube-system-x-my-vcluster   1/1     Running   0          27m</span><br><span class="line">my-vcluster-0                                          2/2     Running   0          52m</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc -n vcluster-my-vcluster</span><br><span class="line">NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">data-my-vcluster-0   Bound    pvc-97bddb5c-47ee-4b62-916a-240c96e84676   5Gi        RWO            px-db          52m</span><br><span class="line"></span><br><span class="line">$ kubectl get ns</span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">vcluster-my-vcluster   Active   54m</span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line">$ kubectl get sc</span><br><span class="line">NAME                                 PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">px-db (default)                      kubernetes.io/portworx-volume   Delete          Immediate           <span class="literal">true</span>                   30h</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>

<p>Connect to vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vcluster connect my-vcluster -n vcluster-my-vcluster</span><br><span class="line">22:08:33 <span class="keyword">done</span> Switched active kube context to vcluster_my-vcluster_vcluster-my-vcluster_kubernetes-admin@cluster.local</span><br><span class="line">22:08:33 warn Since you are using port-forwarding to connect, you will need to leave this terminal open</span><br><span class="line">- Use CTRL+C to <span class="built_in">return</span> to your previous kube context</span><br><span class="line">- Use `kubectl get namespaces` <span class="keyword">in</span> another terminal to access the vcluster</span><br><span class="line">Forwarding from 127.0.0.1:11459 -&gt; 8443</span><br><span class="line">Forwarding from [::1]:11459 -&gt; 8443</span><br><span class="line">Handling connection <span class="keyword">for</span> 11459</span><br></pre></td></tr></table></figure>

<p>Open a new terminal and verify the vcluster nodes:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get nodes -o wide</span><br><span class="line">NAME          STATUS   ROLES    AGE     VERSION        INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION      CONTAINER-RUNTIME</span><br><span class="line">node11  Ready    &lt;none&gt;   4m43s   v1.28.2+k3s1   10.10.62.211   &lt;none&gt;        Fake Kubernetes Image   4.19.76-fakelinux   docker://19.3.12</span><br></pre></td></tr></table></figure>

<p>Note: By default, there is only one node available for vcluster. </p>
<p>Disconnect from vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vcluster disconnect</span><br><span class="line">22:37:06 info Successfully disconnected from vcluster: my-vcluster and switched back to the original context: kubernetes-admin@cluster.local</span><br></pre></td></tr></table></figure>

<p>Open a new bash with the vcluster KUBECONFIG defined:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vcluster connect my-vcluster -n vcluster-my-vcluster -- bash</span><br><span class="line">$ kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                                                                       CLUSTER                                                                    AUTHINFO                                                                   NAMESPACE</span><br><span class="line">*         vcluster_my-vcluster_vcluster-my-vcluster_kubernetes-admin@cluster.local   vcluster_my-vcluster_vcluster-my-vcluster_kubernetes-admin@cluster.local   vcluster_my-vcluster_vcluster-my-vcluster_kubernetes-admin@cluster.local</span><br><span class="line"></span><br><span class="line">$  kubectl get ns</span><br><span class="line">NAME              STATUS   AGE</span><br><span class="line">kube-system       Active   45m</span><br><span class="line">kube-public       Active   45m</span><br><span class="line">kube-node-lease   Active   45m</span><br><span class="line">default           Active   45m</span><br><span class="line"></span><br><span class="line">$ vcluster disconnect</span><br><span class="line">22:51:03 info Successfully disconnected from vcluster: my-vcluster and switched back to the original context: kubernetes-admin@cluster.local</span><br></pre></td></tr></table></figure>

<p>Delete vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$  vcluster list</span><br><span class="line"></span><br><span class="line">       NAME     |      NAMESPACE       | STATUS  | VERSION | CONNECTED |            CREATED            |   AGE    | PRO</span><br><span class="line">  --------------+----------------------+---------+---------+-----------+-------------------------------+----------+------</span><br><span class="line">    my-vcluster | vcluster-my-vcluster | Running | 0.16.4  |           | 2023-10-31 21:40:04 +0000 UTC | 1h12m38s |</span><br><span class="line"></span><br><span class="line">$ kubectl delete namespace vcluster-my-vcluster</span><br><span class="line"></span><br><span class="line">$  vcluster list</span><br><span class="line"></span><br><span class="line">    NAME | NAMESPACE | STATUS | VERSION | CONNECTED | CREATED | AGE | PRO</span><br><span class="line">  -------+-----------+--------+---------+-----------+---------+-----+------</span><br></pre></td></tr></table></figure>

<h2 id="Method-2-Deploy-vcluster-with-vcluster-CLI"><a href="#Method-2-Deploy-vcluster-with-vcluster-CLI" class="headerlink" title="Method 2 - Deploy vcluster with vcluster CLI"></a>Method 2 - Deploy vcluster with vcluster CLI</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> cluster_values.yaml</span><br><span class="line"><span class="built_in">sync</span>:</span><br><span class="line">  services:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  configmaps:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  secrets:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  endpoints:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  pods:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">    ephemeralContainers: <span class="literal">false</span></span><br><span class="line">    status: <span class="literal">false</span></span><br><span class="line">  events:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  persistentvolumeclaims:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  ingresses:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  fake-nodes:</span><br><span class="line">    enabled: <span class="literal">true</span> <span class="comment"># will be ignored if nodes.enabled = true</span></span><br><span class="line">  fake-persistentvolumes:</span><br><span class="line">    enabled: <span class="literal">true</span> <span class="comment"># will be ignored if persistentvolumes.enabled = true</span></span><br><span class="line">  nodes:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">    <span class="comment"># If nodes sync is enabled, and syncAllNodes = true, the virtual cluster</span></span><br><span class="line">    <span class="comment"># will sync all nodes instead of only the ones where some pods are running.</span></span><br><span class="line">    syncAllNodes: <span class="literal">true</span></span><br><span class="line">    <span class="comment"># nodeSelector is used to limit which nodes get synced to the vcluster,</span></span><br><span class="line">    <span class="comment"># and which nodes are used to run vcluster pods.</span></span><br><span class="line">    <span class="comment"># A valid string representation of a label selector must be used.</span></span><br><span class="line">    nodeSelector: <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="comment"># syncNodeChanges allows vcluster user edits of the nodes to be synced down to the host nodes.</span></span><br><span class="line">    <span class="comment"># Write permissions on node resource will be given to the vcluster.</span></span><br><span class="line">    syncNodeChanges: <span class="literal">false</span></span><br><span class="line">  persistentvolumes:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  storageclasses:</span><br><span class="line">    enabled: <span class="literal">false</span></span><br><span class="line">  legacy-storageclasses:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  priorityclasses:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  networkpolicies:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  volumesnapshots:</span><br><span class="line">    enabled: <span class="literal">false</span></span><br><span class="line">  poddisruptionbudgets:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  serviceaccounts:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Scale up etcd</span></span><br><span class="line">etcd:</span><br><span class="line">  replicas: 2</span><br><span class="line">  fsGroup: 12345</span><br><span class="line">  securityContext:</span><br><span class="line">    allowPrivilegeEscalation: <span class="literal">false</span></span><br><span class="line">    capabilities:</span><br><span class="line">      drop:</span><br><span class="line">      - ALL</span><br><span class="line">    readOnlyRootFilesystem: <span class="literal">true</span></span><br><span class="line">    runAsGroup: 12345</span><br><span class="line">    runAsNonRoot: <span class="literal">true</span></span><br><span class="line">    runAsUser: 12345</span><br><span class="line">    seccompProfile:</span><br><span class="line">      <span class="built_in">type</span>: RuntimeDefault</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Scale up controller manager</span></span><br><span class="line">controller:</span><br><span class="line">  replicas: 2</span><br><span class="line">  securityContext:</span><br><span class="line">    allowPrivilegeEscalation: <span class="literal">false</span></span><br><span class="line">    capabilities:</span><br><span class="line">      drop:</span><br><span class="line">      - ALL</span><br><span class="line">    readOnlyRootFilesystem: <span class="literal">true</span></span><br><span class="line">    runAsGroup: 12345</span><br><span class="line">    runAsNonRoot: <span class="literal">true</span></span><br><span class="line">    runAsUser: 12345</span><br><span class="line">    seccompProfile:</span><br><span class="line">      <span class="built_in">type</span>: RuntimeDefault</span><br><span class="line"></span><br><span class="line"><span class="comment"># Scale up api server</span></span><br><span class="line">api:</span><br><span class="line">  replicas: 2</span><br><span class="line">  securityContext:</span><br><span class="line">    allowPrivilegeEscalation: <span class="literal">false</span></span><br><span class="line">    capabilities:</span><br><span class="line">      drop:</span><br><span class="line">      - ALL</span><br><span class="line">    readOnlyRootFilesystem: <span class="literal">true</span></span><br><span class="line">    runAsGroup: 12345</span><br><span class="line">    runAsNonRoot: <span class="literal">true</span></span><br><span class="line">    runAsUser: 12345</span><br><span class="line">    seccompProfile:</span><br><span class="line">      <span class="built_in">type</span>: RuntimeDefault</span><br><span class="line"></span><br><span class="line"><span class="comment"># Scale up DNS server</span></span><br><span class="line">coredns:</span><br><span class="line">  replicas: 2</span><br><span class="line">  securityContext:</span><br><span class="line">    allowPrivilegeEscalation: <span class="literal">false</span></span><br><span class="line">    capabilities:</span><br><span class="line">      drop:</span><br><span class="line">      - ALL</span><br><span class="line">    readOnlyRootFilesystem: <span class="literal">true</span></span><br><span class="line">    runAsGroup: 12345</span><br><span class="line">    runAsNonRoot: <span class="literal">true</span></span><br><span class="line">    runAsUser: 12345</span><br><span class="line">    seccompProfile:</span><br><span class="line">      <span class="built_in">type</span>: RuntimeDefault</span><br><span class="line"></span><br><span class="line">ingress:</span><br><span class="line">  <span class="comment"># Enable ingress record generation</span></span><br><span class="line">  enabled: <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Ingress path type</span></span><br><span class="line">  pathType: ImplementationSpecific</span><br><span class="line">  apiVersion: networking.k8s.io/v1</span><br><span class="line">  ingressClassName: <span class="string">&quot;nginx&quot;</span></span><br><span class="line">  annotations:</span><br><span class="line">    nginx.ingress.kubernetes.io/backend-protocol: HTTPS</span><br><span class="line">    nginx.ingress.kubernetes.io/ssl-passthrough: <span class="string">&quot;true&quot;</span></span><br><span class="line">    nginx.ingress.kubernetes.io/ssl-redirect: <span class="string">&quot;true&quot;</span></span><br><span class="line"></span><br><span class="line">init:</span><br><span class="line">  helm:</span><br><span class="line">    <span class="comment"># public chart</span></span><br><span class="line">    - chart:</span><br><span class="line">        name: metrics-server</span><br><span class="line">        repo: https://kubernetes-sigs.github.io/metrics-server/</span><br><span class="line">        version: 3.9.0</span><br><span class="line">      <span class="comment"># optional field</span></span><br><span class="line">      values: |-</span><br><span class="line">        replicas: 2</span><br><span class="line">        defaultArgs:</span><br><span class="line">          - --cert-dir=/tmp</span><br><span class="line">          - --kubelet-use-node-status-port</span><br><span class="line">          - --metric-resolution=15s</span><br><span class="line">        args:</span><br><span class="line">          - /metrics-server</span><br><span class="line">          - --kubelet-insecure-tls=<span class="literal">true</span></span><br><span class="line">          - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP</span><br><span class="line">      release:</span><br><span class="line">        name: metrics-server</span><br><span class="line">        namespace: kube-system</span><br><span class="line"></span><br><span class="line">  multiNamespaceMode:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">$ vcluster create my-vcluster -f vcluster_values.yaml</span><br><span class="line">23:00:34 info Creating namespace vcluster-my-vcluster</span><br><span class="line">23:00:34 info Create vcluster my-vcluster...</span><br><span class="line">23:00:34 info execute <span class="built_in">command</span>: helm upgrade my-vcluster /tmp/vcluster-0.16.4.tgz-1464010202 --kubeconfig /tmp/2856381900 --namespace vcluster-my-vcluster --install --repository-config=<span class="string">&#x27;&#x27;</span> --values /tmp/901005439 --values vcluster_values.yaml</span><br><span class="line">23:00:35 <span class="keyword">done</span> Successfully created virtual cluster my-vcluster <span class="keyword">in</span> namespace vcluster-my-vcluster</span><br><span class="line">23:00:35 info Waiting <span class="keyword">for</span> vcluster to come up...</span><br><span class="line">23:00:51 warn vcluster is waiting, because vcluster pod my-vcluster-0 has status: ContainerCreating</span><br><span class="line">23:01:02 <span class="keyword">done</span> Switched active kube context to vcluster_my-vcluster_vcluster-my-vcluster_kubernetes-admin@cluster.local</span><br><span class="line">23:01:02 warn Since you are using port-forwarding to connect, you will need to leave this terminal open</span><br><span class="line">- Use CTRL+C to <span class="built_in">return</span> to your previous kube context</span><br><span class="line">- Use `kubectl get namespaces` <span class="keyword">in</span> another terminal to access the vcluster</span><br><span class="line">Forwarding from 127.0.0.1:11704 -&gt; 8443</span><br><span class="line">Forwarding from [::1]:11704 -&gt; 8443</span><br></pre></td></tr></table></figure>

<p>Use CTRL+C to switch back to original context:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                             CLUSTER         AUTHINFO           NAMESPACE</span><br><span class="line">*         kubernetes-admin@cluster.local   cluster.local   kubernetes-admin</span><br></pre></td></tr></table></figure>

<p>Check the vcluster nodes:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vcluster connect my-vcluster -n vcluster-my-vcluster -- bash</span><br><span class="line">$ kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                                                                       CLUSTER                                                                    AUTHINFO                                                                   NAMESPACE</span><br><span class="line">*         vcluster_my-vcluster_vcluster-my-vcluster_kubernetes-admin@cluster.local   vcluster_my-vcluster_vcluster-my-vcluster_kubernetes-admin@cluster.local   vcluster_my-vcluster_vcluster-my-vcluster_kubernetes-admin@cluster.local</span><br><span class="line"></span><br><span class="line">$ kubectl get nodes -o wide</span><br><span class="line">NAME          STATUS   ROLES           AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">node12   Ready    control-plane   9m59s   v1.27.2   10.10.50.155   &lt;none&gt;        CentOS Linux 7 (Core)   5.7.12-1.el7.elrepo.x86_64   docker://23.0.6</span><br><span class="line">node13   Ready    &lt;none&gt;          9m59s   v1.27.2   10.10.9.202    &lt;none&gt;        CentOS Linux 7 (Core)   5.7.12-1.el7.elrepo.x86_64   docker://24.0.6</span><br><span class="line">node14   Ready    &lt;none&gt;          9m59s   v1.27.2   10.10.7.186    &lt;none&gt;        CentOS Linux 7 (Core)   5.7.12-1.el7.elrepo.x86_64   docker://24.0.6</span><br><span class="line">node11   Ready    &lt;none&gt;          9m59s   v1.27.2   10.10.53.174   &lt;none&gt;        CentOS Linux 7 (Core)   5.7.12-1.el7.elrepo.x86_64   docker://23.0.6</span><br></pre></td></tr></table></figure>

<p>Check the node labels:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get nodes -o wide --show-labels</span><br><span class="line">NAME          STATUS   ROLES           AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME   LABELS</span><br><span class="line">node14   Ready    &lt;none&gt;          14m   v1.27.2   10.10.7.186    &lt;none&gt;        CentOS Linux 7 (Core)   5.7.12-1.el7.elrepo.x86_64   docker://24.0.6     beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/fio=<span class="literal">true</span>,kubernetes.io/hostname=node14,kubernetes.io/os=linux</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>

<p>Note: The node label which was created in host cluster is also available in vcluster. In this example, the label is “fio&#x3D;true”</p>
<p>Disconnect from the vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vcluster disconnect</span><br><span class="line">$ <span class="built_in">exit</span></span><br><span class="line">$ kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                             CLUSTER         AUTHINFO           NAMESPACE</span><br><span class="line">*         kubernetes-admin@cluster.local   cluster.local   kubernetes-admin</span><br><span class="line"></span><br><span class="line">$ vcluster list</span><br><span class="line"></span><br><span class="line">       NAME     |      NAMESPACE       | STATUS  | VERSION | CONNECTED |            CREATED            | AGE  | PRO</span><br><span class="line">  --------------+----------------------+---------+---------+-----------+-------------------------------+------+------</span><br><span class="line">    my-vcluster | vcluster-my-vcluster | Running | 0.16.4  |           | 2023-10-31 23:00:35 +0000 UTC | 6m6s |</span><br></pre></td></tr></table></figure>

<p>Create storageclass in host cluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> sc.yaml</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: fio-sc</span><br><span class="line">provisioner: pxd.portworx.com</span><br><span class="line">parameters:</span><br><span class="line">  repl: <span class="string">&quot;1&quot;</span></span><br><span class="line">  sharedv4: <span class="string">&quot;true&quot;</span></span><br><span class="line">  nodes: <span class="string">&quot;20a547ae-b610-4939-94a8-3a87c87ee9b6&quot;</span></span><br><span class="line">reclaimPolicy: Retain</span><br><span class="line">allowVolumeExpansion: <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">$ kubectl apply -f sc.yaml</span><br></pre></td></tr></table></figure>

<p>Verify the storageclass in vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vcluster connect my-vcluster -n vcluster-my-vcluster -- bash</span><br><span class="line"></span><br><span class="line">$ kubectl get sc | grep fio-sc</span><br><span class="line">fio-sc    pxd.portworx.com                Retain          Immediate           <span class="literal">true</span>                   44s</span><br></pre></td></tr></table></figure>

<p>Note: The storageclass created in host cluster is also available in vcluster.</p>
<p>Create namespace in vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl create ns app-fio</span><br><span class="line"></span><br><span class="line">$ kubectl get ns</span><br><span class="line">NAME              STATUS   AGE</span><br><span class="line">default           Active   22m</span><br><span class="line">kube-system       Active   22m</span><br><span class="line">kube-public       Active   22m</span><br><span class="line">kube-node-lease   Active   22m</span><br><span class="line">app-fio           Active   4s</span><br></pre></td></tr></table></figure>

<p>Verify namespace in host cluster context:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                             CLUSTER         AUTHINFO           NAMESPACE</span><br><span class="line">*         kubernetes-admin@cluster.local   cluster.local   kubernetes-admin</span><br><span class="line"></span><br><span class="line">$ kubectl get ns</span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">default                Active   41h</span><br><span class="line">kube-node-lease        Active   41h</span><br><span class="line">kube-public            Active   41h</span><br><span class="line">kube-system            Active   41h</span><br><span class="line">portworx               Active   31h</span><br><span class="line">vcluster-my-vcluster   Active   24m</span><br></pre></td></tr></table></figure>

<p>Note: The created namespace in vcluster does not appear in the host cluster context.</p>
<p>Create fio pod in vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> fio-pod.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: fio-pvc1</span><br><span class="line">  namespace: app-fio</span><br><span class="line">  annotations:</span><br><span class="line">    volume.beta.kubernetes.io/storage-class: fio-sc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 500Gi</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: fio-pod1</span><br><span class="line">  namespace: app-fio</span><br><span class="line">spec:</span><br><span class="line">  affinity:</span><br><span class="line">    nodeAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        nodeSelectorTerms:</span><br><span class="line">        - matchExpressions:</span><br><span class="line">          - key: kubernetes.io/fio</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - <span class="string">&quot;true&quot;</span></span><br><span class="line">  containers:</span><br><span class="line">  - name: fio-pod-container1</span><br><span class="line">    image: ljishen/fio:3.6</span><br><span class="line">    <span class="built_in">command</span>: [ <span class="string">&quot;sleep&quot;</span> ]</span><br><span class="line">    args: [ <span class="string">&quot;30d&quot;</span> ]</span><br><span class="line">    volumeMounts:</span><br><span class="line">      - name: fio-vol1</span><br><span class="line">        mountPath: /mnt/data1</span><br><span class="line">  volumes:</span><br><span class="line">    - name: fio-vol1</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: fio-pvc1</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f fio-pod.yaml</span><br></pre></td></tr></table></figure>

<p>Verfiy POD and PVCs in vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pods -n app-fio -o wide</span><br><span class="line">NAME       READY   STATUS    RESTARTS   AGE     IP               NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">fio-pod1   1/1     Running   0          9m29s   10.10.126.209   node14   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc -n app-fio</span><br><span class="line">NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE</span><br><span class="line">fio-pvc1   Bound    pvc-9997f6a0-b584-4b1b-b520-4ebe1afefffe   500Gi      RWX            fio-sc   4m2s</span><br></pre></td></tr></table></figure>

<p>Connect to the POD in vcluster:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl <span class="built_in">exec</span> -it fio-pod1 -n app-fio -- sh</span><br><span class="line">/ <span class="comment"># hostname</span></span><br><span class="line">fio-pod1</span><br></pre></td></tr></table></figure>

<p>Verify POD and PVCs in the host cluster context:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                             CLUSTER         AUTHINFO           NAMESPACE</span><br><span class="line">*         kubernetes-admin@cluster.local   cluster.local   kubernetes-admin</span><br><span class="line"></span><br><span class="line">$ kubectl get ns</span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">default                Active   41h</span><br><span class="line">kube-node-lease        Active   41h</span><br><span class="line">kube-public            Active   41h</span><br><span class="line">kube-system            Active   41h</span><br><span class="line">portworx               Active   31h</span><br><span class="line">vcluster-my-vcluster   Active   51m</span><br><span class="line"></span><br><span class="line">$ kubectl get pod</span><br><span class="line">No resources found <span class="keyword">in</span> default namespace.</span><br><span class="line">$ kubectl get pvc</span><br><span class="line">No resources found <span class="keyword">in</span> default namespace.</span><br><span class="line"></span><br><span class="line">$ kubectl get pod -n app-fio</span><br><span class="line">No resources found <span class="keyword">in</span> app-fio namespace.</span><br><span class="line">$ kubectl get pvc -n app-fio</span><br><span class="line">No resources found <span class="keyword">in</span> app-fio namespace</span><br></pre></td></tr></table></figure>

<p>Note: The created namespace <em>app-fio</em> doesn’t appear in the host cluster context. Thus, the related POD and PVCs doesn’t appear either.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.vcluster.com/docs/getting-started/setup">https://www.vcluster.com/docs/getting-started/setup</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Binary tree in Python</title>
    <url>/blog/binary-tree-in-python/</url>
    <content><![CDATA[<p>A binary tree is a tree data structure in which each node has at most two children, referred to as the left child and the right child.</p>
<span id="more"></span>

<h2 id="Leetcode-105-Construct-Binary-Tree-from-Preorder-and-Inorder-Traversal"><a href="#Leetcode-105-Construct-Binary-Tree-from-Preorder-and-Inorder-Traversal" class="headerlink" title="[Leetcode 105] Construct Binary Tree from Preorder and Inorder Traversal"></a>[Leetcode 105] Construct Binary Tree from Preorder and Inorder Traversal</h2><p>Given two integer arrays preorder and inorder where preorder is the preorder traversal of a binary tree and inorder is the inorder traversal of the same tree, construct and return the binary tree.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: preorder = [3,9,20,15,7], inorder = [9,3,15,20,7]</span><br><span class="line">Output: [3,9,20,null,null,15,7]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: preorder = [-1], inorder = [-1]</span><br><span class="line">Output: [-1]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; preorder.length &lt;&#x3D; 3000</li>
<li>inorder.length &#x3D;&#x3D; preorder.length</li>
<li>-3000 &lt;&#x3D; preorder[i], inorder[i] &lt;&#x3D; 3000</li>
<li>preorder and inorder consist of unique values.</li>
<li>Each value of inorder also appears in preorder.</li>
<li>preorder is guaranteed to be the preorder traversal of the tree.</li>
<li>inorder is guaranteed to be the inorder traversal of the tree.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buildTree</span>(<span class="params">self, preorder: <span class="type">List</span>[<span class="built_in">int</span>], inorder: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(preorder) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get root node from preorder list</span></span><br><span class="line">        rootVal = preorder[<span class="number">0</span>]</span><br><span class="line">        root = TreeNode(rootVal)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># divide inorder list to left and right subtree</span></span><br><span class="line">        rootIdx = inorder.index(rootVal)</span><br><span class="line">        leftInorder = inorder[:rootIdx]</span><br><span class="line">        rightInorder = inorder[rootIdx + <span class="number">1</span> :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find corresponding preorder left and right subtree</span></span><br><span class="line">        leftPreorder = preorder[<span class="number">1</span> : <span class="built_in">len</span>(leftInorder) + <span class="number">1</span>]</span><br><span class="line">        rightPreorder = preorder[<span class="built_in">len</span>(leftInorder) + <span class="number">1</span> :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># recursively construct left and right subtree</span></span><br><span class="line">        root.left = self.buildTree(leftPreorder, leftInorder)</span><br><span class="line">        root.right = self.buildTree(rightPreorder, rightInorder)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># return the root of tree</span></span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-106-Construct-Binary-Tree-from-Inorder-and-Postorder-Traversal"><a href="#Leetcode-106-Construct-Binary-Tree-from-Inorder-and-Postorder-Traversal" class="headerlink" title="[Leetcode 106] Construct Binary Tree from Inorder and Postorder Traversal"></a>[Leetcode 106] Construct Binary Tree from Inorder and Postorder Traversal</h2><p>Given two integer arrays inorder and postorder where inorder is the inorder traversal of a binary tree and postorder is the postorder traversal of the same tree, construct and return the binary tree.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: inorder = [9,3,15,20,7], postorder = [9,15,7,20,3]</span><br><span class="line">Output: [3,9,20,null,null,15,7]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: inorder = [-1], postorder = [-1]</span><br><span class="line">Output: [-1]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; inorder.length &lt;&#x3D; 3000</li>
<li>postorder.length &#x3D;&#x3D; inorder.length</li>
<li>-3000 &lt;&#x3D; inorder[i], postorder[i] &lt;&#x3D; 3000</li>
<li>inorder and postorder consist of unique values.</li>
<li>Each value of postorder also appears in inorder.</li>
<li>inorder is guaranteed to be the inorder traversal of the tree.</li>
<li>postorder is guaranteed to be the postorder traversal of the tree.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buildTree</span>(<span class="params">self, inorder: <span class="type">List</span>[<span class="built_in">int</span>], postorder: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="comment"># check if empty tree list</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(inorder) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># find the root in postorder list and construct the root node</span></span><br><span class="line">        rootVal = postorder[-<span class="number">1</span>]</span><br><span class="line">        root = TreeNode(rootVal)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># divide the inorder list to left and right subtree</span></span><br><span class="line">        rootIdx = inorder.index(rootVal)</span><br><span class="line">        leftInorder = inorder[:rootIdx]</span><br><span class="line">        rightInorder = inorder[rootIdx + <span class="number">1</span> :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find corresponding postorder list</span></span><br><span class="line">        leftPostorder = postorder[: <span class="built_in">len</span>(leftInorder)]</span><br><span class="line">        rightPostorder = postorder[</span><br><span class="line">            <span class="built_in">len</span>(leftInorder) : <span class="built_in">len</span>(leftInorder) + <span class="built_in">len</span>(rightInorder)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># recursively construct left and right subtree</span></span><br><span class="line">        root.left = self.buildTree(leftInorder, leftPostorder)</span><br><span class="line">        root.right = self.buildTree(rightInorder, rightPostorder)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># return the root of tree</span></span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-129-Sum-Root-to-Leaf-Numbers"><a href="#Leetcode-129-Sum-Root-to-Leaf-Numbers" class="headerlink" title="[Leetcode 129] Sum Root to Leaf Numbers"></a>[Leetcode 129] Sum Root to Leaf Numbers</h2><p>You are given the root of a binary tree containing digits from 0 to 9 only.</p>
<p>Each root-to-leaf path in the tree represents a number.</p>
<p>For example, the root-to-leaf path 1 -&gt; 2 -&gt; 3 represents the number 123.<br>Return the total sum of all root-to-leaf numbers. Test cases are generated so that the answer will fit in a 32-bit integer.</p>
<p>A leaf node is a node with no children.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">      1</span><br><span class="line">    /   \</span><br><span class="line">   2     3</span><br><span class="line"></span><br><span class="line">Input: root = [1,2,3]</span><br><span class="line">Output: 25</span><br><span class="line">Explanation:</span><br><span class="line">The root-to-leaf path 1-&gt;2 represents the number 12.</span><br><span class="line">The root-to-leaf path 1-&gt;3 represents the number 13.</span><br><span class="line">Therefore, sum = 12 + 13 = 25.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>The number of nodes in the tree is in the range [1, 1000].</li>
<li>0 &lt;&#x3D; Node.val &lt;&#x3D; 9</li>
<li>The depth of the tree will not exceed 10.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sumNumbers</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        self.total = <span class="number">0</span></span><br><span class="line">        self.sumNumbersHelper(root, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> self.total</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sumNumbersHelper</span>(<span class="params">self, root, pathSum</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> </span><br><span class="line"></span><br><span class="line">        <span class="comment"># accumulate the path sum</span></span><br><span class="line">        pathSum = pathSum * <span class="number">10</span> + root.val</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</span><br><span class="line">            <span class="comment"># add path sum to result</span></span><br><span class="line">            self.total += pathSum</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># search left subtree</span></span><br><span class="line">            self.sumNumbersHelper(root.left, pathSum)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># search right subtree</span></span><br><span class="line">            self.sumNumbersHelper(root.right, pathSum)</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-226-Invert-Binary-Tree"><a href="#Leetcode-226-Invert-Binary-Tree" class="headerlink" title="[Leetcode 226] Invert Binary Tree"></a>[Leetcode 226] Invert Binary Tree</h2><p>Given the root of a binary tree, invert the tree, and return its root.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: root = [4,2,7,1,3,6,9]</span><br><span class="line">Output: [4,7,2,9,6,3,1]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: root = [2,1,3]</span><br><span class="line">Output: [2,3,1]</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: root = []</span><br><span class="line">Output: []</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>The number of nodes in the tree is in the range [0, 100].</li>
<li>-100 &lt;&#x3D; Node.val &lt;&#x3D; 100</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">invertTree</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line"></span><br><span class="line">        qu = deque()</span><br><span class="line">        qu.append(root)</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(qu) &gt; <span class="number">0</span>:</span><br><span class="line">            node = qu.popleft()</span><br><span class="line">            node.left, node.right = node.right, node.left</span><br><span class="line">            <span class="keyword">if</span> node.left != <span class="literal">None</span>:</span><br><span class="line">                qu.append(node.left)</span><br><span class="line">            <span class="keyword">if</span> node.right != <span class="literal">None</span>:</span><br><span class="line">                qu.append(node.right)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">invertTree_recursive</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line"></span><br><span class="line">        root.left, root.right = root.right, root.left</span><br><span class="line"></span><br><span class="line">        self.invertTree(root.left)</span><br><span class="line">        self.invertTree(root.right)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">invertTree_preorder</span>(<span class="params">self, root: <span class="type">Optional</span>[TreeNode]</span>) -&gt; <span class="type">Optional</span>[TreeNode]:</span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line"></span><br><span class="line">        st = []</span><br><span class="line">        st.append(root)</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(st) &gt; <span class="number">0</span>:</span><br><span class="line">            node = st.pop()</span><br><span class="line">            node.left, node.right = node.right, node.left</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> node.left != <span class="literal">None</span>:</span><br><span class="line">                st.append(node.left)</span><br><span class="line">            <span class="keyword">if</span> node.right != <span class="literal">None</span>:</span><br><span class="line">                st.append(node.right)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>Binary Search</tag>
      </tags>
  </entry>
  <entry>
    <title>CPU cache and cacheline</title>
    <url>/blog/cacheline/</url>
    <content><![CDATA[<p>A CPU cache is a hardware cache which helps reduce the cost from main memory access. It is a smaller and faster memory part which locates closer to a CPU core than the main memory. Most modern CPUs have mulitple level of caches like L1, L2, and L3. When trying to read and write the main memory, the processor will first check if the data already exists in its local cache. <span id="more"></span></p>
<p>The modern CPUs have at least three independent caches:</p>
<ul>
<li>Instruction cache - Used to speed executable instruction fetch</li>
<li>Data cache - Used to speed data fetch and store</li>
<li>Translation lookaside buffer (TLB) - Used to speed virtual-to-physical address translation for both executable instructions and data. The TLB cache is part of the memory management unit (MMU) but not directly related to the CPU cache</li>
</ul>
<p>Let’s take an example to see how the CPU and cache looks like in a physical server.</p>
<p>In the following server, there are 48 CPU cores and each core processor has three levels of caches(L1, L2 and L3). L1 cache has both instruction cache and data cache.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@init531-e43 ~]<span class="comment"># lscpu</span></span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                48</span><br><span class="line">On-line CPU(s) list:   0-47</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    12</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          2</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 106</span><br><span class="line">Model name:            Intel(R) Xeon(R) Gold 5317 CPU @ 3.00GHz</span><br><span class="line">Stepping:              6</span><br><span class="line">CPU MHz:               1506.042</span><br><span class="line">CPU max MHz:           3600.0000</span><br><span class="line">CPU min MHz:           800.0000</span><br><span class="line">BogoMIPS:              6000.00</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             48K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              1280K</span><br><span class="line">L3 cache:              18432K</span><br><span class="line">NUMA node0 CPU(s):     0-11,24-35</span><br><span class="line">NUMA node1 CPU(s):     12-23,36-47</span><br><span class="line">Flags:                 [...]</span><br></pre></td></tr></table></figure>

<p>The <em>getconf</em> command outputs the detail cache size for each level.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@init531-e43 ~]<span class="comment"># getconf -a | grep CACHE</span></span><br><span class="line">LEVEL1_ICACHE_SIZE                 32768</span><br><span class="line">LEVEL1_ICACHE_ASSOC                8</span><br><span class="line">LEVEL1_ICACHE_LINESIZE             64</span><br><span class="line">LEVEL1_DCACHE_SIZE                 49152</span><br><span class="line">LEVEL1_DCACHE_ASSOC                12</span><br><span class="line">LEVEL1_DCACHE_LINESIZE             64</span><br><span class="line">LEVEL2_CACHE_SIZE                  1310720</span><br><span class="line">LEVEL2_CACHE_ASSOC                 20</span><br><span class="line">LEVEL2_CACHE_LINESIZE              64</span><br><span class="line">LEVEL3_CACHE_SIZE                  18874368</span><br><span class="line">LEVEL3_CACHE_ASSOC                 12</span><br><span class="line">LEVEL3_CACHE_LINESIZE              64</span><br><span class="line">LEVEL4_CACHE_SIZE                  0</span><br><span class="line">LEVEL4_CACHE_ASSOC                 0</span><br><span class="line">LEVEL4_CACHE_LINESIZE              0</span><br></pre></td></tr></table></figure>

<p>From the above output, we can see cacheline size for each cache level. It’s all 64 bytes in this example.</p>
<p>When the processor accesses a memory portion which is not already in the cache, it will load a chunk of the memory around the accessed address into the cache such that the cached data could be reused. The chunks of memory loaded by the cache are called cache lines. The size of chunk is called the cache line size. The common cache line sizes are 32, 64 and 128 bytes. </p>
<p>The limited number of cache lines can be held by the cache, which is determined by the cache size. In this example, the size of L1 data cache is 48k and it can hold 768 cache lines(49152&#x2F;64).</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Cache</tag>
      </tags>
  </entry>
  <entry>
    <title>Two pointers technique</title>
    <url>/blog/two-pointers/</url>
    <content><![CDATA[<p>Two pointers is typically used for searching pairs in a sorted array satisfying some condition in linear time.</p>
<span id="more"></span>

<h2 id="Leetcode-15-3Sum"><a href="#Leetcode-15-3Sum" class="headerlink" title="[Leetcode 15] 3Sum"></a>[Leetcode 15] 3Sum</h2><p>Given an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] such that i !&#x3D; j, i !&#x3D; k, and j !&#x3D; k, and nums[i] + nums[j] + nums[k] &#x3D;&#x3D; 0.</p>
<p>Notice that the solution set must not contain duplicate triplets.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [-1,0,1,2,-1,-4]</span><br><span class="line">Output: [[-1,-1,2],[-1,0,1]]</span><br><span class="line">Explanation: </span><br><span class="line">nums[0] + nums[1] + nums[2] = (-1) + 0 + 1 = 0.</span><br><span class="line">nums[1] + nums[2] + nums[4] = 0 + 1 + (-1) = 0.</span><br><span class="line">nums[0] + nums[3] + nums[4] = (-1) + 2 + (-1) = 0.</span><br><span class="line">The distinct triplets are [-1,0,1] and [-1,-1,2].</span><br><span class="line">Notice that the order of the output and the order of the triplets does not matter.</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [0,1,1]</span><br><span class="line">Output: []</span><br><span class="line">Explanation: The only possible triplet does not sum up to 0.</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: nums = [0,0,0]</span><br><span class="line">Output: [[0,0,0]]</span><br><span class="line">Explanation: The only possible triplet sums up to 0.</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>3 &lt;&#x3D; nums.length &lt;&#x3D; 3000</li>
<li>-10^5 &lt;&#x3D; nums[i] &lt;&#x3D; 10^5</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">threeSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        nums.sort()</span><br><span class="line">        triplets = <span class="built_in">set</span>() <span class="comment"># automatically remove duplicates</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums) - <span class="number">2</span>):</span><br><span class="line">            j = i + <span class="number">1</span></span><br><span class="line">            k = <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> j &lt; k:</span><br><span class="line">                <span class="built_in">sum</span> = nums[i] + nums[j] + nums[k]</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">sum</span> &gt; <span class="number">0</span>:</span><br><span class="line">                    k -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">sum</span> &lt; <span class="number">0</span>:</span><br><span class="line">                    j += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    triplets.add((nums[i],nums[j],nums[k]))</span><br><span class="line">                    j += <span class="number">1</span></span><br><span class="line">                    k -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> triplets</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://leetcode.com/tag/two-pointers/">https://leetcode.com/tag/two-pointers/</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
        <tag>Algorithm</tag>
        <tag>Two pointers</tag>
      </tags>
  </entry>
  <entry>
    <title>An Unexpected Journey</title>
    <url>/blog/an-unexpected-journey/</url>
    <content><![CDATA[<p><img src="/images/spy.png"></p>
<p>“Kai, hurry! We’re going to be late to the festival!” Kate called. <span id="more"></span></p>
<p>Her brother Kai was always a slow absent-minded kid ever since he was born. Teachers always said that Kai was different from other children. Perhaps it was because of the many seizures he had as a kid, or it was just that his IQ was lower than average. And as much as she loved her little brother, she couldn’t help but feel embarrassment when her friends made fun of Kai. </p>
<p>It was the annual lunar festival, and they were already behind on schedule. With this heavy traffic, they would be lucky if they got to Chinatown San Francisco by 9:00. Kai was not helping at all. In fact, he was the main reason they were always late to meetings, parties, festivals, etc. He kept on forgetting stuff, so Kate had to help him and get her own belongings ready. When they were finally ready, her mother Sally and father Paul rushed them into the van and off they went. Thank goodness, they got to the festival just as the lunar dragon came out. Everyone cheered except Kai. Kate eyed her brother warily, sensing his discomfort.</p>
<p>“What’s wrong?” She asked.</p>
<p>“I don’t like this. I can sense danger in that dragon,” Kai replied nervously.</p>
<p>Kate rolled her eyes. Kai was always a puny little kid, scared of basically everything and everyone. She told Kai to calm down, and continued enjoying the performance. To her annoyance, Kai wrapped his arms around her waist and kept muttering about death and danger. Just as she was going to tell him to shut up, she did notice something was off. Since they were late, they had to stand in the corner of the plaza, next to the cable and outlets. Suddenly, she heard a crack and the power went off for one of the cords. Her parents had noticed too and they stiffened, as if they knew something more about this. The people gasped as the lights went out and everyone just stood in darkness. People then started screaming, and Kate could hear gunshots. Kai was still standing next to her, silent. She then heard her parents yell, as the cord caught on fire. The entire Chinatown was going to be in flames soon, and she heard Sally yell her name. She tried to locate where the sound was coming from, when suddenly she was picked up by someone behind her. Kai screamed for the person to stop, but he too was picked up by another mysterious figure. Kate fought with all her might, but then she was knocked out by a swift punch to the gut. </p>
<p>When she came to, she was on a bed with Kai sleeping soundly next to her. She looked around, but did not see her parents. Suddenly, a man entered the room. He introduced himself as Cole, and he was a former CIA agent. He explained that the power outage and fire had not been an accident. On the other hand, it was a planned assassination attempt on her family. To her shock, Cole revealed that he used to work with her parents as a spy group for the government. In fact, they were the best generation of spies that the government had ever produced. That all changed when Sally and Paul were married and decided to leave the agency. This was a serious blow to the power of the government, and they had to find replacements. There was no one quite as good as the duo, but one day a man called Luke Jackson was recruited to join. He was said to be the only worthy replacement, and he did not disappoint. Within weeks, he had uncovered many of the darkest secrets of other countries, and even found 3 moles within the agency. He was so trusted by the president that he was made the head of the CIA. Using his charisma and power, he convinced the president to try and destroy the other countries in order to make America the sole power in the world. And he had a plan worthy of Athena herself to turn this idea into reality. </p>
<p>Luke Jackson turned out to have contacts throughout the world. He joined the world government with the sole purpose of causing chaos throughout the world. He had friends working as moles in other national agencies, secretly planting bombs ready to defuse at the right time. If this happened, it would be seen as an act of war by the US and it would be a good reason to declare war on the rest of the world. After hearing this, Kai realized he had to act immediately to save his parents. Luke’s men had kidnapped them and were probably trying to get information from them. He suddenly had horrible thoughts about his parents being subjected to torture. He had to come up with a plan fast. He asked Cole if he knew the whereabouts of Luke. Cole said that Luke always liked to hang out in a bar called the Red Baron. Without wasting any time, Kai and Kate rushed to the bar, only to find it closed. They knew it would be exceedingly risky to break into the bar directly, but they had no choice. Kate made a makeshift paperclip, and Kai skillfully picked the lock. Silently, the siblings snuck into the bar and saw a light upstairs. They could hear screams of pain from their parents, and holding their guns, which they learnt to use from Cole, they burst into the room. They quickly shot the enemies, and freed their parents. Then, they hurried into Cole’s getaway car and they left. </p>
<p>“What did they torture you for?” Cole inquired.</p>
<p>“They wanted to know about a specific mission in the past, Mission 8-109. In that mission, Paul and I were responsible for detonating 3 bombs in the nuclear port of Russia, so that the nuclear bombs would be destroyed without causing casualties. We were the only two people in the world skilled enough to detonate and destroy the planted bombs. Now Luke also wants to know how to do it. He said that we would be a crucial part in his masterplan to take over the world. He has moles in the governments of major countries such as Russia and China, and they have all successfully planted bombs already. He had to make the explosion look like a failed attempt on detonating a nuclear bomb. His plan to do this was to first turn off the nuclear bombs and then detonate the newly planted bombs so that the explosion would destroy the nuclear bombs with them.</p>
<p>In the heart of Washington D.C., Luke Jackson, the enigmatic and cunning head of the CIA, had devised a master plan that could potentially make the United States the sole power in the world. Luke, driven by a twisted sense of patriotism and an unquenchable thirst for dominance, believed that this was the only way to ensure the safety and prosperity of his homeland. His plan involved covert operations, manipulation, and the strategic dismantling of other nations’ infrastructures. Little did Luke know that on the other side of the country, in a quiet suburban town, lived Kai, a prodigious teenager with an unparalleled intellect. Kai, a tech-savvy whiz kid with an insatiable curiosity, stumbled upon classified information while conducting his own research online. The more he uncovered, the clearer it became that Luke’s plan would unleash chaos and jeopardize the delicate balance of global power. </p>
<p>Realizing the gravity of the situation, Kai decided that he couldn’t stand idly by. Armed with his intelligence, resourcefulness, and an unyielding sense of justice, Kai embarked on a mission to stop Luke Jackson and save the world from his destructive ambition. Kai began by discreetly gathering evidence, connecting the dots between seemingly unrelated incidents. As he delved deeper, he discovered a network of shadowy alliances and covert operations orchestrated by Luke. The stakes were higher than he could have imagined, and time was running out. Undeterred, Kai reached out to a few trusted friends who shared his concerns. Together, they formed an underground alliance to counteract Luke’s plans. Each member brought unique skills to the table, ranging from hacking expertise to strategic thinking. </p>
<p>As they worked in the shadows, staying one step ahead of the CIA, Kai’s group became a formidable force against Luke’s machinations. As the tension escalated, the world teetered on the brink of disaster. The media was ablaze with rumors, governments were on edge, and ordinary citizens felt the looming threat. It was a race against time, with Luke tightening his grip on power and Kai and his allies working tirelessly to expose the truth. In a dramatic showdown, Kai and his team managed to infiltrate the heart of the CIA’s operations. Facing off against Luke Jackson, they engaged in a battle of wits, technology, and strategy. Luke, realizing the gravity of his actions, tried to justify his plan as a means to protect the nation. But Kai, armed with irrefutable evidence and a conviction for a better world, exposed the flaws in Luke’s twisted logic. </p>
<p>The world watched in suspense as the truth unfolded, and the tide turned against Luke Jackson. With his plan in ruins and his reputation shattered, Luke was apprehended, and the threat of global chaos was averted. Kai, hailed as a hero, stood as a symbol of resilience and the power of individuals to make a difference. As the world recovered from the brink of disaster, Kai and his allies continued their work, ensuring that the lessons learned from this ordeal would shape a future where power was wielded responsibly and justly.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>Getting started with PostMark</title>
    <url>/blog/postmark/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>Postmark is a benchmark designed to simulate the behavior of mail servers. Postmark consists of three phases. In the first phase a pool of files are created. In the next phase four types of transactions are executed: files are created, deleted, read, and appended to. In the last phase, all files in the pool are deleted.<span id="more"></span></p>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>The PostMark can be run with phoronix-test-suite. Refer to <a href="https://www.flamingbytes.com/blog/phoronix-test-suite/">this blog</a> on how to install phoronix-test-suite.</p>
<h2 id="The-first-run"><a href="#The-first-run" class="headerlink" title="The first run"></a>The first run</h2><p>When you have first run of PostMark, it will have the additional dependent packages installed automatically.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># phoronix-test-suite benchmark postmark</span></span><br><span class="line"></span><br><span class="line">    Evaluating External Test Dependencies ......</span><br><span class="line"></span><br><span class="line">The following dependencies are needed and will be installed:</span><br><span class="line"></span><br><span class="line">- gcc</span><br><span class="line">- gcc-c++</span><br><span class="line">- make</span><br><span class="line">- autoconf</span><br><span class="line">- automake</span><br><span class="line">- patch</span><br><span class="line">- expat-devel</span><br><span class="line">- unzip</span><br><span class="line">- bzip2</span><br><span class="line"></span><br><span class="line">This process may take several minutes.</span><br><span class="line">Please enter your SUDO password below:</span><br><span class="line">Password:</span><br><span class="line"></span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line">Installed:</span><br><span class="line">  autoconf.noarch 0:2.69-11.el7             automake.noarch 0:1.13.4-3.el7</span><br><span class="line">  expat-devel.x86_64 0:2.1.0-15.el7_9       gcc.x86_64 0:4.8.5-44.el7</span><br><span class="line">  gcc-c++.x86_64 0:4.8.5-44.el7             patch.x86_64 0:2.7.1-12.el7_7</span><br><span class="line">  unzip.x86_64 0:6.0-24.el7_9</span><br><span class="line"></span><br><span class="line">Dependency Installed:</span><br><span class="line">  cpp.x86_64 0:4.8.5-44.el7</span><br><span class="line">  glibc-devel.x86_64 0:2.17-326.el7_9</span><br><span class="line">  glibc-headers.x86_64 0:2.17-326.el7_9</span><br><span class="line">  kernel-headers.x86_64 0:3.10.0-1160.102.1.el7</span><br><span class="line">  libmpc.x86_64 0:1.0.1-3.el7</span><br><span class="line">  libstdc++-devel.x86_64 0:4.8.5-44.el7</span><br><span class="line">  m4.x86_64 0:1.4.16-10.el7</span><br><span class="line">  mpfr.x86_64 0:3.1.1-4.el7</span><br><span class="line">  perl-Data-Dumper.x86_64 0:2.145-3.el7</span><br><span class="line">  perl-Test-Harness.noarch 0:3.28-3.el7</span><br><span class="line">  perl-Thread-Queue.noarch 0:3.02-2.el7</span><br><span class="line"></span><br><span class="line">Dependency Updated:</span><br><span class="line">  expat.x86_64 0:2.1.0-15.el7_9              glibc.x86_64 0:2.17-326.el7_9</span><br><span class="line">  glibc-common.x86_64 0:2.17-326.el7_9</span><br><span class="line"></span><br><span class="line">Complete!</span><br><span class="line"></span><br><span class="line">Phoronix Test Suite v10.8.4</span><br><span class="line"></span><br><span class="line">    To Install:    pts/postmark-1.1.2</span><br><span class="line"></span><br><span class="line">    Determining File Requirements ........................................................................................................................................................................................................</span><br><span class="line">    Searching Download Caches ............................................................................................................................................................................................................</span><br><span class="line"></span><br><span class="line">    1 Test To Install</span><br><span class="line">        1 File To Download [0.01MB]</span><br><span class="line">        1MB Of Disk Space Is Needed</span><br><span class="line">        2 Seconds Estimated Install Time</span><br><span class="line"></span><br><span class="line">    pts/postmark-1.1.2:</span><br><span class="line">        Test Installation 1 of 1</span><br><span class="line">        1 File Needed [0.01 MB]</span><br><span class="line">        Downloading: postmark_1.51.orig.tar.gz                                                                                                                                                                                    [0.01MB]</span><br><span class="line">        Downloading ......................................................................................................................................................................................................................</span><br><span class="line">        Approximate Install Size: 1 MB</span><br><span class="line">        Estimated Install Time: 2 Seconds</span><br><span class="line">        Installing Test @ 23:56:49</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure>

<h2 id="Result-file-location"><a href="#Result-file-location" class="headerlink" title="Result file location"></a>Result file location</h2><p>By default, the result from benchmarking is saved under &#x2F;var&#x2F;lib&#x2F;phoronix-test-suite. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># ls -la /var/lib/phoronix-test-suite</span></span><br><span class="line">total 20</span><br><span class="line">drwxr-xr-x.  8 root root  167 Nov 14 03:07 .</span><br><span class="line">drwxr-xr-x. 34 root root 4096 Nov 13 23:31 ..</span><br><span class="line">-rw-------.  1 root root 8290 Nov 14 03:07 core.pt2so</span><br><span class="line">-rw-r--r--.  1 root root  979 Nov 14 03:07 graph-config.json</span><br><span class="line">drwxr-xr-x.  3 root root   17 Nov 13 23:32 installed-tests</span><br><span class="line">drwxr-xr-x.  2 root root    6 Nov 13 23:31 modules</span><br><span class="line">drwxr-xr-x.  2 root root    6 Nov 13 23:31 modules-data</span><br><span class="line">drwxr-xr-x.  6 root root   55 Nov 13 23:31 test-profiles</span><br><span class="line">drwxr-xr-x.  3 root root   38 Nov 13 23:57 test-results</span><br><span class="line">drwxr-xr-x.  4 root root   30 Nov 13 23:31 test-suites</span><br></pre></td></tr></table></figure>

<p>This can be modified by editing &#x2F;etc&#x2F;phoronix-test-suite.xml</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># vim /etc/phoronix-test-suite.xml</span></span><br><span class="line">&lt;ResultsDirectory&gt;/root/phoronix-test-suite/test-results/&lt;/ResultsDirectory&gt;</span><br></pre></td></tr></table></figure>

<h2 id="Batch-mode"><a href="#Batch-mode" class="headerlink" title="Batch mode"></a>Batch mode</h2><p>If you don’t want to answer the prompted questions during the test execution, you can setup batch mode before test execution.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># phoronix-test-suite batch-setup</span></span><br><span class="line"></span><br><span class="line">These are the default configuration options <span class="keyword">for</span> when running the Phoronix Test Suite <span class="keyword">in</span> a batch mode (i.e. running phoronix-test-suite batch-benchmark universe). Running <span class="keyword">in</span> a batch mode is designed to be as autonomous as possible, except <span class="keyword">for</span> <span class="built_in">where</span> you<span class="string">&#x27;d like any end-user interaction.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Save test results when in batch mode (Y/n): y</span></span><br><span class="line"><span class="string">    Open the web browser automatically when in batch mode (y/N): n</span></span><br><span class="line"><span class="string">    Auto upload the results to OpenBenchmarking.org (Y/n): n</span></span><br><span class="line"><span class="string">    Prompt for test identifier (Y/n): n</span></span><br><span class="line"><span class="string">    Prompt for test description (Y/n): n</span></span><br><span class="line"><span class="string">    Prompt for saved results file-name (Y/n): n</span></span><br><span class="line"><span class="string">    Run all test options (Y/n): n</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Batch settings saved.</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>

<p>After setting up batch mode, you can run the test as below.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment">#  phoronix-test-suite batch-benchmark pts/postmark</span></span><br><span class="line"></span><br><span class="line">    Evaluating External Test Dependencies ......</span><br><span class="line"></span><br><span class="line">Phoronix Test Suite v10.8.4</span><br><span class="line"></span><br><span class="line">    Installed:     pts/postmark-1.1.2</span><br><span class="line"></span><br><span class="line">System Information</span><br><span class="line"></span><br><span class="line">  PROCESSOR:              2 x Intel Xeon Gold 5317 @ 3.60GHz</span><br><span class="line">    Core Count:           24</span><br><span class="line">    Thread Count:         48</span><br><span class="line">    Extensions:           SSE 4.2</span><br><span class="line">                          + AVX512_VNNI</span><br><span class="line">                          + AVX512CD</span><br><span class="line">                          + AVX2</span><br><span class="line">                          + AVX</span><br><span class="line">                          + RDRAND</span><br><span class="line">                          + FSGSBASE</span><br><span class="line">    Cache Size:           18 MB</span><br><span class="line">    Microcode:            0xd000390</span><br><span class="line">    Core Family:          Ice Lake</span><br><span class="line">    Scaling Driver:       acpi-cpufreq conservative (Boost: Enabled)</span><br><span class="line"></span><br><span class="line">  GRAPHICS:               ASPEED</span><br><span class="line">    Screen:               1024x768</span><br><span class="line"></span><br><span class="line">  MOTHERBOARD:            Supermicro X12DDW-A6 v1.01</span><br><span class="line">    BIOS Version:         1.5</span><br><span class="line">    Chipset:              Intel Device 0998</span><br><span class="line">    Network:              2 x Mellanox MT27800 + 4 x Intel I350 + 2 x Mellanox MT2892</span><br><span class="line"></span><br><span class="line">  MEMORY:                 8 x 64 GB DDR4-2933MT/s Samsung M393A8G40BB4-CWE</span><br><span class="line"></span><br><span class="line">  DISK:                   4 x 1600GB Micron_7450_MTFDKCC1T6TFS + 6 x 1920GB Micron_5400_MTFD</span><br><span class="line">    File-System:          xfs</span><br><span class="line">    Mount Options:        attr2 inode64 logbsize=32k logbufs=8 noquota relatime rw seclabel</span><br><span class="line">    Disk Scheduler:       NONE</span><br><span class="line">    Disk Details:         Block Size: 4096</span><br><span class="line"></span><br><span class="line">  OPERATING SYSTEM:       CentOS Linux 7</span><br><span class="line">    Kernel:               5.7.12-1.el7.elrepo.x86_64 (x86_64)</span><br><span class="line">    Compiler:             GCC 4.8.5 20150623</span><br><span class="line">    Security:             SELinux</span><br><span class="line">                          + itlb_multihit: Not affected</span><br><span class="line">                          + l1tf: Not affected</span><br><span class="line">                          + mds: Not affected</span><br><span class="line">                          + meltdown: Not affected</span><br><span class="line">                          + spec_store_bypass: Mitigation of SSB disabled via prctl and seccomp</span><br><span class="line">                          + spectre_v1: Mitigation of usercopy/swapgs barriers and __user pointer sanitization</span><br><span class="line">                          + spectre_v2: Mitigation of Enhanced IBRS IBPB: conditional RSB filling</span><br><span class="line">                          + srbds: Not affected</span><br><span class="line">                          + tsx_async_abort: Not affected</span><br><span class="line"></span><br><span class="line">PostMark 1.51:</span><br><span class="line">    pts/postmark-1.1.2</span><br><span class="line">    Test 1 of 1</span><br><span class="line">    Estimated Trial Run Count:    3</span><br><span class="line">    Estimated Time To Completion: 3 Minutes [01:15 UTC]</span><br><span class="line">        Started Run 1 @ 01:13:06</span><br><span class="line">        Started Run 2 @ 01:14:02</span><br><span class="line">        Started Run 3 @ 01:14:58</span><br><span class="line"></span><br><span class="line">    Disk Transaction Performance:</span><br><span class="line">        4901</span><br><span class="line">        4807</span><br><span class="line">        4901</span><br><span class="line"></span><br><span class="line">    Average: 4870 TPS</span><br><span class="line">    Deviation: 1.11%</span><br></pre></td></tr></table></figure>

<h2 id="Compare-multiple-runs"><a href="#Compare-multiple-runs" class="headerlink" title="Compare multiple runs"></a>Compare multiple runs</h2><p>In the case you need to tweak some settings and run it again, and you want to compare the results for multipe runs. You can use the <em>phoronix-test-suite merge-results</em> command.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># ls -ltr test-results/</span></span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 5 root root 88 Nov 14 01:15 2023-11-14-0113</span><br><span class="line">drwxr-xr-x. 5 root root 88 Nov 14 01:25 2023-11-14-0122</span><br><span class="line"></span><br><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># phoronix-test-suite merge-results 2023-11-14-0113 2023-11-14-0122</span></span><br><span class="line"></span><br><span class="line">Merged Results Saved To: /home/wali/phoronix-test-suite/test-results/merge-7632/composite.xml</span><br><span class="line"></span><br><span class="line">    Do you want to view the <span class="built_in">test</span> results? (Y/n): n</span><br><span class="line"></span><br><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># ls -ltr test-results/</span></span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 5 root root 88 Nov 14 01:15 2023-11-14-0113</span><br><span class="line">drwxr-xr-x. 5 root root 88 Nov 14 01:25 2023-11-14-0122</span><br><span class="line">drwxr-xr-x. 2 root root 27 Nov 14 02:11 merge-7789</span><br></pre></td></tr></table></figure>

<h2 id="Export-result"><a href="#Export-result" class="headerlink" title="Export result"></a>Export result</h2><p>The test result can be exported to the following format.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># phoronix-test-suite -h</span></span><br><span class="line">[...]</span><br><span class="line">    result-file-raw-to-csv [Test Result]</span><br><span class="line">    result-file-to-csv     [Test Result]</span><br><span class="line">    result-file-to-html    [Test Result]</span><br><span class="line">    result-file-to-json    [Test Result]</span><br><span class="line">    result-file-to-pdf     [Test Result]</span><br><span class="line">    result-file-to-suite   [Test Result]</span><br><span class="line">    result-file-to-text    [Test Result]</span><br></pre></td></tr></table></figure>

<p>To export it as html, you can run the following command.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># phoronix-test-suite result-file-to-html merge-7789</span></span><br></pre></td></tr></table></figure>

<p><img src="/images/phoronix-test-suite-result-1.png" alt="Image"><br><img src="/images/phoronix-test-suite-result-2.png" alt="Image"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://openbenchmarking.org/test/pts/postmark">https://openbenchmarking.org/test/pts/postmark</a></li>
<li><a href="https://www.filesystems.org/docs/auto-pilot/Postmark.html">https://www.filesystems.org/docs/auto-pilot/Postmark.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>PostMark</tag>
      </tags>
  </entry>
  <entry>
    <title>How to install phoronix test suite on CentOS</title>
    <url>/blog/phoronix-test-suite/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>The <strong>Phoronix Test Suite</strong> is the most comprehensive testing and benchmarking platform available for Linux, Solaris, macOS, Windows, and BSD operating systems. The Phoronix Test Suite allows for carrying out tests in a fully automated manner from test installation to execution and reporting. All tests are meant to be easily reproducible, easy-to-use, and support fully automated execution. The Phoronix Test Suite is open-source under the GNU GPLv3 license and is developed by Phoronix Media in cooperation with partners.<span id="more"></span></p>
<p>The Phoronix Test Suite has access to more than 450 test profiles and over 100 test suites via OpenBenchmarking.org. If there is a test not currently covered by the Phoronix Test Suite, new tests can be quickly added via its extensible architecture with each profile just being comprised of XML files and a few simple bash scripts.</p>
<h2 id="Install-phoronix-test-suite-on-CentOS"><a href="#Install-phoronix-test-suite-on-CentOS" class="headerlink" title="Install phoronix-test-suite on CentOS"></a>Install phoronix-test-suite on CentOS</h2><p>OS and kernel version:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># cat /etc/centos-release</span></span><br><span class="line">CentOS Linux release 7.9.2009 (Core)</span><br><span class="line"></span><br><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># uname -r</span></span><br><span class="line">5.7.12-1.el7.elrepo.x86_64</span><br></pre></td></tr></table></figure>

<p>The only mandatory requirement for the Phoronix Test Suite on Linux, Solaris, *BSD, Hurd, and macOS operating systems is PHP CLI. Many of the benchmarking profiles do require the standard development tools&#x2F;libraries (GCC compiler, etc) and other common programs. However, on many Linux distributions and operating systems the Phoronix Test Suite is able to use the software’s package management system for installing these additional dependencies.</p>
<p>To install PHP dependencies:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 ~]<span class="comment"># yum install wget php-cli php-xml bzip2</span></span><br></pre></td></tr></table></figure>

<p>To install the <a href="https://phoronix-test-suite.com/?k=downloads">latest phoronix-test-suite</a>:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 ~]<span class="comment"># wget https://phoronix-test-suite.com/releases/phoronix-test-suite-10.8.4.tar.gz</span></span><br><span class="line"></span><br><span class="line">[root@vm1 ~]<span class="comment"># tar zxf phoronix-test-suite-10.8.4.tar.gz</span></span><br><span class="line">[root@vm1 ~]<span class="comment"># cd phoronix-test-suite</span></span><br><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># ls</span></span><br><span class="line">AUTHORS  ChangeLog  COPYING  deploy  documentation  install.bat  install_macos  install-sh  ob-cache  phoronix-test-suite  phoronix-test-suite.bat  pts-core  README.md  release-highlights.md  SECURITY.md</span><br><span class="line"></span><br><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># ./install-sh</span></span><br><span class="line"><span class="built_in">which</span>: no xdg-mime <span class="keyword">in</span> (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)</span><br><span class="line"></span><br><span class="line">Phoronix Test Suite Installation Completed</span><br><span class="line"></span><br><span class="line">Executable File: /usr/bin/phoronix-test-suite</span><br><span class="line">Documentation: /usr/share/doc/phoronix-test-suite/</span><br><span class="line">Phoronix Test Suite Files: /usr/share/phoronix-test-suite/</span><br></pre></td></tr></table></figure>

<h2 id="Verify-the-installation"><a href="#Verify-the-installation" class="headerlink" title="Verify the installation"></a>Verify the installation</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># phoronix-test-suite -h</span></span><br><span class="line"></span><br><span class="line">Phoronix Test Suite v10.8.4</span><br><span class="line"></span><br><span class="line">The **Phoronix Test Suite** is the most comprehensive testing and benchmarking platform available <span class="keyword">for</span> Linux, Solaris, macOS, Windows, and BSD operating systems. The Phoronix Test Suite allows <span class="keyword">for</span> carrying out tests <span class="keyword">in</span> a fully automated manner from <span class="built_in">test</span> installation to execution and reporting. All tests are meant to be easily reproducible, easy-to-use, and support fully automated execution. The Phoronix Test Suite is open-source under the GNU GPLv3 license and is developed by Phoronix Media <span class="keyword">in</span> cooperation with partners.</span><br><span class="line"></span><br><span class="line">View the included documentation or visit https://www.phoronix-test-suite.com/ <span class="keyword">for</span> full details.</span><br><span class="line"></span><br><span class="line">SYSTEM</span><br><span class="line"></span><br><span class="line">    interactive</span><br><span class="line">    php-conf</span><br><span class="line">    shell</span><br><span class="line">    system-info</span><br><span class="line">    system-properties</span><br><span class="line">    system-sensors</span><br><span class="line"></span><br><span class="line">TEST INSTALLATION</span><br><span class="line"></span><br><span class="line">    force-install         [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    install               [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    install-dependencies  [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    make-download-cache</span><br><span class="line">    remove-installed-test [Test]</span><br><span class="line"></span><br><span class="line">TESTING</span><br><span class="line"></span><br><span class="line">    benchmark             [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    estimate-install-time [Test | Suite | OpenBenchmarking ID | Test Result]</span><br><span class="line">    estimate-run-time     [Test | Suite | OpenBenchmarking ID | Test Result]</span><br><span class="line">    finish-run            [Test Result]</span><br><span class="line">    run                   [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    run-random-tests</span><br><span class="line">    run-subset            [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    run-tests-in-suite    [Suite]</span><br><span class="line">    stress-batch-run      [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    stress-run            [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    strict-benchmark      [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    strict-run            [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line"></span><br><span class="line">BATCH TESTING</span><br><span class="line"></span><br><span class="line">    batch-benchmark   [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    batch-install     [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    batch-run         [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    batch-setup</span><br><span class="line">    default-benchmark [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    default-run       [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    dry-run           [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    internal-run      [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line"></span><br><span class="line">OPENBENCHMARKING.ORG</span><br><span class="line"></span><br><span class="line">    clone-result                  [OpenBenchmarking ID]  ...</span><br><span class="line">    dump-suites-to-git</span><br><span class="line">    dump-tests-to-git</span><br><span class="line">    enable-repo</span><br><span class="line">    list-recommended-tests</span><br><span class="line">    make-openbenchmarking-cache</span><br><span class="line">    ob-test-profile-analyze</span><br><span class="line">    openbenchmarking-changes</span><br><span class="line">    openbenchmarking-login</span><br><span class="line">    openbenchmarking-refresh</span><br><span class="line">    openbenchmarking-repositories</span><br><span class="line">    openbenchmarking-uploads</span><br><span class="line">    recently-added-tests</span><br><span class="line">    upload-result                 [Test Result]</span><br><span class="line">    upload-test-profile</span><br><span class="line">    upload-test-suite             [Suite]</span><br><span class="line"></span><br><span class="line">INFORMATION</span><br><span class="line"></span><br><span class="line">    info                          [Test | Suite | OpenBenchmarking ID | Test Result]</span><br><span class="line">    intersect                     [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    list-all-tests</span><br><span class="line">    list-available-suites</span><br><span class="line">    list-available-tests</span><br><span class="line">    list-available-virtual-suites</span><br><span class="line">    list-cached-tests</span><br><span class="line">    list-installed-dependencies</span><br><span class="line">    list-installed-suites</span><br><span class="line">    list-installed-tests</span><br><span class="line">    list-missing-dependencies</span><br><span class="line">    list-not-installed-tests</span><br><span class="line">    list-possible-dependencies</span><br><span class="line">    list-saved-results</span><br><span class="line">    list-test-status</span><br><span class="line">    list-test-usage</span><br><span class="line">    search</span><br><span class="line">    test-to-suite-map</span><br><span class="line"></span><br><span class="line">ASSET CREATION</span><br><span class="line"></span><br><span class="line">    build-suite</span><br><span class="line">    create-test-profile</span><br><span class="line">    debug-benchmark           [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    debug-install             [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    debug-result-parser       [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    debug-test-download-links [Test | Suite | OpenBenchmarking ID | Test Result]</span><br><span class="line">    download-test-files       [Test | Suite | OpenBenchmarking ID | Test Result]  ...</span><br><span class="line">    dump-documentation</span><br><span class="line">    inspect-test-profile      [Test]</span><br><span class="line">    rebuild-test-suite        [Suite]</span><br><span class="line">    validate-result-file</span><br><span class="line">    validate-test-profile     [Test]</span><br><span class="line">    validate-test-suite       [Suite]</span><br><span class="line"></span><br><span class="line">RESULT MANAGEMENT</span><br><span class="line"></span><br><span class="line">    auto-sort-result-file                      [Test Result]</span><br><span class="line">    compare-results-to-baseline                [Test Result] [Test Result]</span><br><span class="line">    compare-results-two-way                    [Test Result]</span><br><span class="line">    edit-result-file                           [Test Result]</span><br><span class="line">    extract-from-result-file                   [Test Result]</span><br><span class="line">    keep-results-in-result-file                [Test Result]</span><br><span class="line">    merge-results                              [Test Result]  ...</span><br><span class="line">    remove-incomplete-results-from-result-file [Test Result]</span><br><span class="line">    remove-result                              [Test Result]</span><br><span class="line">    remove-result-from-result-file             [Test Result]</span><br><span class="line">    remove-results-from-result-file            [Test Result]</span><br><span class="line">    remove-run-from-result-file                [Test Result]</span><br><span class="line">    rename-identifier-in-result-file           [Test Result]</span><br><span class="line">    rename-result-file                         [Test Result]</span><br><span class="line">    reorder-result-file                        [Test Result]</span><br><span class="line">    show-result                                [Test Result]</span><br><span class="line"></span><br><span class="line">OTHER</span><br><span class="line"></span><br><span class="line">    commands</span><br><span class="line">    debug-dependency-handler</span><br><span class="line">    debug-render-test</span><br><span class="line">    debug-self-test</span><br><span class="line">    <span class="built_in">help</span></span><br><span class="line">    version</span><br><span class="line"></span><br><span class="line">RESULT ANALYSIS</span><br><span class="line"></span><br><span class="line">    analyze-run-times      [Test Result]</span><br><span class="line">    executive-summary      [Test Result]</span><br><span class="line">    result-file-confidence [Test Result]</span><br><span class="line">    result-file-stats      [Test Result]</span><br><span class="line">    wins-and-losses        [Test Result]</span><br><span class="line">    workload-topology      [Test Result]</span><br><span class="line"></span><br><span class="line">MODULES</span><br><span class="line"></span><br><span class="line">    auto-load-module</span><br><span class="line">    list-modules</span><br><span class="line">    module-info      [Phoronix Test Suite Module]</span><br><span class="line">    module-setup     [Phoronix Test Suite Module]</span><br><span class="line">    test-module      [Phoronix Test Suite Module]</span><br><span class="line">    unload-module</span><br><span class="line"></span><br><span class="line">DEBUGGING</span><br><span class="line"></span><br><span class="line">    check-tests                   [Test]</span><br><span class="line">    diagnostics</span><br><span class="line">    dump-file-info</span><br><span class="line">    dump-openbenchmarking-indexes</span><br><span class="line">    dump-phodevi-smart-cache</span><br><span class="line">    dump-possible-options</span><br><span class="line">    dump-unhandled-dependencies</span><br><span class="line">    list-failed-installs</span><br><span class="line">    list-test-errors</span><br><span class="line">    list-unsupported-tests</span><br><span class="line"></span><br><span class="line">USER CONFIGURATION</span><br><span class="line"></span><br><span class="line">    enterprise-setup</span><br><span class="line">    network-info</span><br><span class="line">    network-setup</span><br><span class="line">    user-config-reset</span><br><span class="line">    user-config-set</span><br><span class="line">    variables</span><br><span class="line"></span><br><span class="line">RESULT EXPORT</span><br><span class="line"></span><br><span class="line">    result-file-raw-to-csv [Test Result]</span><br><span class="line">    result-file-to-csv     [Test Result]</span><br><span class="line">    result-file-to-html    [Test Result]</span><br><span class="line">    result-file-to-json    [Test Result]</span><br><span class="line">    result-file-to-pdf     [Test Result]</span><br><span class="line">    result-file-to-suite   [Test Result]</span><br><span class="line">    result-file-to-text    [Test Result]</span><br><span class="line"></span><br><span class="line">PHOROMATIC</span><br><span class="line"></span><br><span class="line">    start-phoromatic-server</span><br><span class="line"></span><br><span class="line">RESULT VIEWER</span><br><span class="line"></span><br><span class="line">    start-result-viewer</span><br></pre></td></tr></table></figure>

<h2 id="List-all-available-test-suites-and-tests"><a href="#List-all-available-test-suites-and-tests" class="headerlink" title="List all available test suites and tests"></a>List all available test suites and tests</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># /usr/bin/phoronix-test-suite list-available-suites</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Phoronix Test Suite v10.8.4</span><br><span class="line">Available Suites</span><br><span class="line"></span><br><span class="line">  pts/audio-encoding               - Audio Encoding                   System</span><br><span class="line">  pts/av1                          - AV1                              System</span><br><span class="line">  pts/bioinformatics               - Bioinformatics                   System</span><br><span class="line">  pts/browsers                     - Web Browsers                     System</span><br><span class="line">  pts/cad                          - CAD                              System</span><br><span class="line">  pts/chess                        - Chess Test Suite                 Processor</span><br><span class="line">  pts/compilation                  - Timed Code Compilation           System</span><br><span class="line">  pts/compression                  - Compression Tests                Processor</span><br><span class="line">  pts/creator                      - Creator Workloads                System</span><br><span class="line">  pts/cryptocurrency               - Cryptocurrency Benchmarks, CPU Mining Tests Processor</span><br><span class="line">  pts/cryptography                 - Cryptography                     Processor</span><br><span class="line">  pts/database                     - Database Test Suite              System</span><br><span class="line">  pts/desktop-graphics             - Desktop Graphics                 System</span><br><span class="line">  pts/disk                         - Disk Test Suite                  Disk</span><br><span class="line">  pts/electronic-design            - Electronic Design                System</span><br><span class="line">  pts/finance                      - Finance                          System</span><br><span class="line">  pts/game-dev                     - Game Development                 System</span><br><span class="line">  pts/hpc                          - HPC - High Performance Computing System</span><br><span class="line">  pts/imaging                      - Imaging                          System</span><br><span class="line">  pts/internet-speed               - Internet Speed                   System</span><br><span class="line">  pts/java                         - Java                             System</span><br><span class="line">  pts/kernel                       - Common Kernel Benchmarks         System</span><br><span class="line">  pts/linear-algebra               - Linear Algebra                   System</span><br><span class="line">  pts/machine-learning             - Machine Learning                 System</span><br><span class="line">  pts/memory                       - Memory Test Suite                Memory</span><br><span class="line">  pts/molecular-dynamics           - Molecular Dynamics               System</span><br><span class="line">  pts/mpi                          - MPI Benchmarks                   System</span><br><span class="line">  pts/network                      - Networking Test Suite            Network</span><br><span class="line">  pts/nvidia-gpu-compute           - NVIDIA GPU Compute               Graphics</span><br><span class="line">  pts/ocr                          - OCR                              System</span><br><span class="line">  pts/oneapi                       - Intel oneAPI                     System</span><br><span class="line">  pts/opencl                       - OpenCL                           System</span><br><span class="line">  pts/productivity                 - Productivity                     System</span><br><span class="line">  pts/programmer                   - Programmer / Developer System Benchmarks System</span><br><span class="line">  pts/python                       - Python                           System</span><br><span class="line">  pts/quantum-mechanics            - Quantum Mechanics                System</span><br><span class="line">  pts/raytracing                   - Raytracing                       System</span><br><span class="line">  pts/renderers                    - Renderers                        Processor</span><br><span class="line">  pts/scientific-computing         - Scientific Computing             System</span><br><span class="line">  pts/sdr                          - Software Defined Radio           System</span><br><span class="line">  pts/server                       - Server                           System</span><br><span class="line">  pts/speech                       - Speech                           System</span><br><span class="line">  pts/steam                        - Steam                            Graphics</span><br><span class="line">  pts/telephony                    - Telephony                        System</span><br><span class="line">  pts/texture-compression          - Texture Compression              System</span><br><span class="line">  pts/unigine                      - Unigine Test Suite               Graphics</span><br><span class="line">  pts/video-encoding               - Video Encoding                   System</span><br><span class="line">  pts/vulkan-compute               - Vulkan Compute                   Graphics</span><br><span class="line">  pts/vulkan-rt                    - Vulkan Ray-Tracing               Graphics</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># /usr/bin/phoronix-test-suite list-available-tests</span></span><br><span class="line">Phoronix Test Suite v10.8.4</span><br><span class="line">Available Tests</span><br><span class="line"></span><br><span class="line">pts/ai-benchmark            AI Benchmark Alpha                                System</span><br><span class="line">pts/aircrack-ng             Aircrack-ng                                       Processor</span><br><span class="line">pts/amg                     Algebraic Multi-Grid Benchmark                    Processor</span><br><span class="line">pts/aobench                 AOBench                                           Processor</span><br><span class="line">pts/aom-av1                 AOM AV1                                           Processor</span><br><span class="line">pts/apache                  Apache HTTP Server                                System</span><br><span class="line">pts/apache-iotdb            Apache IoTDB                                      System</span><br><span class="line">pts/apache-siege            Apache Siege                                      System</span><br><span class="line">pts/appleseed               Appleseed                                         System</span><br><span class="line">pts/arrayfire               ArrayFire                                         Processor</span><br><span class="line">pts/askap                   ASKAP                                             System</span><br><span class="line">pts/asmfish                 asmFish                                           Processor</span><br><span class="line">pts/astcenc                 ASTC Encoder                                      System</span><br><span class="line">pts/avifenc                 libavif avifenc                                   Processor</span><br><span class="line">pts/axs2mlperf              axs2mlperf                                        System</span><br><span class="line">pts/basemark                Basemark GPU                                      System</span><br><span class="line">pts/basis                   Basis Universal                                   System</span><br><span class="line">pts/blake2                  BLAKE2                                            Processor</span><br><span class="line">pts/blender                 Blender                                           System</span><br><span class="line">pts/blogbench               BlogBench                                         Disk</span><br><span class="line">pts/blosc                   C-Blosc                                           Processor</span><br><span class="line">pts/bork                    Bork File Encrypter                               Processor</span><br><span class="line">pts/botan                   Botan                                             Processor</span><br><span class="line">pts/brl-cad                 BRL-CAD                                           System</span><br><span class="line">pts/build-apache            Timed Apache Compilation                          Processor</span><br><span class="line">pts/build-clash             Timed Clash Compilation                           Processor</span><br><span class="line">pts/build-eigen             Timed Eigen Compilation                           Processor</span><br><span class="line">pts/build-erlang            Timed Erlang/OTP Compilation                      Processor</span><br><span class="line">pts/build-ffmpeg            Timed FFmpeg Compilation                          Processor</span><br><span class="line">pts/build-gcc               Timed GCC Compilation                             Processor</span><br><span class="line">pts/build-gdb               Timed GDB GNU Debugger Compilation                Processor</span><br><span class="line">pts/build-gem5              Timed Gem5 Compilation                            Processor</span><br><span class="line">pts/build-godot             Timed Godot Game Engine Compilation               Processor</span><br><span class="line">pts/build-imagemagick       Timed ImageMagick Compilation                     Processor</span><br><span class="line">pts/build-linux-kernel      Timed Linux Kernel Compilation                    Processor</span><br><span class="line">pts/build-llvm              Timed LLVM Compilation                            Processor</span><br><span class="line">pts/build-mesa              Timed Mesa Compilation                            Processor</span><br><span class="line">pts/build-mplayer           Timed MPlayer Compilation                         Processor</span><br><span class="line">pts/build-nodejs            Timed Node.js Compilation                         Processor</span><br><span class="line">pts/build-php               Timed PHP Compilation                             Processor</span><br><span class="line">pts/build-python            Timed CPython Compilation                         Processor</span><br><span class="line">pts/build-wasmer            Timed Wasmer Compilation                          Processor</span><br><span class="line">pts/build2                  Build2                                            Processor</span><br><span class="line">pts/bullet                  Bullet Physics Engine                             Processor</span><br><span class="line">pts/byte                    BYTE Unix Benchmark                               Processor</span><br><span class="line">pts/c-ray                   C-Ray                                             Processor</span><br><span class="line">pts/cachebench              CacheBench                                        Processor</span><br><span class="line">pts/caffe                   Caffe                                             System</span><br><span class="line">pts/cassandra               Apache Cassandra                                  System</span><br><span class="line">pts/chia-vdf                Chia Blockchain VDF                               Processor</span><br><span class="line">pts/clickhouse              ClickHouse                                        System</span><br><span class="line">pts/clomp                   CLOMP                                             Processor</span><br><span class="line">pts/cloudsuite-da           CloudSuite Data Analytics                         System</span><br><span class="line">pts/cloudsuite-ga           CloudSuite Graph Analytics                        System</span><br><span class="line">pts/cloudsuite-ma           CloudSuite In-Memory Analytics                    System</span><br><span class="line">pts/cloudsuite-ms           CloudSuite Media Streaming                        System</span><br><span class="line">pts/cloudsuite-ws           CloudSuite Web Serving                            System</span><br><span class="line">pts/cloverleaf              CloverLeaf                                        Processor</span><br><span class="line">pts/cockroach               CockroachDB                                       System</span><br><span class="line">pts/comd-cl                 CoMD OpenCL                                       System</span><br><span class="line">pts/compilebench            Compile Bench                                     Disk</span><br><span class="line">pts/compress-7zip           7-Zip Compression                                 Processor</span><br><span class="line">pts/compress-gzip           Gzip Compression                                  Processor</span><br><span class="line">pts/compress-lz4            LZ4 Compression                                   Processor</span><br><span class="line">pts/compress-pbzip2         Parallel BZIP2 Compression                        Processor</span><br><span class="line">pts/compress-rar            RAR Compression                                   System</span><br><span class="line">pts/compress-xz             XZ Compression                                    Processor</span><br><span class="line">pts/compress-zstd           Zstd Compression                                  Processor</span><br><span class="line">pts/core-latency            Core-Latency                                      Processor</span><br><span class="line">pts/coremark                Coremark                                          Processor</span><br><span class="line">pts/couchdb                 Apache CouchDB                                    System</span><br><span class="line">pts/cp2k                    CP2K Molecular Dynamics                           Processor</span><br><span class="line">pts/cpp-perf-bench          CppPerformanceBenchmarks                          System</span><br><span class="line">pts/cpuminer-opt            Cpuminer-Opt                                      Processor</span><br><span class="line">pts/crafty                  Crafty                                            Processor</span><br><span class="line">pts/cryptopp                Crypto++                                          Processor</span><br><span class="line">pts/ctx-clock               ctx_clock                                         System</span><br><span class="line">pts/cyclictest              Cyclictest                                        System</span><br><span class="line">pts/cython-bench            Cython Benchmark                                  Processor</span><br><span class="line">pts/dacapobench             DaCapo Benchmark                                  Processor</span><br><span class="line">pts/daphne                  Darmstadt Automotive Parallel Heterogeneous Suite System</span><br><span class="line">pts/dav1d                   dav1d                                             Processor</span><br><span class="line">pts/dbench                  Dbench                                            Disk</span><br><span class="line">pts/deeprec                 DeepRec                                           System</span><br><span class="line">pts/deepsparse              Neural Magic DeepSparse                           System</span><br><span class="line">pts/deepspeech              DeepSpeech                                        Processor</span><br><span class="line">pts/dolfyn                  Dolfyn                                            Processor</span><br><span class="line">pts/draco                   Google Draco                                      System</span><br><span class="line">pts/dragonflydb             Dragonflydb                                       System</span><br><span class="line">pts/duckdb                  DuckDB                                            System</span><br><span class="line">pts/easywave                easyWave                                          Processor</span><br><span class="line">pts/ebizzy                  ebizzy                                            Processor</span><br><span class="line">pts/ecp-candle              ECP-CANDLE                                        System</span><br><span class="line">pts/embree                  Embree                                            Processor</span><br><span class="line">pts/encode-flac             FLAC Audio Encoding                               Processor</span><br><span class="line">pts/encode-mp3              LAME MP3 Encoding                                 Processor</span><br><span class="line">pts/encode-opus             Opus Codec Encoding                               Processor</span><br><span class="line">pts/encode-wavpack          WavPack Audio Encoding                            System</span><br><span class="line">pts/encodec                 EnCodec                                           System</span><br><span class="line">pts/espeak                  eSpeak-NG Speech Engine                           Processor</span><br><span class="line">pts/etcd                    etcd                                              System</span><br><span class="line">pts/etcpak                  Etcpak                                            Processor</span><br><span class="line">pts/ethr                    Ethr                                              Network</span><br><span class="line">pts/etqw-demo-iqc           ET: Quake Wars Image Quality                      System</span><br><span class="line">pts/faiss                   Faiss                                             System</span><br><span class="line">pts/fast-cli                fast-cli                                          Network</span><br><span class="line">pts/ffmpeg                  FFmpeg                                            Processor</span><br><span class="line">pts/ffte                    FFTE                                              Processor</span><br><span class="line">pts/fftw                    FFTW                                              Processor</span><br><span class="line">pts/fhourstones             Fhourstones                                       Processor</span><br><span class="line">pts/financebench            FinanceBench                                      System</span><br><span class="line">pts/fio                     Flexible IO Tester                                Disk</span><br><span class="line">pts/fs-mark                 FS-Mark                                           Disk</span><br><span class="line">pts/gcrypt                  Gcrypt Library                                    Processor</span><br><span class="line">pts/geekbench               Geekbench                                         System</span><br><span class="line">pts/git                     Git                                               System</span><br><span class="line">pts/glibc-bench             Glibc Benchmarks                                  OS</span><br><span class="line">pts/gmpbench                GNU GMP GMPbench                                  Processor</span><br><span class="line">pts/gnupg                   GnuPG                                             System</span><br><span class="line">pts/go-benchmark            Go Benchmarks                                     Processor</span><br><span class="line">pts/gpaw                    GPAW                                              System</span><br><span class="line">pts/graph500                Graph500                                          System</span><br><span class="line">pts/graphics-magick         GraphicsMagick                                    Processor</span><br><span class="line">pts/gromacs                 GROMACS                                           System</span><br><span class="line">pts/hackbench               Hackbench                                         Processor</span><br><span class="line">pts/hadoop                  Apache Hadoop                                     System</span><br><span class="line">pts/hammerdb-mariadb        HammerDB - MariaDB                                System</span><br><span class="line">pts/hammerdb-postgresql     HammerDB - PostgreSQL                             System</span><br><span class="line">pts/hbase                   Apache HBase                                      System</span><br><span class="line">pts/hdparm-read             hdparm Timed Disk Reads                           Disk</span><br><span class="line">pts/heffte                  HeFFTe - Highly Efficient FFT <span class="keyword">for</span> Exascale        Processor</span><br><span class="line">pts/helsing                 Helsing                                           Processor</span><br><span class="line">pts/himeno                  Himeno Benchmark                                  Processor</span><br><span class="line">pts/hint                    Hierarchical INTegration                          System</span><br><span class="line">pts/hmmer                   Timed HMMer Search                                Processor</span><br><span class="line">pts/hpcc                    HPC Challenge                                     Processor</span><br><span class="line">pts/hpcg                    High Performance Conjugate Gradient               Processor</span><br><span class="line">pts/idle                    Timed Idle                                        System</span><br><span class="line">pts/idle-power-usage        Idle Power Usage                                  System</span><br><span class="line">pts/incompact3d             Xcompact3d Incompact3d                            Processor</span><br><span class="line">pts/indigobench             IndigoBench                                       System</span><br><span class="line">pts/influxdb                InfluxDB                                          System</span><br><span class="line">pts/intel-mlc               Intel Memory Latency Checker                      Memory</span><br><span class="line">pts/intel-mpi               Intel MPI Benchmarks                              System</span><br><span class="line">pts/intel-tensorflow        Intel TensorFlow                                  System</span><br><span class="line">pts/ior                     IOR                                               Disk</span><br><span class="line">pts/ipc-benchmark           IPC_benchmark                                     Processor</span><br><span class="line">pts/iperf                   iPerf                                             Network</span><br><span class="line">pts/java-gradle-perf        Java Gradle Build                                 Processor</span><br><span class="line">pts/java-jmh                Java JMH                                          System</span><br><span class="line">pts/java-scimark2           Java SciMark                                      Processor</span><br><span class="line">pts/jgfxbat                 Java Graphics Basic Acceptance Test               Processor</span><br><span class="line">pts/john-the-ripper         John The Ripper                                   Processor</span><br><span class="line">pts/jpegxl                  JPEG XL libjxl                                    Processor</span><br><span class="line">pts/jpegxl-decode           JPEG XL Decoding libjxl                           Processor</span><br><span class="line">pts/juliagpu                JuliaGPU                                          System</span><br><span class="line">pts/keydb                   KeyDB                                             System</span><br><span class="line">pts/kripke                  Kripke                                            System</span><br><span class="line">pts/kvazaar                 Kvazaar                                           Processor</span><br><span class="line">pts/laghos                  Laghos                                            Processor</span><br><span class="line">pts/lammps                  LAMMPS Molecular Dynamics Simulator               Processor</span><br><span class="line">pts/lczero                  LeelaChessZero                                    Processor</span><br><span class="line">pts/leveldb                 LevelDB                                           Disk</span><br><span class="line">pts/libgav1                 libgav1                                           Processor</span><br><span class="line">pts/libraw                  LibRaw                                            Processor</span><br><span class="line">pts/libxsmm                 libxsmm                                           Processor</span><br><span class="line">pts/liquid-dsp              Liquid-DSP                                        Processor</span><br><span class="line">pts/luajit                  LuaJIT                                            Processor</span><br><span class="line">pts/luaradio                LuaRadio                                          Processor</span><br><span class="line">pts/lulesh                  LULESH                                            Processor</span><br><span class="line">pts/lulesh-cl               Lulesh OpenCL                                     System</span><br><span class="line">pts/luxcorerender           LuxCoreRender                                     Processor</span><br><span class="line">pts/luxmark                 LuxMark                                           System</span><br><span class="line">pts/lzbench                 lzbench                                           Processor</span><br><span class="line">pts/m-queens                m-queens                                          Processor</span><br><span class="line">pts/mafft                   Timed MAFFT Alignment                             Processor</span><br><span class="line">pts/mandelbulbgpu           MandelbulbGPU                                     System</span><br><span class="line">pts/mandelgpu               MandelGPU                                         System</span><br><span class="line">pts/mbw                     MBW                                               Memory</span><br><span class="line">pts/memcached               Memcached                                         System</span><br><span class="line">pts/memtier-benchmark       Redis 7.0.12 + memtier_benchmark                  System</span><br><span class="line">pts/mencoder                Mencoder                                          Processor</span><br><span class="line">pts/minibude                miniBUDE                                          Processor</span><br><span class="line">pts/minife                  miniFE                                            Processor</span><br><span class="line">pts/minion                  Minion                                            Processor</span><br><span class="line">pts/mkl-dnn                 oneDNN MKL-DNN                                    Processor</span><br><span class="line">pts/mlpack                  Mlpack Benchmark                                  System</span><br><span class="line">pts/mnn                     Mobile Neural Network                             System</span><br><span class="line">pts/mocassin                Monte Carlo Simulations of Ionised Nebulae        Processor</span><br><span class="line">pts/montage                 Montage Astronomical Image Mosaic Engine          Processor</span><br><span class="line">pts/mpcbench                GNU MPC                                           Processor</span><br><span class="line">pts/mrbayes                 Timed MrBayes Analysis                            Processor</span><br><span class="line">pts/mt-dgemm                ACES DGEMM                                        Processor</span><br><span class="line">pts/multichase              Multichase Pointer Chaser                         Processor</span><br><span class="line">pts/mutex                   BenchmarkMutex                                    System</span><br><span class="line">pts/mysqlslap               MariaDB                                           System</span><br><span class="line">pts/n-queens                N-Queens                                          Processor</span><br><span class="line">pts/namd                    NAMD                                              Processor</span><br><span class="line">pts/natron                  Natron                                            System</span><br><span class="line">pts/ncnn                    NCNN                                              System</span><br><span class="line">pts/neat                    Nebular Empirical Analysis Tool                   Processor</span><br><span class="line">pts/neatbench               NeatBench                                         System</span><br><span class="line">pts/nekrs                   nekRS                                             Processor</span><br><span class="line">pts/nero2d                  Open FMM Nero2D                                   Processor</span><br><span class="line">pts/netperf                 Netperf                                           Network</span><br><span class="line">pts/nettle                  Nettle                                            Processor</span><br><span class="line">pts/network-loopback        Loopback TCP Network Performance                  Network</span><br><span class="line">pts/nginx                   nginx                                             System</span><br><span class="line">pts/ngspice                 Ngspice                                           Processor</span><br><span class="line">pts/node-express-loadtest   Node.js Express HTTP Load Test                    Processor</span><br><span class="line">pts/node-octane             Node.js Octane Benchmark                          Processor</span><br><span class="line">pts/node-web-tooling        Node.js V8 Web Tooling Benchmark                  Processor</span><br><span class="line">pts/novabench               Novabench                                         System</span><br><span class="line">pts/npb                     NAS Parallel Benchmarks                           Processor</span><br><span class="line">pts/numenta-nab             Numenta Anomaly Benchmark                         System</span><br><span class="line">pts/numpy                   Numpy Benchmark                                   Processor</span><br><span class="line">pts/nuttcp                  Nuttcp                                            Network</span><br><span class="line">pts/nwchem                  NWChem                                            Processor</span><br><span class="line">pts/oidn                    Intel Open Image Denoise                          Processor</span><br><span class="line">pts/oneapi-level-zero       oneAPI Level Zero Tests                           System</span><br><span class="line">pts/onednn                  oneDNN                                            Processor</span><br><span class="line">pts/onnx                    ONNX Runtime                                      System</span><br><span class="line">pts/open-porous-media       Open Porous Media                                 Processor</span><br><span class="line">pts/opencv                  OpenCV                                            System</span><br><span class="line">pts/opencv-bench            OpenCV Benchmark                                  Processor</span><br><span class="line">pts/opendwarfs              OpenDwarfs                                        System</span><br><span class="line">pts/openems                 OpenEMS                                           System</span><br><span class="line">pts/openfoam                OpenFOAM                                          Processor</span><br><span class="line">pts/openjpeg                OpenJPEG                                          Processor</span><br><span class="line">pts/openradioss             OpenRadioss                                       Processor</span><br><span class="line">pts/openssl                 OpenSSL                                           Processor</span><br><span class="line">pts/openvino                OpenVINO                                          System</span><br><span class="line">pts/openvkl                 OpenVKL                                           Processor</span><br><span class="line">pts/optcarrot               Optcarrot                                         System</span><br><span class="line">pts/osbench                 OSBench                                           OS</span><br><span class="line">pts/ospray                  OSPRay                                            Processor</span><br><span class="line">pts/ospray-studio           OSPRay Studio                                     Processor</span><br><span class="line">pts/palabos                 Palabos                                           Processor</span><br><span class="line">pts/parboil                 Parboil                                           Processor</span><br><span class="line">pts/pennant                 Pennant                                           Processor</span><br><span class="line">pts/perf-bench              perf-bench                                        OS</span><br><span class="line">pts/perl-benchmark          Perl Benchmarks                                   Processor</span><br><span class="line">pts/petsc                   PETSc                                             System</span><br><span class="line">pts/pgbench                 PostgreSQL                                        System</span><br><span class="line">pts/php                     PHP Micro Benchmarks                              System</span><br><span class="line">pts/phpbench                PHPBench                                          System</span><br><span class="line">pts/pjdfstest               Pjdfstest                                         Disk</span><br><span class="line">pts/pjsip                   PJSIP                                             Processor</span><br><span class="line">pts/plaidml                 PlaidML                                           System</span><br><span class="line">pts/pmbench                 pmbench                                           Memory</span><br><span class="line">pts/polybench-c             PolyBench-C                                       Processor</span><br><span class="line">pts/polyhedron              Polyhedron Fortran Benchmarks                     Processor</span><br><span class="line">pts/postmark                PostMark                                          Disk</span><br><span class="line">pts/povray                  POV-Ray                                           Processor</span><br><span class="line">pts/powertop-wakeups        Powertop Wakeups                                  Processor</span><br><span class="line">pts/primesieve              Primesieve                                        Processor</span><br><span class="line">pts/pts-self-test           Phoronix Test Suite Self Test                     System</span><br><span class="line">pts/pybench                 PyBench                                           System</span><br><span class="line">pts/pyhpc                   PyHPC Benchmarks                                  System</span><br><span class="line">pts/pymongo-inserts         PyMongo Inserts                                   System</span><br><span class="line">pts/pyperformance           PyPerformance                                     System</span><br><span class="line">pts/qe                      Quantum ESPRESSO                                  Processor</span><br><span class="line">pts/qmcpack                 QMCPACK                                           Processor</span><br><span class="line">pts/qmlbench                Qmlbench                                          System</span><br><span class="line">pts/quadray                 QuadRay                                           Processor</span><br><span class="line">pts/quantlib                QuantLib                                          Processor</span><br><span class="line">pts/rabbitmq                RabbitMQ                                          System</span><br><span class="line">pts/radiance                Radiance Benchmark                                Processor</span><br><span class="line">pts/ramspeed                RAMspeed SMP                                      Memory</span><br><span class="line">pts/rav1e                   rav1e                                             Processor</span><br><span class="line">pts/rays1bench              rays1bench                                        Processor</span><br><span class="line">pts/rbenchmark              R Benchmark                                       Processor</span><br><span class="line">pts/redis                   Redis                                             System</span><br><span class="line">pts/relion                  RELION                                            Processor</span><br><span class="line">pts/remhos                  Remhos                                            Processor</span><br><span class="line">pts/renaissance             Renaissance                                       Processor</span><br><span class="line">pts/rnnoise                 RNNoise                                           Processor</span><br><span class="line">pts/rocksdb                 RocksDB                                           System</span><br><span class="line">pts/rodinia                 Rodinia                                           Processor</span><br><span class="line">pts/rust-mandel             Rust Mandelbrot                                   Processor</span><br><span class="line">pts/rust-prime              Rust Prime Benchmark                              Processor</span><br><span class="line">pts/schbench                Schbench                                          System</span><br><span class="line">pts/scikit-learn            Scikit-Learn                                      System</span><br><span class="line">pts/scimark2                SciMark                                           Processor</span><br><span class="line">pts/securemark              SecureMark                                        Processor</span><br><span class="line">pts/serial-loopback         Serial Loopback Test                              System</span><br><span class="line">pts/simdjson                simdjson                                          Processor</span><br><span class="line">pts/smallpt                 Smallpt                                           Processor</span><br><span class="line">pts/smallpt-gpu             SmallPT GPU                                       System</span><br><span class="line">pts/smart                   SMART Disk Self-Report                            Disk</span><br><span class="line">pts/smhasher                SMHasher                                          Processor</span><br><span class="line">pts/sockperf                Sockperf                                          OS</span><br><span class="line">pts/spacy                   spaCy                                             System</span><br><span class="line">pts/spark                   Apache Spark                                      System</span><br><span class="line">pts/spec-cpu2017            SPEC CPU 2017                                     System</span><br><span class="line">pts/spec-jbb2015            SPECjbb 2015                                      System</span><br><span class="line">pts/specfem3d               SPECFEM3D                                         Processor</span><br><span class="line">pts/speedtest-cli           speedtest-cli                                     Network</span><br><span class="line">pts/sqlite                  SQLite                                            Disk</span><br><span class="line">pts/sqlite-speedtest        SQLite Speedtest                                  System</span><br><span class="line">pts/srslte                  srsLTE                                            Processor</span><br><span class="line">pts/srsran                  srsRAN Project                                    Processor</span><br><span class="line">pts/stargate                Stargate Digital Audio Workstation                Processor</span><br><span class="line">pts/startup-time            Application Start-up Time                         Disk</span><br><span class="line">pts/stockfish               Stockfish                                         Processor</span><br><span class="line">pts/stream                  Stream                                            Memory</span><br><span class="line">pts/stream-dynamic          Stream-Dynamic                                    Memory</span><br><span class="line">pts/stress-ng               Stress-NG                                         System</span><br><span class="line">pts/stressapptest           Stressful Application Test                        Memory</span><br><span class="line">pts/sudokut                 Sudokut                                           Processor</span><br><span class="line">pts/sunflow                 Sunflow Rendering System                          System</span><br><span class="line">pts/svt-av1                 SVT-AV1                                           Processor</span><br><span class="line">pts/svt-hevc                SVT-HEVC                                          Processor</span><br><span class="line">pts/svt-vp9                 SVT-VP9                                           Processor</span><br><span class="line">pts/swet                    Swet                                              Processor</span><br><span class="line">pts/synthmark               Google SynthMark                                  Processor</span><br><span class="line">pts/sysbench                Sysbench                                          System</span><br><span class="line">pts/system-decompress-bzip2 System BZIP2 Decompression                        Processor</span><br><span class="line">pts/system-decompress-gzip  System GZIP Decompression                         Processor</span><br><span class="line">pts/system-decompress-tiff  System Libtiff Decompression                      Processor</span><br><span class="line">pts/system-decompress-xz    System XZ Decompression                           Processor</span><br><span class="line">pts/system-libjpeg          System JPEG Library Decode                        Processor</span><br><span class="line">pts/system-libxml2          System Libxml2 Parsing                            Processor</span><br><span class="line">pts/systemd-boot-kernel     Systemd Kernel Boot Time                          Processor</span><br><span class="line">pts/systemd-boot-total      Systemd Total Boot Time                           System</span><br><span class="line">pts/systemd-boot-userspace  Systemd Userspace Boot Time                       Processor</span><br><span class="line">pts/t-test1                 t-test1                                           Memory</span><br><span class="line">pts/tachyon                 Tachyon                                           Processor</span><br><span class="line">pts/tensorflow              TensorFlow                                        System</span><br><span class="line">pts/tensorflow-lite         TensorFlow Lite                                   System</span><br><span class="line">pts/tidb                    TiDB Community Server                             System</span><br><span class="line">pts/tinymembench            Tinymembench                                      Memory</span><br><span class="line">pts/tiobench                Threaded I/O Tester                               Disk</span><br><span class="line">pts/tjbench                 libjpeg-turbo tjbench                             System</span><br><span class="line">pts/tnn                     TNN                                               System</span><br><span class="line">pts/toktx                   KTX-Software toktx                                System</span><br><span class="line">pts/toybrot                 toyBrot Fractal Generator                         Processor</span><br><span class="line">pts/tscp                    TSCP                                              Processor</span><br><span class="line">pts/ttsiod-renderer         TTSIOD 3D Renderer                                Processor</span><br><span class="line">pts/tungsten                Tungsten Renderer                                 Processor</span><br><span class="line">pts/unpack-firefox          Unpacking Firefox                                 System</span><br><span class="line">pts/unpack-linux            Unpacking The Linux Kernel                        Disk</span><br><span class="line">pts/uvg266                  uvg266                                            Processor</span><br><span class="line">pts/v-ray                   Chaos Group V-RAY                                 System</span><br><span class="line">pts/viennacl                ViennaCL                                          System</span><br><span class="line">pts/vosk                    VOSK Speech Recognition Toolkit                   Processor</span><br><span class="line">pts/vpxenc                  VP9 libvpx Encoding                               Processor</span><br><span class="line">pts/vvenc                   VVenC                                             Processor</span><br><span class="line">pts/webp                    WebP Image Encode                                 Processor</span><br><span class="line">pts/webp2                   WebP2 Image Encode                                Processor</span><br><span class="line">pts/whisper-cpp             Whisper.cpp                                       System</span><br><span class="line">pts/will-it-scale           will-it-scale                                     OS</span><br><span class="line">pts/wrf                     WRF                                               System</span><br><span class="line">pts/x264                    x264                                              Processor</span><br><span class="line">pts/x265                    x265                                              Processor</span><br><span class="line">pts/xmrig                   Xmrig                                             Processor</span><br><span class="line">pts/xsbench                 Xsbench                                           System</span><br><span class="line">pts/xsbench-cl              Xsbench OpenCL                                    System</span><br><span class="line">pts/y-cruncher              Y-Cruncher                                        Processor</span><br><span class="line">pts/yafaray                 YafaRay                                           Processor</span><br><span class="line">pts/yugabytedb              YugabyteDB                                        System</span><br><span class="line">pts/z3                      Z3 Theorem Prover                                 Processor</span><br><span class="line">pts/zendnn-tensorflow       AMD ZenDNN TensorFlow                             System</span><br><span class="line">system/apache               Apache Benchmark</span><br><span class="line">system/blender              Blender                                           System</span><br><span class="line">system/blogbench            BlogBench                                         Disk</span><br><span class="line">system/cephfs-rados         CephFS RADOS Benchmark                            System</span><br><span class="line">system/clpeak               clpeak                                            System</span><br><span class="line">system/compress-lzma        LZMA Compression</span><br><span class="line">system/compress-pbzip2      PBZIP2 Compression</span><br><span class="line">system/compress-zstd        Zstd Compression                                  Processor</span><br><span class="line">system/cryptsetup           Cryptsetup                                        System</span><br><span class="line">system/darktable            Darktable                                         System</span><br><span class="line">system/dbench               Dbench                                            Disk</span><br><span class="line">system/ethminer             Ethereum Ethminer                                 System</span><br><span class="line">system/fio                  Flexible IO Tester                                Disk</span><br><span class="line">system/gegl                 GEGL                                              System</span><br><span class="line">system/gimp                 GIMP                                              System</span><br><span class="line">system/gmic                 GMIC                                             System</span><br><span class="line">system/gnupg                GnuPG                                             Processor</span><br><span class="line">system/gnuradio             GNU Radio                                         Processor</span><br><span class="line">system/hugin                Hugin                                             System</span><br><span class="line">system/inkscape             Inkscape                                          System</span><br><span class="line">system/iozone               IOzone                                            Disk</span><br><span class="line">system/libreoffice          LibreOffice                                       System</span><br><span class="line">system/mpv                  MPV                                               System</span><br><span class="line">system/nginx                Nginx                                             System</span><br><span class="line">system/ocrmypdf             OCRMyPDF                                          System</span><br><span class="line">system/octave-benchmark     GNU Octave Benchmark                              System</span><br><span class="line">system/openscad             OpenSCAD                                          System</span><br><span class="line">system/openssl              OpenSSL                                           Processor</span><br><span class="line">system/opm                  Open Porous Media                                 Processor</span><br><span class="line">system/rawtherapee          RawTherapee                                       System</span><br><span class="line">system/redis                Redis Memtier / Redis Benchmark                   System</span><br><span class="line">system/rsvg                 librsvg                                           System</span><br><span class="line">system/selenium             Selenium                                          System</span><br><span class="line">system/selenium-top-sites   Time To Load + View Popular Websites              System</span><br><span class="line">system/sqlite               SQLite</span><br><span class="line">system/tesseract-ocr        Tesseract OCR                                     System</span><br><span class="line">system/wireguard            WireGuard + Linux Networking Stack Stress Test    Network</span><br><span class="line">git/aom-av1                 AOM AV1                                           Processor</span><br><span class="line">git/dav1d                   dav1d                                             Processor</span><br><span class="line">git/rav1e                   rav1e                                             Processor</span><br><span class="line">git/svt-av1                 SVT-AV1                                           Processor</span><br><span class="line">git/svt-hevc                SVT-HEVC                                          Processor</span><br><span class="line">git/svt-vp9                 SVT-VP9                                           Processor</span><br><span class="line">git/vpxenc                  VP9 libvpx Encoding                               Processor</span><br><span class="line">git/x265                    x265                                              Processor</span><br></pre></td></tr></table></figure>

<h2 id="Get-info-for-testsuite-test"><a href="#Get-info-for-testsuite-test" class="headerlink" title="Get info for testsuite&#x2F;test"></a>Get info for testsuite&#x2F;test</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># phoronix-test-suite info disk</span></span><br><span class="line"></span><br><span class="line">Phoronix Test Suite v10.8.4</span><br><span class="line">Disk Test Suite</span><br><span class="line"></span><br><span class="line">Suite Description:  This <span class="built_in">test</span> suite is designed to contain real-world disk and file-system tests.</span><br><span class="line"></span><br><span class="line">Run Identifier:     pts/disk-1.3.2</span><br><span class="line">Suite Version:      1.3.2</span><br><span class="line">Maintainer:         Michael Larabel</span><br><span class="line">Status:</span><br><span class="line">Suite Type:         Disk</span><br><span class="line">Unique Tests:       8</span><br><span class="line">Contained Tests:</span><br><span class="line">                    SQLite              Threads / Copies: 1</span><br><span class="line">                    SQLite              Threads / Copies: 2</span><br><span class="line">                    SQLite              Threads / Copies: 4</span><br><span class="line">                    SQLite              Threads / Copies: 8</span><br><span class="line">                    SQLite              Threads / Copies: 16</span><br><span class="line">                    SQLite              Threads / Copies: 32</span><br><span class="line">                    SQLite              Threads / Copies: 48</span><br><span class="line">                    FS-Mark             Test: 1000 Files, 1MB Size</span><br><span class="line">                    FS-Mark             Test: 1000 Files, 1MB Size, No Sync/FSync</span><br><span class="line">                    FS-Mark             Test: 5000 Files, 1MB Size, 4 Threads</span><br><span class="line">                    FS-Mark             Test: 4000 Files, 32 Sub Dirs, 1MB Size</span><br><span class="line">                    Compile Bench       Test: Initial Create</span><br><span class="line">                    Compile Bench       Test: Compile</span><br><span class="line">                    Compile Bench       Test: Read Compiled Tree</span><br><span class="line">                    IOR                 Block Size: 2MB - Disk Target: Default Test Directory</span><br><span class="line">                    IOR                 Block Size: 2MB - Disk Target: /home</span><br><span class="line">                    IOR                 Block Size: 2MB - Disk Target: /</span><br><span class="line">                    IOR                 Block Size: 4MB - Disk Target: Default Test Directory</span><br><span class="line">                    IOR                 Block Size: 4MB - Disk Target: /home</span><br><span class="line">                    IOR                 Block Size: 4MB - Disk Target: /</span><br><span class="line">                    IOR                 Block Size: 8MB - Disk Target: Default Test Directory</span><br><span class="line">                    IOR                 Block Size: 8MB - Disk Target: /home</span><br><span class="line">                    IOR                 Block Size: 8MB - Disk Target: /</span><br><span class="line">                    IOR                 Block Size: 16MB - Disk Target: Default Test Directory</span><br><span class="line">                    IOR                 Block Size: 16MB - Disk Target: /home</span><br><span class="line">                    IOR                 Block Size: 16MB - Disk Target: /</span><br><span class="line">                    IOR                 Block Size: 32MB - Disk Target: Default Test Directory</span><br><span class="line">                    IOR                 Block Size: 32MB - Disk Target: /home</span><br><span class="line">                    IOR                 Block Size: 32MB - Disk Target: /</span><br><span class="line">                    IOR                 Block Size: 64MB - Disk Target: Default Test Directory</span><br><span class="line">                    IOR                 Block Size: 64MB - Disk Target: /home</span><br><span class="line">                    IOR                 Block Size: 64MB - Disk Target: /</span><br><span class="line">                    IOR                 Block Size: 256MB - Disk Target: Default Test Directory</span><br><span class="line">                    IOR                 Block Size: 256MB - Disk Target: /home</span><br><span class="line">                    IOR                 Block Size: 256MB - Disk Target: /</span><br><span class="line">                    IOR                 Block Size: 512MB - Disk Target: Default Test Directory</span><br><span class="line">                    IOR                 Block Size: 512MB - Disk Target: /home</span><br><span class="line">                    IOR                 Block Size: 512MB - Disk Target: /</span><br><span class="line">                    IOR                 Block Size: 1024MB - Disk Target: Default Test Directory</span><br><span class="line">                    IOR                 Block Size: 1024MB - Disk Target: /home</span><br><span class="line">                    IOR                 Block Size: 1024MB - Disk Target: /</span><br><span class="line">                    IOzone              8GB Write Performance</span><br><span class="line">                    Dbench              1 Clients</span><br><span class="line">                    Dbench              12 Clients</span><br><span class="line">                    PostMark</span><br><span class="line">                    Flexible IO Tester  Type: Random Read - IO Engine: Linux AIO - Buffered: No - Direct: Yes - Block Size: 4KB - Disk Target: Default Test Directory</span><br><span class="line">                    Flexible IO Tester  Type: Random Read - IO Engine: Linux AIO - Buffered: No - Direct: Yes - Block Size: 2MB - Disk Target: Default Test Directory</span><br><span class="line">                    Flexible IO Tester  Type: Random Write - IO Engine: Linux AIO - Buffered: No - Direct: Yes - Block Size: 4KB - Disk Target: Default Test Directory</span><br><span class="line">                    Flexible IO Tester  Type: Random Write - IO Engine: Linux AIO - Buffered: No - Direct: Yes - Block Size: 2MB - Disk Target: Default Test Directory</span><br><span class="line">                    Flexible IO Tester  Type: Sequential Read - IO Engine: Linux AIO - Buffered: No - Direct: Yes - Block Size: 4KB - Disk Target: Default Test Directory</span><br><span class="line">                    Flexible IO Tester  Type: Sequential Read - IO Engine: Linux AIO - Buffered: No - Direct: Yes - Block Size: 2MB - Disk Target: Default Test Directory</span><br><span class="line">                    Flexible IO Tester  Type: Sequential Write - IO Engine: Linux AIO - Buffered: No - Direct: Yes - Block Size: 4KB - Disk Target: Default Test Directory</span><br><span class="line">                    Flexible IO Tester  Type: Sequential Write - IO Engine: Linux AIO - Buffered: No - Direct: Yes - Block Size: 2MB - Disk Target: Default Test Directory</span><br><span class="line">                    60 Tests / 8 Unique Tests</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@vm1 phoronix-test-suite]<span class="comment"># phoronix-test-suite info postmark</span></span><br><span class="line"></span><br><span class="line">Phoronix Test Suite v10.8.4</span><br><span class="line">PostMark 1.51</span><br><span class="line"></span><br><span class="line">Run Identifier:          pts/postmark-1.1.2</span><br><span class="line">Profile Version:         1.1.2</span><br><span class="line">Maintainer:              Michael Larabel</span><br><span class="line">Test Type:               Disk</span><br><span class="line">Software Type:           Utility</span><br><span class="line">License Type:            Free</span><br><span class="line">Test Status:             Verified</span><br><span class="line">Supported Platforms:     Linux, Solaris, MacOSX, BSD</span><br><span class="line">Project Web-Site:</span><br><span class="line">Estimated Run-Time:      150 Seconds</span><br><span class="line">Estimated Install Time:  2 Seconds</span><br><span class="line">Download Size:           0.01 MB</span><br><span class="line">Environment Size:        1 MB</span><br><span class="line"></span><br><span class="line">Description: This is a <span class="built_in">test</span> of NetApp<span class="string">&#x27;s PostMark benchmark designed to simulate small-file testing similar to the tasks endured by web and mail servers. This test profile will set PostMark to perform 25,000 transactions with 500 files simultaneously with the file sizes ranging between 5 and 512 kilobytes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">OpenBenchmarking.org Test Profile: https://openbenchmarking.org/test/pts/postmark-1.1.2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Test Installed:    Yes</span></span><br><span class="line"><span class="string">Last Run:          2023-11-14</span></span><br><span class="line"><span class="string">Install Time:      2 Seconds</span></span><br><span class="line"><span class="string">Install Size:      120 Bytes</span></span><br><span class="line"><span class="string">Average Run-Time:  2 Minutes, 30 Seconds</span></span><br><span class="line"><span class="string">Latest Run-Time:   13 Minutes, 41 Seconds</span></span><br><span class="line"><span class="string">Times Run:         2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Software Dependencies:</span></span><br><span class="line"><span class="string">- C/C++ Compiler Toolchain</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">OpenBenchmarking.org Overview Metrics:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Disk Transaction Performance</span></span><br><span class="line"><span class="string">[Performance Overview] Average Deviation Between Runs: 1.62% Sample Analysis Count: 14756</span></span><br><span class="line"><span class="string">    Comparison of 14,756 OpenBenchmarking.org samples since 26 February 2011 to 12 February 2022; median result: 3363 TPS. Box plot of samples:</span></span><br><span class="line"><span class="string">    [ |------*---------------------------------################################################!##################*#############################*----------------------------------*------------------------------------------|              ]</span></span><br><span class="line"><span class="string">             ^ 10th Percentile: 340                                                         60th Percentile: 4076 ^</span></span><br><span class="line"><span class="string">                                                                                                                          75th Percentile: 5173 ^</span></span><br><span class="line"><span class="string">                                                                                                                                                             90th Percentile: 6466 ^</span></span><br><span class="line"><span class="string">[Run-Time Requirements] Average Run-Time: 8 Minutes</span></span><br><span class="line"><span class="string">    Comparison of 3,384 OpenBenchmarking.org samples since 26 February 2011 to 12 February 2022; median result: 198 Seconds. Box plot of samples:</span></span><br><span class="line"><span class="string">    [                                                                              |------------------------------------------------------------------------------------------------------------------*----------------------#*#####!#*#*|   ]</span></span><br><span class="line"><span class="string">                                                                                                                                                                                 10th Percentile: 840 ^  25th Percentile: 334 ^</span></span><br><span class="line"><span class="string">                                                                                                                                                                                                                 60th Percentile: 163 ^</span></span><br><span class="line"><span class="string">                                                                                                                                                                                                                   75th Percentile: 130 ^</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">OpenBenchmarking.org Workload Analysis:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Shared Libraries Used By This Test: libc.so.6</span></span><br><span class="line"><span class="string">Notable Instructions Used By Test On Capable CPUs: MMX, SSE, SSE2</span></span><br><span class="line"><span class="string">Instructions Possible On Capable CPUs With Extra Compiler Flags: MMX, SSE, SSE2, AVX, AVX2, AVX512</span></span><br><span class="line"><span class="string">Honors CFLAGS/CXXFLAGS: Yes</span></span><br><span class="line"><span class="string">Tested CPU Architectures: aarch64, armv5tel, armv6l, armv7l, i386, i686, mips, mips64, ppc, ppc64, ppc64le, sparc64, x86_64</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Test Profile Change History:</span></span><br><span class="line"><span class="string">v1.1.2 - 8 February 2020</span></span><br><span class="line"><span class="string">Update download mirrors.</span></span><br><span class="line"><span class="string">v1.1.1 - 28 May 2018</span></span><br><span class="line"><span class="string">Update download URLs.</span></span><br><span class="line"><span class="string">v1.1.0 - 24 January 2013</span></span><br><span class="line"><span class="string">Update PostMark download link that was dead, increase the intensity of the PostMark disk run case.</span></span><br><span class="line"><span class="string">v1.0.0 - 6 December 2010</span></span><br><span class="line"><span class="string">Initial import into OpenBenchmarking.org</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Results Containing This Test</span></span><br><span class="line"><span class="string">postmark-result-20231112 postmark-result-20231112</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Test Suites Containing This Test</span></span><br><span class="line"><span class="string">pts/disk   Disk Test Suite</span></span><br><span class="line"><span class="string">pts/kernel Common Kernel Benchmarks</span></span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://phoronix-test-suite.com/">https://phoronix-test-suite.com/</a></li>
<li><a href="https://github.com/phoronix-test-suite/phoronix-test-suite/">https://github.com/phoronix-test-suite/phoronix-test-suite/</a></li>
<li><a href="https://github.com/phoronix-test-suite/phoronix-test-suite/blob/master/documentation/phoronix-test-suite.md">Getting started with phoronix-test-suite</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Benchmarking</category>
      </categories>
      <tags>
        <tag>Phoronix-test-suite</tag>
      </tags>
  </entry>
  <entry>
    <title>sysbench</title>
    <url>/blog/sysbench/</url>
    <content><![CDATA[<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sysbench /root/trg/sysbench/src/lua/oltp_delete.lua --mysql-user=root --mysql-password=0000abc! --mysql-socket=/ssddata/dmain/mysqld.sock --threads=32 --report-interval=10 --rand-type=uniform --time=60 --table_size=1000000 --tables=10 prepare</span><br><span class="line">sysbench /root/trg/sysbench/src/lua/oltp_delete.lua --mysql-user=root --mysql-password=0000abc! --mysql-socket=/ssddata/dmain/mysqld.sock --threads=32 --report-interval=10 --rand-type=uniform --time=60 --table_size=1000000 --tables=10 run</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://github.com/akopytov/sysbench/tree/master">https://github.com/akopytov/sysbench/tree/master</a></li>
<li><a href="https://github.com/akopytov/sysbench/blob/master/src/lua/oltp_common.lua">https://github.com/akopytov/sysbench/blob/master/src/lua/oltp_common.lua</a></li>
<li><a href="https://github.com/akopytov/sysbench/blob/master/src/lua/oltp_delete.lua">https://github.com/akopytov/sysbench/blob/master/src/lua/oltp_delete.lua</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>A collection of top classic coding questions</title>
    <url>/blog/top-classic-coding-questions/</url>
    <content><![CDATA[<p>This is a list of top classic coding questions which help understand various data structure and algorithms.</p>
<span id="more"></span>

<h2 id="Leetcode-206-Reverse-Linked-List"><a href="#Leetcode-206-Reverse-Linked-List" class="headerlink" title="[Leetcode 206] Reverse Linked List"></a>[Leetcode 206] Reverse Linked List</h2><p>Given the head of a singly linked list, reverse the list, and return the reversed list.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [1,2,3,4,5]</span><br><span class="line">Output: [5,4,3,2,1]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [1,2]</span><br><span class="line">Output: [2,1]</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = []</span><br><span class="line">Output: []</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>The number of nodes in the list is the range [0, 5000].</li>
<li>-5000 &lt;&#x3D; Node.val &lt;&#x3D; 5000</li>
</ul>
<p>Follow up: A linked list can be reversed either iteratively or recursively. Could you implement both?</p>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># Time: O(n) Space: O(1)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        prev, curr = <span class="literal">None</span>, head</span><br><span class="line">        <span class="keyword">while</span> curr:</span><br><span class="line">            <span class="built_in">next</span> = curr.<span class="built_in">next</span> <span class="comment"># save next node before we break the pointer</span></span><br><span class="line">            curr.<span class="built_in">next</span> = prev <span class="comment"># point to previous node</span></span><br><span class="line">            prev = curr <span class="comment"># shift previous pointer </span></span><br><span class="line">            curr = <span class="built_in">next</span> <span class="comment"># shift current pointer</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> prev</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Time: O(n) Space: O(1)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseList_recursive</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># e.g.</span></span><br><span class="line">        <span class="comment">#   1 -&gt; 2 -&gt; 3 -&gt; 4</span></span><br><span class="line">        <span class="comment">#   ^    ^</span></span><br><span class="line">        <span class="comment"># head  head.next</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#   4  -&gt;  3  -&gt;  2    -&gt;     1   -&gt; None</span></span><br><span class="line">        <span class="comment">#   ^             ^           ^</span></span><br><span class="line">        <span class="comment"># newHead        head.next   head</span></span><br><span class="line">        newHead = head</span><br><span class="line">        <span class="keyword">if</span> head.<span class="built_in">next</span>:</span><br><span class="line">            newHead = self.reverseList(head.<span class="built_in">next</span>)</span><br><span class="line">            head.<span class="built_in">next</span>.<span class="built_in">next</span> = head</span><br><span class="line">        head.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> newHead</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode-25-Reverse-Nodes-in-k-Group"><a href="#Leetcode-25-Reverse-Nodes-in-k-Group" class="headerlink" title="[Leetcode 25] Reverse Nodes in k-Group"></a>[Leetcode 25] Reverse Nodes in k-Group</h2><p>Given the head of a linked list, reverse the nodes of the list k at a time, and return the modified list.</p>
<p>k is a positive integer and is less than or equal to the length of the linked list. If the number of nodes is not a multiple of k then left-out nodes, in the end, should remain as it is.</p>
<p>You may not alter the values in the list’s nodes, only nodes themselves may be changed.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [1,2,3,4,5], k = 2</span><br><span class="line">Output: [2,1,4,3,5]</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: head = [1,2,3,4,5], k = 3</span><br><span class="line">Output: [3,2,1,4,5]</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>The number of nodes in the list is n.</li>
<li>1 &lt;&#x3D; k &lt;&#x3D; n &lt;&#x3D; 5000</li>
<li>0 &lt;&#x3D; Node.val &lt;&#x3D; 1000</li>
</ul>
<p>Follow-up: Can you solve the problem in O(1) extra memory space?</p>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseKGroup</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode], k: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        <span class="comment"># check if reverse is needed</span></span><br><span class="line">        curr = head</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> curr:</span><br><span class="line">                <span class="keyword">return</span> head <span class="comment"># no need to reverse, thus return the original head</span></span><br><span class="line">            curr = curr.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Reverse the sub-list with k nodes</span></span><br><span class="line">        <span class="comment"># e.g.</span></span><br><span class="line">        <span class="comment"># None  1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6, k = 3</span></span><br><span class="line">        <span class="comment"># ^     ^</span></span><br><span class="line">        <span class="comment"># prev  head(curr) </span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># After reverse the first 3 nodes,</span></span><br><span class="line">        <span class="comment"># 3 -&gt; 2 -&gt; 1 -&gt; 4 -&gt; 5 -&gt; 6</span></span><br><span class="line">        <span class="comment"># ^         ^    ^</span></span><br><span class="line">        <span class="comment"># prev      head curr</span></span><br><span class="line">        prev, curr = <span class="literal">None</span>, head</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            <span class="built_in">next</span> = curr.<span class="built_in">next</span> <span class="comment"># save the next before we break the pointer</span></span><br><span class="line">            curr.<span class="built_in">next</span> = prev <span class="comment"># point to previous node(reverse)</span></span><br><span class="line">            prev = curr <span class="comment"># shift previous pointer</span></span><br><span class="line">            curr = <span class="built_in">next</span> <span class="comment"># shift current pointer </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># after reverse, the curr pointer points to the &quot;head&quot; of next k nodes, </span></span><br><span class="line">        <span class="comment"># and the previous pointer points to the new &quot;head&quot; of the reversed k nodes.</span></span><br><span class="line">        head.<span class="built_in">next</span> = self.reverseKGroup(curr, k) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># the previous pointer points to the head of reversed list</span></span><br><span class="line">        <span class="keyword">return</span> prev</span><br></pre></td></tr></table></figure>

<h2 id="Leetcoe-146-LRU-Cache"><a href="#Leetcoe-146-LRU-Cache" class="headerlink" title="[Leetcoe 146] LRU Cache"></a>[Leetcoe 146] LRU Cache</h2><p>Design a data structure that follows the constraints of a Least Recently Used (LRU) cache.</p>
<p>Implement the LRUCache class:</p>
<ul>
<li>LRUCache(int capacity) Initialize the LRU cache with positive size capacity.</li>
<li>int get(int key) Return the value of the key if the key exists, otherwise return -1.</li>
<li>void put(int key, int value) Update the value of the key if the key exists. Otherwise, add the key-value pair to the cache. If the number of keys exceeds the capacity from this operation, evict the least recently used key.</li>
</ul>
<p>The functions get and put must each run in O(1) average time complexity.</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input</span><br><span class="line">[&quot;LRUCache&quot;, &quot;put&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;put&quot;, &quot;get&quot;, &quot;get&quot;, &quot;get&quot;]</span><br><span class="line">[[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]</span><br><span class="line">Output</span><br><span class="line">[null, null, null, 1, null, -1, null, -1, 3, 4]</span><br><span class="line"></span><br><span class="line">Explanation</span><br><span class="line">LRUCache lRUCache = new LRUCache(2);</span><br><span class="line">lRUCache.put(1, 1); // cache is &#123;1=1&#125;</span><br><span class="line">lRUCache.put(2, 2); // cache is &#123;1=1, 2=2&#125;</span><br><span class="line">lRUCache.get(1);    // return 1</span><br><span class="line">lRUCache.put(3, 3); // LRU key was 2, evicts key 2, cache is &#123;1=1, 3=3&#125;</span><br><span class="line">lRUCache.get(2);    // returns -1 (not found)</span><br><span class="line">lRUCache.put(4, 4); // LRU key was 1, evicts key 1, cache is &#123;4=4, 3=3&#125;</span><br><span class="line">lRUCache.get(1);    // return -1 (not found)</span><br><span class="line">lRUCache.get(3);    // return 3</span><br><span class="line">lRUCache.get(4);    // return 4</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; capacity &lt;&#x3D; 3000</li>
<li>0 &lt;&#x3D; key &lt;&#x3D; 10^4</li>
<li>0 &lt;&#x3D; value &lt;&#x3D; 10^5</li>
<li>At most 2 * 10^5 calls will be made to get and put.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LRUCache</span>:</span><br><span class="line">    <span class="comment"># double-linked list</span></span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">Node</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key=<span class="literal">None</span>, val=<span class="literal">None</span></span>):</span><br><span class="line">            self.key = key</span><br><span class="line">            self.val = val</span><br><span class="line">            self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">            self.prev = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, capacity: <span class="built_in">int</span></span>):</span><br><span class="line">        self.cap = capacity</span><br><span class="line">        self.cache = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dummy head and tail for the doubled-linked list</span></span><br><span class="line">        self.head = self.Node()</span><br><span class="line">        self.tail = self.Node()</span><br><span class="line">        self.head.<span class="built_in">next</span> = self.tail</span><br><span class="line">        self.tail.prev = self.head</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addNode</span>(<span class="params">self, node</span>):</span><br><span class="line">        node1 = self.head.<span class="built_in">next</span></span><br><span class="line">        node.<span class="built_in">next</span> = node1</span><br><span class="line">        node1.prev = node</span><br><span class="line">        self.head.<span class="built_in">next</span> = node</span><br><span class="line">        node.prev = self.head</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deleteNode</span>(<span class="params">self, node</span>):</span><br><span class="line">        prev = node.prev</span><br><span class="line">        <span class="built_in">next</span> = node.<span class="built_in">next</span></span><br><span class="line">        prev.<span class="built_in">next</span> = <span class="built_in">next</span></span><br><span class="line">        <span class="built_in">next</span>.prev = prev</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, key: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> self.cache:</span><br><span class="line">            node = self.cache[key]</span><br><span class="line">            val = node.val</span><br><span class="line">            <span class="keyword">del</span> self.cache[key]</span><br><span class="line">            self.deleteNode(node)</span><br><span class="line">            self.addNode(node)</span><br><span class="line">            self.cache[key] = self.head.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">return</span> val</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">put</span>(<span class="params">self, key: <span class="built_in">int</span>, value: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> self.cache:</span><br><span class="line">            node = self.cache[key]</span><br><span class="line">            <span class="keyword">del</span> self.cache[key]</span><br><span class="line">            self.deleteNode(node)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># delete the least recently used node when the capacity is full</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.cache) == self.cap:</span><br><span class="line">            <span class="keyword">del</span> self.cache[self.tail.prev.key]</span><br><span class="line">            self.deleteNode(self.tail.prev)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># insert the most recent used node to the front</span></span><br><span class="line">        self.addNode(self.Node(key, value))</span><br><span class="line">        self.cache[key] = self.head.<span class="built_in">next</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Your LRUCache object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"># obj = LRUCache(capacity)</span></span><br><span class="line"><span class="comment"># param_1 = obj.get(key)</span></span><br><span class="line"><span class="comment"># obj.put(key,value)</span></span><br></pre></td></tr></table></figure>

<h3 id="Leetcode-224-Basic-Calculator"><a href="#Leetcode-224-Basic-Calculator" class="headerlink" title="[Leetcode 224] Basic Calculator"></a>[Leetcode 224] Basic Calculator</h3><p>Given a string s representing a valid expression, implement a basic calculator to evaluate it, and return the result of the evaluation.</p>
<p>Note: You are not allowed to use any built-in function which evaluates strings as mathematical expressions, such as eval().</p>
<p>Example 1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;1 + 1&quot;</span><br><span class="line">Output: 2</span><br></pre></td></tr></table></figure>

<p>Example 2:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot; 2 - (3 + 4) &quot;</span><br><span class="line">Output: -5</span><br></pre></td></tr></table></figure>

<p>Example 3:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input: s = &quot;(1+(4+5+2)-3)+(6+8)&quot;</span><br><span class="line">Output: 23</span><br></pre></td></tr></table></figure>

<p>Constraints:</p>
<ul>
<li>1 &lt;&#x3D; s.length &lt;&#x3D; 3 * 10^5</li>
<li>s consists of digits, ‘+’, ‘-‘, ‘(‘, ‘)’, and ‘ ‘.</li>
<li>s represents a valid expression.</li>
<li>‘+’ is not used as a unary operation (i.e., “+1” and “+(2 + 3)” is invalid).</li>
<li>‘-‘ could be used as a unary operation (i.e., “-1” and “-(2 + 3)” is valid).</li>
<li>There will be no two consecutive operators in the input.</li>
<li>Every number and running calculation will fit in a signed 32-bit integer.</li>
</ul>
<p>Solution:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calculate</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Refer to https://leetcode.com/problems/basic-calculator/solutions/546092/simple-python-solution-using-stack-with-explanation-inline/?envType=study-plan-v2&amp;envId=top-interview-150</span></span><br><span class="line"><span class="string">        1. Take 3 containers:</span></span><br><span class="line"><span class="string">        num -&gt; to store current num value only</span></span><br><span class="line"><span class="string">        sign -&gt; to store sign value, initially +1</span></span><br><span class="line"><span class="string">        res -&gt; to store sum</span></span><br><span class="line"><span class="string">        When ( comes these containers used for calculate sum of intergers within () brackets.</span></span><br><span class="line"><span class="string">        --------------------</span></span><br><span class="line"><span class="string">        2. When c is + or -</span></span><br><span class="line"><span class="string">        Move num to res, because we need to empty num for next integer value.</span></span><br><span class="line"><span class="string">        set num = 0</span></span><br><span class="line"><span class="string">        sign = update with c</span></span><br><span class="line"><span class="string">        --------------------</span></span><br><span class="line"><span class="string">        3. When c is &#x27;(&#x27;</span></span><br><span class="line"><span class="string">        Here, we need num, res, sign to calculate sum of integers within ()</span></span><br><span class="line"><span class="string">        So, move num and sign to stack =&gt; [num, sign]</span></span><br><span class="line"><span class="string">        Now reset - res = 0, num = 0, sign = 1 (default)</span></span><br><span class="line"><span class="string">        --------------------</span></span><br><span class="line"><span class="string">        4. When c is &#x27;)&#x27; -&gt; 2-(3+4), Here res=3, num=4, sign=1 stack [2, -] </span></span><br><span class="line"><span class="string">        res +=sign*num -&gt; calculate sum for num first, then pop items from stack, res=7</span></span><br><span class="line"><span class="string">        res *=stack.pop() - &gt; Pop sign(+ or -) to multiply with res, res = 7*(-1)</span></span><br><span class="line"><span class="string">        res +=stack.pop() - &gt; Pop integer and add with prev. sum, res = -7 + 2 - 5</span></span><br><span class="line"><span class="string">        --------------------</span></span><br><span class="line"><span class="string">        Simple Example: 2 - 3</span></span><br><span class="line"><span class="string">        Initially res will have 2,i.e. res = 2</span></span><br><span class="line"><span class="string">        then store &#x27;-&#x27; in sign. it will be used when 3 comes. ie. sign = -1</span></span><br><span class="line"><span class="string">        Now 3 comes =&gt; res = res + num*sign</span></span><br><span class="line"><span class="string">        Return statement: res+num*sign =&gt; res = 2 + 3(-1) = 2 - 3 = -1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num = <span class="number">0</span></span><br><span class="line">        sign = <span class="number">1</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        stack = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)): <span class="comment"># iterate characters</span></span><br><span class="line">            c = s[i]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> c.isdigit(): </span><br><span class="line">                <span class="comment"># parse the consecutive digits. e.g. 23 -&gt; 2 * 10 + 3</span></span><br><span class="line">                num = num*<span class="number">10</span> + <span class="built_in">int</span>(c) </span><br><span class="line">            </span><br><span class="line">            <span class="keyword">elif</span> c <span class="keyword">in</span> <span class="string">&#x27;-+&#x27;</span>: <span class="comment"># check for - and +</span></span><br><span class="line">                <span class="comment"># calculate the temporary result before &#x27;+&#x27; or &#x27;-&#x27;</span></span><br><span class="line">                res += num*sign</span><br><span class="line">                <span class="comment"># save the new sign</span></span><br><span class="line">                sign = -<span class="number">1</span> <span class="keyword">if</span> c == <span class="string">&#x27;-&#x27;</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">                <span class="comment"># reset num to 0 for parsing the next operand</span></span><br><span class="line">                num = <span class="number">0</span></span><br><span class="line">            <span class="keyword">elif</span> c == <span class="string">&#x27;(&#x27;</span>:</span><br><span class="line">                <span class="comment"># we need res and sign to save the temporary result within &#x27;()&#x27;. Reset sign to default 1.</span></span><br><span class="line">                stack.append(res)</span><br><span class="line">                stack.append(sign)</span><br><span class="line">                res = <span class="number">0</span></span><br><span class="line">                sign = <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> c == <span class="string">&#x27;)&#x27;</span>:</span><br><span class="line">                <span class="comment"># calculate the result within &#x27;()&#x27;</span></span><br><span class="line">                <span class="comment"># e.g. 2 -(3 + 4) -&gt; res = 3, num = 4, sign = 1, stack = [2, -]</span></span><br><span class="line">                res +=sign*num <span class="comment"># res = 3 * 1 + 4 = 7</span></span><br><span class="line">                res *=stack.pop() <span class="comment"># res = 7 * (-1) = -7</span></span><br><span class="line">                res +=stack.pop() <span class="comment"># res = -7 + 2 = -5</span></span><br><span class="line">                num = <span class="number">0</span> <span class="comment"># reset num to 0</span></span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> res + num*sign</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">calculate_recursive</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">i</span>):</span><br><span class="line">            res, digits, sign = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(s):</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> s[i].isdigit():</span><br><span class="line">                    <span class="comment"># parse the operand</span></span><br><span class="line">                    digits = digits * <span class="number">10</span> + <span class="built_in">int</span>(s[i])</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">elif</span> s[i] <span class="keyword">in</span> <span class="string">&#x27;+-&#x27;</span>:</span><br><span class="line">                    <span class="comment"># evaluate the subresult before the next operator</span></span><br><span class="line">                    res += digits * sign</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># reset digits to 0</span></span><br><span class="line">                    digits = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># reset operator sign</span></span><br><span class="line">                    sign = <span class="number">1</span> <span class="keyword">if</span> s[i] == <span class="string">&#x27;+&#x27;</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">elif</span> s[i] == <span class="string">&#x27;(&#x27;</span>:</span><br><span class="line">                    <span class="comment"># evalute the sub results in &#x27;()&#x27; and return the index of &#x27;)&#x27;</span></span><br><span class="line">                    subres, i = evaluate(i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># accumulate the result</span></span><br><span class="line">                    res += subres * sign</span><br><span class="line"></span><br><span class="line">                <span class="keyword">elif</span> s[i] == <span class="string">&#x27;)&#x27;</span>:</span><br><span class="line">                    <span class="comment"># evaluate the subresult before &#x27;)&#x27;</span></span><br><span class="line">                    res += digits * sign</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># return the result and index</span></span><br><span class="line">                    <span class="keyword">return</span> res, i</span><br><span class="line">                </span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> res + digits * sign        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> evaluate(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Leetcode</tag>
        <tag>Algorithm</tag>
        <tag>Data structure</tag>
      </tags>
  </entry>
  <entry>
    <title>Saving the Multiverse</title>
    <url>/blog/saving-the-multiverse/</url>
    <content><![CDATA[<p><img src="/images/multiverse.png"></p>
<p>Adventure comes in many forms. Some you expect, some you don’t. Some have you excited, some make you dread. But if there’s one thing all my adventures have in common, it’s that they all somehow end up with my life being in danger. <span id="more"></span></p>
<p>I am not some ordinary kid. In fact, I might as well be extraordinary. To other kids at Mission High School, I am just the “smart kid” that everyone wants to be. While they need to stay up till 2 a.m. studying for the exam the next day, I am just wasting away on my Nintendo. At least that’s what they think. And they aren’t wrong. I do spend more time gaming than studying, but I have a reason. Once again, I am not your daily high schooler. In fact, I’m not even a normal human. That was what I used to be.</p>
<p>I am Captain America. I am Iron Man. I am Mario. I am Bob the Builder. I am Sonic. I am… yes, you probably get the idea now. You’re also probably thinking, am I ok? Well, I wasn’t joking when I said I was every video game character ever. You see, on my 16th birthday, I wished to have the power to be anything I wanted. I got that wish granted, sort of. That night, I dreamed that a mysterious hooded man flew to me and said that I would play a bigger role than I ever imagined. The next day, just as I opened my new Nintendo that my dad bought for me, I was suddenly sucked into the game. I was on the Nintendo, except it wasn’t what I expected to see on a Nintendo. I was presented with a road map of games to play, in that specific order. The first game was Marvel Ultimate Alliance 3: The Black Order. I stepped on the tile(literally) and I was instantly teleported into the game. I was sent through pipes similar to those in Super Mario Bros, and ended up in a dark alley. However, I was no longer myself. I was wearing a red, white, and blue army uniform with a five pointed star in the middle. I even had a helmet on. Yes, I had become Captain America.</p>
<p>Everything was slowly becoming clearer. I was seeing everything through the perspective of the video game character. On the left side of my screen, there was a goal. It said to capture Ebony Maw and Corvus Glaive. While I wasn’t very good at memorizing math formulas and vocabulary words, I always had a photographic memory when it came to video games and movies. Ebony Maw and Corvus Glaive were members of the Black Order, a team of aliens that served Thanos, an evil titan that wanted to gather the 6 infinity stones. I had a map of where I should go. Apparently, there was the Avengers Tower 4 miles away from my alley.<br>With my new body, running seemed like a breeze. I felt like I could even beat Usain Bolt in a race. I ran at a comfortable pace of 30 miles per hour, and in a mere 8 minutes, I was at the Avengers Tower. This was not some happy reunion though, as the tower was in ruins. I walked inside the debris and rubble, only to find it deserted. A stranger walked past me, and gave me a disappointed glance. I was confused and horrified. The Avengers were always my role models, but now were they all dead? I asked the nearest person what happened to the Avengers, but he just looked at me as if I was stupid.</p>
<p>“Was that a joke? They’re gone, captured by Thanos and his Black Order! This all happened because you were missing, Cap!”</p>
<p>I just stood there, speechless. Suddenly, a complex plan appeared in my head. Perhaps it was because of my newly modified brain, but now I could form battle plans quickly. Even as Captain America, I couldn’t take the entire Black Order, as there was Corvus Glaive, Proxima Midnight, Cull Obsidian, and Ebony Maw, backed up by millions of monsters. But if I captured Corvus and Ebony, who were the two most important generals, I could use them to bargain with Thanos for the rest of the Avengers. </p>
<p>With the plan in mind, I began researching the whereabouts of Sanctuary II, the ship of Thanos. It was easier than expected, as in a few days Thanos would be coming down to Earth to retrieve the Space Stone, which was currently in the possession of SHIELD. Thanos had already gotten the Time Stone and Mind Stone from Dr. Strange and Vision respectively. I wasn’t sure if he had the reality, soul, and power stone, however. His power was already immense, so I needed every bit of help I could get. I knew that the space stone was encased in the Tesseract, so I needed to convince Shield to give me it. Little did I know of the betrayal going on from within.</p>
<p>The moment I stepped into SHIELD headquarters, I could smell the air of treason and guilt. Everyone looked nervous when they saw me, as if they were hiding something. My suspicions grew as I saw their hands go to their weapons. Alexander Pierce, the head of SHIELD, greeted me with a “Kill him!” My battle reflexes immediately took over as I saw everyone pulling out their guns and I quickly duck behind a desk for protection. I found out that my entire outfit was bullet-proof when the bullets bounced harmlessly off. With renewed confidence, I took out my vibranium shield and started throwing it everywhere. People dropped like flies and soon I had everyone surrender. Alexander agreed to hand over the Tesseract in exchange for his life being spared. I took out the Space Stone from the Tesseract, and merged its power with myself. I couldn’t help but notice that Alexander hadn’t used any of his most powerful weapons or bombs. It seemed a bit too easy just for him to surrender like that. Suddenly, a man rushed in and said that Thanos had arrived. Alexander had a cruel smile on his face that made me realize that I had been tricked. Thanos was arriving today to retrieve the Space Stone, along with the Black Order.</p>
<p>I was about to make a run for it when the doors opened once more as Ebony Maw and Corvus Glaive entered alone. Thanos was not with them. They explained that the Mad Titan was not here as he had other things to do. They were sent to get the stone for him. All this time, I had been hiding behind a pillar in the building. Realizing that this was my chance, I pounced from behind and tackled the two of them. Before Ebony Maw could use his dark magic, I smacked him on the head with my shield, instantly knocking him unconscious. Corvus Glaive got out his weapon, the Cosmic Glaive, and tried to stab me with it. I blocked with my shield and swiftly took him down with a wheel kick to the head. I carried the two bodies back to Sanctuary II, which shocked Thanos, who was sitting comfortably on his throne. I held his two generals at gunpoint, forcing Thanos to release the Avengers along with the Infinity Stones. He reluctantly agreed and did everything I asked for. </p>
<p>After completing this mission, I was sent back home. As much as I had enjoyed that adventure, I knew that there was always a chance of me being killed in the process of completing a quest. Also, I still had to face the challenge that terrified all high schoolers, calculus homework. However, when I sat down, I realized that I instantly solved every problem in the homework. Getting my powers seemed to have greatly improved my brainpower. Perhaps the enhanced IQ of the superheroes had saved my memory. Or maybe it was a secret birthday gift from God. Either way, this was going to be a huge help for me, as I could spend more time gaming than studying. </p>
<p>A sudden thought popped into my head. What were my parents going to think if I just suddenly disappeared and went inside of the game. To see what happened during this process, I set up a camera to record the entire scene. I called my parents and then quickly went into the video game. After waiting for 10 minutes, I left the game. I was suddenly reunited with my physical body, which had been talking to my mom. </p>
<p>“Why are you still gaming?”She asked.</p>
<p>“Uhm, I’m almost done with this level. Besides, I finished my calculus homework already.”</p>
<p>My mom was shocked, but after seeing all the correct homework, she let me resume my game. I quickly checked on the video recording, and was appalled to see that my body had turned on autorun when I went into the game. It acted just like what I normally would do, so my parents wouldn’t be suspicious of anything. Also, despite the fact that I waited for 10 minutes, in real life it had only been 1 minute. Satisfied, I reentered the game to accept my next quest.<br>As time passed, the excitement faded and saving universes in video games felt more like a suicidal job with no pay. Each quest was more dangerous than the one before, and I felt like I was at death’s door. In the God of War, I was Kratos, and fought the entire Olympic Pantheon. I had to go through immense pain after being stabbed multiple times by Zeus and Ares. There was no one to talk to about my adventures, as they would not believe me. Then I asked myself why I was even bothering to save people at the risk of my own life. I decided that I could just leave the video game characters and just chill with my enhanced knowledge. School would be a breeze for me so I could just get a job easily. </p>
<p>For a few months, I did that. However, I was beginning to feel increasingly guilty about what I had done. One night, I was visited by the same hooded man that had given me my powers.</p>
<p>“What have you done, fool? I gave you your powers so you could use them for good, not so you could just waste time! Do you have any idea of how many people died as a result of your laziness?”</p>
<p>Shocked, I listened to the man go on and on about the horrors that happened. After all that, I was horrified by what I had done. The hooded man informed me that there was a evil mastermind behind all of this, the Celestial, who was a primordial deity that controlled everything in the multiverse. After defeating all the heroes in the video games, he can finally be freed from his bonds and released into this world.</p>
<p>“So, what will you decide to do? If you are not going to use your powers for good, I will take them away from you and give it to someone worthy.  You can continue your delinquent lifestyle. However, if you choose to keep your powers, you will have to do your job of protecting the multiverse. This will become your lifetime job, which means that you shall get an annual pay. You will have the same breaks as normal people in the United States and your annual pay will be one million dollars per year.”</p>
<p>I was content with this offer and from then on, worked tirelessly to stop the Celestial. Little by little, game by game, success was closer.</p>
]]></content>
      <categories>
        <category>Young Author</category>
      </categories>
      <tags>
        <tag>Writing</tag>
      </tags>
  </entry>
  <entry>
    <title>How to connect to AKS cluster nodes</title>
    <url>/blog/how-to-connect-to-aks-cluster-nodes/</url>
    <content><![CDATA[<p>One method to connect to the AKS cluster nodes is to create an interactive shell, aka debug pod.<span id="more"></span></p>
<p>To list your nodes, use the kubectl get nodes command:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME                                STATUS   ROLES   AGE    VERSION</span><br><span class="line">aks-nodepool1-36793237-vmss000000   Ready    agent   171m   v1.27.7</span><br><span class="line">aks-nodepool1-36793237-vmss000001   Ready    agent   170m   v1.27.7</span><br><span class="line">aks-nodepool1-36793237-vmss000002   Ready    agent   156m   v1.27.7</span><br><span class="line">aks-torpedo-83020050-vmss000000     Ready    agent   151m   v1.27.7</span><br></pre></td></tr></table></figure>

<p>Use the kubectl debug command to run a container image on the node to connect to it. The following command starts a privileged container on your node and connects to it.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl debug node/aks-nodepool1-36793237-vmss000000 -it --image=mcr.microsoft.com/dotnet/runtime-deps:6.0</span><br><span class="line">Creating debugging pod node-debugger-aks-nodepool1-36793237-vmss000000-nj9kd with container debugger on node aks-nodepool1-36793237-vmss000000.</span><br><span class="line">If you don<span class="string">&#x27;t see a command prompt, try pressing enter.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">root@aks-nodepool1-36793237-vmss000000:/# chroot /host</span></span><br><span class="line"><span class="string"># hostname</span></span><br><span class="line"><span class="string">aks-nodepool1-36793237-vmss000000</span></span><br></pre></td></tr></table></figure>

<p>When you are done with a debugging pod, enter the exit command to end the interactive shell session. After the interactive container session closes, delete the pod used for access with kubectl delete pod.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># exit</span></span><br><span class="line">root@aks-nodepool1-36793237-vmss000000:/<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME                                                    READY   STATUS      RESTARTS   AGE</span><br><span class="line">node-debugger-aks-nodepool1-36793237-vmss000000-nj9kd   0/1     Completed   0          62s</span><br><span class="line"></span><br><span class="line">$ kubectl delete pod node-debugger-aks-nodepool1-36793237-vmss000000-nj9kd</span><br><span class="line">pod <span class="string">&quot;node-debugger-aks-nodepool1-36793237-vmss000000-nj9kd&quot;</span> deleted</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://learn.microsoft.com/en-us/azure/aks/node-access">https://learn.microsoft.com/en-us/azure/aks/node-access</a></li>
<li><a href="https://learn.microsoft.com/en-us/azure/virtual-machines/ssh-keys-azure-cli">https://learn.microsoft.com/en-us/azure/virtual-machines/ssh-keys-azure-cli</a></li>
<li><a href="https://learn.microsoft.com/en-us/azure/virtual-machines/ssh-keys-portal">https://learn.microsoft.com/en-us/azure/virtual-machines/ssh-keys-portal</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>AKS</tag>
      </tags>
  </entry>
  <entry>
    <title>off cpu flame graph</title>
    <url>/blog/off-cpu-flame-graph/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Perf</tag>
        <tag>Flame Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>Understanding EC2 Instance and Storage Volume Types</title>
    <url>/blog/understanding-ec2-instance-types/</url>
    <content><![CDATA[<h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>When you launch an instance, the instance type you specify determines the hardware of the host computer used for your instance, such as CPU, memory, storage and network. <span id="more"></span></p>
<p>Amazon EC2 dedicates some resources, such as CPU, memory, and instance storage, to a particular instance. And it shares other resources, such as the network and the disk subsystem, among instances.</p>
<h2 id="Instance-type-naming-convention"><a href="#Instance-type-naming-convention" class="headerlink" title="Instance type naming convention"></a>Instance type naming convention</h2><p>Amazon EC2 instance types are named based on their family, generation, processor family, additional capabilities, and size.</p>
<p><img src="/images/instance-type-name.png" alt="Image"></p>
<p>Instance families:</p>
<ul>
<li>C – Compute optimized</li>
<li>D – Dense storage</li>
<li>F – FPGA</li>
<li>G – Graphics intensive</li>
<li>Hpc – High performance computing</li>
<li>I – Storage optimized</li>
<li>Im – Storage optimized with a one to four ratio of vCPU to memory</li>
<li>Is – Storage optimized with a one to six ratio of vCPU to memory</li>
<li>Inf – AWS Inferentia</li>
<li>M – General purpose</li>
<li>Mac – macOS</li>
<li>P – GPU accelerated</li>
<li>R – Memory optimized</li>
<li>T – Burstable performance</li>
<li>Trn – AWS Trainium</li>
<li>U – High memory</li>
<li>VT – Video transcoding</li>
<li>X – Memory intensive</li>
</ul>
<p>Processor families:</p>
<ul>
<li>a – AMD processors</li>
<li>g – AWS Graviton processors</li>
<li>i – Intel processors</li>
</ul>
<p>Additional capabilities:</p>
<ul>
<li>b – EBS optimized</li>
<li>d – Instance store volumes</li>
<li>n – Network and EBS optimized</li>
<li>e – Extra storage or memory</li>
<li>z – High performance</li>
<li>q – Qualcomm inference accelerators</li>
<li>flex – Flex instance</li>
</ul>
<h2 id="Available-Instance-Types"><a href="#Available-Instance-Types" class="headerlink" title="Available Instance Types"></a>Available Instance Types</h2><p>Amazon EC2 provides a wide selection of instance types optimized to fit different use cases.</p>
<ul>
<li>General purpose</li>
<li>Compute optimized</li>
<li>Memory optimized</li>
<li>Storage optimized</li>
<li>Accelerated computing</li>
<li>High-performance computing</li>
</ul>
<h2 id="Instances-built-on-the-Nitro-System"><a href="#Instances-built-on-the-Nitro-System" class="headerlink" title="Instances built on the Nitro System"></a>Instances built on the Nitro System</h2><p>The Nitro System provides bare metal capabilities that eliminate virtualization overhead and support workloads that require full access to host hardware. Bare metal instances are well suited for the following:</p>
<ul>
<li>Workloads that require access to low-level hardware features (for example, Intel VT) that are not available or fully supported in virtualized environments</li>
<li>Applications that require a non-virtualized environment for licensing or support</li>
</ul>
<p>There are both virtualized instances and baremetal instances(name as *.metal) built on Nitro system.</p>
<p>In general, you can just use the virtualized instances unless you know what you need with the baremetal instances.</p>
<h2 id="Storage-volume-types"><a href="#Storage-volume-types" class="headerlink" title="Storage volume types"></a>Storage volume types</h2><p>Some instance types support EBS volumes and instance store volumes, while other instance types support only EBS volumes. </p>
<p>The following is an exmaple of instance type m6idn.12xlarge. When you launch it, the NVMe instance store volumes(2x 1425GB NVMe SSD) are automatically attached to the instance. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ip-172-18-160-187 /]<span class="comment"># lsblk</span></span><br><span class="line">NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">nvme0n1       259:0    0  100G  0 disk</span><br><span class="line">├─nvme0n1p1   259:2    0  100G  0 part /</span><br><span class="line">└─nvme0n1p128 259:3    0    1M  0 part</span><br><span class="line">nvme1n1       259:1    0  1.3T  0 disk</span><br><span class="line">nvme2n1       259:4    0  1.3T  0 disk</span><br></pre></td></tr></table></figure>

<p>In this instance type, the attached NVME volume can achieve the 230k IOPS as below.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ip-172-18-160-187 /]<span class="comment"># fio --blocksize=4k --ioengine=libaio --readwrite=randwrite --filesize=10G --group_reporting --direct=1 --iodepth=128 --randrepeat=1 --name=job1 --filename=/dev/nvme1n1</span></span><br><span class="line">job1: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=128</span><br><span class="line">fio-2.14</span><br><span class="line">Starting 1 process</span><br><span class="line">Jobs: 1 (f=1): [w(1)] [100.0% <span class="keyword">done</span>] [0KB/787.6MB/0KB /s] [0/202K/0 iops] [eta 00m:00s]</span><br><span class="line">job1: (groupid=0, <span class="built_in">jobs</span>=1): err= 0: pid=266: Fri Jan 26 23:18:07 2024</span><br><span class="line">  write: io=10240MB, bw=917710KB/s, iops=229427, runt= 11426msec</span><br><span class="line">    slat (usec): min=0, max=195, avg= 3.73, stdev=10.48</span><br><span class="line">    clat (usec): min=203, max=1199, avg=553.89, stdev=149.49</span><br><span class="line">     lat (usec): min=204, max=1200, avg=557.62, stdev=150.69</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  258],  5.00th=[  262], 10.00th=[  266], 20.00th=[  286],</span><br><span class="line">     | 30.00th=[  604], 40.00th=[  620], 50.00th=[  628], 60.00th=[  628],</span><br><span class="line">     | 70.00th=[  636], 80.00th=[  644], 90.00th=[  652], 95.00th=[  660],</span><br><span class="line">     | 99.00th=[  676], 99.50th=[  676], 99.90th=[  684], 99.95th=[  684],</span><br><span class="line">     | 99.99th=[  748]</span><br><span class="line">    lat (usec) : 250=0.04%, 500=21.03%, 750=78.92%, 1000=0.01%</span><br><span class="line">    lat (msec) : 2=0.01%</span><br><span class="line">  cpu          : usr=13.83%, sys=35.84%, ctx=128620, majf=8, minf=11</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.1%</span><br><span class="line">     issued    : total=r=0/w=2621440/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=128</span><br><span class="line"></span><br><span class="line">Run status group 0 (all <span class="built_in">jobs</span>):</span><br><span class="line">  WRITE: io=10240MB, aggrb=917710KB/s, minb=917710KB/s, maxb=917710KB/s, mint=11426msec, maxt=11426msec</span><br><span class="line"></span><br><span class="line">Disk stats (<span class="built_in">read</span>/write):</span><br><span class="line">  nvme1n1: ios=41/2616064, merge=0/0, ticks=3/1305473, in_queue=1305476, util=99.30%</span><br></pre></td></tr></table></figure>

<p>You can also attach EBS volumes(e.g. nvme3n1) as needed.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ip-172-18-160-187 /]<span class="comment"># lsblk</span></span><br><span class="line">NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">nvme0n1       259:0    0  100G  0 disk</span><br><span class="line">├─nvme0n1p1   259:2    0  100G  0 part /</span><br><span class="line">└─nvme0n1p128 259:3    0    1M  0 part</span><br><span class="line">nvme1n1       259:1    0  1.3T  0 disk</span><br><span class="line">nvme2n1       259:4    0  1.3T  0 disk</span><br><span class="line">nvme3n1       259:5    0  500G  0 disk</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Cloud Storage</category>
      </categories>
      <tags>
        <tag>AWS</tag>
      </tags>
  </entry>
  <entry>
    <title>How to disable I/O merge for Linux block device</title>
    <url>/blog/how-to-disable-iomerge-for-Linux-block-device/</url>
    <content><![CDATA[<h2 id="iomerges-parameter"><a href="#iomerges-parameter" class="headerlink" title="iomerges parameter"></a>iomerges parameter</h2><p>In Linux, the file <strong>nomerges</strong> under &#x2F;sys&#x2F;block&#x2F;xxx&#x2F;queue&#x2F; enables the user to disable the lookup logic involved with I&#x2F;O merging requests in the block layer. <span id="more"></span></p>
<p>By default, all merges are enabled.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> /sys/block/sdb/queue/nomerges</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<p>When set to 1, only simple one-hit merges will be tried. When set to 2, no merge algorithms will be tried (including one-hit or more complex tree&#x2F;hash lookups).</p>
<h2 id="Tuning-exercise"><a href="#Tuning-exercise" class="headerlink" title="Tuning exercise"></a>Tuning exercise</h2><p>Firstly, we run 4k sequential write benchmarking with fio on a SSD disk. The write IOPS achieved is 104k as below. However, it doesn’t really tell us how many 4k sequential writes can be handled by this SSD disk because the 4k writes are merged to larger blocksize, ~20k in this case.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> /sys/block/sdb/queue/nomerges</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">$ fio --ioengine=libaio --blocksize=4k --readwrite=write --filesize=1G --end_fsync=1 --iodepth=128 --direct=1 --group_reporting --numjobs=1 --name=fiojob1 --filename=/dev/sdb</span><br><span class="line">  write: IOPS=104k, BW=406MiB/s (426MB/s)(1024MiB/2522msec)</span><br><span class="line"></span><br><span class="line">$ iostat -ktdx 1 sdb</span><br><span class="line">01/28/2024 12:57:51 AM</span><br><span class="line">Device:   rrqm/s   wrqm/s     r/s    w/s        rkB/s   wkB/s       avgrq-sz   avgqu-sz   await   r_await   w_await  svctm  %util</span><br><span class="line">sdb       0.00     62222.00   0.00   41947.00   0.00    416708.00   19.87      27.62      0.66    0.00      0.66     0.02   100.00</span><br></pre></td></tr></table></figure>

<p>Now, we disable I&#x2F;O merges and rerun the same fio benchmarking. The write IOPS achieved is 79.2k which reflects the actual capabilty to handle 4k sequential writes for this SSD disk.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 2 &gt; /sys/block/sdb/queue/nomerges</span><br><span class="line">$ <span class="built_in">cat</span> /sys/block/sdb/queue/nomerges</span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">$ fio --ioengine=libaio --blocksize=4k --readwrite=write --filesize=1G --end_fsync=1 --iodepth=128 --direct=1 --group_reporting --numjobs=1 --name=fiojob1 --filename=/dev/sdb</span><br><span class="line">  write: IOPS=79.2k, BW=309MiB/s (324MB/s)(1024MiB/3311msec)</span><br><span class="line"></span><br><span class="line">$ iostat -ktdx 1 sdb</span><br><span class="line">01/28/2024 01:00:24 AM</span><br><span class="line">Device:   rrqm/s   wrqm/s   r/s    w/s        rkB/s   wkB/s       avgrq-sz   avgqu-sz   await   r_await   w_await  svctm   %util</span><br><span class="line">sdb       0.00     0.00     0.00   78923.00   0.00    315692.00   8.00       41.15      0.52    0.00      0.52     0.01    100.20</span><br></pre></td></tr></table></figure>

<p>Note: The fio and iostat command above should be run from different terminal in order to monitor the disk I&#x2F;O statistics in real time.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.kernel.org/doc/Documentation/block/queue-sysfs.txt">https://www.kernel.org/doc/Documentation/block/queue-sysfs.txt</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Tuning</tag>
      </tags>
  </entry>
  <entry>
    <title>Does 100% utilization really mean the disk bottleneck?</title>
    <url>/blog/a-trap-in-iostat-util/</url>
    <content><![CDATA[<p>From man page of iostat in Linux, <strong>%util</strong> is explained as below.</p>
<blockquote>
<p>%util:</p>
<p>Percentage of elapsed time during which I&#x2F;O requests were issued to the device (bandwidth utilization for the device). Device saturation occurs when this value is close to 100%.</p>
</blockquote>
<p>I have seen many cases people misuse this metric when to tell if the system runs into disk I&#x2F;O bottleneck. <span id="more"></span></p>
<p>To explain what could be wrong with %util, let’s start from a fio benchmark run.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ fio --ioengine=libaio --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --iodepth=128 --direct=1 --group_reporting -time_based --runtime=60 --numjobs=1 --name=fiojob1 --filename=/dev/nvme1n1</span><br><span class="line">write: io=2555.8MB, bw=43615KB/s, iops=10903, runt= 60004msec</span><br></pre></td></tr></table></figure>

<p>During the fio run, we can examine the iostat output to understand the disk utilization.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Device: rrqm/s wrqm/s r/s w/s   rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util</span><br><span class="line">nvme1n1 0      0      0   10993 0     43972 8        30.13    2.74  0       2.74    0.09  100.08</span><br></pre></td></tr></table></figure>

<p>From the iostat output, the disk utilization is 100%. Can we conclude the system is running into disk I&#x2F;O bottleneck. <strong>Not yet!!</strong></p>
<p>The fio benchmark runs on a AWS EC2 instance which has one EBS volume attached. The IOPS limit is set to <strong>16000</strong> when to create the EBS volume. Obviously, the ceiling is not reached yet since the w&#x2F;s achieved is only <strong>10993</strong>. But, why the disk utilization is already <strong>100%</strong>.</p>
<p>The problem is due to <strong>parallelism</strong>.</p>
<p>In the man page of iostat, when it says <strong>%util</strong>, it means “Percentage of elapsed time during which I&#x2F;O requests were issued to the device”. It also means “Device saturation occurs when this value is close to 100%”. This is NOT correct for the modern SSD disk any more.</p>
<p>For the tranditional magnetic hard disk, due to the nature of spining disk head, it can only handle one I&#x2F;O at a time. The above statement makes sense. However, for the modern SSD disk, it can handle multiple I&#x2F;Os at a time. When the disk is 100% busy, it might be continuously handling one or more write requests during that sampling period. In other words, it doesn’t mean the SSD disk can’t handle extra I&#x2F;Os even if it’s already 100% busy servicing I&#x2F;O requests. </p>
<p>For the previous fio run, in order to push the disk limit, we need a way to increase I&#x2F;O parallelism. To achieve this, we can keep increasing numjobs from 1 to 2, or more.</p>
<p><strong>numjobs&#x3D;2:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ fio --ioengine=libaio --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --iodepth=128 --direct=1 --group_reporting -time_based --runtime=60 --numjobs=2 --name=fiojob1 --filename=/dev/nvme1n1</span><br><span class="line">write: io=3122.2MB, bw=53283KB/s, iops=13320, runt= 60003msec</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Device: rrqm/s wrqm/s r/s w/s     rkB/s wkB/s   avgrq-sz avgqu-sz await r_await w_await svctm %util</span><br><span class="line">nvme1n1 0      0      0   13412.2 0     53648.8 8        44.84    3.34  0       3.34    0.07  100</span><br></pre></td></tr></table></figure>

<p>Now, this w&#x2F;s is increased from 10993 to 13412. We can push more until the limit 16000.</p>
<p><strong>numjobs&#x3D;4:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ fio --ioengine=libaio --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --iodepth=128 --direct=1 --group_reporting -time_based --runtime=60 --numjobs=4 --name=fiojob1 --filename=/dev/nvme1n1</span><br><span class="line">write: io=3798.2MB, bw=64817KB/s, iops=16204, runt= 60004msec</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Device: rrqm/s wrqm/s r/s w/s     rkB/s wkB/s   avgrq-sz avgqu-sz await r_await w_await svctm %util</span><br><span class="line">nvme1n1 0      0      0   16001.6 0     64006.4 8        50.15    3.13  0       3.13    0.06  100</span><br></pre></td></tr></table></figure>

<p>To this point, the disk reaches its IOPS limit 16000 and it is finally saturated.</p>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Observability</tag>
      </tags>
  </entry>
  <entry>
    <title>The complete explanation for await, svctm and %util in iostat</title>
    <url>/blog/ultimate-explanation-for-iostat/</url>
    <content><![CDATA[<p><strong>iostat</strong> is a Linux I&#x2F;O performance monitoring utility. It’s very commonly used to analyze device utilization. <span id="more"></span></p>
<h2 id="proc-diskstats"><a href="#proc-diskstats" class="headerlink" title="&#x2F;proc&#x2F;diskstats"></a>&#x2F;proc&#x2F;diskstats</h2><p>The statistics fields in iostat are calculated based on the I&#x2F;O statistics of block devices in &#x2F;proc&#x2F;diskstats.</p>
<p>Each line in the &#x2F;proc&#x2F;diskstats file contains the following 14 fields. More fields are added in kernel 4.18 and later.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1  major number</span><br><span class="line">2  minor number</span><br><span class="line">3  device name</span><br><span class="line">4  reads completed successfully</span><br><span class="line">5  reads merged</span><br><span class="line">6  sectors read</span><br><span class="line">7  time spent reading (ms)</span><br><span class="line">8  writes completed</span><br><span class="line">9  writes merged</span><br><span class="line">10  sectors written</span><br><span class="line">11  time spent writing (ms)</span><br><span class="line">12  I/Os currently in progress</span><br><span class="line">13  time spent doing I/Os (ms)</span><br><span class="line">14  weighted time spent doing I/Os (ms)</span><br></pre></td></tr></table></figure>

<p>We will need to know these statistics later when we learn how iostat calculates its fields.</p>
<h2 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a>iostat</h2><p>Now, let’s use fio load generator to benchamrk a AWS EBS gp3 volume and examine the iostat report.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ fio --ioengine=libaio --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --iodepth=128 --direct=1 --group_reporting -time_based --runtime=60 --numjobs=1 --name=fiojob1 --filename=/dev/nvme1n1</span><br><span class="line">write: io=2555.8MB, bw=43615KB/s, iops=10903, runt= 60004msec</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Device: rrqm/s wrqm/s r/s w/s   rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util</span><br><span class="line">nvme1n1 0      0      0   10993 0     43972 8        30.13    2.74  0       2.74    0.09  100</span><br></pre></td></tr></table></figure>

<p>As we can see above, it achieves 10903 iops and the disk is 100% utilized.</p>
<p>You may naturally conclude that the disk runs into bottleneck since it’s 100% busy. Is this really true? Before we answer this question, let’s firstly understand how iostat computes the statistics fields.</p>
<h2 id="How-are-the-iostat-fields-calculated"><a href="#How-are-the-iostat-fields-calculated" class="headerlink" title="How are the iostat fields calculated?"></a>How are the iostat fields calculated?</h2><p>Since the basic fields, such as r&#x2F;s and w&#x2F;s, are very straightforward, we will mainly focus on the following three extended fields because they are commonly used to identify the disk bottlenck.</p>
<ul>
<li>await</li>
<li>svctm</li>
<li>%util</li>
</ul>
<p>From iostat source code, to calcualte the <strong>total number of read and write IOs</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">n_ios  = blkio.rd_ios + blkio.wr_ios;</span><br></pre></td></tr></table></figure>

<p>To calculate the <strong>total amount of time(ms) waiting in queue</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">n_ticks = blkio.rd_ticks + blkio.wr_ticks;</span><br></pre></td></tr></table></figure>

<p>To calculate the <strong>average I&#x2F;O wait time(ms)</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wait = n_ios ? n_ticks / n_ios : 0.0;</span><br></pre></td></tr></table></figure>

<p>To calcualte the <strong>average I&#x2F;O service time(ms)</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">svc_t = n_ios ? blkio.ticks / n_ios : 0.0;</span><br></pre></td></tr></table></figure>

<p>Note: the blkio.ticks is calculated based on the field 13 “time spent doing I&#x2F;Os (ms)” in &#x2F;proc&#x2F;diskstats.</p>
<p>To calculate the <strong>disk utilization</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">busy = 100.0 * blkio.ticks / deltams; /* percentage! */</span><br></pre></td></tr></table></figure>

<h2 id="Two-traps-in-iostat"><a href="#Two-traps-in-iostat" class="headerlink" title="Two traps in iostat"></a>Two traps in iostat</h2><p>With above implementaion, there are actually two traps when to use them to identify disk bottleneck.</p>
<ul>
<li><strong>svctm</strong> - average I&#x2F;O service time(ms)</li>
<li><strong>%util</strong> - disk utilization</li>
</ul>
<p>Let’s try to increase the fio numjobs from 1 to 4.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ fio --ioengine=libaio --blocksize=4k --readwrite=write --filesize=10G --end_fsync=1 --iodepth=128 --direct=1 --group_reporting -time_based --runtime=60 --numjobs=4 --name=fiojob1 --filename=/dev/nvme1n1</span><br><span class="line">write: io=3798.2MB, bw=64817KB/s, iops=16204, runt= 60004msec</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Device: rrqm/s wrqm/s r/s w/s     rkB/s wkB/s   avgrq-sz avgqu-sz await r_await w_await svctm %util</span><br><span class="line">nvme1n1 0      0      0   16001.6 0     64006.4 8        50.15    3.13  0       3.13    0.06  100</span><br></pre></td></tr></table></figure>

<p>Comparing the 1 fio job and 4 jobs runs:</p>
<ul>
<li>The w&#x2F;s increases from 10903 to 16001 even though the disk utilization is 100% for the two runs. It means, with one job run, the disk is not fully saturated yet.</li>
<li>The average I&#x2F;O service time(svctm) reduces from 0.09 to 0.06.</li>
</ul>
<p>Literally, the disk could respond the request in 0.09ms under ligher load, and 0.06ms under heavier load. This seems unlikely and it’s not what iostat supposes to tell us on the average disk service time.</p>
<p>And from iostat man page, you may see this warning.</p>
<blockquote>
<p>svctm<br>    The average service time (in milliseconds) for I&#x2F;O requests that were issued to the device. Warning! Do not trust this field any more.  This field will be removed in a future sysstat version.</p>
</blockquote>
<p>For the traditional spining harddisk, the I&#x2F;O has to be serialized due to the nature of disk head movement from one disk platter location to another. That means only one I&#x2F;O can be serviced at once. In such case, the svctm can reflect how fast each I&#x2F;O is responded.</p>
<p>For the modern SSD disk, it won’t be true any more because the disk can respond to multiple IOs at once. Even if it’s 100% utilized, it just means during that period, the disk is busy responding I&#x2F;O requests. It doesn’t always mean the disk is already saturated.</p>
<p>In order to idenitfy if the disk is completely saturated(peaked), the only way is to offer more work to do in parallel. In the above example, by increasing fio numjobs from 1 to 4, the w&#x2F;s is peaked at 16001 which is align with the specfied iops for the EBS gp3 volume.</p>
<p>In a word, both the svctm and %util fields in iostat are misleading for the modern SSD storage system. They should be used with extra care. </p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://www.kernel.org/doc/Documentation/iostats.txt">I&#x2F;O statistics fields</a></li>
<li><a href="https://www.kernel.org/doc/Documentation/ABI/testing/procfs-diskstats">&#x2F;proc&#x2F;diskstats</a></li>
<li><a href="https://github.com/i4oolish/iostat/blob/master/iostat.c#L219">iostat.c</a></li>
</ul>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Performance</category>
      </categories>
      <tags>
        <tag>Observability</tag>
      </tags>
  </entry>
  <entry>
    <title>How to upgrade Linux kernel on Ubuntu</title>
    <url>/blog/how-to-install-Linux-kernel-on-Ubuntu/</url>
    <content><![CDATA[<h2 id="Method-1-Use-System-Update-Process-Automatic-Procedure"><a href="#Method-1-Use-System-Update-Process-Automatic-Procedure" class="headerlink" title="Method 1: Use System Update Process (Automatic Procedure)"></a>Method 1: Use System Update Process (Automatic Procedure)</h2><ol>
<li><p>Update System Packages</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo apt update</span><br></pre></td></tr></table></figure>

<p>This command retrieves information about the latest available package versions from the repositories configured on the system.</p>
</li>
<li><p>Run the upgrade</p>
<p>Run the following command to upgrade the installed packages, including the kernel, to the latest versions available in the repositories:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo apt upgrade</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Method-2-Manually-Update-the-Kernel-Advanced-Procedure"><a href="#Method-2-Manually-Update-the-Kernel-Advanced-Procedure" class="headerlink" title="Method 2: Manually Update the Kernel (Advanced Procedure)"></a>Method 2: Manually Update the Kernel (Advanced Procedure)</h2><p>There’s another procedure for selecting and installing a new kernel using Mainline. This will take some risk from the untested kernel in mainline.</p>
<ol>
<li><p>Add the PPA maintained by cappelikan to the list of software sources or repositories</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo add-apt-repository ppa:cappelikan/ppa</span><br></pre></td></tr></table></figure>
</li>
<li><p>Refresh the database</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo apt update</span><br></pre></td></tr></table></figure>
</li>
<li><p>Install Mainline utility</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo apt install mainline</span><br></pre></td></tr></table></figure>
</li>
<li><p>Launch Mainline utility</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$sudo</span> mainline</span><br><span class="line">mainline 1.4.9</span><br><span class="line"></span><br><span class="line">Mainline Kernels 1.4.9 - install kernel packages from https://kernel.ubuntu.com/mainline/</span><br><span class="line"></span><br><span class="line">Syntax: mainline &lt;<span class="built_in">command</span>&gt; [options]</span><br><span class="line"></span><br><span class="line">Commands</span><br><span class="line"></span><br><span class="line">  check               Check <span class="keyword">for</span> kernel updates</span><br><span class="line">  notify              Check <span class="keyword">for</span> kernel updates and send a desktop notification</span><br><span class="line">  list                List the available kernels</span><br><span class="line">  list-installed      List the installed kernels</span><br><span class="line">  install-latest      Install the latest mainline kernel</span><br><span class="line">  install-minor       Install the latest mainline kernel without going to a new major version</span><br><span class="line">  install &lt;names&gt;     Install the specified kernels(1)(2)</span><br><span class="line">  uninstall &lt;names&gt;   Uninstall the specified kernels(1)(2)</span><br><span class="line">  uninstall-old       Uninstall all but the highest installed version(2)</span><br><span class="line">  download &lt;names&gt;    Download the specified kernels(1)</span><br><span class="line">  lock &lt;names&gt;        Lock the specified kernels(1)</span><br><span class="line">  unlock &lt;names&gt;      Unlock the specified kernels(1)</span><br><span class="line">  delete-cache        Delete the cached info about available kernels</span><br><span class="line">  write-config        Write the given include/exclude &amp; previous-majors options to the config file</span><br><span class="line">  <span class="built_in">help</span>                This <span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">Options</span><br><span class="line"></span><br><span class="line">  --include-rc        Include release-candidate and unstable releases</span><br><span class="line">  --exclude-rc        Exclude release-candidate and unstable releases</span><br><span class="line">  --include-flavors   Include flavors other than <span class="string">&quot;generic&quot;</span></span><br><span class="line">  --exclude-flavors   Exclude flavors other than <span class="string">&quot;generic&quot;</span></span><br><span class="line">  --include-invalid   Include failed/incomplete builds</span><br><span class="line">  --exclude-invalid   Exclude failed/incomplete builds</span><br><span class="line">  --previous-majors <span class="comment"># Include # (or &quot;all&quot; or &quot;none&quot;) previous major versions</span></span><br><span class="line">  --include-all       Short <span class="keyword">for</span> <span class="string">&quot;--include-rc --include-flavors --include-invalid --previous-majors all&quot;</span></span><br><span class="line">  --exclude-all       Short <span class="keyword">for</span> <span class="string">&quot;--exclude-rc --exclude-flavors --exclude-invalid --previous-majors none&quot;</span></span><br><span class="line">  -n|--dry-run        Don<span class="string">&#x27;t actually install or uninstall</span></span><br><span class="line"><span class="string">  -v|--verbose [#]    Set verbosity level to #, or increment by 1</span></span><br><span class="line"><span class="string">  --pause             Pause and require keypress before exiting</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Notes</span></span><br><span class="line"><span class="string">(1) One or more version strings taken from the output of &quot;list&quot;</span></span><br><span class="line"><span class="string">    comma, pipe, colon, or space separated. (space requires quotes or backslashes)</span></span><br><span class="line"><span class="string">(2) Locked kernels and the currently running kernel are ignored</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">mainline: done</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>List the installed kernel versions</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo mainline list-installed</span><br><span class="line">mainline 1.4.9</span><br><span class="line">Installed Kernels:</span><br><span class="line">linux-image-5.15.0-94-generic</span><br><span class="line">linux-image-6.5.0-17-generic</span><br><span class="line">linux-image-generic</span><br><span class="line">linux-image-generic-hwe-22.04</span><br><span class="line">mainline: <span class="keyword">done</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>List the available kernel versions</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo mainline list</span><br></pre></td></tr></table></figure>
</li>
<li><p>Install the kernel</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo mainline install 6.3</span><br><span class="line"></span><br><span class="line">$ sudo mainline list-installed</span><br><span class="line">mainline 1.4.9</span><br><span class="line">Installed Kernels:</span><br><span class="line">linux-image-5.15.0-94-generic</span><br><span class="line">linux-image-6.5.0-17-generic</span><br><span class="line">linux-image-generic</span><br><span class="line">linux-image-generic-hwe-22.04</span><br><span class="line">linux-image-unsigned-5.7.12-050712-generic</span><br><span class="line">linux-image-unsigned-6.3.0-060300-generic</span><br><span class="line">mainline: <span class="keyword">done</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Boot system with newly installed kernel</p>
<p>Reboot the system and hold the Shift key or Esc key, depending on the Linux distribution.</p>
<p><img src="/images/ubuntu-install-1.png"></p>
<p>Select Advanced options for Ubuntu and choose the kernel to boot up.</p>
<p><img src="/images/ubuntu-install-2.png"></p>
</li>
</ol>
]]></content>
      <categories>
        <category>Tech</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Kernel</tag>
      </tags>
  </entry>
  <entry>
    <title>How to use kubectl to manage kubernetes cluster contexts</title>
    <url>/blog/how-to-use-kubectl-to-manage-k8s-cluster-contexts/</url>
    <content><![CDATA[<h2 id="Kubernetes-context"><a href="#Kubernetes-context" class="headerlink" title="Kubernetes context"></a>Kubernetes context</h2><p>A context in kubernetes is used to group access parameters under a convenient name. Each context has three parameters: cluster, namespace, and user. By default, the <a href="https://www.flamingbytes.com/blog/using-kubeconfig-to-configure-access-to-remote-kubernetes-cluster/">kubectl</a> command-line tool uses parameters from the current context to communicate with the cluster.</p>
<p>In this article, we will learn how to use kubectl to manage the cluster contexts and access the different clusters.</p>
<p><strong>To list all the cluster contexts:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl config get-contexts </span><br><span class="line">  CURRENT                                          NAME                                             CLUSTER       AUTHINFO      NAMESPACE</span><br><span class="line">* arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1</span><br></pre></td></tr></table></figure>

<p><strong>To get the current cluster context:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl config current-context</span><br><span class="line">arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1</span><br></pre></td></tr></table></figure>

<p><strong>To add and switch to a different cluster context:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ aws eks update-kubeconfig --name cluster-2 --region us-west-2</span><br><span class="line">Added new context arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-2 to /Users/cluster/.kube/config</span><br><span class="line"></span><br><span class="line">$ kubectl config get-contexts </span><br><span class="line">  CURRENT                                          NAME                                             CLUSTER       AUTHINFO      NAMESPACE</span><br><span class="line">  arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1</span><br><span class="line">* arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-2   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-2   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-2</span><br></pre></td></tr></table></figure>

<p><strong>To switch to an existing cluster context:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl config use-context arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1</span><br><span class="line">Switched to context <span class="string">&quot;arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1&quot;</span>.</span><br><span class="line"></span><br><span class="line">$ kubectl config get-contexts </span><br><span class="line">  CURRENT                                           NAME                                              CLUSTER       AUTHINFO      NAMESPACE*</span><br><span class="line">* arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1</span><br><span class="line">  arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-2   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-2   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-2</span><br></pre></td></tr></table></figure>

<p><strong>To delete an cluster context:</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl config delete-context arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-2</span><br><span class="line"></span><br><span class="line">$ kubectl config get-contexts </span><br><span class="line">  CURRENT                                           NAME                                              CLUSTER       AUTHINFO      NAMESPACE</span><br><span class="line">* arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1   arn:aws:eks:us-west-2:xxxxxx:cluster/cluster-1</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tech</category>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
</search>
