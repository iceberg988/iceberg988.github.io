<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/logo/FlamingBytes-icon-64x64-1.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo/FlamingBytes-icon-64x64-1.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo/FlamingBytes-icon-64x64-1.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.flamingbytes.com","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"default"},"fold":{"enable":true,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Stay hungry, stay foolish">
<meta property="og:type" content="website">
<meta property="og:title" content="FlamingBytes">
<meta property="og:url" content="https://www.flamingbytes.com/page/11/index.html">
<meta property="og:site_name" content="FlamingBytes">
<meta property="og:description" content="Stay hungry, stay foolish">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="relentlesstorm">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://www.flamingbytes.com/page/11/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/11/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>FlamingBytes</title>
  







<!-- Google Adsense-->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8578408127828851"
     crossorigin="anonymous"></script>

<!-- Google tag (gtag.js) for analytics-->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B8PQ47L2H0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-B8PQ47L2H0');
</script>

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">FlamingBytes</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">10</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">99</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">259</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="relentlesstorm"
      src="/images/logo/FlamingBytes-icon-64x64-1.png">
  <p class="site-author-name" itemprop="name">relentlesstorm</p>
  <div class="site-description" itemprop="description">Stay hungry, stay foolish</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">259</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">99</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:relentlesstorm@gmail.com" title="E-Mail → mailto:relentlesstorm@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://www.flamingbytes.com/blog/cgroups-limit-memory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo/FlamingBytes-icon-64x64-1.png">
      <meta itemprop="name" content="relentlesstorm">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlamingBytes">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | FlamingBytes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/cgroups-limit-memory/" class="post-title-link" itemprop="url">Using cgroups to limit Memory usage</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-12-04 13:08:00" itemprop="dateCreated datePublished" datetime="2022-12-04T13:08:00-08:00">2022-12-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-22 11:28:20" itemprop="dateModified" datetime="2023-10-22T11:28:20-07:00">2023-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/" itemprop="url" rel="index"><span itemprop="name">Tech</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>The memory controller isolates the memory behaviour of a group of tasks from the rest of the system.</p>
<pre><code>$ mount | egrep &quot;/cgroup |/memory&quot;
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)

$ lssubsys -am | grep memory
memory /sys/fs/cgroup/memory
</code></pre>
<p>In this post, we will learn how to use the following control files to limit and monitor memory usage for the user tasks.</p>
<ul>
<li>memory.usage_in_bytes - show current usage for memory</li>
<li>memory.limit_in_bytes - set&#x2F;show limit of memory usage</li>
</ul>
<h2 id="Create-memory-control-group"><a href="#Create-memory-control-group" class="headerlink" title="Create memory control group"></a>Create memory control group</h2><p>Install libcgroup package to manage cgroups:</p>
<pre><code>$ yum install libcgroup libcgroup-tools
</code></pre>
<p>Create memory control group:</p>
<pre><code>$ cgcreate -g memory:/memlimited
$ lscgroup | grep memory
memory:/
memory:/memlimited

$ ls /sys/fs/cgroup/memory/memlimited/
cgroup.clone_children           memory.kmem.slabinfo                memory.memsw.failcnt             memory.soft_limit_in_bytes
cgroup.event_control            memory.kmem.tcp.failcnt             memory.memsw.limit_in_bytes      memory.stat
cgroup.procs                    memory.kmem.tcp.limit_in_bytes      memory.memsw.max_usage_in_bytes  memory.swappiness
memory.failcnt                  memory.kmem.tcp.max_usage_in_bytes  memory.memsw.usage_in_bytes      memory.usage_in_bytes
memory.force_empty              memory.kmem.tcp.usage_in_bytes      memory.move_charge_at_immigrate  memory.use_hierarchy
memory.kmem.failcnt             memory.kmem.usage_in_bytes          memory.numa_stat                 notify_on_release
memory.kmem.limit_in_bytes      memory.limit_in_bytes               memory.oom_control               tasks
memory.kmem.max_usage_in_bytes  memory.max_usage_in_bytes           memory.pressure_level
</code></pre>
<h2 id="Limit-the-memory-usage"><a href="#Limit-the-memory-usage" class="headerlink" title="Limit the memory usage"></a>Limit the memory usage</h2><h3 id="Using-control-files-directly"><a href="#Using-control-files-directly" class="headerlink" title="Using control files directly"></a>Using control files directly</h3><pre><code>$ echo 32G &gt; /sys/fs/cgroup/memory/memlimited/memory.limit_in_bytes
$ cat /sys/fs/cgroup/memory/memlimited/memory.limit_in_bytes
34359738368
</code></pre>
<h3 id="Using-libcgroup-tools"><a href="#Using-libcgroup-tools" class="headerlink" title="Using libcgroup tools"></a>Using libcgroup tools</h3><p>Limit the memory usage:</p>
<pre><code>$ cgset -r memory.limit_in_bytes=32G memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 34359738368
</code></pre>
<h2 id="Verify-the-memory-usage"><a href="#Verify-the-memory-usage" class="headerlink" title="Verify the memory usage"></a>Verify the memory usage</h2><h3 id="Unlimit-the-memory-usage"><a href="#Unlimit-the-memory-usage" class="headerlink" title="Unlimit the memory usage"></a>Unlimit the memory usage</h3><pre><code>$ cgset -r memory.limit_in_bytes=-1 memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 9223372036854771712
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code>$ echo 3 &gt; /proc/sys/vm/drop_caches
$ cgexec -g memory:memlimited fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is unlimited:</p>
<pre><code>$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
14143488
14143488
819499008
13288558592
25772953600
38258790400
50776608768
55638511616
55638429696
55638478848
55638528000
55638577152
55210229760
55210229760
^C
</code></pre>
<h3 id="Limit-the-memory-usage-to-32GB"><a href="#Limit-the-memory-usage-to-32GB" class="headerlink" title="Limit the memory usage to 32GB"></a>Limit the memory usage to 32GB</h3><pre><code>$ cgset -r memory.limit_in_bytes=32G memlimited
$ cgget -r memory.limit_in_bytes memlimited
memlimited:
memory.limit_in_bytes: 34359738368
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code>$ echo 3 &gt; /proc/sys/vm/drop_caches
$ cgexec -g memory:memlimited fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is limited to 32GB:</p>
<pre><code>$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
9134080
6819614720
18048208896
29089763328
34359726080
34359672832
34359607296
34359717888
34359635968
34359619584
34280120320
34280120320
^C
</code></pre>
<p>From vmstat output, the cache usage is limited to 32GB. There is also swapping out activity due the memory pressure.</p>
<pre><code>$ vmstat 5 -t
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 UTC
 0  0      0 1053891776    968 142992    0    0     0    30    0    0  0  0 100  0  0 2022-12-23 22:04:30
 2  0      0 1045755200    968 8266836    0    0  1697     0 3789  727  2  1 98  0  0 2022-12-23 22:04:35
 1  0      0 1034800320    976 19222628    0    0     0     2 1770  575  0  1 99  0  0 2022-12-23 22:04:40
 1  0      0 1024034304    984 29988256    0    0     0 163843 2999  780  0  1 99  0  0 2022-12-23 22:04:45
 1  1      0 1020353664    992 33665560    0    0     0 985500 8317 2414  0  1 98  0  0 2022-12-23 22:04:50
 1  1 317440 1020354112   1000 33666764    0 63474     3 1518376 18524 2604  0  1 98  1  0 2022-12-23 22:04:55
 1  1 340480 1020353472   1008 33666660    0 4571     0 1602014 15377 2381  0  1 98  1  0 2022-12-23 22:05:00
 2  0 340480 1020355904   1016 33667212   39    0    39 1632866 22166 70618  0  1 98  0  0 2022-12-23 22:05:05
 1  0 340480 1020356032   1020 33667228    0    0     0 1861355 27437 108029  0  1 99  0  0 2022-12-23 22:05:10
 2  0 340480 1020356224   1024 33667236    0    0     0 1874438 28732 111364  0  1 98  0  0 2022-12-23 22:05:15
 0  0    512 1020432704   1024 33599516  212    0   550 915429 13644 56805  0  1 99  0  0 2022-12-23 22:05:20
 0  0    512 1020432960   1028 33599516    0    0     3     4  165  149  0  0 100  0  0 2022-12-23 22:05:25
^C
</code></pre>
<h3 id="Limit-the-memory-usage-for-the-tasks-in-the-current-bash"><a href="#Limit-the-memory-usage-for-the-tasks-in-the-current-bash" class="headerlink" title="Limit the memory usage for the tasks in the current bash"></a>Limit the memory usage for the tasks in the current bash</h3><pre><code>$ cat /sys/fs/cgroup/memory/memlimited/tasks
$ echo $$ &gt; /sys/fs/cgroup/memory/memlimited/tasks
$ cat /sys/fs/cgroup/memory/memlimited/tasks
27875
28889

$ ps -ef | egrep &quot;27875|29023&quot; | grep -v grep
root     27875 27873  0 21:11 pts/0    00:00:17 -bash
root     29026 27875  0 22:11 pts/0    00:00:00 ps -ef
</code></pre>
<p>Use fio to write 50G data:</p>
<pre><code>$ fio --blocksize=64k --ioengine=libaio --readwrite=write --filesize=50G --group_reporting --direct=0 --iodepth=128 --end_fsync=1 --name=job1 --filename=/mnt/fio.dat
</code></pre>
<p>Verify the memory usage is limited to 32GB:</p>
<pre><code>$ while true; do cat /sys/fs/cgroup/memory/memlimited/memory.usage_in_bytes; sleep 5; done
22835200
22835200
22835200
10150678528
21453983744
32693850112
34359607296
34359607296
34359738368
34359730176
34359730176
34359660544
34280062976
^C


$ vmstat 5 -t
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 UTC
 0  0    512 1053886784   1004 144860    0    0     0    31    0    0  0  0 100  0  0 2022-12-23 22:09:17
 0  0    512 1053886528   1004 144824    0    0     0     0  143  148  0  0 100  0  0 2022-12-23 22:09:22
 1  0    512 1050907776   1004 3109688    0    0  1660     3 2690  566  1  0 98  0  0 2022-12-23 22:09:27
 1  0    512 1039836288   1012 14180528    0    0     0     2 1699  508  0  1 99  0  0 2022-12-23 22:09:32
 1  0    512 1028822720   1020 25194536    0    0     0     2 1625  494  0  1 99  0  0 2022-12-23 22:09:37
 1  1    512 1020348800   1028 33665604    0    0     0 385026 4478 1313  0  1 99  0  0 2022-12-23 22:09:42
 1  1  46080 1020340224   1036 33676796    0 9090     0 1563114 13781 2301  0  1 98  1  0 2022-12-23 22:09:47
 1  1 340480 1020338688   1044 33676544    0 58904     0 1557223 19212 2580  0  1 98  1  0 2022-12-23 22:09:52
 1  1 340480 1020337792   1052 33676376   33    0    33 1481028 14133 5776  0  1 98  1  0 2022-12-23 22:09:57
 1  0 340480 1020342208   1056 33676396    0    0   184 1877767 27447 114955  0  1 98  0  0 2022-12-23 22:10:02
 1  1 340480 1020341056   1056 33677100    0    0     0 1817494 27635 101675  0  1 98  0  0 2022-12-23 22:10:07
 1  0 195144 1020350208   1060 33676988   83    0    83 1872111 27068 113280  0  1 98  0  0 2022-12-23 22:10:12
^C
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://docs.kernel.org/admin-guide/cgroup-v1/memory.html">https://docs.kernel.org/admin-guide/cgroup-v1/memory.html</a></li>
<li><a target="_blank" rel="noopener" href="https://engineering.linkedin.com/blog/2016/08/don_t-let-linux-control-groups-uncontrolled">https://engineering.linkedin.com/blog/2016/08/don_t-let-linux-control-groups-uncontrolled</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://www.flamingbytes.com/blog/cockroachdb-performance-benchmarking/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo/FlamingBytes-icon-64x64-1.png">
      <meta itemprop="name" content="relentlesstorm">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlamingBytes">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | FlamingBytes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/cockroachdb-performance-benchmarking/" class="post-title-link" itemprop="url">CockroachDB performance benchmarking</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-12-04 08:00:00" itemprop="dateCreated datePublished" datetime="2022-12-04T08:00:00-08:00">2022-12-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-22 11:28:20" itemprop="dateModified" datetime="2023-10-22T11:28:20-07:00">2023-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/" itemprop="url" rel="index"><span itemprop="name">Tech</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/Benchmarking/" itemprop="url" rel="index"><span itemprop="name">Benchmarking</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="CockroachDB-key-concepts"><a href="#CockroachDB-key-concepts" class="headerlink" title="CockroachDB key concepts"></a>CockroachDB key concepts</h2><p>Range -	CockroachDB stores all user data (tables, indexes, etc.) and almost all system data in a giant sorted map of key-value pairs. This keyspace is divided into “ranges”, contiguous chunks of the keyspace, so that every key can always be found in a single range.</p>
<p>From a SQL perspective, a table and its secondary indexes initially map to a single range, where each key-value pair in the range represents a single row in the table (also called the primary index because the table is sorted by the primary key) or a single row in a secondary index. As soon as that range reaches 512 MiB in size, it splits into two ranges. This process continues for these new ranges as the table and its indexes continue growing.</p>
<pre><code>[root@host1 ~]# tail -f /var/log/cockroachdb_logs/cockroach.log
I220908 17:54:02.220498 5523606 kv/kvserver/pkg/kv/kvserver/replica_command.go:420 ⋮ [n1,split,s1,r499/1:‹/Table/113/1/4&#123;394/6…-693/6…&#125;›] 1700  initiating a split of this range at key ‹/Table/113/1/4402/59627› [r501] (‹512 MiB above threshold size 512 MiB›)‹›
</code></pre>
<p>Replica -	CockroachDB replicates each range (3 times by default) and stores each replica on a different node.</p>
<p>Refer to <a target="_blank" rel="noopener" href="https://www.cockroachlabs.com/docs/stable/architecture/reads-and-writes-overview.html#cockroachdb-architecture-terms">here</a> for more.</p>
<p><img src="/images/cockroachdb-arch-terms.png" alt="Image"></p>
<h2 id="Production-checklist"><a href="#Production-checklist" class="headerlink" title="Production checklist"></a>Production checklist</h2><p>Check <a target="_blank" rel="noopener" href="https://www.cockroachlabs.com/docs/stable/recommended-production-settings.html">here</a> for the important recommendations for production deployments of CockroachDB. The following only lists some of the recommended settings.</p>
<h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><ul>
<li>Disable Linux memory swapping. Over-allocating memory on production machines can lead to unexpected performance issues when pages have to be read back into memory.</li>
<li>For production deployments, set –cache to 25% or higher. Avoid setting –cache and –max-sql-memory to a combined value of more than 75% of a machine’s total RAM. Doing so increases the risk of memory-related failures.</li>
</ul>
<h3 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h3><ul>
<li>The maximum recommended storage capacity per node is 2.5 TiB, regardless of the number of vCPUs.</li>
<li>Use dedicated volumes for the CockroachDB store. Do not share the store volume with any other I&#x2F;O activity.</li>
<li>Store CockroachDB log files in a separate volume from the main data store so that logging is not impacted by I&#x2F;O throttling.</li>
<li>The recommended Linux filesystems are ext4 and XFS.</li>
</ul>
<h3 id="Disk-I-O"><a href="#Disk-I-O" class="headerlink" title="Disk I&#x2F;O"></a>Disk I&#x2F;O</h3><ul>
<li>Use sysbench to benchmark IOPS on your cluster. If IOPS decrease, add more nodes to your cluster to increase IOPS.</li>
<li>Do not use LVM in the I&#x2F;O path. Dynamically resizing CockroachDB store volumes can result in significant performance degradation. Using LVM snapshots in lieu of CockroachDB backup and restore is also not supported.</li>
<li>The optimal configuration for striping more than one device is RAID 10. RAID 0 and 1 are also acceptable from a performance perspective.</li>
</ul>
<h3 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h3><p>When starting a node, two main flags are used to control its network connections:</p>
<ul>
<li>–listen-addr determines which address(es) to listen on for connections from other nodes and clients.</li>
<li>–advertise-addr determines which address to tell other nodes to use.</li>
</ul>
<p><img src="/images/cockroachdb-network.png" alt="Image"></p>
<h3 id="Load-balancing"><a href="#Load-balancing" class="headerlink" title="Load balancing"></a>Load balancing</h3><p>Each CockroachDB node is an equally suitable SQL gateway to a cluster, but to ensure client performance and reliability, it’s important to use load balancing:</p>
<ul>
<li>Performance: Load balancers spread client traffic across nodes. This prevents any one node from being overwhelmed by requests and improves overall cluster performance (queries per second).</li>
<li>Reliability: Load balancers decouple client health from the health of a single CockroachDB node. To ensure that traffic is not directed to failed nodes or nodes that are not ready to receive requests, load balancers should use CockroachDB’s readiness health check.</li>
</ul>
<h3 id="Cache-and-SQL-memory-size"><a href="#Cache-and-SQL-memory-size" class="headerlink" title="Cache and SQL memory size"></a>Cache and SQL memory size</h3><p>CockroachDB manages its own memory caches, independently of the operating system. These are configured via the –cache and –max-sql-memory flags.</p>
<p>Each node has a default cache size of 128MiB that is passively consumed. The default was chosen to facilitate development and testing, where users are likely to run multiple CockroachDB nodes on a single machine. Increasing the cache size will generally improve the node’s read performance.</p>
<p>Each node has a default SQL memory size of 25%. This memory is used as-needed by active operations to store temporary data for SQL queries.</p>
<ul>
<li>Increasing a node’s cache size will improve the node’s read performance.</li>
<li>Increasing a node’s SQL memory size will increase the number of simultaneous client connections it allows, as well as the node’s capacity for in-memory processing of rows when using ORDER BY, GROUP BY, DISTINCT, joins, and window functions.</li>
</ul>
<p>You can check cache size and SQL memory pool size in the log. In the following example output, it matches with the specified 25% cache and SQL memory size setting.</p>
<pre><code>$ vim cockroach.log
I220905 17:49:33.671304 1 server/config.go:487 ⋮ [n?] 6  system total memory: 1008 GiB
I220905 17:49:33.671318 1 server/config.go:489 ⋮ [n?] 7  server configuration:
I220905 17:49:33.671318 1 server/config.go:489 ⋮ [n?] 7 +‹max offset             500000000›
I220905 17:49:33.671318 1 server/config.go:489 ⋮ [n?] 7 +‹cache size             252 GiB›
I220905 17:49:33.671318 1 server/config.go:489 ⋮ [n?] 7 +‹SQL memory pool size   252 GiB›
</code></pre>
<h2 id="CockroachDB-workloads"><a href="#CockroachDB-workloads" class="headerlink" title="CockroachDB workloads"></a>CockroachDB workloads</h2><p><img src="/images/rockroachdb_workloads.png" alt="Image"></p>
<h3 id="bank-workload"><a href="#bank-workload" class="headerlink" title="bank workload"></a>bank workload</h3><pre><code>$ cockroach workload init bank &#39;postgresql://root@host1:26257?sslmode=disable&#39;
I220831 23:52:14.593170 1 workload/workloadsql/dataload.go:146  [-] 1  imported bank (0s, 1000 rows)
I220831 23:52:14.609421 1 workload/workloadsql/workloadsql.go:136  [-] 2  starting 9 splits


$ cockroach workload run bank --duration=1m &#39;postgresql://root@host1:26257?sslmode=disable&#39;
I220831 23:52:55.378492 1 workload/cli/run.go:414  [-] 1  creating load generator...
I220831 23:52:55.380594 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 2.103665ms)
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
    1.0s        0         2929.8         2936.4      3.0     15.2     50.3    234.9 transfer
    2.0s        0         3387.3         3161.8      3.4     13.6     23.1     54.5 transfer
    3.0s        0         2755.2         3026.3      3.9     13.6     23.1    302.0 transfer
    4.0s        0         3295.5         3093.6      3.3     13.6     33.6    469.8 transfer
    5.0s        0         3536.5         3182.2      3.5     12.1     18.9     35.7 transfer
    6.0s        0         3558.1         3244.8      3.5     11.0     21.0     39.8 transfer
    7.0s        0         3566.9         3290.8      3.7     11.0     17.8     39.8 transfer
    8.0s        0         3317.6         3294.2      3.7     12.6     24.1     62.9 transfer
    9.0s        0         2992.9         3260.7      4.1     14.7     22.0     39.8 transfer
   10.0s        0         3628.2         3297.4      3.5     11.5     19.9     39.8 transfer
   11.0s        0         3604.5         3325.3      3.5     11.5     17.8     37.7 transfer
   12.0s        0         3668.4         3353.9      3.5     11.0     17.8     31.5 transfer
   13.0s        0         3485.9         3364.1      3.7     11.5     18.9     31.5 transfer
   14.0s        0         3377.9         3365.1      3.7     12.6     21.0     56.6 transfer
   15.0s        0         3084.4         3346.3      4.1     13.6     21.0     92.3 transfer
   16.0s        0         3650.1         3365.3      3.4     11.5     17.8     44.0 transfer
   17.0s        0         3662.7         3382.8      3.5     11.5     17.8     37.7 transfer
   18.0s        0         3461.8         3387.2      3.7     11.0     19.9    159.4 transfer
   19.0s        0         3426.3         3389.3      3.7     12.1     19.9     41.9 transfer
   20.0s        0         3196.0         3379.6      3.9     13.1     19.9     35.7 transfer
&lt;omitted..&gt;

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
   60.0s        0         206551         3442.5      4.6      3.7     12.1     19.9    469.8  transfer

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
   60.0s        0         206551         3442.5      4.6      3.7     12.1     19.9    469.8
</code></pre>
<h3 id="TPC-C-workload"><a href="#TPC-C-workload" class="headerlink" title="TPC-C workload"></a>TPC-C workload</h3><blockquote>
<p>–warehouses</p>
<blockquote>
<p>The number of warehouses for loading initial data, at approximately 200 MB per warehouse.</p>
<p>Applicable commands: init or run</p>
<p>Default: 1</p>
</blockquote>
</blockquote>
<blockquote>
<p>–workers</p>
<blockquote>
<p>The number of concurrent workers.</p>
<p>Applicable commands: init or run</p>
<p>Default: –warehouses * 10</p>
</blockquote>
</blockquote>
<p>The number os wareshouses can be specified by “–warehouses” option. By default, only one warehouse is created.</p>
<pre><code>$ cockroach workload init tpcc &#39;postgresql://root@host1:26257?sslmode=disable&#39;
Error: failed insert into warehouse: pq: duplicate key value violates unique constraint &quot;warehouse_pkey&quot;

$ cockroach workload init tpcc &#39;postgresql://root@host1:26257?sslmode=disable&#39; --drop
I220901 00:32:56.365593 1 workload/workloadsql/dataload.go:146  [-] 1  imported warehouse (0s, 1 rows)
I220901 00:32:56.372727 1 workload/workloadsql/dataload.go:146  [-] 2  imported district (0s, 10 rows)
I220901 00:32:57.034349 1 workload/workloadsql/dataload.go:146  [-] 3  imported customer (1s, 30000 rows)
I220901 00:32:57.248382 1 workload/workloadsql/dataload.go:146  [-] 4  imported history (0s, 30000 rows)
I220901 00:32:57.462320 1 workload/workloadsql/dataload.go:146  [-] 5  imported order (0s, 30000 rows)
I220901 00:32:57.490476 1 workload/workloadsql/dataload.go:146  [-] 6  imported new_order (0s, 9000 rows)
I220901 00:32:57.921658 1 workload/workloadsql/dataload.go:146  [-] 7  imported item (0s, 100000 rows)
I220901 00:32:59.267268 1 workload/workloadsql/dataload.go:146  [-] 8  imported stock (1s, 100000 rows)
I220901 00:33:00.916392 1 workload/workloadsql/dataload.go:146  [-] 9  imported order_line (2s, 300343 rows)


$ cockroach workload run tpcc --duration=10m &#39;postgresql://root@host1:26257?sslmode=disable&#39;
I220901 00:34:17.147411 1 workload/cli/run.go:414  [-] 1  creating load generator...
Initializing 2 connections...
Initializing 0 idle connections...
Initializing 10 workers and preparing statements...
I220901 00:34:17.151817 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 4.411152ms)
&lt;omitted..&gt;

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0             12            0.0     32.6     31.5     44.0     44.0     44.0  delivery

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0            120            0.2     17.9     17.8     26.2     41.9     54.5  newOrder

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0             13            0.0      6.7      5.8     14.2     15.2     15.2  orderStatus

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0            131            0.2     10.8     11.0     14.7     24.1     29.4  payment

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0             13            0.0     10.8     10.5     15.7     15.7     15.7  stockLevel

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  600.0s        0            289            0.5     14.4     13.1     28.3     41.9     54.5
Audit check 9.2.1.7: SKIP: not enough delivery transactions to be statistically significant
Audit check 9.2.2.5.1: SKIP: not enough orders to be statistically significant
Audit check 9.2.2.5.2: SKIP: not enough orders to be statistically significant
Audit check 9.2.2.5.5: SKIP: not enough payments to be statistically significant
Audit check 9.2.2.5.6: SKIP: not enough order status transactions to be statistically significant
Audit check 9.2.2.5.3: PASS
Audit check 9.2.2.5.4: PASS

_elapsed_______tpmC____efc__avg(ms)__p50(ms)__p90(ms)__p95(ms)__p99(ms)_pMax(ms)
  600.0s       12.0  93.3%     17.9     17.8     23.1     26.2     41.9     54.5
</code></pre>
<h3 id="YCSB-workload"><a href="#YCSB-workload" class="headerlink" title="YCSB workload"></a>YCSB workload</h3><pre><code>$ cockroach workload init ycsb &#39;postgresql://root@host1:26257?sslmode=disable&#39;
I220831 23:57:53.909658 1 workload/workloadsql/dataload.go:146  [-] 1  imported usertable (2s, 10000 rows)


$ cockroach workload run ycsb --duration=10m &#39;postgresql://root@host1:26257?sslmode=disable&#39;
I220831 23:58:24.410319 1 workload/cli/run.go:414  [-] 1  creating load generator...
I220831 23:58:24.427528 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 17.212701ms)
_elapsed___errors__ops/sec(inst)___ops/sec(cum)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)
    1.0s        0        11011.5        11243.7      1.1      2.4      3.8     11.0 read
    1.0s        0          581.7          593.9      3.0      5.5      7.3     12.6 update
    2.0s        0        12105.1        11674.4      1.1      1.8      3.4     10.0 read
    2.0s        0          636.9          615.4      2.8      3.9      6.6     11.0 update
    3.0s        0        11590.6        11646.5      1.1      2.0      4.5     14.2 read
    3.0s        0          607.1          612.7      2.8      6.0      9.4     12.1 update
    4.0s        0        11813.5        11688.2      1.1      1.9      3.5     13.1 read
    4.0s        0          622.0          615.0      2.8      4.5      7.6     13.6 update
    5.0s        0        11959.5        11742.5      1.1      2.0      3.3      8.1 read
    5.0s        0          607.0          613.4      2.6      4.7      7.1     12.1 update
    6.0s        0        12186.2        11816.5      1.0      1.8      3.5     12.6 read
    6.0s        0          638.0          617.5      2.6      4.1     10.0     16.8 update
    7.0s        0        11815.8        11816.3      1.1      2.0      3.7      8.4 read
    7.0s        0          646.0          621.6      2.8      4.7      6.8      9.4 update
    8.0s        0        11850.8        11820.6      1.1      2.0      3.3      7.6 read
    8.0s        0          617.9          621.1      2.8      4.5      6.6      9.4 update
    9.0s        0        11713.5        11808.7      1.1      2.0      3.9     11.0 read
    9.0s        0          609.1          619.8      2.8      4.5      7.6     11.5 update
   10.0s        0        11949.6        11822.8      1.1      2.0      3.3     13.1 read
   10.0s        0          622.0          620.0      2.8      4.2      6.0      8.4 update

&lt;omitted..&gt;

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0        8576709        14294.5      1.0      1.0      1.6      2.9     62.9  read

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__total
  600.0s        0         451026          751.7      2.5      2.5      3.8      6.6     27.3  update

_elapsed___errors_____ops(total)___ops/sec(cum)__avg(ms)__p50(ms)__p95(ms)__p99(ms)_pMax(ms)__result
  600.0s        0        9027735        15046.2      1.1      1.0      2.2      3.5     62.9
</code></pre>
<h2 id="Restart-CockroachDB-cluster"><a href="#Restart-CockroachDB-cluster" class="headerlink" title="Restart CockroachDB cluster"></a>Restart CockroachDB cluster</h2><ol>
<li><p>Stop the process on each node</p>
<p> $ ps -ef | grep cock | grep -v grep<br> root     12217     1 99 Sep06 ?        2-20:28:09 cockroach start –log-dir&#x3D;&#x2F;var&#x2F;log&#x2F;cockroachdb_logs –store&#x3D;&#x2F;mnt&#x2F;cockroachdb_mnt1 –insecure –advertise-addr&#x3D;host1 –join&#x3D;host1,host2,host3 –cache&#x3D;.25 –max-sql-memory&#x3D;.25</p>
<p> $ kill -9 12217<br> $ ps -ef | grep cock | grep -v grep</p>
</li>
<li><p>Start the process on each node</p>
<p> $ cockroach start –store&#x3D;&#x2F;mnt&#x2F;cockroanchdb_mnt1 –insecure –advertise-addr&#x3D;host1 –join&#x3D;host1,host2,host3 –cache&#x3D;.25 -max-sql-memory&#x3D;.25 –background</p>
</li>
</ol>
<h2 id="CockroachDB-commands"><a href="#CockroachDB-commands" class="headerlink" title="CockroachDB commands"></a>CockroachDB commands</h2><p>List node IDs:</p>
<pre><code>$ cockroach node ls --insecure
  id
------
   1
   2
   3
(3 rows)
</code></pre>
<p>Show node status:</p>
<pre><code>$ cockroach node status --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+----------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:23:09.022236 |          | true         | true
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:23:09.048582 |          | true         | true
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:23:09.402081 |          | true         | true
(3 rows)
</code></pre>
<p>Show status and range&#x2F;replica details:</p>
<pre><code>$ cockroach node status --ranges --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live | replicas_leaders | replicas_leaseholders | ranges | ranges_unavailable | ranges_underreplicated
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:23:36.022463 |          | true         | true    |         306 |                   306 |    941 |                  0 |                      0
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:23:36.048508 |          | true         | true    |              310 |                   310 |    941 |                  0 |                      0
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:23:36.40421  |          | true         | true    |              325 |                   325 |    941 |                  0 |                      0
(3 rows)
</code></pre>
<p>Show status and disk usage details:</p>
<pre><code>$ cockroach node status --stats --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live |  live_bytes  |  key_bytes  | value_bytes  | intent_bytes | system_bytes
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------+--------------+-------------+--------------+--------------+---------------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:23:45.022008 |          | true         | true    | 246665681135 | 47930398573 | 220956161393 |        40331 |       812555
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:23:45.048777 |          | true         | true    | 246665683880 | 47930401894 | 220956192532 |        28025 |       817028
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:23:45.40156  |          | true         | true    | 246666008478 | 47930560065 | 220957658233 |        74382 |       741028
(3 rows)
</code></pre>
<p>Show status and decommissioning details for active and inactive nodes:</p>
<pre><code>$ cockroach node status --decommission --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live | gossiped_replicas | is_decommissioning | membership | is_draining
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:24:03.043432 |          | true         | true    |               941 | false              | active     | false
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:24:03.048948 |          | true         | true    |               941 | false              | active     | false
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:24:03.401953 |          | true         | true    |               941 | false              | active     | false
(3 rows)
</code></pre>
<p>Show complete status details for active and inactive nodes:</p>
<pre><code>$ cockroach node status --all --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live | replicas_leaders | replicas_leaseholders | ranges | ranges_unavailable | ranges_underreplicated |  live_bytes  |  key_bytes  | value_bytes  | intent_bytes | system_bytes | gossiped_replicas | is_decommissioning | membership | is_draining
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+---------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:24:07.539077 |          |         true |    true |              306 |                   306 |    942 |                  0 |                      0 | 246677946612 | 47939212836 | 221027696131 |        81398 |       722359 |        941        |              false |   active   |    false
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-03 16:24:12.04249  | 2022-09-03 22:24:07.548886 |          |         true |    true |              311 |                   311 |    942 |                  0 |                      0 | 246677952989 | 47939228198 | 221027812276 |        67816 |       731258 |        941        |              false |   active   |    false
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-03 16:24:12.390521 | 2022-09-03 22:24:07.901765 |          |         true |    true |              325 |                   325 |    942 |                  0 |                      0 | 246678252011 | 47939394591 | 221029217819 |        55249 |       869542 |        941        |              false |   active   |    false
(3 rows)
</code></pre>
<p>Show status details for a specific node:</p>
<pre><code>$ cockroach node status 1 --insecure
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+----------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-03 16:24:12.001861 | 2022-09-03 22:24:30.044382 |          | true         | true
(1 row)
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/cockroachdb/cockroach/blob/master/docs/design.md">https://github.com/cockroachdb/cockroach/blob/master/docs/design.md</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://www.flamingbytes.com/blog/cgroup-limit-cpu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo/FlamingBytes-icon-64x64-1.png">
      <meta itemprop="name" content="relentlesstorm">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlamingBytes">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | FlamingBytes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/cgroup-limit-cpu/" class="post-title-link" itemprop="url">Using cgroups to limit CPU utilization</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-12-03 12:09:00" itemprop="dateCreated datePublished" datetime="2022-12-03T12:09:00-08:00">2022-12-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-22 11:28:20" itemprop="dateModified" datetime="2023-10-22T11:28:20-07:00">2023-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/" itemprop="url" rel="index"><span itemprop="name">Tech</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Intro-to-cgroups"><a href="#Intro-to-cgroups" class="headerlink" title="Intro to cgroups"></a>Intro to cgroups</h2><p>Cgroups(control groups) make it possible to allocate system resources such as CPU time, memory, disk I&#x2F;O and network bandwidth, or combinations of them,  among a group of tasks(processes) running on a system.</p>
<p>The following commands output the available subsystems(resource controllers) for the cgroups. Each subsystem has a bunch of tunables to control the resource allocation.</p>
<pre><code>$ lssubsys -am
cpuset /sys/fs/cgroup/cpuset
cpu,cpuacct /sys/fs/cgroup/cpu,cpuacct
blkio /sys/fs/cgroup/blkio
memory /sys/fs/cgroup/memory
devices /sys/fs/cgroup/devices
freezer /sys/fs/cgroup/freezer
net_cls,net_prio /sys/fs/cgroup/net_cls,net_prio
perf_event /sys/fs/cgroup/perf_event
hugetlb /sys/fs/cgroup/hugetlb
pids /sys/fs/cgroup/pids
rdma /sys/fs/cgroup/rdma

$ mount | grep cgroup
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
</code></pre>
<h2 id="CPU-subsystem-and-tunables"><a href="#CPU-subsystem-and-tunables" class="headerlink" title="CPU subsystem and tunables"></a>CPU subsystem and tunables</h2><h3 id="Ceiling-enforcement-parameters"><a href="#Ceiling-enforcement-parameters" class="headerlink" title="Ceiling enforcement parameters"></a>Ceiling enforcement parameters</h3><p>cpu.cfs_period_us</p>
<p>specifies a period of time in microseconds (µs, represented here as “us”) for how regularly a cgroup’s access to CPU resources should be reallocated. If tasks in a cgroup should be able to access a single CPU for 0.2 seconds out of every 1 second, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 1000000. The upper limit of the cpu.cfs_quota_us parameter is 1 second and the lower limit is 1000 microseconds.</p>
<p>cpu.cfs_quota_us</p>
<p>specifies the total amount of time in microseconds (µs, represented here as “us”) for which all tasks in a cgroup can run during one period (as defined by cpu.cfs_period_us). As soon as tasks in a cgroup use up all the time specified by the quota, they are throttled for the remainder of the time specified by the period and not allowed to run until the next period. If tasks in a cgroup should be able to access a single CPU for 0.2 seconds out of every 1 second, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 1000000. Note that the quota and period parameters operate on a CPU basis. To allow a process to fully utilize two CPUs, for example, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 100000.</p>
<p>Setting the value in cpu.cfs_quota_us to -1 indicates that the cgroup does not adhere to any CPU time restrictions. This is also the default value for every cgroup (except the root cgroup).</p>
<h3 id="Relative-shares-parameter"><a href="#Relative-shares-parameter" class="headerlink" title="Relative shares parameter"></a>Relative shares parameter</h3><p>cpu.shares</p>
<p>contains an integer value that specifies a relative share of CPU time available to the tasks in a cgroup. For example, tasks in two cgroups that have cpu.shares set to 100 will receive equal CPU time, but tasks in a cgroup that has cpu.shares set to 200 receive twice the CPU time of tasks in a cgroup where cpu.shares is set to 100. The value specified in the cpu.shares file must be 2 or higher.</p>
<p>Note that shares of CPU time are distributed per all CPU cores on multi-core systems. Even if a cgroup is limited to less than 100% of CPU on a multi-core system, it may use 100% of each individual CPU core.</p>
<p>Using relative shares to specify CPU access has two implications on resource management that should be considered:</p>
<p>Because the CFS does not demand equal usage of CPU, it is hard to predict how much CPU time a cgroup will be allowed to utilize. When tasks in one   cgroup are idle and are not using any CPU time, the leftover time is collected in a global pool of unused CPU cycles. Other cgroups are allowed to borrow CPU cycles from this pool.</p>
<p>The actual amount of CPU time that is available to a cgroup can vary depending on the number of cgroups that exist on the system. If a cgroup has a relative share of 1000 and two other cgroups have a relative share of 500, the first cgroup receives 50% of all CPU time in cases when processes in all cgroups attempt to use 100% of the CPU. However, if another cgroup is added with a relative share of 1000, the first cgroup is only allowed 33% of the CPU (the rest of the cgroups receive 16.5%, 16.5%, and 33% of CPU).</p>
<h2 id="Using-libcgroup-tools"><a href="#Using-libcgroup-tools" class="headerlink" title="Using libcgroup tools"></a>Using libcgroup tools</h2><p>Install libcgroup package to manage cgroups:</p>
<pre><code>$ yum install libcgroup libcgroup-tools
</code></pre>
<p>List the cgroups:</p>
<pre><code>$ lscgroup
hugetlb:/
cpu,cpuacct:/
cpuset:/
blkio:/
memory:/
freezer:/
net_cls,net_prio:/
pids:/
rdma:/
perf_event:/
devices:/
devices:/system.slice
devices:/system.slice/irqbalance.service
devices:/system.slice/systemd-udevd.service
devices:/system.slice/polkit.service
devices:/system.slice/chronyd.service
devices:/system.slice/auditd.service
devices:/system.slice/tuned.service
devices:/system.slice/systemd-journald.service
devices:/system.slice/sshd.service
devices:/system.slice/crond.service
devices:/system.slice/NetworkManager.service
devices:/system.slice/rsyslog.service
devices:/system.slice/abrtd.service
devices:/system.slice/lvm2-lvmetad.service
devices:/system.slice/postfix.service
devices:/system.slice/dbus.service
devices:/system.slice/system-getty.slice
devices:/system.slice/systemd-logind.service
devices:/system.slice/abrt-oops.service

$ ls /sys/fs/cgroup
blkio  cpuacct      cpuset   freezer  memory   net_cls,net_prio  perf_event  rdma
cpu    cpu,cpuacct  devices  hugetlb  net_cls  net_prio          pids        systemd
</code></pre>
<p>Create the cgroup:</p>
<pre><code>$ cgcreate -g cpu:/cpulimited

$ lscgroup | grep cpulimited
cpu,cpuacct:/cpulimited

$ ls cpulimited/
cgroup.clone_children  cpuacct.usage_percpu       cpu.cfs_period_us  cpu.stat
cgroup.procs           cpuacct.usage_percpu_sys   cpu.cfs_quota_us   notify_on_release
cpuacct.stat           cpuacct.usage_percpu_user  cpu.rt_period_us   tasks
cpuacct.usage          cpuacct.usage_sys          cpu.rt_runtime_us
cpuacct.usage_all      cpuacct.usage_user         cpu.shares
</code></pre>
<p>Limit CPU utilization by percentage:</p>
<pre><code>$ lscpu | grep ^CPU\(s\):
CPU(s):                96

$ cgset -r cpu.cfs_quota_us=200000 cpulimited
</code></pre>
<p>Check the cgroup settings:</p>
<pre><code>$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: 200000

$ cgget -g cpu:cpulimited
cpulimited:
cpu.cfs_period_us: 100000
cpu.stat: nr_periods 2
    nr_throttled 0
    throttled_time 0
cpu.shares: 1024
cpu.cfs_quota_us: 200000
cpu.rt_runtime_us: 0
cpu.rt_period_us: 1000000
</code></pre>
<p>Delete the cgroup:</p>
<pre><code>$ cgdelete cpu,cpuacct:/cpulimited
</code></pre>
<h2 id="Verify-the-CPU-utilization-with-fio-workload"><a href="#Verify-the-CPU-utilization-with-fio-workload" class="headerlink" title="Verify the CPU utilization with fio workload"></a>Verify the CPU utilization with fio workload</h2><p>Create a fio job file:</p>
<pre><code>$ cat burn_cpu.job
[burn_cpu]
# Don&#39;t transfer any data, just burn CPU cycles
ioengine=cpuio
# Stress the CPU at 100%
cpuload=100
# Make 4 clones of the job
numjobs=4
</code></pre>
<p>Run the fio jobs without CPU limit:</p>
<pre><code>$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: -1

$ cgexec -g cpu:cpulimited fio burn_cpu.job
</code></pre>
<p>Check the CPU usage:</p>
<pre><code>$ top
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
13775 root      20   0 1079912   4016   2404 R 100.0  0.0   0:11.65 fio
13776 root      20   0 1079916   4004   2392 R 100.0  0.0   0:11.65 fio
13777 root      20   0 1079920   4004   2392 R 100.0  0.0   0:11.65 fio
13778 root      20   0 1079924   4004   2392 R 100.0  0.0   0:11.65 fio
</code></pre>
<p>The CPU utilization is 400% for the 4 fio jobs when there is no CPU limit set. Note that, there is totally 9600% CPU bandwidth available.</p>
<p>Limit the CPU utilization to 200%:</p>
<pre><code>$ cgset -r cpu.cfs_quota_us=200000 cpulimited

$ cgget -r cpu.cfs_quota_us cpulimited
cpulimited:
cpu.cfs_quota_us: 200000
</code></pre>
<p>Run the fio jobs again:</p>
<pre><code>$ cgexec -g cpu:cpulimited fio burn_cpu.job
</code></pre>
<p>Check the CPU usage:</p>
<pre><code>$ top
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
12908 root      20   0 1079916   3948   2336 R  50.3  0.0   0:06.91 fio
12909 root      20   0 1079920   3948   2336 R  50.0  0.0   0:06.88 fio
12910 root      20   0 1079924   3948   2336 R  50.0  0.0   0:06.93 fio
12907 root      20   0 1079912   3948   2336 R  49.3  0.0   0:06.86 fio
</code></pre>
<p>The CPU utilization is 200% for the 4 fio jobs when the CPU utilization is limited to 200%.</p>
<p>Check the processes are running on which CPU cores:</p>
<pre><code>$ mpstat -P ALL 5 | awk &#39;&#123;if ($3==&quot;CPU&quot; || $NF&lt;99)print;&#125;&#39;

12:40:32 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
12:40:37 AM  all    2.11    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   97.89
12:40:37 AM    0   20.52    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   79.48
12:40:37 AM    1   50.60    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.40
12:40:37 AM    2   50.10    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.90
12:40:37 AM   24   29.74    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   70.26
12:40:37 AM   87   50.20    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.80

12:40:37 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
12:40:42 AM  all    2.11    0.00    0.01    0.00    0.00    0.00    0.00    0.00    0.00   97.88
12:40:42 AM    0   11.49    0.00    0.20    0.00    0.00    0.00    0.00    0.00    0.00   88.31
12:40:42 AM    1   50.60    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.40
12:40:42 AM    2   50.30    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.70
12:40:42 AM   24   38.97    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   61.03
12:40:42 AM   87   49.90    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   50.10
</code></pre>
<p>The 4 fio jobs are running on 5 CPU cores with total utilization of 200%. So, it indicates this method limits the total CPU utilization out of all the CPU cores. However, the number of CPU cores is not limited.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://docs.kernel.org/admin-guide/cgroup-v1/cgroups.html">https://docs.kernel.org/admin-guide/cgroup-v1/cgroups.html</a></li>
<li><a target="_blank" rel="noopener" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpu">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux&#x2F;6&#x2F;html&#x2F;resource_management_guide&#x2F;sec-cpu</a></li>
<li><a target="_blank" rel="noopener" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/chap-using_libcgroup_tools">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux&#x2F;7&#x2F;html&#x2F;resource_management_guide&#x2F;chap-using_libcgroup_tools</a></li>
<li><a target="_blank" rel="noopener" href="https://scoutapm.com/blog/restricting-process-cpu-usage-using-nice-cpulimit-and-cgroups">https://scoutapm.com/blog/restricting-process-cpu-usage-using-nice-cpulimit-and-cgroups</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://www.flamingbytes.com/blog/deploy-cockroachdb-in-kubernetes-cluster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo/FlamingBytes-icon-64x64-1.png">
      <meta itemprop="name" content="relentlesstorm">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlamingBytes">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | FlamingBytes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/deploy-cockroachdb-in-kubernetes-cluster/" class="post-title-link" itemprop="url">Deploy CockroachDB in kubernetes cluster</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-12-03 08:00:00" itemprop="dateCreated datePublished" datetime="2022-12-03T08:00:00-08:00">2022-12-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-22 11:28:20" itemprop="dateModified" datetime="2023-10-22T11:28:20-07:00">2023-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/" itemprop="url" rel="index"><span itemprop="name">Tech</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Deploy-Kubernetes-cluster"><a href="#Deploy-Kubernetes-cluster" class="headerlink" title="Deploy Kubernetes cluster"></a>Deploy Kubernetes cluster</h2><pre><code>$ kubectl get nodes
NAME            STATUS   ROLES    AGE    VERSION
node0   Ready    &lt;none&gt;   115d   v1.19.2
node1   Ready    &lt;none&gt;   115d   v1.19.2
node2   Ready    &lt;none&gt;   115d   v1.19.2
node3   Ready    master   115d   v1.19.2
</code></pre>
<h2 id="Start-CockroachDB-cluster"><a href="#Start-CockroachDB-cluster" class="headerlink" title="Start CockroachDB cluster"></a>Start CockroachDB cluster</h2><p>Start the CockroachDB nodes with a configuration file that has been customized for performance:</p>
<pre><code>$ curl -O https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/performance/cockroachdb-statefulset-insecure.yaml

$ kubectl create -f cockroachdb-statefulset-insecure.yaml
service/cockroachdb-public created
service/cockroachdb created
poddisruptionbudget.policy/cockroachdb-budget created
statefulset.apps/cockroachdb created
</code></pre>
<p>Confirm that three pods are Running successfully. Note that they will not be considered Ready until after the cluster has been initialized:</p>
<pre><code>$ kubectl get pods
NAME            READY     STATUS    RESTARTS   AGE
cockroachdb-0   0/1       Running   0          2m
cockroachdb-1   0/1       Running   0          2m
cockroachdb-2   0/1       Running   0          2m
</code></pre>
<p>Confirm that the persistent volumes and corresponding claims were created successfully for all three pods:</p>
<pre><code>$ kubectl get pvc
NAME                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
datadir-cockroachdb-0   Bound    pvc-ac210538-2a20-4d99-9b5d-c5a03d733b4d   1Ti        RWO            px-repl1       35s
datadir-cockroachdb-1   Bound    pvc-54f24a59-a063-46bb-9645-d4292eb483a3   1Ti        RWO            px-repl1       25s
datadir-cockroachdb-2   Bound    pvc-0ebd7fa7-dc36-4c77-ab9d-003d10680f56   1Ti        RWO            px-repl1       15s

$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                           STORAGECLASS   REASON   AGE
pvc-0ebd7fa7-dc36-4c77-ab9d-003d10680f56   1Ti        RWO            Delete           Bound    default/datadir-cockroachdb-2   px-repl1                17s
pvc-54f24a59-a063-46bb-9645-d4292eb483a3   1Ti        RWO            Delete           Bound    default/datadir-cockroachdb-1   px-repl1                27s
pvc-ac210538-2a20-4d99-9b5d-c5a03d733b4d   1Ti        RWO            Delete           Bound    default/datadir-cockroachdb-0   px-repl1                27s
</code></pre>
<p>Before initialize the CockroachDB cluster, check the network(use ping command) to make sure each host can communicate with each other. Otherwise, you may run into connection failure between cluster nodes.</p>
<p>In my case, I want to run the cluster over the private network which doesn’t have DNS setup. So, I just add the private IP addresses and Kubernetes qualified hostnames and CockroachDB node names in &#x2F;etc&#x2F;hosts file on each host as below.</p>
<pre><code>10.0.0.1 ip-10-10-0-1 cockroachdb-0.cockroachdb
10.0.0.2 ip-10-10-0-2 cockroachdb-1.cockroachdb
10.0.0.3 ip-10-10-0-3 cockroachdb-2.cockroachdb
</code></pre>
<p>Use the provided cluster-init.yaml file to perform a one-time initialization that joins the CockroachDB nodes into a single cluster:</p>
<pre><code>$ kubectl create \
-f https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/cluster-init.yaml
</code></pre>
<p>Confirm that cluster initialization has completed successfully. The job should be considered successful and the Kubernetes pods should soon be considered Ready:</p>
<pre><code>$ kubectl get job cluster-init
NAME           COMPLETIONS   DURATION   AGE
cluster-init   1/1           7s         27s
kubectl get pods
NAME                 READY   STATUS      RESTARTS   AGE
cluster-init-cqf8l   0/1     Completed   0          56s
cockroachdb-0        1/1     Running     0          7m51s
cockroachdb-1        1/1     Running     0          7m51s
cockroachdb-2        1/1     Running     0          7m51s
</code></pre>
<h2 id="Use-the-built-in-SQL-client"><a href="#Use-the-built-in-SQL-client" class="headerlink" title="Use the built-in SQL client"></a>Use the built-in SQL client</h2><pre><code>$ kubectl run cockroachdb -it --image=cockroachdb/cockroach:v22.1.7 --rm --restart=Never -- sql --insecure --host=cockroachdb-public

root@cockroachdb-public:26257/defaultdb&gt; show databases;
  database_name | owner | primary_region | regions | survival_goal
----------------+-------+----------------+---------+----------------
  defaultdb     | root  | NULL           | &#123;&#125;      | NULL
  postgres      | root  | NULL           | &#123;&#125;      | NULL
  system        | node  | NULL           | &#123;&#125;      | NULL
(3 rows)


Time: 5ms total (execution 4ms / network 1ms)
</code></pre>
<h2 id="Run-TPC-C-workload-on-the-CockroachDB-cluster"><a href="#Run-TPC-C-workload-on-the-CockroachDB-cluster" class="headerlink" title="Run TPC-C workload on the CockroachDB cluster"></a>Run TPC-C workload on the CockroachDB cluster</h2><pre><code>$ kubectl run cockroachdb -it --image=cockroachdb/cockroach:v22.1.7 --rm --restart=Never -- bash
[root@cockroachdb cockroach]# cockroach node ls --insecure --host=cockroachdb-public
  id
------
   1
   2
   3
(3 rows)

[root@cockroachdb cockroach]# cockroach workload init tpcc &quot;postgresql://root@cockroachdb-public:26257?sslmode=disable&quot; --warehouses 10 --drop
I220922 19:48:15.543311 1 workload/workloadsql/dataload.go:146  [-] 1  imported warehouse (0s, 10 rows)
I220922 19:48:15.580634 1 workload/workloadsql/dataload.go:146  [-] 2  imported district (0s, 100 rows)
I220922 19:48:20.098986 1 workload/workloadsql/dataload.go:146  [-] 3  imported customer (5s, 300000 rows)
I220922 19:48:21.601978 1 workload/workloadsql/dataload.go:146  [-] 4  imported history (2s, 300000 rows)
I220922 19:48:24.317881 1 workload/workloadsql/dataload.go:146  [-] 5  imported order (3s, 300000 rows)
I220922 19:48:24.540100 1 workload/workloadsql/dataload.go:146  [-] 6  imported new_order (0s, 90000 rows)
I220922 19:48:24.998829 1 workload/workloadsql/dataload.go:146  [-] 7  imported item (0s, 100000 rows)
I220922 19:48:34.737491 1 workload/workloadsql/dataload.go:146  [-] 8  imported stock (10s, 1000000 rows)
I220922 19:48:49.030366 1 workload/workloadsql/dataload.go:146  [-] 9  imported order_line (14s, 3001222 rows)

[root@cockroachdb cockroach]# cockroach sql --insecure --database tpcc --host=cockroachdb-public:26257 -e &#39;show tables&#39;
  schema_name | table_name | type  | owner | estimated_row_count | locality
--------------+------------+-------+-------+---------------------+-----------
  public      | customer   | table | root  |              300000 | NULL
  public      | district   | table | root  |                 100 | NULL
  public      | history    | table | root  |              300000 | NULL
  public      | item       | table | root  |              100000 | NULL
  public      | new_order  | table | root  |               90000 | NULL
  public      | order      | table | root  |              300000 | NULL
  public      | order_line | table | root  |             3001222 | NULL
  public      | stock      | table | root  |             1000000 | NULL
  public      | warehouse  | table | root  |                  10 | NULL
(9 rows)

[root@cockroachdb cockroach]# cockroach workload run tpcc --warehouses=10 --ramp=10s --duration=20s postgres://root@cockroachdb-public:26257?sslmode=disable
I220922 19:52:41.633625 1 workload/cli/run.go:414  [-] 1  creating load generator...
Initializing 20 connections...
Initializing 0 idle connections...
Initializing 100 workers and preparing statements...
I220922 19:52:51.646000 1 workload/cli/run.go:445  [-] 2  creating load generator... done (took 10.012371773s)

_elapsed_______tpmC____efc__avg(ms)__p50(ms)__p90(ms)__p95(ms)__p99(ms)_pMax(ms)
   20.0s      153.0 119.0%     38.2     44.0     48.2     48.2     50.3     52.4
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.cockroachlabs.com/docs/v22.1/deploy-cockroachdb-with-kubernetes-insecure">https://www.cockroachlabs.com/docs/v22.1/deploy-cockroachdb-with-kubernetes-insecure</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cockroachlabs.com/docs/stable/kubernetes-performance.html">https://www.cockroachlabs.com/docs/stable/kubernetes-performance.html</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://www.flamingbytes.com/blog/setup-cockroachdb-in-docker-container/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo/FlamingBytes-icon-64x64-1.png">
      <meta itemprop="name" content="relentlesstorm">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlamingBytes">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | FlamingBytes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/setup-cockroachdb-in-docker-container/" class="post-title-link" itemprop="url">Setup CockroachDB in docker container</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-12-02 08:00:00" itemprop="dateCreated datePublished" datetime="2022-12-02T08:00:00-08:00">2022-12-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-22 11:28:20" itemprop="dateModified" datetime="2023-10-22T11:28:20-07:00">2023-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/" itemprop="url" rel="index"><span itemprop="name">Tech</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Create-docker-volume"><a href="#Create-docker-volume" class="headerlink" title="Create docker volume"></a>Create docker volume</h2><p>Create docker volume on each host:</p>
<pre><code>[root@host1 ~]# docker volume create --driver local --opt type=ext4 --opt device=/dev/nvme2n1 vol1
[root@host1 ~]# mkfs.ext4 /dev/nvme2n1
[root@host1 ~]# docker inspect vol1
[
    &#123;
        &quot;CreatedAt&quot;: &quot;2022-09-13T01:10:50Z&quot;,
        &quot;Driver&quot;: &quot;local&quot;,
        &quot;Labels&quot;: &#123;&#125;,
        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/vol1/_data&quot;,
        &quot;Name&quot;: &quot;vol1&quot;,
        &quot;Options&quot;: &#123;
            &quot;device&quot;: &quot;/dev/nvme2n1&quot;,
            &quot;type&quot;: &quot;ext4&quot;
        &#125;,
        &quot;Scope&quot;: &quot;local&quot;
    &#125;
]
</code></pre>
<h2 id="Start-CockroachDB-instance"><a href="#Start-CockroachDB-instance" class="headerlink" title="Start CockroachDB instance"></a>Start CockroachDB instance</h2><p>Start the CockroachDB instance on each host:</p>
<pre><code>[root@host1 ~]# docker run -d --cpus=&quot;0.25&quot; --memory=&quot;32g&quot; --name=roach1 --hostname=host1 -p 26257:26257 -p 8080:8080 -v &quot;vol1:/cockroach/cockroach-data&quot; cockroachdb/cockroach:v22.1.6 start --insecure --join=host1,host2,host3

[root@host2 ~]# docker run -d --cpus=&quot;0.25&quot; --memory=&quot;32g&quot; --name=roach2 --hostname=host2 -p 26257:26257 -p 8080:8080 -v &quot;vol2:/cockroach/cockroach-data&quot; cockroachdb/cockroach:v22.1.6 start --insecure --join=host1,host2,host3

[root@host3 ~]# docker run -d --cpus=&quot;0.25&quot; --memory=&quot;32g&quot; --name=roach3 --hostname=host3 -p 26257:26257 -p 8080:8080 -v &quot;vol3:/cockroach/cockroach-data&quot; cockroachdb/cockroach:v22.1.6 start --insecure --join=host1,host2,host3
</code></pre>
<h2 id="Initialize-the-CockroachDB-cluster"><a href="#Initialize-the-CockroachDB-cluster" class="headerlink" title="Initialize the CockroachDB cluster"></a>Initialize the CockroachDB cluster</h2><p>Initialize the cluster on any of the cluster nodes:</p>
<pre><code>[root@host1 ~]# docker exec -it roach1 ./cockroach init --insecure
</code></pre>
<p>Check the cluster status:</p>
<pre><code>[root@host1 ~]# docker exec -it roach1 ./cockroach node ls --insecure
  id
------
   1
   2
   3
(3 rows)

[root@host1 ~]# docker exec -it roach1 ./cockroach node status --insecure
  id |     address      |   sql_address    |  build  |         started_at         |         updated_at         | locality | is_available | is_live
-----+------------------+------------------+---------+----------------------------+----------------------------+----------+--------------+----------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-13 01:09:19.857864 | 2022-09-13 01:10:22.870301 |          | true         | true
   2 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-13 01:09:20.668787 | 2022-09-13 01:10:23.671303 |          | true         | true
   3 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-13 01:09:20.910602 | 2022-09-13 01:10:19.415254 |          | true         | true
(3 rows)

[root@host1 ~]# docker exec -it roach1 df -h
Filesystem               Size  Used Avail Use% Mounted on
overlay                   50G  7.1G   43G  15% /
tmpfs                     64M     0   64M   0% /dev
tmpfs                    504G     0  504G   0% /sys/fs/cgroup
shm                       64M     0   64M   0% /dev/shm
/dev/nvme2n1             1.5T  1.6G  1.4T   1% /cockroach/cockroach-data
/dev/mapper/centos-root   50G  7.1G   43G  15% /etc/hosts
tmpfs                    504G     0  504G   0% /proc/acpi
tmpfs                    504G     0  504G   0% /proc/scsi
tmpfs                    504G     0  504G   0% /sys/firmware

[root@host1 ~]# mount | grep nvme2n1
/dev/nvme2n1 on /var/lib/docker/volumes/vol1/_data type ext4 (rw,relatime)
</code></pre>
<h2 id="Check-the-cluster-startup-logs"><a href="#Check-the-cluster-startup-logs" class="headerlink" title="Check the cluster startup logs"></a>Check the cluster startup logs</h2><pre><code>[root@host1 ~]# docker exec -it roach1 grep &#39;node starting&#39; cockroach-data/logs/cockroach.log -A 11
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +CockroachDB node starting at 2022-09-13 00:57:22.532397342 +0000 UTC (took 110.6s)
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +build:               CCL v22.1.6 @ 2022/08/23 17:05:04 (go1.17.11)
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +webui:               ‹http://host1:8080›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +sql:                 ‹postgresql://root@host1:26257/defaultdb?sslmode=disable›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +sql (JDBC):          ‹jdbc:postgresql://host1:26257/defaultdb?sslmode=disable&amp;user=root›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +RPC client flags:    ‹/cockroach/cockroach &lt;client cmd&gt; --host=host1:26257 --insecure›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +logs:                ‹/cockroach/cockroach-data/logs›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +temp dir:            ‹/cockroach/cockroach-data/cockroach-temp1471904785›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +external I/O path:   ‹/cockroach/cockroach-data/extern›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +store[0]:            ‹path=/cockroach/cockroach-data›
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +storage engine:      pebble
I220913 00:57:22.532570 151 1@cli/start.go:1028 ⋮ [n1] 252 +clusterID:           ‹8276f8fb-30f2-4712-ad4e-f008d382372d›
</code></pre>
<h2 id="Load-the-tpcc-dataset"><a href="#Load-the-tpcc-dataset" class="headerlink" title="Load the tpcc dataset"></a>Load the tpcc dataset</h2><pre><code>[root@host1 ~]# docker exec -it roach1 ./cockroach workload init tpcc &quot;postgresql://root@host1:26257?sslmode=disable&quot; --warehouses 2500 --drop

[root@host2 ~]# iostat -ktdx 2 nvme2n1
09/13/2022 01:14:00 AM
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00   239.50    0.00 2112.00     0.00 167118.00   158.26     0.27    0.13    0.00    0.13   0.25  53.00

09/13/2022 01:14:02 AM
Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
nvme2n1           0.00   268.00    0.00 2369.50     0.00 185266.00   156.38     0.30    0.13    0.00    0.13   0.24  56.50
</code></pre>
<p>The following are the elapsed time to load dataset for 2500 warehouses.</p>
<pre><code>I220913 04:28:20.633547 1 workload/workloadsql/dataload.go:146  [-] 1  imported warehouse (0s, 2500 rows)
I220913 04:28:20.900607 1 workload/workloadsql/dataload.go:146  [-] 2  imported district (0s, 25000 rows)
I220913 04:46:27.360882 7 1@util/log/log_flush.go:99  [-] 3  hangup received, flushing logs
I220913 04:59:37.468628 1 workload/workloadsql/dataload.go:146  [-] 4  imported customer (31m17s, 75000000 rows)
I220913 05:06:54.166160 1 workload/workloadsql/dataload.go:146  [-] 5  imported history (7m17s, 75000000 rows)
I220913 09:17:27.487420 1 workload/workloadsql/dataload.go:146  [-] 6  imported order (4h10m33s, 75000000 rows)
I220913 09:18:29.376933 1 workload/workloadsql/dataload.go:146  [-] 7  imported new_order (1m2s, 22500000 rows)
I220913 09:18:29.975510 1 workload/workloadsql/dataload.go:146  [-] 8  imported item (1s, 100000 rows)
I220913 10:07:29.447874 1 workload/workloadsql/dataload.go:146  [-] 9  imported stock (48m59s, 250000000 rows)
I220913 11:18:52.140400 1 workload/workloadsql/dataload.go:146  [-] 10  imported order_line (1h11m23s, 750022630 rows)
</code></pre>
<p>The following shows the used file system size for the 2500 warehouses.</p>
<pre><code>[root@host1 ~]# docker exec -it roach1 df -h
Filesystem               Size  Used Avail Use% Mounted on
overlay                   50G  7.1G   43G  15% /
tmpfs                     64M     0   64M   0% /dev
tmpfs                    504G     0  504G   0% /sys/fs/cgroup
shm                       64M     0   64M   0% /dev/shm
/dev/nvme2n1             1.5T   49G  1.4T   4% /cockroach/cockroach-data
/dev/mapper/centos-root   50G  7.1G   43G  15% /etc/hosts
tmpfs                    504G     0  504G   0% /proc/acpi
tmpfs                    504G     0  504G   0% /proc/scsi
tmpfs                    504G     0  504G   0% /sys/firmware
</code></pre>
<h2 id="Run-the-tpcc-workload"><a href="#Run-the-tpcc-workload" class="headerlink" title="Run the tpcc workload"></a>Run the tpcc workload</h2><p>Run tpcc workload from remote host:</p>
<pre><code>[root@host4 ~]# cat addr
postgres://root@host1:26257?sslmode=disable postgres://root@host2:26257?sslmode=disable postgres://root@host3:26257?sslmode=disable
[root@host4 ~]# cockroach workload run tpcc --warehouses=2500 --ramp=5m --duration=30m $(cat addr)
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.cockroachlabs.com/docs/v22.1/start-a-local-cluster-in-docker-linux">https://www.cockroachlabs.com/docs/v22.1/start-a-local-cluster-in-docker-linux</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://www.flamingbytes.com/blog/setup-cockroachdb-cluster-with-haproxy-load-balancing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo/FlamingBytes-icon-64x64-1.png">
      <meta itemprop="name" content="relentlesstorm">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlamingBytes">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | FlamingBytes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/setup-cockroachdb-cluster-with-haproxy-load-balancing/" class="post-title-link" itemprop="url">Setup CockroachDB cluster with HAProxy load balancing</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-12-01 08:00:00" itemprop="dateCreated datePublished" datetime="2022-12-01T08:00:00-08:00">2022-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-22 11:28:20" itemprop="dateModified" datetime="2023-10-22T11:28:20-07:00">2023-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/" itemprop="url" rel="index"><span itemprop="name">Tech</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Each CockroachDB node is an equally suitable SQL gateway to your cluster, but to ensure client performance and reliability, it’s important to use load balancing.</p>
<ul>
<li><p>Performance: Load balancers spread client traffic across nodes. This prevents any one node from being overwhelmed by requests and improves overall cluster performance (queries per second).</p>
</li>
<li><p>Reliability: Load balancers decouple client health from the health of a single CockroachDB node. In cases where a node fails, the load balancer redirects client traffic to available nodes.</p>
</li>
</ul>
<p>HAProxy is one of the most popular open-source TCP load balancers, and CockroachDB includes a built-in command for generating a configuration file that is preset to work with your running cluster.</p>
<p>With a single load balancer, client connections are resilient to node failure, but the load balancer itself is a point of failure. It’s therefore best to make load balancing resilient as well by using multiple load balancing instances, with a mechanism like floating IPs or DNS to select load balancers for clients.</p>
<p>For performance and availability reasons, it is not recommended to run the HAproxy on the same node as the cockroachDB.</p>
<p>This post shows a detailed steps how to deploy a HAProxy load balancer for CockroachDB cluster.</p>
<h2 id="Deploy-the-CockroachDB-cluster"><a href="#Deploy-the-CockroachDB-cluster" class="headerlink" title="Deploy the CockroachDB cluster"></a>Deploy the CockroachDB cluster</h2><pre><code>[root@host1 ~]# ps -ef | grep cockroach | grep -v grep
root      4783     1 99 18:59 ?        2-17:35:59 cockroach start --log-dir=/var/log/cockroachdb_logs/1 --store=/mnt/cockroachdb_mnt1 --insecure --listen-addr=host1:26257 --http-addr=host1:8080 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      4876     1 95 18:59 ?        03:33:42 cockroach start --log-dir=/var/log/cockroachdb_logs/2 --store=/mnt/cockroachdb_mnt2 --insecure --listen-addr=host1:26258 --http-addr=host1:8081 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      4932     1 77 18:59 ?        02:53:13 cockroach start --log-dir=/var/log/cockroachdb_logs/3 --store=/mnt/cockroachdb_mnt3 --insecure --listen-addr=host1:26259 --http-addr=host1:8082 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      5049     1 99 18:59 ?        06:02:17 cockroach start --log-dir=/var/log/cockroachdb_logs/4 --store=/mnt/cockroachdb_mnt4 --insecure --listen-addr=host1:26260 --http-addr=host1:8083 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      5131     1 94 18:59 ?        03:31:51 cockroach start --log-dir=/var/log/cockroachdb_logs/5 --store=/mnt/cockroachdb_mnt5 --insecure --listen-addr=host1:26261 --http-addr=host1:8084 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      5220     1 72 18:59 ?        02:42:39 cockroach start --log-dir=/var/log/cockroachdb_logs/6 --store=/mnt/cockroachdb_mnt6 --insecure --listen-addr=host1:26262 --http-addr=host1:8085 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      5309     1 94 18:59 ?        03:30:22 cockroach start --log-dir=/var/log/cockroachdb_logs/7 --store=/mnt/cockroachdb_mnt7 --insecure --listen-addr=host1:26263 --http-addr=host1:8086 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB
root      5358     1 58 18:59 ?        02:11:20 cockroach start --log-dir=/var/log/cockroachdb_logs/8 --store=/mnt/cockroachdb_mnt8 --insecure --listen-addr=host1:26264 --http-addr=host1:8087 --join=host1:26257,host1:26258,host1:26259,host1:26260,host1:26261,host1:26262,host1:26263,host1:26264,host2:26257,host2:26258,host2:26259,host2:26260,host2:26261,host2:26262,host2:26263,host2:26264,host3:26257,host3:26258,host3:26259,host3:26260,host3:26261,host3:26262,host3:26263,host3:26264 --cache=2GiB --max-sql-memory=2GiB

[root@host1 ~]# cockroach node status --insecure --host=host1:26257
  id |       address       |     sql_address     |  build  |         started_at         |         updated_at         | locality | is_available | is_live
-----+---------------------+---------------------+---------+----------------------------+----------------------------+----------+--------------+----------
   1 | host1:26257 | host1:26257 | v22.1.6 | 2022-09-17 18:59:37.989384 | 2022-09-17 22:43:39.507057 |          | true         | true
   2 | host1:26258 | host1:26258 | v22.1.6 | 2022-09-17 18:59:39.128398 | 2022-09-17 22:43:40.640431 |          | true         | true
   3 | host1:26259 | host1:26259 | v22.1.6 | 2022-09-17 18:59:39.374127 | 2022-09-17 22:43:40.885678 |          | true         | true
   4 | host1:26260 | host1:26260 | v22.1.6 | 2022-09-17 18:59:39.726667 | 2022-09-17 22:43:36.73931  |          | true         | true
   5 | host1:26261 | host1:26261 | v22.1.6 | 2022-09-17 18:59:39.971549 | 2022-09-17 22:43:36.983538 |          | true         | true
   6 | host1:26262 | host1:26262 | v22.1.6 | 2022-09-17 18:59:40.225358 | 2022-09-17 22:43:37.235768 |          | true         | true
   7 | host1:26263 | host1:26263 | v22.1.6 | 2022-09-17 18:59:40.447688 | 2022-09-17 22:43:37.459962 |          | true         | true
   8 | host1:26264 | host1:26264 | v22.1.6 | 2022-09-17 18:59:40.677217 | 2022-09-17 22:43:37.690883 |          | true         | true
   9 | host2:26257 | host2:26257 | v22.1.6 | 2022-09-17 18:59:41.604348 | 2022-09-17 22:43:38.613662 |          | true         | true
  10 | host2:26258 | host2:26258 | v22.1.6 | 2022-09-17 18:59:41.904071 | 2022-09-17 22:43:38.912773 |          | true         | true
  11 | host2:26259 | host2:26259 | v22.1.6 | 2022-09-17 18:59:42.194446 | 2022-09-17 22:43:39.202688 |          | true         | true
  12 | host2:26260 | host2:26260 | v22.1.6 | 2022-09-17 18:59:42.512201 | 2022-09-17 22:43:39.519727 |          | true         | true
  13 | host2:26261 | host2:26261 | v22.1.6 | 2022-09-17 18:59:42.850974 | 2022-09-17 22:43:39.860063 |          | true         | true
  14 | host2:26262 | host2:26262 | v22.1.6 | 2022-09-17 18:59:43.177162 | 2022-09-17 22:43:40.191082 |          | true         | true
  15 | host2:26263 | host2:26263 | v22.1.6 | 2022-09-17 18:59:43.543973 | 2022-09-17 22:43:40.553146 |          | true         | true
  16 | host2:26264 | host2:26264 | v22.1.6 | 2022-09-17 18:59:43.858993 | 2022-09-17 22:43:40.86705  |          | true         | true
  17 | host3:26257 | host3:26257 | v22.1.6 | 2022-09-17 18:59:44.807641 | 2022-09-17 22:43:37.315927 |          | true         | true
  18 | host3:26258 | host3:26258 | v22.1.6 | 2022-09-17 18:59:45.119533 | 2022-09-17 22:43:37.628497 |          | true         | true
  19 | host3:26259 | host3:26259 | v22.1.6 | 2022-09-17 18:59:45.508535 | 2022-09-17 22:43:38.015714 |          | true         | true
  20 | host3:26260 | host3:26260 | v22.1.6 | 2022-09-17 18:59:45.901631 | 2022-09-17 22:43:38.409443 |          | true         | true
  21 | host3:26261 | host3:26261 | v22.1.6 | 2022-09-17 18:59:46.191724 | 2022-09-17 22:43:38.700531 |          | true         | true
  22 | host3:26262 | host3:26262 | v22.1.6 | 2022-09-17 18:59:46.540504 | 2022-09-17 22:43:39.048942 |          | true         | true
  23 | host3:26263 | host3:26263 | v22.1.6 | 2022-09-17 18:59:46.944353 | 2022-09-17 22:43:39.453175 |          | true         | true
  24 | host3:26264 | host3:26264 | v22.1.6 | 2022-09-17 18:59:47.274458 | 2022-09-17 22:43:39.782378 |          | true         | true
(24 rows)
</code></pre>
<h2 id="Generate-HAProxy-configuration-file"><a href="#Generate-HAProxy-configuration-file" class="headerlink" title="Generate HAProxy configuration file"></a>Generate HAProxy configuration file</h2><p>Install HAProxy on one of the CockroachDB nodes:</p>
<pre><code>[root@host1 ~]# yum install haproxy
[root@host1 ~]# haproxy -v
HA-Proxy version 1.5.18 2016/05/10
Copyright 2000-2016 Willy Tarreau &lt;willy@haproxy.org&gt;
</code></pre>
<p>Generate the HAProxy configuration file:</p>
<pre><code>[root@host1 ~]# cockroach gen haproxy --host=host1:26257 --insecure

[root@host1 ~]# cat haproxy.cfg

global
  maxconn 4096

defaults
    mode                tcp

    # Timeout values should be configured for your specific use.
    # See: https://cbonte.github.io/haproxy-dconv/1.8/configuration.html#4-timeout%20connect

    # With the timeout connect 5 secs,
    # if the backend server is not responding, haproxy will make a total
    # of 3 connection attempts waiting 5s each time before giving up on the server,
    # for a total of 15 seconds.
    retries             2
    timeout connect     5s

    # timeout client and server govern the maximum amount of time of TCP inactivity.
    # The server node may idle on a TCP connection either because it takes time to
    # execute a query before the first result set record is emitted, or in case of
    # some trouble on the server. So these timeout settings should be larger than the
    # time to execute the longest (most complex, under substantial concurrent workload)
    # query, yet not too large so truly failed connections are lingering too long
    # (resources associated with failed connections should be freed reasonably promptly).
    timeout client      10m
    timeout server      10m

    # TCP keep-alive on client side. Server already enables them.
    option              clitcpka

listen psql
    bind :26257
    mode tcp
    balance roundrobin
    option httpchk GET /health?ready=1
    server cockroach1 host1:26257 check port 8080
    server cockroach2 host1:26258 check port 8081
    server cockroach3 host1:26259 check port 8082
    server cockroach4 host1:26260 check port 8083
    server cockroach5 host1:26261 check port 8084
    server cockroach6 host1:26262 check port 8085
    server cockroach7 host1:26263 check port 8086
    server cockroach8 host1:26264 check port 8087
    server cockroach9 host2:26257 check port 8080
    server cockroach10 host2:26258 check port 8081
    server cockroach11 host2:26259 check port 8082
    server cockroach12 host2:26260 check port 8083
    server cockroach13 host2:26261 check port 8084
    server cockroach14 host2:26262 check port 8085
    server cockroach15 host2:26263 check port 8086
    server cockroach16 host2:26264 check port 8087
    server cockroach17 host3:26257 check port 8080
    server cockroach18 host3:26258 check port 8081
    server cockroach19 host3:26259 check port 8082
    server cockroach20 host3:26260 check port 8083
    server cockroach21 host3:26261 check port 8084
    server cockroach22 host3:26262 check port 8085
    server cockroach23 host3:26263 check port 8086
    server cockroach24 host3:26264 check port 8087
</code></pre>
<h2 id="Start-HAProxy"><a href="#Start-HAProxy" class="headerlink" title="Start HAProxy"></a>Start HAProxy</h2><p>It’s not recommended to deploy HAProxy on the same node as CockroachDB for performance and availability reasons. Thus, we are going to start the HAProxy  on a dedicated node.</p>
<p>Connect to the dedicated HAProxy node and install the proxy on it:</p>
<pre><code>[root@host4 ~]# yum install haproxy
</code></pre>
<p>Start HAproxy with the generated configuration file:</p>
<pre><code>[root@host4 ~]# scp host1:/root/haproxy.cfg ./
[root@host4 ~]# haproxy -f haproxy.cfg
</code></pre>
<h2 id="Verify-the-HAProxy"><a href="#Verify-the-HAProxy" class="headerlink" title="Verify the HAProxy"></a>Verify the HAProxy</h2><p>The Load Balancer distributes the requests sent to the cluster equally between the different nodes.</p>
<p>From primary node, we can check which node is being connected as below:</p>
<pre><code>[root@host1 ~]# cockroach sql --host=host4 --insecure
root@host4:26257/defaultdb&gt; SHOW node_id;
  node_id
-----------
  1

root@host4:26257/defaultdb&gt; exit
[root@host1 ~]# cockroach sql --host=host4 --insecure
root@host4:26257/defaultdb&gt; SHOW node_id;
  node_id
-----------
  2

[root@host1 ~]# cockroach sql --host=host4 --insecure
root@host4:26257/defaultdb&gt; SHOW node_id;
  node_id
-----------
  3
</code></pre>
<h2 id="Run-TPCC-through-the-HAProxy-load-balancer"><a href="#Run-TPCC-through-the-HAProxy-load-balancer" class="headerlink" title="Run TPCC through the HAProxy load balancer"></a>Run TPCC through the HAProxy load balancer</h2><pre><code>[root@host4 cockroachdb]# cockroach workload run tpcc --warehouses=5000 --split --scatter --ramp=30s --duration=5m &#39;postgresql://root@host4:26257/tpcc?sslmode=disable&#39;
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.cockroachlabs.com/docs/v22.1/deploy-cockroachdb-on-premises-insecure">https://www.cockroachlabs.com/docs/v22.1/deploy-cockroachdb-on-premises-insecure</a></li>
<li><a target="_blank" rel="noopener" href="https://www.scaleway.com/en/docs/tutorials/setup-cockroachdb-cluster/">Install a multi-node Cockroach database with HA Proxy</a></li>
<li><a target="_blank" rel="noopener" href="http://www.haproxy.org/">http://www.haproxy.org/</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://www.flamingbytes.com/blog/how-to-delete-partition-in-linux/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo/FlamingBytes-icon-64x64-1.png">
      <meta itemprop="name" content="relentlesstorm">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlamingBytes">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | FlamingBytes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/how-to-delete-partition-in-linux/" class="post-title-link" itemprop="url">How to delete partition in Linux</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-11-11 13:17:00" itemprop="dateCreated datePublished" datetime="2022-11-11T13:17:00-08:00">2022-11-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-22 11:28:20" itemprop="dateModified" datetime="2023-10-22T11:28:20-07:00">2023-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/" itemprop="url" rel="index"><span itemprop="name">Tech</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Identify-the-disk-which-contains-the-partitions"><a href="#Identify-the-disk-which-contains-the-partitions" class="headerlink" title="Identify the disk which contains the partitions"></a>Identify the disk which contains the partitions</h2><pre><code>$ lsblk
NAME                         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sr0                           11:0    1   9.5G  0 rom
nvme0n1                      259:6    0   1.5T  0 disk
├─nvme0n1p1                  259:7    0   200M  0 part /boot/efi
├─nvme0n1p2                  259:8    0     1G  0 part /boot
└─nvme0n1p3                  259:9    0   1.5T  0 part
  ├─centos_init500--c11-root 253:0    0    50G  0 lvm  /
  ├─centos_init500--c11-swap 253:1    0     4G  0 lvm  [SWAP]
  └─centos_init500--c11-home 253:2    0   1.4T  0 lvm  /home
nvme1n1                      259:10   0   1.5T  0 disk
├─nvme1n1p1                  259:13   0   100M  0 part
├─nvme1n1p5                  259:14   0     4G  0 part
├─nvme1n1p6                  259:15   0     4G  0 part
├─nvme1n1p7                  259:17   0 119.9G  0 part
└─nvme1n1p8                  259:18   0   1.3T  0 part


$ fdisk -l
[..]
Disk /dev/nvme1n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x7f33a7d2

        Device Boot      Start         End      Blocks   Id  System
/dev/nvme1n1p1               1  3125627534  1562813767   ee  GPT
[..]
</code></pre>
<h2 id="Delete-the-partitions"><a href="#Delete-the-partitions" class="headerlink" title="Delete the partitions"></a>Delete the partitions</h2><p>Select the disk that contains the partitions you intend to delete:</p>
<pre><code>$ fdisk /dev/nvme1n1
Welcome to fdisk (util-linux 2.23.2).

Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.
</code></pre>
<p>Delete the partition:</p>
<pre><code>Command (m for help): d
Selected partition 1
Partition 1 is deleted
</code></pre>
<p><strong>NOTE:</strong> The partition is automatically selected if there are no other partitions on disk. If the disk contains multiple partitions, select a partition that you want to delete.</p>
<p>Verify the paritions:</p>
<pre><code>Command (m for help): p

Disk /dev/nvme1n1: 1600.3 GB, 1600321314816 bytes, 3125627568 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x7f33a7d2

        Device Boot      Start         End      Blocks   Id  System
</code></pre>
<p>Save the change and quit fdisk:</p>
<pre><code>Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
</code></pre>
<p>Verify the partitions with lsblk:</p>
<pre><code>$ lsblk
NAME                         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0                           11:0    1  9.5G  0 rom
nvme0n1                      259:6    0  1.5T  0 disk
├─nvme0n1p1                  259:7    0  200M  0 part /boot/efi
├─nvme0n1p2                  259:8    0    1G  0 part /boot
└─nvme0n1p3                  259:9    0  1.5T  0 part
  ├─centos_init500--c11-root 253:0    0   50G  0 lvm  /
  ├─centos_init500--c11-swap 253:1    0    4G  0 lvm  [SWAP]
  └─centos_init500--c11-home 253:2    0  1.4T  0 lvm  /home
nvme1n1                      259:10   0  1.5T  0 disk
</code></pre>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://www.flamingbytes.com/blog/useful-es-apis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo/FlamingBytes-icon-64x64-1.png">
      <meta itemprop="name" content="relentlesstorm">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlamingBytes">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | FlamingBytes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/useful-es-apis/" class="post-title-link" itemprop="url">Useful Elastic cluster APIs</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-10-28 12:37:00" itemprop="dateCreated datePublished" datetime="2022-10-28T12:37:00-07:00">2022-10-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-22 11:28:20" itemprop="dateModified" datetime="2023-10-22T11:28:20-07:00">2023-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/" itemprop="url" rel="index"><span itemprop="name">Tech</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          Useful Elastic cluster APIs
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/blog/useful-es-apis/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://www.flamingbytes.com/blog/benchmark-es-cluster-with-rally/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo/FlamingBytes-icon-64x64-1.png">
      <meta itemprop="name" content="relentlesstorm">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlamingBytes">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | FlamingBytes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/benchmark-es-cluster-with-rally/" class="post-title-link" itemprop="url">Benchmarking Elasticsearch cluster with Rally</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-10-26 12:34:00" itemprop="dateCreated datePublished" datetime="2022-10-26T12:34:00-07:00">2022-10-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-22 11:28:20" itemprop="dateModified" datetime="2023-10-22T11:28:20-07:00">2023-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/" itemprop="url" rel="index"><span itemprop="name">Tech</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/Benchmarking/" itemprop="url" rel="index"><span itemprop="name">Benchmarking</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Install-Rally-on-each-node"><a href="#Install-Rally-on-each-node" class="headerlink" title="Install Rally on each node"></a>Install Rally on each node</h2><p>Prerequisites</p>
<pre><code>$ yum update
$ yum install openssl-devel bzip2-devel libffi-devel
$ yum groupinstall &quot;Development Tools&quot;
</code></pre>
<p>Install Python 3.8+</p>
<pre><code>$ wget https://www.python.org/ftp/python/3.8.15/Python-3.8.15.tar.xz
$ tar xf Python-3.8.15.tar.xz
$ vim Python-3.8.15/Modules/Setup
SSL=/usr/local/ssl
_ssl _ssl.c \
        -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
        -L$(SSL)/lib -lssl -lcrypto
$ mv Python-3.8.15 /usr/src
$ cd /usr/src/Python-3.8.15/
$ ./configure --enable-optimizations
$ make altinstall
$ python3.8 -m ssl
$ pip3.8 install --upgrade pip
</code></pre>
<p>Install Git (Not required for load generator node)</p>
<pre><code>$ yum install libcurl-devel
$ wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.38.1.tar.xz
$ tar xvf git-2.38.1.tar.xz
$ cd git-2.38.1/
$ make configure
$ ./configure --prefix=/usr/local
$ make install
$ git --version
git version 2.38.1
</code></pre>
<p>Install JDK (Not required for load generator node)</p>
<pre><code>$ yum install java
$ java -version
openjdk version &quot;1.8.0_352&quot;
</code></pre>
<p>Install esrally</p>
<pre><code>$ pip3.8 install esrally
$ esrally --version
esrally 2.6.0
</code></pre>
<p>Elasticsearch can not be launched as root. Create a non-root user on each node.</p>
<pre><code>$ groupadd es
$ useradd es -g es
$ passwd es
$ cd /home/es
$ su - es
</code></pre>
<p>Set JAVA_HOME path</p>
<pre><code>$ vim .bash_profile
JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
export JAVA_HOME
$ source .bash_profile
$ echo $JAVA_HOME
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
</code></pre>
<h2 id="Benchmarking-a-Single-Node"><a href="#Benchmarking-a-Single-Node" class="headerlink" title="Benchmarking a Single Node"></a>Benchmarking a Single Node</h2><p>Install Elasticsearch</p>
<pre><code>$ esrally install --distribution-version=7.17.0 --node-name=&quot;rally-node-0&quot; --network-host=&quot;127.0.0.1&quot; --http-port=39200 --master-nodes=&quot;rally-node-0&quot; --seed-hosts=&quot;127.0.0.1:39300&quot;

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Downloading Elasticsearch 7.17.0 (297.0 MB total size)                       [100%]
&#123;
  &quot;installation-id&quot;: &quot;10735bfa-f1b8-44c4-8e7f-8932c8daa201&quot;
&#125;

--------------------------------
[INFO] SUCCESS (took 10 seconds)
--------------------------------
</code></pre>
<p>Start the Elasticsearch node</p>
<pre><code>$ export INSTALLATION_ID=10735bfa-f1b8-44c4-8e7f-8932c8daa201
$ export RACE_ID=$(uuidgen)
$ esrally start --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot; --race-id=&quot;$&#123;RACE_ID&#125;&quot;

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/


-------------------------------
[INFO] SUCCESS (took 3 seconds)
-------------------------------
</code></pre>
<p>Run a benchmark</p>
<pre><code>$ esrally race --pipeline=benchmark-only --target-host=127.0.0.1:39200 --track=geonames --challenge=append-no-conflicts-index-only --on-error=abort --race-id=$&#123;RACE_ID&#125;

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Race id is [4e93324a-9326-49e8-be72-9f77cc837657]
[INFO] Downloading track data (252.9 MB total size)                               [100.0%]
[INFO] Decompressing track data from [/home/es/.rally/benchmarks/data/geonames/documents-2.json.bz2] to [/home/es/.rally/benchmarks/data/geonames/documents-2.json] (resulting size: [3.30] GB) ... [OK]
[INFO] Preparing file offset table for [/home/es/.rally/benchmarks/data/geonames/documents-2.json] ... [OK]
[INFO] Racing on track [geonames], challenge [append-no-conflicts-index-only] and car [&#39;external&#39;] with version [7.17.0].

Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [100% done]
Running force-merge                                                            [100% done]
Running wait-until-merges-finish                                               [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                         Metric |         Task |           Value |   Unit |
|---------------------------------------------------------------:|-------------:|----------------:|-------:|
|                     Cumulative indexing time of primary shards |              |    12.5856      |    min |
|             Min cumulative indexing time across primary shards |              |     0.0190333   |    min |
|          Median cumulative indexing time across primary shards |              |     2.49065     |    min |
|             Max cumulative indexing time across primary shards |              |     2.61288     |    min |
|            Cumulative indexing throttle time of primary shards |              |     0.0081      |    min |
|    Min cumulative indexing throttle time across primary shards |              |     0           |    min |
| Median cumulative indexing throttle time across primary shards |              |     0           |    min |
|    Max cumulative indexing throttle time across primary shards |              |     0.0061      |    min |
|                        Cumulative merge time of primary shards |              |     6.75938     |    min |
|                       Cumulative merge count of primary shards |              |    55           |        |
|                Min cumulative merge time across primary shards |              |     0           |    min |
|             Median cumulative merge time across primary shards |              |     1.32735     |    min |
|                Max cumulative merge time across primary shards |              |     1.47518     |    min |
|               Cumulative merge throttle time of primary shards |              |     1.72393     |    min |
|       Min cumulative merge throttle time across primary shards |              |     0           |    min |
|    Median cumulative merge throttle time across primary shards |              |     0.323533    |    min |
|       Max cumulative merge throttle time across primary shards |              |     0.42085     |    min |
|                      Cumulative refresh time of primary shards |              |     2.97073     |    min |
|                     Cumulative refresh count of primary shards |              |   246           |        |
|              Min cumulative refresh time across primary shards |              |     0.00185     |    min |
|           Median cumulative refresh time across primary shards |              |     0.5943      |    min |
|              Max cumulative refresh time across primary shards |              |     0.599217    |    min |
|                        Cumulative flush time of primary shards |              |     0.237767    |    min |
|                       Cumulative flush count of primary shards |              |     8           |        |
|                Min cumulative flush time across primary shards |              |     0.00241667  |    min |
|             Median cumulative flush time across primary shards |              |     0.0473      |    min |
|                Max cumulative flush time across primary shards |              |     0.0492167   |    min |
|                                        Total Young Gen GC time |              |    14.009       |      s |
|                                       Total Young Gen GC count |              |  1159           |        |
|                                          Total Old Gen GC time |              |     3.491       |      s |
|                                         Total Old Gen GC count |              |    66           |        |
|                                                     Store size |              |     3.20482     |     GB |
|                                                  Translog size |              |     3.07336e-07 |     GB |
|                                         Heap used for segments |              |     1.00685     |     MB |
|                                       Heap used for doc values |              |     0.0602913   |     MB |
|                                            Heap used for terms |              |     0.77124     |     MB |
|                                            Heap used for norms |              |     0.104492    |     MB |
|                                           Heap used for points |              |     0           |     MB |
|                                    Heap used for stored fields |              |     0.0708237   |     MB |
|                                                  Segment count |              |   141           |        |
|                                    Total Ingest Pipeline count |              |     0           |        |
|                                     Total Ingest Pipeline time |              |     0           |      s |
|                                   Total Ingest Pipeline failed |              |     0           |        |
|                                                 Min Throughput | index-append | 86654           | docs/s |
|                                                Mean Throughput | index-append | 87091.9         | docs/s |
|                                              Median Throughput | index-append | 87152.4         | docs/s |
|                                                 Max Throughput | index-append | 87276.3         | docs/s |
|                                        50th percentile latency | index-append |   315.247       |     ms |
|                                        90th percentile latency | index-append |   573.935       |     ms |
|                                        99th percentile latency | index-append |  1139.89        |     ms |
|                                       100th percentile latency | index-append |  1153.22        |     ms |
|                                   50th percentile service time | index-append |   315.247       |     ms |
|                                   90th percentile service time | index-append |   573.935       |     ms |
|                                   99th percentile service time | index-append |  1139.89        |     ms |
|                                  100th percentile service time | index-append |  1153.22        |     ms |
|                                                     error rate | index-append |     0           |      % |


---------------------------------
[INFO] SUCCESS (took 255 seconds)
---------------------------------
</code></pre>
<p>Stop the Elasticsearch node</p>
<pre><code>$ esrally stop --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot;
</code></pre>
<p>If you only want to shutdown the node but don’t want to delete the node and the data, pass –preserve-install additionally.</p>
<h2 id="Benchmarking-a-Cluster"><a href="#Benchmarking-a-Cluster" class="headerlink" title="Benchmarking a Cluster"></a>Benchmarking a Cluster</h2><p>Install and start Elasticsearch on each cluster node</p>
<pre><code>$ esrally install --distribution-version=7.17.0 --node-name=&quot;rally-node-0&quot; --network-host=&quot;10.10.10.2&quot; --http-port=39200 --master-nodes=&quot;rally-node-0,rally-node-1,rally-node-2&quot; --seed-hosts=&quot;10.10.10.2:39300,10.10.10.3:39300,10.10.10.4:39300&quot;
[INFO] Downloading Elasticsearch 7.17.0 (297.0 MB total size)  [100%]
&#123;
  &quot;installation-id&quot;: &quot;aa826112-d371-4f09-9b68-f9084e7c9e0b&quot;
&#125;
</code></pre>
<p>Generate a race id on one of the nodes</p>
<pre><code>$ uuidgen
734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f
</code></pre>
<p><strong>Note</strong>: The same race id is set on all the nodes including the one where will generate load.</p>
<p>Start the cluster by running the following command on each node</p>
<pre><code>$ export INSTALLATION_ID=aa826112-d371-4f09-9b68-f9084e7c9e0b
$ export RACE_ID=734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f
$ esrally start --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot; --race-id=&quot;$&#123;RACE_ID&#125;&quot;
</code></pre>
<p><strong>Note</strong>: The INSTALLATION_ID is specific to each node and the RACI_ID is identical for all the nodes.</p>
<p>Once the cluster is started, check the cluster status with the _cat&#x2F;health API</p>
<pre><code>[es@node1 ~]$ curl http://10.10.10.2:39200/_cat/health\?v
epoch      timestamp cluster         status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1667015754 03:55:54  rally-benchmark green           3         3      6   3    0    0        0             0                  -                100.0%
</code></pre>
<p>On each cluster node, check the elastic process and port</p>
<pre><code>$ ps -ef | egrep -i &quot;rally|elastic&quot; | grep -v grep
es        2258     1 91 20:52 ?        00:58:39 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j2.formatMsgNoLookups=true -Djava.locale.providers=SPI,JRE -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.io.tmpdir=/tmp/elasticsearch-1322059221495755520 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/heapdump -XX:ErrorFile=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/hs_err_pid%p.log -XX:+ExitOnOutOfMemoryError -XX:MaxDirectMemorySize=536870912 -Des.path.home=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0 -Des.path.conf=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/config -Des.distribution.flavor=default -Des.distribution.type=tar -Des.bundled_jdk=true -cp /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/lib/* org.elasticsearch.bootstrap.Elasticsearch -d -p ./pid
es        2285  2258  0 20:52 ?        00:00:00 /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/modules/x-pack-ml/platform/linux-x86_64/bin/controller

$ netstat -anop | grep 39200
tcp6       0      0 10.10.10.2:39200       :::*                    LISTEN      2258/java            off (0.00/0/0)
</code></pre>
<p>Start the benchmark on the load generator node (remember to set the race id there)</p>
<pre><code>[es@node1 ~]$ export RACE_ID=734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f
[es@node1 ~]$ esrally race --pipeline=benchmark-only --target-host=10.10.10.2:39200,10.10.10.3:39200,10.10.10.4:39200 --track=geonames --challenge=append-no-conflicts --on-error=abort --race-id=$&#123;RACE_ID&#125;
    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Race id is [734bb4b3-8b7a-4c0b-9fa6-aaeb4659569f]
[INFO] Racing on track [geonames], challenge [append-no-conflicts] and car [&#39;external&#39;] with version [7.17.0].

[WARNING] merges_total_time is 420149 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] merges_total_throttled_time is 81765 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] indexing_total_time is 825388 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] refresh_total_time is 76340 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
[WARNING] flush_total_time is 10787 ms indicating that the cluster is not in a defined clean state. Recorded index time metrics may be misleading.
Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [100% done]
Running refresh-after-index                                                    [100% done]
Running force-merge                                                            [100% done]
Running refresh-after-force-merge                                              [100% done]
Running wait-until-merges-finish                                               [100% done]
Running index-stats                                                            [100% done]
Running node-stats                                                             [100% done]
Running default                                                                [100% done]
Running term                                                                   [100% done]
Running phrase                                                                 [100% done]
Running country_agg_uncached                                                   [100% done]
Running country_agg_cached                                                     [100% done]
Running scroll                                                                 [100% done]
Running expression                                                             [100% done]
Running painless_static                                                        [100% done]
Running painless_dynamic                                                       [100% done]
Running decay_geo_gauss_function_score                                         [100% done]
Running decay_geo_gauss_script_score                                           [100% done]
Running field_value_function_score                                             [100% done]
Running field_value_script_score                                               [100% done]
Running large_terms                                                            [100% done]
Running large_filtered_terms                                                   [100% done]
Running large_prohibited_terms                                                 [100% done]
Running desc_sort_population                                                   [100% done]
Running asc_sort_population                                                    [100% done]
Running asc_sort_with_after_population                                         [100% done]
Running desc_sort_geonameid                                                    [100% done]
Running desc_sort_with_after_geonameid                                         [100% done]
Running asc_sort_geonameid                                                     [100% done]
Running asc_sort_with_after_geonameid                                          [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                         Metric |                           Task |          Value |    Unit |
|---------------------------------------------------------------:|-------------------------------:|---------------:|--------:|
|                     Cumulative indexing time of primary shards |                                |   13.3055      |     min |
|             Min cumulative indexing time across primary shards |                                |    0           |     min |
|          Median cumulative indexing time across primary shards |                                |    2.68572     |     min |
|             Max cumulative indexing time across primary shards |                                |    2.74885     |     min |
|            Cumulative indexing throttle time of primary shards |                                |    0           |     min |
|    Min cumulative indexing throttle time across primary shards |                                |    0           |     min |
| Median cumulative indexing throttle time across primary shards |                                |    0           |     min |
|    Max cumulative indexing throttle time across primary shards |                                |    0           |     min |
|                        Cumulative merge time of primary shards |                                |    4.82182     |     min |
|                       Cumulative merge count of primary shards |                                |   57           |         |
|                Min cumulative merge time across primary shards |                                |    0           |     min |
|             Median cumulative merge time across primary shards |                                |    0.984917    |     min |
|                Max cumulative merge time across primary shards |                                |    1.06472     |     min |
|               Cumulative merge throttle time of primary shards |                                |    0.978367    |     min |
|       Min cumulative merge throttle time across primary shards |                                |    0           |     min |
|    Median cumulative merge throttle time across primary shards |                                |    0.195508    |     min |
|       Max cumulative merge throttle time across primary shards |                                |    0.265933    |     min |
|                      Cumulative refresh time of primary shards |                                |    1.20573     |     min |
|                     Cumulative refresh count of primary shards |                                |  148           |         |
|              Min cumulative refresh time across primary shards |                                |    3.33333e-05 |     min |
|           Median cumulative refresh time across primary shards |                                |    0.258775    |     min |
|              Max cumulative refresh time across primary shards |                                |    0.283433    |     min |
|                        Cumulative flush time of primary shards |                                |    0.172783    |     min |
|                       Cumulative flush count of primary shards |                                |   11           |         |
|                Min cumulative flush time across primary shards |                                |    1.66667e-05 |     min |
|             Median cumulative flush time across primary shards |                                |    0.0345      |     min |
|                Max cumulative flush time across primary shards |                                |    0.0385167   |     min |
|                                        Total Young Gen GC time |                                |   16.263       |       s |
|                                       Total Young Gen GC count |                                | 2821           |         |
|                                          Total Old Gen GC time |                                |    2.312       |       s |
|                                         Total Old Gen GC count |                                |   41           |         |
|                                                     Store size |                                |    3.03867     |      GB |
|                                                  Translog size |                                |    3.58559e-07 |      GB |
|                                         Heap used for segments |                                |    0.701981    |      MB |
|                                       Heap used for doc values |                                |    0.0314178   |      MB |
|                                            Heap used for terms |                                |    0.54541     |      MB |
|                                            Heap used for norms |                                |    0.0736694   |      MB |
|                                           Heap used for points |                                |    0           |      MB |
|                                    Heap used for stored fields |                                |    0.0514832   |      MB |
|                                                  Segment count |                                |  102           |         |
|                                    Total Ingest Pipeline count |                                |    0           |         |
|                                     Total Ingest Pipeline time |                                |    0           |       s |
|                                   Total Ingest Pipeline failed |                                |    0           |         |
|                                                     error rate |                   index-append |    0           |       % |
|                                                 Min Throughput |                    index-stats |   90.01        |   ops/s |
|                                                Mean Throughput |                    index-stats |   90.02        |   ops/s |
|                                              Median Throughput |                    index-stats |   90.02        |   ops/s |
|                                                 Max Throughput |                    index-stats |   90.04        |   ops/s |
|                                        50th percentile latency |                    index-stats |    5.16153     |      ms |
|                                        90th percentile latency |                    index-stats |    6.00114     |      ms |
|                                        99th percentile latency |                    index-stats |    6.61081     |      ms |
|                                      99.9th percentile latency |                    index-stats |   10.4064      |      ms |
|                                       100th percentile latency |                    index-stats |   10.8105      |      ms |
|                                   50th percentile service time |                    index-stats |    4.00402     |      ms |
|                                   90th percentile service time |                    index-stats |    4.6339      |      ms |
|                                   99th percentile service time |                    index-stats |    5.10083     |      ms |
|                                 99.9th percentile service time |                    index-stats |    9.17415     |      ms |
|                                  100th percentile service time |                    index-stats |    9.22474     |      ms |
[..]

[WARNING] No throughput metrics available for [index-append]. Likely cause: The benchmark ended already during warmup.

----------------------------------
[INFO] SUCCESS (took 4008 seconds)
----------------------------------
</code></pre>
<p>Shutdown the cluster on each node</p>
<pre><code>$ esrally stop --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot;
</code></pre>
<p><strong>Note</strong>: If you only want to shutdown the node but don’t want to delete the node and the data, add the option “–preserve-install” additionally.</p>
<h2 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h2><p>Elasticsearch start failure due to max virtual memory is too low</p>
<pre><code>$ cat /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/rally-benchmark.log
[..]
[2022-10-28T16:58:41,041][ERROR][o.e.b.Bootstrap          ] [rally-node-0] node validation exception
[1] bootstrap checks failed. You must address the points described in the following [1] lines before starting Elasticsearch.
bootstrap check failure [1] of [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
[..]

$ sysctl -a | grep max_map_count
vm.max_map_count = 65530
</code></pre>
<p>To fix this issue, change the kernel parameter</p>
<pre><code>$ vim /etc/sysctl.conf
vm.max_map_count=1048576
$ sysctl -p
vm.max_map_count = 1048576
</code></pre>
<p>Restart Elasticsearch and verify the process and port</p>
<pre><code>$ esrally stop --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot; --preserve-install
$ esrally start --installation-id=&quot;$&#123;INSTALLATION_ID&#125;&quot; --race-id=&quot;$&#123;RACE_ID&#125;&quot;

$ netstat -anop | grep 39200
tcp6       0      0 10.10.10.2:39200       :::*                    LISTEN      23726/java           off (0.00/0/0)

$ ps -ef | egrep -i &quot;rally|elastic&quot; | grep -v grep
es       23726     1 18 18:01 ?        00:00:42 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j2.formatMsgNoLookups=true -Djava.locale.providers=SPI,JRE -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.io.tmpdir=/tmp/elasticsearch-7969870787666953814 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/heapdump -XX:ErrorFile=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/hs_err_pid%p.log -XX:+ExitOnOutOfMemoryError -XX:MaxDirectMemorySize=536870912 -Des.path.home=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0 -Des.path.conf=/home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/config -Des.distribution.flavor=default -Des.distribution.type=tar -Des.bundled_jdk=true -cp /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/lib/* org.elasticsearch.bootstrap.Elasticsearch -d -p ./pid
es       23752 23726  0 18:01 ?        00:00:00 /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/install/elasticsearch-7.17.0/modules/x-pack-ml/platform/linux-x86_64/bin/controller
</code></pre>
<p>Elasticsearch start failure due to max file descriptors is too low</p>
<pre><code>$ cat /home/es/.rally/benchmarks/races/aa826112-d371-4f09-9b68-f9084e7c9e0b/rally-node-0/logs/server/rally-benchmark.log
[..]
[2022-10-28T18:02:09,107][ERROR][o.e.b.Bootstrap          ] [rally-node-1] node validation exception
[1] bootstrap checks failed. You must address the points described in the following [1] lines before starting Elasticsearch.
bootstrap check failure [1] of [1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]
[..]
</code></pre>
<p>To fix this issue, change the max open files value in &#x2F;etc&#x2F;security&#x2F;limits.conf</p>
<pre><code>$ vim /etc/security/limits.conf
*              soft     nofile          1048576
*              hard     nofile          1048576
</code></pre>
<p>Exit and login back to the shell to see the changed value</p>
<pre><code>ulimit -a  | grep &quot;open files&quot;
open files                      (-n) 1048576
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://esrally.readthedocs.io/en/stable/">Rally User Guide</a></li>
<li><a target="_blank" rel="noopener" href="https://elasticsearch-benchmarks.elastic.co/#">https://elasticsearch-benchmarks.elastic.co/#</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/elastic/rally-eventdata-track">https://github.com/elastic/rally-eventdata-track</a></li>
<li><a target="_blank" rel="noopener" href="https://esrally.readthedocs.io/en/stable/configuration.html#reporting">https://esrally.readthedocs.io/en/stable/configuration.html#reporting</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://www.flamingbytes.com/blog/getting-started-with-esrally/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo/FlamingBytes-icon-64x64-1.png">
      <meta itemprop="name" content="relentlesstorm">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FlamingBytes">
      <meta itemprop="description" content="Stay hungry, stay foolish">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | FlamingBytes">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/getting-started-with-esrally/" class="post-title-link" itemprop="url">Getting started with Elastic Rally benchmark</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-10-25 12:30:00" itemprop="dateCreated datePublished" datetime="2022-10-25T12:30:00-07:00">2022-10-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-22 11:28:20" itemprop="dateModified" datetime="2023-10-22T11:28:20-07:00">2023-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/" itemprop="url" rel="index"><span itemprop="name">Tech</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Tech/Benchmarking/" itemprop="url" rel="index"><span itemprop="name">Benchmarking</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Rally is the macrobenchmarking framework for Elasticsearch. This post follows the instructions <a target="_blank" rel="noopener" href="https://esrally.readthedocs.io/en/stable/install.html">here</a> to install Rally and run the very first benchmark(aka race).</p>
<h3 id="Install-python"><a href="#Install-python" class="headerlink" title="Install python"></a>Install python</h3><p>Python 3.8+ including pip3 is required for Rally.</p>
<pre><code>$ yum update
$ yum install openssl-devel bzip2-devel libffi-devel
$ yum groupinstall &quot;Development Tools&quot;

$ curl -O https://www.python.org/ftp/python/3.8.1/Python-3.8.1.tgz
$ tar zxf Python-3.8.1.tgz
$ mv Python-3.8.1 /usr/src
$ cd /usr/src/Python-3.8.1/
$ vim Modules/Setup 
SSL=/usr/local/ssl
_ssl _ssl.c \
        -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
        -L$(SSL)/lib -lssl -lcrypto

$ ./configure --enable-optimizations
$ make altinstall
$ python3.8 -m ssl

$ pip3 -V
pip 22.3 from /usr/local/lib/python3.8/site-packages/pip (python 3.8)
</code></pre>
<p><strong>Note</strong>: If you do not uncomment the 4 lines in Modules&#x2F;Setup, you would fail to install Rally with the ssl module unavailable issue as mentioned in the following troubleshooting section.</p>
<h3 id="Install-git"><a href="#Install-git" class="headerlink" title="Install git"></a>Install git</h3><p>Git is not required if all of the following conditions are met:</p>
<ul>
<li><p>You are using Rally only as a load generator (–pipeline&#x3D;benchmark-only) or you are referring to Elasticsearch configurations with –team-path.</p>
</li>
<li><p>You create your own tracks and refer to them with –track-path.</p>
</li>
<li><p>In all other cases, Rally requires git 1.9 or better. Verify with git –version</p>
<p>  $ yum -y remove git<br>  $ yum -y remove git-*<br>  $ yum install git<br>  $ git version<br>  git version 2.38.1</p>
</li>
</ul>
<h2 id="Install-JDK"><a href="#Install-JDK" class="headerlink" title="Install JDK"></a>Install JDK</h2><p>A JDK is required on all machines where you want to launch Elasticsearch. If you use Rally just as a load generator to benchmark remote clusters, no JDK is required. Refer to <a target="_blank" rel="noopener" href="https://www.elastic.co/support/matrix#matrix_jvm">here</a> to determine the appropriate JDK version to run Elasticsearch.</p>
<pre><code>$ yum install java
$ java -version
openjdk version &quot;1.8.0_345&quot;
OpenJDK Runtime Environment (build 1.8.0_345-b01)
OpenJDK 64-Bit Server VM (build 25.345-b01, mixed mode)
</code></pre>
<p>To <a target="_blank" rel="noopener" href="https://www.oracle.com/java/technologies/javase/jdk17-archive-downloads.html">download</a> and install a specific java version:</p>
<pre><code>$ wget https://download.oracle.com/java/17/archive/jdk-17.0.5_linux-x64_bin.rpm
$ rpm -ivh jdk-17.0.5_linux-x64_bin.rpm
$ java -version
java version &quot;17.0.5&quot; 2022-10-18 LTS
Java(TM) SE Runtime Environment (build 17.0.5+9-LTS-191)
Java HotSpot(TM) 64-Bit Server VM (build 17.0.5+9-LTS-191, mixed mode, sharing)

$ rpm -qi jdk-17-17.0.5-ga.x86_64
Name        : jdk-17
Epoch       : 2000
Version     : 17.0.5
Release     : ga
Architecture: x86_64
Install Date: Wed 02 Nov 2022 03:58:42 PM PDT
Group       : Development/Tools
Size        : 316751437
License     : https://java.com/freeuselicense
Signature   : RSA/SHA256, Tue 13 Sep 2022 09:36:17 AM PDT, Key ID 72f97b74ec551f03
Source RPM  : jdk-17-17.0.5-ga.src.rpm
Build Date  : Tue 13 Sep 2022 09:35:27 AM PDT
Build Host  : java.com
Relocations : /usr/java
Vendor      : Oracle Corporation
URL         : http://www.oracle.com/technetwork/java/javase/overview/index.html
Summary     : Java Platform Standard Edition Development Kit
Description :
The Java Platform Standard Edition Development Kit (JDK) includes both
the runtime environment (Java virtual machine, the Java platform classes
and supporting files) and development tools (compilers, debuggers,
tool libraries and other tools).
</code></pre>
<p>With java 1.8.0, you can run Elasticsearch 7.17.x or lower version.</p>
<p>To find the JDK, Rally expects the environment variable JAVA_HOME to be set on all targeted machines. To have more specific control, for example when you want to benchmark across a wide range of Elasticsearch releases, you can also set JAVAx_HOME where x is the major version of a JDK (e.g. JAVA8_HOME would point to a JDK 8 installation). Rally will then choose the highest supported JDK per version of Elasticsearch that is available.</p>
<pre><code>$ export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
$ echo $JAVA_HOME
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
</code></pre>
<h2 id="Install-Rally"><a href="#Install-Rally" class="headerlink" title="Install Rally"></a>Install Rally</h2><pre><code>$ pip3.8 install --upgrade pip

$ pip3.8 install esrally

$ esrally -h
usage: esrally [-h] [--version] &#123;race,list,info,create-track,generate,compare,download,install,start,stop&#125; ...

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

 You Know, for Benchmarking Elasticsearch.

optional arguments:
  -h, --help            show this help message and exit
  --version             show program&#39;s version number and exit

subcommands:
  &#123;race,list,info,create-track,generate,compare,download,install,start,stop&#125;
    race                Run a benchmark
    list                List configuration options
    info                Show info about a track
    create-track        Create a Rally track from existing data
    generate            Generate artifacts
    compare             Compare two races
    download            Downloads an artifact
    install             Installs an Elasticsearch node locally
    start               Starts an Elasticsearch node locally
    stop                Stops an Elasticsearch node locally

Find out more about Rally at https://esrally.readthedocs.io/en/2.6.0/


$ esrally list tracks

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available tracks:

Name              Description                                                              Documents    Compressed Size    Uncompressed Size    Default Challenge        All Challenges
----------------  -----------------------------------------------------------------------  -----------  -----------------  -------------------  -----------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
dense_vector      Benchmark for dense vector indexing and search                           10,000,000   7.2 GB             19.5 GB              index-and-search         index-and-search
elastic/endpoint  Endpoint track                                                           0            0 bytes            0 bytes              default                  default
elastic/logs      Track for simulating logging workloads                                   14,009,078   N/A                N/A                  logging-indexing         cross-clusters-search,logging-disk-usage,logging-indexing-querying,logging-indexing,logging-querying,logging-snapshot-mount,logging-snapshot-restore,logging-snapshot,many-shards-quantitative,many-shards-snapshots
elastic/security  Track for simulating Elastic Security workloads                          77,513,777   N/A                N/A                  security-querying        index-alert-source-events,security-indexing-querying,security-indexing,security-querying
eql               EQL benchmarks based on endgame index of SIEM demo cluster               60,782,211   4.5 GB             109.2 GB             default                  default,index-sorting
geonames          POIs from Geonames                                                       11,396,503   252.9 MB           3.3 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts,significant-text
geopoint          Point coordinates from PlanetOSM                                         60,844,404   482.1 MB           2.3 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts
geopointshape     Point coordinates from PlanetOSM indexed as geoshapes                    60,844,404   470.8 MB           2.6 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts
geoshape          Shapes from PlanetOSM                                                    84,220,567   17.0 GB            58.7 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-big
http_logs         HTTP server log data                                                     247,249,096  1.2 GB             31.1 GB              append-no-conflicts      append-no-conflicts,runtime-fields,append-no-conflicts-index-only,append-sorted-no-conflicts,append-index-only-with-ingest-pipeline,update,append-no-conflicts-index-reindex-only
metricbeat        Metricbeat data                                                          1,079,600    87.7 MB            1.2 GB               append-no-conflicts      append-no-conflicts
nested            StackOverflow Q&amp;A stored as nested docs                                  11,203,029   663.3 MB           3.4 GB               nested-search-challenge  nested-search-challenge,index-only
noaa              Global daily weather measurements from NOAA                              33,659,481   949.4 MB           9.0 GB               append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,aggs,filter-aggs
nyc_taxis         Taxi rides in New York in 2015                                           165,346,692  4.5 GB             74.3 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts-index-only,update,append-ml,aggs
percolator        Percolator benchmark based on AOL queries                                2,000,000    121.1 kB           104.9 MB             append-no-conflicts      append-no-conflicts
pmc               Full text benchmark with academic papers from PMC                        574,199      5.5 GB             21.7 GB              append-no-conflicts      append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts,append-fast-with-conflicts,indexing-querying
so                Indexing benchmark using up to questions and answers from StackOverflow  36,062,278   8.9 GB             33.1 GB              append-no-conflicts      append-no-conflicts,transform,frequent-items
so_vector         Benchmark for vector search with StackOverflow data                      2,000,000    12.3 GB            32.2 GB              index-and-search         index-and-search
sql               SQL query performance based on NOAA Weather data                         33,659,481   949.4 MB           9.0 GB               sql                      sql
tsdb              metricbeat information for elastic-app k8s cluster                       116,633,698  N/A                123.0 GB             append-no-conflicts      append-no-conflicts,downsample

-------------------------------
[INFO] SUCCESS (took 3 seconds)
-------------------------------


$ esrally list cars

    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

Available cars:

Name                     Type    Description
-----------------------  ------  --------------------------------------
16gheap                  car     Sets the Java heap to 16GB
1gheap                   car     Sets the Java heap to 1GB
24gheap                  car     Sets the Java heap to 24GB
2gheap                   car     Sets the Java heap to 2GB
4gheap                   car     Sets the Java heap to 4GB
8gheap                   car     Sets the Java heap to 8GB
defaults                 car     Sets the Java heap to 1GB
basic-license            mixin   Basic License
debug-non-safepoints     mixin   More accurate CPU profiles
ea                       mixin   Enables Java assertions
fp                       mixin   Preserves frame pointers
g1gc                     mixin   Enables the G1 garbage collector
parallelgc               mixin   Enables the Parallel garbage collector
trial-license            mixin   Trial License
unpooled                 mixin   Enables Netty&#39;s unpooled allocator
x-pack-ml                mixin   X-Pack Machine Learning
x-pack-monitoring-http   mixin   X-Pack Monitoring (HTTP exporter)
x-pack-monitoring-local  mixin   X-Pack Monitoring (local exporter)
x-pack-security          mixin   X-Pack Security
zgc                      mixin   Enables the ZGC garbage collector

-------------------------------
[INFO] SUCCESS (took 6 seconds)
-------------------------------
</code></pre>
<h2 id="Run-the-first-race-with-Rally"><a href="#Run-the-first-race-with-Rally" class="headerlink" title="Run the first race with Rally"></a>Run the first race with Rally</h2><p>A “race” in Rally is the execution of a benchmarking experiment. You can choose different benchmarking scenarios (called tracks) for your benchmarks.</p>
<p>Rally should be run as a non-root user. We create a user “es” to run the following race.</p>
<pre><code>$ groupadd es
$ useradd es -g es
$ passwd es
$ cd /home/es


$ su - es
$ export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.352.b08-2.el7_9.x86_64/jre
$ esrally race --distribution-version=7.17.0 --track=geonames
    ____        ____
   / __ \____ _/ / /_  __
  / /_/ / __ `/ / / / / /
 / _, _/ /_/ / / / /_/ /
/_/ |_|\__,_/_/_/\__, /
                /____/

[INFO] Race id is [d3102b91-ac10-4383-b6a4-7b98d2831af7]
[INFO] Preparing for race ...
[INFO] Downloading Elasticsearch 7.17.0 (297.0 MB total size)                       [100%]
[INFO] Downloading track data (252.9 MB total size)                               [100.0%]
[INFO] Decompressing track data from [/home/es/.rally/benchmarks/data/geonames/documents-2.json.bz2] to [/home/es/.rally/benchmarks/data/geonames/documents-2.json] (resulting size: [3.30] GB) ...  [OK]
[INFO] Preparing file offset table for [/home/es/.rally/benchmarks/data/geonames/documents-2.json] ... [OK]
[INFO] Racing on track [geonames], challenge [append-no-conflicts] and car [&#39;defaults&#39;] with version [7.17.0].

Running delete-index                                                           [100% done]
Running create-index                                                           [100% done]
Running check-cluster-health                                                   [100% done]
Running index-append                                                           [100% done]
Running refresh-after-index                                                    [100% done]
Running force-merge                                                            [100% done]
Running refresh-after-force-merge                                              [100% done]
Running wait-until-merges-finish                                               [100% done]
Running index-stats                                                            [100% done]
Running node-stats                                                             [100% done]
Running default                                                                [100% done]
Running term                                                                   [100% done]
Running phrase                                                                 [100% done]
Running country_agg_uncached                                                   [100% done]
Running country_agg_cached                                                     [100% done]
Running scroll                                                                 [100% done]
Running expression                                                             [100% done]
Running painless_static                                                        [100% done]
Running painless_dynamic                                                       [100% done]
Running decay_geo_gauss_function_score                                         [100% done]
Running decay_geo_gauss_script_score                                           [100% done]
Running field_value_function_score                                             [100% done]
Running field_value_script_score                                               [100% done]
Running large_terms                                                            [100% done]
Running large_filtered_terms                                                   [100% done]
Running large_prohibited_terms                                                 [100% done]
Running desc_sort_population                                                   [100% done]
Running asc_sort_population                                                    [100% done]
Running asc_sort_with_after_population                                         [100% done]
Running desc_sort_geonameid                                                    [100% done]
Running desc_sort_with_after_geonameid                                         [100% done]
Running asc_sort_geonameid                                                     [100% done]
Running asc_sort_with_after_geonameid                                          [100% done]

------------------------------------------------------
    _______             __   _____
   / ____(_)___  ____ _/ /  / ___/_________  ________
  / /_  / / __ \/ __ `/ /   \__ \/ ___/ __ \/ ___/ _ \
 / __/ / / / / / /_/ / /   ___/ / /__/ /_/ / /  /  __/
/_/   /_/_/ /_/\__,_/_/   /____/\___/\____/_/   \___/
------------------------------------------------------

|                                                         Metric |                           Task |           Value |    Unit |
|---------------------------------------------------------------:|-------------------------------:|----------------:|--------:|
|                     Cumulative indexing time of primary shards |                                |    12.7087      |     min |
|             Min cumulative indexing time across primary shards |                                |     0.0196167   |     min |
|          Median cumulative indexing time across primary shards |                                |     2.5376      |     min |
|             Max cumulative indexing time across primary shards |                                |     2.58153     |     min |
|            Cumulative indexing throttle time of primary shards |                                |     0.0108667   |     min |
|    Min cumulative indexing throttle time across primary shards |                                |     0           |     min |
| Median cumulative indexing throttle time across primary shards |                                |     0           |     min |
|    Max cumulative indexing throttle time across primary shards |                                |     0.00731667  |     min |
|                        Cumulative merge time of primary shards |                                |     7.13778     |     min |
|                       Cumulative merge count of primary shards |                                |    52           |         |
|                Min cumulative merge time across primary shards |                                |     0           |     min |
|             Median cumulative merge time across primary shards |                                |     1.20084     |     min |
|                Max cumulative merge time across primary shards |                                |     2.20237     |     min |
|               Cumulative merge throttle time of primary shards |                                |     1.59453     |     min |
|       Min cumulative merge throttle time across primary shards |                                |     0           |     min |
|    Median cumulative merge throttle time across primary shards |                                |     0.2181      |     min |
|       Max cumulative merge throttle time across primary shards |                                |     0.569333    |     min |
|                      Cumulative refresh time of primary shards |                                |     2.89785     |     min |
|                     Cumulative refresh count of primary shards |                                |   256           |         |
|              Min cumulative refresh time across primary shards |                                |     0.00186667  |     min |
|           Median cumulative refresh time across primary shards |                                |     0.577325    |     min |
|              Max cumulative refresh time across primary shards |                                |     0.599333    |     min |
|                        Cumulative flush time of primary shards |                                |     0.2239      |     min |
|                       Cumulative flush count of primary shards |                                |    14           |         |
|                Min cumulative flush time across primary shards |                                |     0.0023      |     min |
|             Median cumulative flush time across primary shards |                                |     0.0471833   |     min |
|                Max cumulative flush time across primary shards |                                |     0.05055     |     min |
|                                        Total Young Gen GC time |                                |    18.57        |       s |
|                                       Total Young Gen GC count |                                |  2243           |         |
|                                          Total Old Gen GC time |                                |     3.541       |       s |
|                                         Total Old Gen GC count |                                |    66           |         |
|                                                     Store size |                                |     2.82211     |      GB |
|                                                  Translog size |                                |     3.07336e-07 |      GB |
|                                         Heap used for segments |                                |     0.733753    |      MB |
|                                       Heap used for doc values |                                |     0.0489769   |      MB |
|                                            Heap used for terms |                                |     0.557007    |      MB |
|                                            Heap used for norms |                                |     0.0753784   |      MB |
|                                           Heap used for points |                                |     0           |      MB |
|                                    Heap used for stored fields |                                |     0.0523911   |      MB |
|                                                  Segment count |                                |   103           |         |
|                                    Total Ingest Pipeline count |                                |     0           |         |
|                                     Total Ingest Pipeline time |                                |     0           |       s |
|                                   Total Ingest Pipeline failed |                                |     0           |         |
|                                                 Min Throughput |                   index-append | 87315.7         |  docs/s |
|                                                Mean Throughput |                   index-append | 87373.7         |  docs/s |
|                                              Median Throughput |                   index-append | 87368.1         |  docs/s |
|                                                 Max Throughput |                   index-append | 87440.9         |  docs/s |
|                                        50th percentile latency |                   index-append |   316.067       |      ms |
|                                        90th percentile latency |                   index-append |   458.448       |      ms |
|                                        99th percentile latency |                   index-append |  1152.53        |      ms |
|                                       100th percentile latency |                   index-append |  1316.07        |      ms |
|                                   50th percentile service time |                   index-append |   316.067       |      ms |
|                                   90th percentile service time |                   index-append |   458.448       |      ms |
|                                   99th percentile service time |                   index-append |  1152.53        |      ms |
|                                  100th percentile service time |                   index-append |  1316.07        |      ms |
|                                                     error rate |                   index-append |     0           |       % |
|                                                 Min Throughput |                    index-stats |    89.91        |   ops/s |
|                                                Mean Throughput |                    index-stats |    89.95        |   ops/s |
|                                              Median Throughput |                    index-stats |    89.95        |   ops/s |
|                                                 Max Throughput |                    index-stats |    89.97        |   ops/s |
|                                        50th percentile latency |                    index-stats |     4.36948     |      ms |
|                                        90th percentile latency |                    index-stats |     5.06188     |      ms |
|                                        99th percentile latency |                    index-stats |     5.51726     |      ms |
|                                      99.9th percentile latency |                    index-stats |     7.79772     |      ms |
|                                       100th percentile latency |                    index-stats |     9.64821     |      ms |
|                                   50th percentile service time |                    index-stats |     3.16338     |      ms |
|                                   90th percentile service time |                    index-stats |     3.67796     |      ms |
|                                   99th percentile service time |                    index-stats |     3.86689     |      ms |
|                                 99.9th percentile service time |                    index-stats |     4.13559     |      ms |
|                                  100th percentile service time |                    index-stats |     6.9374      |      ms |
[..]                                                 

----------------------------------
[INFO] SUCCESS (took 4199 seconds)
----------------------------------
</code></pre>
<p>You can save this report also to a file by using –report-file&#x3D;&#x2F;path&#x2F;to&#x2F;your&#x2F;report.md and save it as CSV with –report-format&#x3D;csv.</p>
<p>What did Rally just do?</p>
<ul>
<li>It downloaded and started Elasticsearch 7.17.0</li>
<li>It downloaded the relevant data for the geonames track</li>
<li>It ran the actual benchmark</li>
<li>And finally it reported the results</li>
</ul>
<h2 id="Rally-Configuration"><a href="#Rally-Configuration" class="headerlink" title="Rally Configuration"></a>Rally Configuration</h2><p>Rally stores its configuration in the file .rally&#x2F;rally.ini which is automatically created the first time Rally is executed.</p>
<pre><code>$ pwd
/home/es/.rally/benchmarks
$ ls
data  distributions  races  teams  tracks
$ ls distributions/
elasticsearch-7.17.0-linux-x86_64.tar.gz

$ ls tracks/default/
download.sh  elastic  eql  geonames  geopoint  geopointshape  geoshape  http_logs  metricbeat  nested  noaa  nyc_taxis  percolator  pmc  README.md  so  sql

$ ls races/d3102b91-ac10-4383-b6a4-7b98d2831af7/
race.json  rally-node-0

$ ls teams/default/
cars  LICENSE  NOTICE  plugins  README.md

$ ls -l data/geonames/
total 3723472
-rw-rw-r-- 1 es es 3547613828 Oct 27 17:01 documents-2.json
-rw-rw-r-- 1 es es  265208777 Oct 27 17:00 documents-2.json.bz2
-rw-rw-r-- 1 es es       4250 Oct 27 17:02 documents-2.json.offset


$ cat /home/es/.rally/rally.ini
[meta]
config.version = 17

[system]
env.name = local

[node]
root.dir = /home/es/.rally/benchmarks
src.root.dir = /home/es/.rally/benchmarks/src

[source]
remote.repo.url = https://github.com/elastic/elasticsearch.git
elasticsearch.src.subdir = elasticsearch

[benchmarks]
local.dataset.cache = /home/es/.rally/benchmarks/data

[reporting]
datastore.type = in-memory
datastore.host =
datastore.port =
datastore.secure = False
datastore.user =
datastore.password =


[tracks]
default.url = https://github.com/elastic/rally-tracks

[teams]
default.url = https://github.com/elastic/rally-teams

[defaults]
preserve_benchmark_candidate = false

[distributions]
release.cache = true
</code></pre>
<p>The benchmark data directory can be changed by modifying root.dir in rally.ini.</p>
<ul>
<li>root.dir (default: “~&#x2F;.rally&#x2F;benchmarks”): Rally uses this directory to store all benchmark-related data. It assumes that it has complete control over this directory and any of its subdirectories.</li>
<li>src.root.dir (default: “~&#x2F;.rally&#x2F;benchmarks&#x2F;src”): The directory where the source code of Elasticsearch or any plugins is checked out. Only relevant for benchmarks from sources.</li>
</ul>
<h2 id="Uninstall-python3"><a href="#Uninstall-python3" class="headerlink" title="Uninstall python3"></a>Uninstall python3</h2><p>The following are the optional commands in the case you need to uninstall python3 in CentOS.</p>
<pre><code>$ whereis python3
python3: /usr/bin/python3 /usr/bin/python3.6 /usr/bin/python3.6m /usr/lib/python3.6 /usr/lib64/python3.6 /usr/local/bin/python3.11 /usr/local/bin/python3.11-config /usr/local/lib/python3.11 /usr/include/python3.6m /usr/share/man/man1/python3.1.gz

$ whereis pip3
pip3: /usr/local/bin/pip3 /usr/local/bin/pip3.8 /usr/local/bin/pip3.10

$ rpm -qa | grep python3 --&gt; Only needed if you installed python3 by yum package installer.

$ whereis python3 |xargs rm -frv
$ whereis pip3 |xargs rm -frv
</code></pre>
<h2 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h2><ul>
<li><p>“WARNING: pip is configured with locations that require TLS&#x2F;SSL, however the ssl module in Python is not available.”</p>
<p>  $ pip3.8 install esrally<br>  WARNING: pip is configured with locations that require TLS&#x2F;SSL, however the ssl module in Python is not available.<br>  WARNING: Retrying (Retry(total&#x3D;4, connect&#x3D;None, read&#x3D;None, redirect&#x3D;None, status&#x3D;None)) after connection broken by ‘SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)’: &#x2F;simple&#x2F;esrally&#x2F;<br>  WARNING: Retrying (Retry(total&#x3D;3, connect&#x3D;None, read&#x3D;None, redirect&#x3D;None, status&#x3D;None)) after connection broken by ‘SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)’: &#x2F;simple&#x2F;esrally&#x2F;<br>  WARNING: Retrying (Retry(total&#x3D;2, connect&#x3D;None, read&#x3D;None, redirect&#x3D;None, status&#x3D;None)) after connection broken by ‘SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)’: &#x2F;simple&#x2F;esrally&#x2F;<br>  WARNING: Retrying (Retry(total&#x3D;1, connect&#x3D;None, read&#x3D;None, redirect&#x3D;None, status&#x3D;None)) after connection broken by ‘SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)’: &#x2F;simple&#x2F;esrally&#x2F;<br>  WARNING: Retrying (Retry(total&#x3D;0, connect&#x3D;None, read&#x3D;None, redirect&#x3D;None, status&#x3D;None)) after connection broken by ‘SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)’: &#x2F;simple&#x2F;esrally&#x2F;<br>  Could not fetch URL <a target="_blank" rel="noopener" href="https://pypi.org/simple/esrally/">https://pypi.org/simple/esrally/</a>: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host&#x3D;’pypi.org’, port&#x3D;443): Max retries exceeded with url: &#x2F;simple&#x2F;esrally&#x2F; (Caused by SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)) - skipping<br>  ERROR: Could not find a version that satisfies the requirement esrally (from versions: none)<br>  ERROR: No matching distribution found for esrally<br>  WARNING: pip is configured with locations that require TLS&#x2F;SSL, however the ssl module in Python is not available.<br>  Could not fetch URL <a target="_blank" rel="noopener" href="https://pypi.org/simple/pip/">https://pypi.org/simple/pip/</a>: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host&#x3D;’pypi.org’, port&#x3D;443): Max retries exceeded with url: &#x2F;simple&#x2F;pip&#x2F; (Caused by SSLError(“Can’t connect to HTTPS URL because the SSL module is not available.”)) - skipping<br>  WARNING: There was an error checking the latest version of pip.</p>
</li>
</ul>
<p>To fix this, install the python after un-commenting the four lines in Modules&#x2F;Setup.</p>
<pre><code>$ curl -O https://www.python.org/ftp/python/3.8.1/Python-3.8.1.tgz
$ tar zxf Python-3.8.1.tgz
$ mv Python-3.8.1 /usr/src
$ cd /usr/src/Python-3.8.1/
$ vim Modules/Setup 
SSL=/usr/local/ssl
_ssl _ssl.c \
    -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
    -L$(SSL)/lib -lssl -lcrypto

$ ./configure --enable-optimizations
$ make altinstall
$ python3.8 -m ssl
</code></pre>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/blog/announcing-rally-benchmarking-for-elasticsearch?spm=a2c65.11461447.0.0.e26a498cbHyowi">https://www.elastic.co/blog/announcing-rally-benchmarking-for-elasticsearch?spm=a2c65.11461447.0.0.e26a498cbHyowi</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/elastic/rally">https://github.com/elastic/rally</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/elastic/rally-tracks">https://github.com/elastic/rally-tracks</a></li>
<li><a target="_blank" rel="noopener" href="https://esrally.readthedocs.io/en/stable/quickstart.html">https://esrally.readthedocs.io/en/stable/quickstart.html</a></li>
<li><a target="_blank" rel="noopener" href="https://esrally.readthedocs.io/en/stable/install.html">https://esrally.readthedocs.io/en/stable/install.html</a></li>
<li><a target="_blank" rel="noopener" href="https://esrally.readthedocs.io/en/stable/configuration.html">https://esrally.readthedocs.io/en/stable/configuration.html</a></li>
<li><a target="_blank" rel="noopener" href="https://esrally.readthedocs.io/en/stable/race.html">https://esrally.readthedocs.io/en/stable/race.html</a></li>
<li><a target="_blank" rel="noopener" href="https://esrally.readthedocs.io/en/stable/faq.html">https://esrally.readthedocs.io/en/stable/faq.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.python.org/ftp/python/">https://www.python.org/ftp/python/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/support/matrix#matrix_jvm">https://www.elastic.co/support/matrix#matrix_jvm</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/56552390/how-to-fix-ssl-module-in-python-is-not-available-in-centos">https://stackoverflow.com/questions/56552390/how-to-fix-ssl-module-in-python-is-not-available-in-centos</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.searchhub.io/how-to-setup-elasticsearch-benchmarking">https://blog.searchhub.io/how-to-setup-elasticsearch-benchmarking</a></li>
<li><a target="_blank" rel="noopener" href="https://techviewleo.com/install-java-openjdk-on-rocky-linux-centos/">https://techviewleo.com/install-java-openjdk-on-rocky-linux-centos/</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/page/10/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/12/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">relentlesstorm</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
